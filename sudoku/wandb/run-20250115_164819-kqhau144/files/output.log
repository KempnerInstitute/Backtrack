tokenized dataset DatasetDict({
    train: Dataset({
        features: ['input_ids'],
        num_rows: 26444
    })
    val: Dataset({
        features: ['input_ids'],
        num_rows: 28
    })
})
[2025-01-15 16:48:33,779] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/n/home05/sqin/self-correct/sudoku/train.py:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 0.9272, 'grad_norm': 8.125, 'learning_rate': 1.5128593040847203e-08, 'epoch': 0.0003025260928755105}
{'loss': 1.0307, 'grad_norm': 8.375, 'learning_rate': 3.0257186081694406e-08, 'epoch': 0.000605052185751021}
{'loss': 1.0687, 'grad_norm': 8.4375, 'learning_rate': 4.5385779122541606e-08, 'epoch': 0.0009075782786265315}
{'loss': 1.0218, 'grad_norm': 8.25, 'learning_rate': 6.051437216338881e-08, 'epoch': 0.001210104371502042}
{'loss': 1.0295, 'grad_norm': 8.0625, 'learning_rate': 7.564296520423602e-08, 'epoch': 0.0015126304643775525}
{'loss': 1.2817, 'grad_norm': 9.0, 'learning_rate': 9.077155824508321e-08, 'epoch': 0.001815156557253063}
{'loss': 1.0628, 'grad_norm': 8.5, 'learning_rate': 1.059001512859304e-07, 'epoch': 0.0021176826501285734}
{'loss': 1.3377, 'grad_norm': 8.8125, 'learning_rate': 1.2102874432677762e-07, 'epoch': 0.002420208743004084}
{'loss': 1.0881, 'grad_norm': 8.0625, 'learning_rate': 1.3615733736762482e-07, 'epoch': 0.002722734835879595}
{'loss': 1.0953, 'grad_norm': 8.1875, 'learning_rate': 1.5128593040847204e-07, 'epoch': 0.003025260928755105}
{'eval_valid_loss': 0.3109419345855713, 'eval_valid_runtime': 2.917, 'eval_valid_samples_per_second': 9.599, 'eval_valid_steps_per_second': 4.799, 'epoch': 0.003025260928755105}
Traceback (most recent call last):
  File "/n/home05/sqin/self-correct/sudoku/train.py", line 202, in <module>
  File "/n/home05/sqin/self-correct/sudoku/train.py", line 189, in main
    trainer.train()
  File "/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/transformers/trainer.py", line 2536, in _inner_training_loop
    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt