Tokenizer vocab size: 109
Model size: 23,165,440
train_files ['/n/netscratch/dam_lab/Lab/sqin/reason/sudoku/data/sudoku_data_0_10000.json', '/n/netscratch/dam_lab/Lab/sqin/reason/sudoku/data/sudoku_data_10000_20000.json', '/n/netscratch/dam_lab/Lab/sqin/reason/sudoku/data/sudoku_data_20000_30000.json', '/n/netscratch/dam_lab/Lab/sqin/reason/sudoku/data/sudoku_data_30000_40000.json', '/n/netscratch/dam_lab/Lab/sqin/reason/sudoku/data/sudoku_data_40000_50000.json', '/n/netscratch/dam_lab/Lab/sqin/reason/sudoku/data/sudoku_data_50000_60000.json', '/n/netscratch/dam_lab/Lab/sqin/reason/sudoku/data/sudoku_data_60000_70000.json', '/n/netscratch/dam_lab/Lab/sqin/reason/sudoku/data/sudoku_data_70000_80000.json', '/n/netscratch/dam_lab/Lab/sqin/reason/sudoku/data/sudoku_data_80000_90000.json', '/n/netscratch/dam_lab/Lab/sqin/reason/sudoku/data/sudoku_data_90000_100000.json']
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
















Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100000/100000 [00:34<00:00, 4120.24 examples/s]
tokenized dataset DatasetDict({
    train: Dataset({
        features: ['input_ids'],
        num_rows: 100000
    })
    val: Dataset({
        features: ['input_ids'],
        num_rows: 100
    })
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100000/100000 [00:35<00:00, 2791.47 examples/s]
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/n/home05/sqin/self-correct/sudoku/train.py:194: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1881: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)
  overflow_gpu = get_accelerator().ByteTensor([overflow])
{'loss': 4.6879, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.0004}
{'loss': 4.6766, 'grad_norm': 34.56405316082795, 'learning_rate': 3e-09, 'epoch': 0.0008}
{'loss': 4.6777, 'grad_norm': 34.152337718388466, 'learning_rate': 1.3e-08, 'epoch': 0.0012}
{'loss': 4.6813, 'grad_norm': 34.21968792191695, 'learning_rate': 2.3e-08, 'epoch': 0.0016}
{'loss': 4.6781, 'grad_norm': 34.040781471077025, 'learning_rate': 3.3000000000000004e-08, 'epoch': 0.002}
{'loss': 4.6725, 'grad_norm': 34.021539597072554, 'learning_rate': 4.3e-08, 'epoch': 0.0024}
{'loss': 4.6475, 'grad_norm': 34.09430736781959, 'learning_rate': 5.3000000000000005e-08, 'epoch': 0.0028}
{'loss': 4.6322, 'grad_norm': 34.00124592461678, 'learning_rate': 6.300000000000001e-08, 'epoch': 0.0032}
{'loss': 4.565, 'grad_norm': 33.50811441560764, 'learning_rate': 7.3e-08, 'epoch': 0.0036}
{'loss': 4.5271, 'grad_norm': 33.42260798452605, 'learning_rate': 8.3e-08, 'epoch': 0.004}
{'eval_valid_loss': 4.4765625, 'eval_valid_runtime': 0.1262, 'eval_valid_samples_per_second': 792.539, 'eval_valid_steps_per_second': 198.135, 'epoch': 0.004}
{'loss': 4.4732, 'grad_norm': 32.83729061412815, 'learning_rate': 9.3e-08, 'epoch': 0.0044}
{'loss': 4.402, 'grad_norm': 31.698185430150556, 'learning_rate': 1.0300000000000001e-07, 'epoch': 0.0048}
{'loss': 4.3393, 'grad_norm': 31.37888065974853, 'learning_rate': 1.13e-07, 'epoch': 0.0052}
{'loss': 4.2711, 'grad_norm': 30.701879609065323, 'learning_rate': 1.23e-07, 'epoch': 0.0056}
{'loss': 4.2018, 'grad_norm': 29.895055894124727, 'learning_rate': 1.33e-07, 'epoch': 0.006}
{'loss': 4.1301, 'grad_norm': 28.609931877879134, 'learning_rate': 1.4300000000000002e-07, 'epoch': 0.0064}
{'loss': 4.0494, 'grad_norm': 27.49129098491175, 'learning_rate': 1.53e-07, 'epoch': 0.0068}
{'loss': 3.9553, 'grad_norm': 25.703328511554222, 'learning_rate': 1.63e-07, 'epoch': 0.0072}
{'loss': 3.8752, 'grad_norm': 23.973305320313198, 'learning_rate': 1.73e-07, 'epoch': 0.0076}
{'loss': 3.7946, 'grad_norm': 21.54744192460587, 'learning_rate': 1.83e-07, 'epoch': 0.008}
{'eval_valid_loss': 3.62890625, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1119.042, 'eval_valid_steps_per_second': 279.761, 'epoch': 0.008}
{'loss': 3.7193, 'grad_norm': 20.662276781079914, 'learning_rate': 1.9300000000000002e-07, 'epoch': 0.0084}
{'loss': 3.6293, 'grad_norm': 17.704305099385405, 'learning_rate': 2.03e-07, 'epoch': 0.0088}
{'loss': 3.5591, 'grad_norm': 15.983653360821835, 'learning_rate': 2.1300000000000001e-07, 'epoch': 0.0092}
{'loss': 3.4823, 'grad_norm': 13.77316818796238, 'learning_rate': 2.2300000000000002e-07, 'epoch': 0.0096}
{'loss': 3.4173, 'grad_norm': 11.906856531428518, 'learning_rate': 2.3300000000000003e-07, 'epoch': 0.01}
{'loss': 3.3558, 'grad_norm': 9.942883658657594, 'learning_rate': 2.43e-07, 'epoch': 0.0104}
{'loss': 3.2964, 'grad_norm': 8.70711504629346, 'learning_rate': 2.53e-07, 'epoch': 0.0108}
{'loss': 3.2393, 'grad_norm': 7.601329599950788, 'learning_rate': 2.63e-07, 'epoch': 0.0112}
{'loss': 3.1847, 'grad_norm': 6.838793910929197, 'learning_rate': 2.73e-07, 'epoch': 0.0116}
{'loss': 3.1319, 'grad_norm': 6.494859005768357, 'learning_rate': 2.8300000000000003e-07, 'epoch': 0.012}
{'eval_valid_loss': 3.02734375, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1109.155, 'eval_valid_steps_per_second': 277.289, 'epoch': 0.012}
{'loss': 3.0798, 'grad_norm': 6.170941231505717, 'learning_rate': 2.9300000000000004e-07, 'epoch': 0.0124}
{'loss': 3.0323, 'grad_norm': 6.031172549289051, 'learning_rate': 3.0300000000000005e-07, 'epoch': 0.0128}
{'loss': 2.9766, 'grad_norm': 6.049450333310757, 'learning_rate': 3.1300000000000006e-07, 'epoch': 0.0132}
{'loss': 2.9272, 'grad_norm': 6.007539790081658, 'learning_rate': 3.2300000000000007e-07, 'epoch': 0.0136}
{'loss': 2.876, 'grad_norm': 6.104215394455949, 'learning_rate': 3.330000000000001e-07, 'epoch': 0.014}
{'loss': 2.8229, 'grad_norm': 6.183102750291839, 'learning_rate': 3.43e-07, 'epoch': 0.0144}
{'loss': 2.7707, 'grad_norm': 6.186530851316036, 'learning_rate': 3.53e-07, 'epoch': 0.0148}
{'loss': 2.7183, 'grad_norm': 6.122978231967022, 'learning_rate': 3.63e-07, 'epoch': 0.0152}
{'loss': 2.6624, 'grad_norm': 6.164384889089029, 'learning_rate': 3.73e-07, 'epoch': 0.0156}
{'loss': 2.6142, 'grad_norm': 6.079842538065137, 'learning_rate': 3.8300000000000003e-07, 'epoch': 0.016}
{'eval_valid_loss': 2.44921875, 'eval_valid_runtime': 0.0885, 'eval_valid_samples_per_second': 1129.526, 'eval_valid_steps_per_second': 282.382, 'epoch': 0.016}
{'loss': 2.5602, 'grad_norm': 5.969919090217929, 'learning_rate': 3.9300000000000004e-07, 'epoch': 0.0164}
{'loss': 2.5053, 'grad_norm': 6.018976843253996, 'learning_rate': 4.0300000000000005e-07, 'epoch': 0.0168}
{'loss': 2.4592, 'grad_norm': 5.574982289427266, 'learning_rate': 4.1300000000000006e-07, 'epoch': 0.0172}
{'loss': 2.412, 'grad_norm': 5.437305551099488, 'learning_rate': 4.23e-07, 'epoch': 0.0176}
{'loss': 2.3628, 'grad_norm': 5.134675055249577, 'learning_rate': 4.33e-07, 'epoch': 0.018}
{'loss': 2.3178, 'grad_norm': 5.082982093826589, 'learning_rate': 4.4300000000000004e-07, 'epoch': 0.0184}
{'loss': 2.2752, 'grad_norm': 4.801721137889913, 'learning_rate': 4.5300000000000005e-07, 'epoch': 0.0188}
{'loss': 2.2353, 'grad_norm': 4.654346825975548, 'learning_rate': 4.6300000000000006e-07, 'epoch': 0.0192}
{'loss': 2.1943, 'grad_norm': 4.4385744325362015, 'learning_rate': 4.7300000000000007e-07, 'epoch': 0.0196}
{'loss': 2.1591, 'grad_norm': 4.123656693559562, 'learning_rate': 4.830000000000001e-07, 'epoch': 0.02}
{'eval_valid_loss': 2.017578125, 'eval_valid_runtime': 0.0956, 'eval_valid_samples_per_second': 1045.945, 'eval_valid_steps_per_second': 261.486, 'epoch': 0.02}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 2.1274, 'grad_norm': 3.891449597692999, 'learning_rate': 4.93e-07, 'epoch': 0.0204}
{'loss': 2.0933, 'grad_norm': 3.7503330480182546, 'learning_rate': 5.03e-07, 'epoch': 0.0208}
{'loss': 2.063, 'grad_norm': 3.7642715206044097, 'learning_rate': 5.13e-07, 'epoch': 0.0212}
{'loss': 2.0352, 'grad_norm': 3.37210725509421, 'learning_rate': 5.23e-07, 'epoch': 0.0216}
{'loss': 2.0115, 'grad_norm': 3.089239363993007, 'learning_rate': 5.33e-07, 'epoch': 0.022}
{'loss': 1.9881, 'grad_norm': 3.157694896601373, 'learning_rate': 5.43e-07, 'epoch': 0.0224}
{'loss': 1.967, 'grad_norm': 2.82109084312735, 'learning_rate': 5.53e-07, 'epoch': 0.0228}
{'loss': 1.9434, 'grad_norm': 2.6990038293492975, 'learning_rate': 5.63e-07, 'epoch': 0.0232}
{'loss': 1.9274, 'grad_norm': 2.6537836714120906, 'learning_rate': 5.730000000000001e-07, 'epoch': 0.0236}
{'loss': 1.9072, 'grad_norm': 2.627845419755668, 'learning_rate': 5.830000000000001e-07, 'epoch': 0.024}
{'eval_valid_loss': 1.826171875, 'eval_valid_runtime': 0.0933, 'eval_valid_samples_per_second': 1071.928, 'eval_valid_steps_per_second': 267.982, 'epoch': 0.024}
{'loss': 1.8932, 'grad_norm': 2.316871109131581, 'learning_rate': 5.930000000000001e-07, 'epoch': 0.0244}
{'loss': 1.8778, 'grad_norm': 2.5905354268057974, 'learning_rate': 6.030000000000001e-07, 'epoch': 0.0248}
{'loss': 1.8651, 'grad_norm': 2.2893232971672464, 'learning_rate': 6.130000000000001e-07, 'epoch': 0.0252}
{'loss': 1.8532, 'grad_norm': 2.055376548323048, 'learning_rate': 6.230000000000001e-07, 'epoch': 0.0256}
{'loss': 1.842, 'grad_norm': 2.273617829056428, 'learning_rate': 6.33e-07, 'epoch': 0.026}
{'loss': 1.8307, 'grad_norm': 1.9322189327433688, 'learning_rate': 6.43e-07, 'epoch': 0.0264}
{'loss': 1.8213, 'grad_norm': 1.9866614329342607, 'learning_rate': 6.53e-07, 'epoch': 0.0268}
{'loss': 1.8132, 'grad_norm': 1.8045202669051719, 'learning_rate': 6.63e-07, 'epoch': 0.0272}
{'loss': 1.8026, 'grad_norm': 1.8180453452370684, 'learning_rate': 6.730000000000001e-07, 'epoch': 0.0276}
{'loss': 1.797, 'grad_norm': 1.6685313006475306, 'learning_rate': 6.830000000000001e-07, 'epoch': 0.028}
{'eval_valid_loss': 1.7470703125, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1115.029, 'eval_valid_steps_per_second': 278.757, 'epoch': 0.028}
{'loss': 1.7901, 'grad_norm': 1.851914525268448, 'learning_rate': 6.930000000000001e-07, 'epoch': 0.0284}
{'loss': 1.7866, 'grad_norm': 1.5752869666265847, 'learning_rate': 7.030000000000001e-07, 'epoch': 0.0288}
{'loss': 1.7768, 'grad_norm': 1.6599728404438716, 'learning_rate': 7.130000000000001e-07, 'epoch': 0.0292}
{'loss': 1.771, 'grad_norm': 1.6312649251174693, 'learning_rate': 7.230000000000001e-07, 'epoch': 0.0296}
{'loss': 1.768, 'grad_norm': 1.4730495546175453, 'learning_rate': 7.330000000000001e-07, 'epoch': 0.03}
{'loss': 1.7641, 'grad_norm': 1.5166082560139147, 'learning_rate': 7.430000000000001e-07, 'epoch': 0.0304}
{'loss': 1.7567, 'grad_norm': 1.5626977163629359, 'learning_rate': 7.530000000000001e-07, 'epoch': 0.0308}
{'loss': 1.7532, 'grad_norm': 1.5038553055891068, 'learning_rate': 7.630000000000001e-07, 'epoch': 0.0312}
{'loss': 1.7492, 'grad_norm': 1.4743181547382087, 'learning_rate': 7.73e-07, 'epoch': 0.0316}
{'loss': 1.7448, 'grad_norm': 1.577437587486155, 'learning_rate': 7.83e-07, 'epoch': 0.032}
{'eval_valid_loss': 1.7060546875, 'eval_valid_runtime': 0.0883, 'eval_valid_samples_per_second': 1131.904, 'eval_valid_steps_per_second': 282.976, 'epoch': 0.032}
{'loss': 1.7403, 'grad_norm': 1.4855755343043726, 'learning_rate': 7.93e-07, 'epoch': 0.0324}
{'loss': 1.7344, 'grad_norm': 1.443589039960127, 'learning_rate': 8.03e-07, 'epoch': 0.0328}
{'loss': 1.7323, 'grad_norm': 1.4732112344693709, 'learning_rate': 8.13e-07, 'epoch': 0.0332}
{'loss': 1.7308, 'grad_norm': 1.4513955489400778, 'learning_rate': 8.23e-07, 'epoch': 0.0336}
{'loss': 1.7248, 'grad_norm': 1.4551265324470728, 'learning_rate': 8.33e-07, 'epoch': 0.034}
{'loss': 1.7222, 'grad_norm': 1.557395711238437, 'learning_rate': 8.43e-07, 'epoch': 0.0344}
{'loss': 1.7178, 'grad_norm': 1.4346161465629659, 'learning_rate': 8.53e-07, 'epoch': 0.0348}
{'loss': 1.7151, 'grad_norm': 1.4458546227475824, 'learning_rate': 8.63e-07, 'epoch': 0.0352}
{'loss': 1.7129, 'grad_norm': 1.4184511470180199, 'learning_rate': 8.73e-07, 'epoch': 0.0356}
{'loss': 1.7089, 'grad_norm': 1.4243954285784093, 'learning_rate': 8.830000000000001e-07, 'epoch': 0.036}
{'eval_valid_loss': 1.673828125, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1122.817, 'eval_valid_steps_per_second': 280.704, 'epoch': 0.036}
{'loss': 1.7035, 'grad_norm': 1.4520051676424075, 'learning_rate': 8.930000000000001e-07, 'epoch': 0.0364}
{'loss': 1.7007, 'grad_norm': 1.4307625564157191, 'learning_rate': 9.030000000000001e-07, 'epoch': 0.0368}
{'loss': 1.6987, 'grad_norm': 1.42803760470878, 'learning_rate': 9.130000000000001e-07, 'epoch': 0.0372}
{'loss': 1.6986, 'grad_norm': 1.4727317464383214, 'learning_rate': 9.23e-07, 'epoch': 0.0376}
{'loss': 1.6925, 'grad_norm': 1.4578501749638983, 'learning_rate': 9.33e-07, 'epoch': 0.038}
{'loss': 1.6918, 'grad_norm': 1.4057529948720113, 'learning_rate': 9.43e-07, 'epoch': 0.0384}
{'loss': 1.6881, 'grad_norm': 1.4145706894316055, 'learning_rate': 9.53e-07, 'epoch': 0.0388}
{'loss': 1.6851, 'grad_norm': 1.4235554042793734, 'learning_rate': 9.630000000000001e-07, 'epoch': 0.0392}
{'loss': 1.6839, 'grad_norm': 1.4602079482338153, 'learning_rate': 9.73e-07, 'epoch': 0.0396}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.6796, 'grad_norm': 1.457827938300647, 'learning_rate': 9.830000000000002e-07, 'epoch': 0.04}
{'eval_valid_loss': 1.64453125, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1116.935, 'eval_valid_steps_per_second': 279.234, 'epoch': 0.04}
{'loss': 1.6743, 'grad_norm': 1.5330609614072042, 'learning_rate': 9.93e-07, 'epoch': 0.0404}
{'loss': 1.6727, 'grad_norm': 1.4676222085614303, 'learning_rate': 1.002e-06, 'epoch': 0.0408}
{'loss': 1.67, 'grad_norm': 1.4826304008870996, 'learning_rate': 1.012e-06, 'epoch': 0.0412}
{'loss': 1.6674, 'grad_norm': 1.4918131708302413, 'learning_rate': 1.0220000000000001e-06, 'epoch': 0.0416}
{'loss': 1.6646, 'grad_norm': 1.4629258557751479, 'learning_rate': 1.032e-06, 'epoch': 0.042}
{'loss': 1.6634, 'grad_norm': 1.5111573554426487, 'learning_rate': 1.0420000000000001e-06, 'epoch': 0.0424}
{'loss': 1.66, 'grad_norm': 1.5145912098111334, 'learning_rate': 1.052e-06, 'epoch': 0.0428}
{'loss': 1.6601, 'grad_norm': 1.6565487430262504, 'learning_rate': 1.0620000000000002e-06, 'epoch': 0.0432}
{'loss': 1.6584, 'grad_norm': 1.4816093643173351, 'learning_rate': 1.072e-06, 'epoch': 0.0436}
{'loss': 1.6516, 'grad_norm': 1.542106034736991, 'learning_rate': 1.0820000000000002e-06, 'epoch': 0.044}
{'eval_valid_loss': 1.6171875, 'eval_valid_runtime': 0.0942, 'eval_valid_samples_per_second': 1061.639, 'eval_valid_steps_per_second': 265.41, 'epoch': 0.044}
{'loss': 1.6397, 'grad_norm': 1.5761840564583225, 'learning_rate': 1.092e-06, 'epoch': 0.0444}
{'loss': 1.6417, 'grad_norm': 1.5359816601129899, 'learning_rate': 1.1020000000000002e-06, 'epoch': 0.0448}
{'loss': 1.6444, 'grad_norm': 1.4889025392605204, 'learning_rate': 1.1120000000000001e-06, 'epoch': 0.0452}
{'loss': 1.6427, 'grad_norm': 1.5336611507021969, 'learning_rate': 1.122e-06, 'epoch': 0.0456}
{'loss': 1.6356, 'grad_norm': 1.5750489991755572, 'learning_rate': 1.1320000000000001e-06, 'epoch': 0.046}
{'loss': 1.6344, 'grad_norm': 1.537433128482455, 'learning_rate': 1.142e-06, 'epoch': 0.0464}
{'loss': 1.6323, 'grad_norm': 1.532660125747355, 'learning_rate': 1.1520000000000002e-06, 'epoch': 0.0468}
{'loss': 1.6382, 'grad_norm': 1.4584186521901663, 'learning_rate': 1.162e-06, 'epoch': 0.0472}
{'loss': 1.6293, 'grad_norm': 1.539101682444975, 'learning_rate': 1.1720000000000002e-06, 'epoch': 0.0476}
{'loss': 1.6228, 'grad_norm': 1.48205752286613, 'learning_rate': 1.182e-06, 'epoch': 0.048}
{'eval_valid_loss': 1.591796875, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.846, 'eval_valid_steps_per_second': 279.961, 'epoch': 0.048}
{'loss': 1.6255, 'grad_norm': 1.5436839655216323, 'learning_rate': 1.1920000000000002e-06, 'epoch': 0.0484}
{'loss': 1.6175, 'grad_norm': 1.5165579311188837, 'learning_rate': 1.202e-06, 'epoch': 0.0488}
{'loss': 1.614, 'grad_norm': 1.5271739597044247, 'learning_rate': 1.2120000000000002e-06, 'epoch': 0.0492}
{'loss': 1.6158, 'grad_norm': 1.5264612279954939, 'learning_rate': 1.2220000000000001e-06, 'epoch': 0.0496}
{'loss': 1.6166, 'grad_norm': 1.5085187540224863, 'learning_rate': 1.2320000000000002e-06, 'epoch': 0.05}
{'loss': 1.6259, 'grad_norm': 1.5023625080070302, 'learning_rate': 1.2420000000000001e-06, 'epoch': 0.0504}
{'loss': 1.6095, 'grad_norm': 1.7581020392272315, 'learning_rate': 1.2520000000000003e-06, 'epoch': 0.0508}
{'loss': 1.607, 'grad_norm': 1.53974202205671, 'learning_rate': 1.2620000000000002e-06, 'epoch': 0.0512}
{'loss': 1.6068, 'grad_norm': 1.5228454515354846, 'learning_rate': 1.2720000000000003e-06, 'epoch': 0.0516}
{'loss': 1.6045, 'grad_norm': 1.5386197281053797, 'learning_rate': 1.2820000000000002e-06, 'epoch': 0.052}
{'eval_valid_loss': 1.5732421875, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1129.082, 'eval_valid_steps_per_second': 282.271, 'epoch': 0.052}
{'loss': 1.6106, 'grad_norm': 1.563611423268279, 'learning_rate': 1.2920000000000003e-06, 'epoch': 0.0524}
{'loss': 1.5986, 'grad_norm': 1.474414637003706, 'learning_rate': 1.3020000000000002e-06, 'epoch': 0.0528}
{'loss': 1.5969, 'grad_norm': 1.4571948611568266, 'learning_rate': 1.3120000000000003e-06, 'epoch': 0.0532}
{'loss': 1.597, 'grad_norm': 1.4548555997653327, 'learning_rate': 1.3220000000000002e-06, 'epoch': 0.0536}
{'loss': 1.6068, 'grad_norm': 1.453238188016712, 'learning_rate': 1.3320000000000003e-06, 'epoch': 0.054}
{'loss': 1.5894, 'grad_norm': 1.4649705768358907, 'learning_rate': 1.3420000000000002e-06, 'epoch': 0.0544}
{'loss': 1.5902, 'grad_norm': 1.4507031460281872, 'learning_rate': 1.352e-06, 'epoch': 0.0548}
{'loss': 1.6042, 'grad_norm': 1.3962456692574667, 'learning_rate': 1.362e-06, 'epoch': 0.0552}
{'loss': 1.5963, 'grad_norm': 1.495360406369295, 'learning_rate': 1.372e-06, 'epoch': 0.0556}
{'loss': 1.5872, 'grad_norm': 1.557202090735291, 'learning_rate': 1.382e-06, 'epoch': 0.056}
{'eval_valid_loss': 1.55859375, 'eval_valid_runtime': 0.0944, 'eval_valid_samples_per_second': 1059.4, 'eval_valid_steps_per_second': 264.85, 'epoch': 0.056}
{'loss': 1.5848, 'grad_norm': 1.5945416288958567, 'learning_rate': 1.392e-06, 'epoch': 0.0564}
{'loss': 1.5869, 'grad_norm': 1.6071605122254973, 'learning_rate': 1.402e-06, 'epoch': 0.0568}
{'loss': 1.5964, 'grad_norm': 1.4140379374041048, 'learning_rate': 1.412e-06, 'epoch': 0.0572}
{'loss': 1.5784, 'grad_norm': 1.3839303444596107, 'learning_rate': 1.4220000000000001e-06, 'epoch': 0.0576}
{'loss': 1.5714, 'grad_norm': 1.3976880841284032, 'learning_rate': 1.432e-06, 'epoch': 0.058}
{'loss': 1.5777, 'grad_norm': 1.5800213137529038, 'learning_rate': 1.4420000000000001e-06, 'epoch': 0.0584}
{'loss': 1.5723, 'grad_norm': 1.4597801530343222, 'learning_rate': 1.452e-06, 'epoch': 0.0588}
{'loss': 1.568, 'grad_norm': 1.383946212642932, 'learning_rate': 1.4620000000000001e-06, 'epoch': 0.0592}
{'loss': 1.5831, 'grad_norm': 1.4987038933452235, 'learning_rate': 1.472e-06, 'epoch': 0.0596}
{'loss': 1.567, 'grad_norm': 1.4208980442321861, 'learning_rate': 1.4820000000000002e-06, 'epoch': 0.06}
{'eval_valid_loss': 1.5478515625, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.326, 'eval_valid_steps_per_second': 279.081, 'epoch': 0.06}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.5729, 'grad_norm': 1.3691978497048938, 'learning_rate': 1.492e-06, 'epoch': 0.0604}
{'loss': 1.5668, 'grad_norm': 1.4757850734768194, 'learning_rate': 1.5020000000000002e-06, 'epoch': 0.0608}
{'loss': 1.5779, 'grad_norm': 1.4639886432876386, 'learning_rate': 1.512e-06, 'epoch': 0.0612}
{'loss': 1.5688, 'grad_norm': 1.2717389900712077, 'learning_rate': 1.5220000000000002e-06, 'epoch': 0.0616}
{'loss': 1.5674, 'grad_norm': 1.3855457153186403, 'learning_rate': 1.5320000000000001e-06, 'epoch': 0.062}
{'loss': 1.5573, 'grad_norm': 1.283646540331128, 'learning_rate': 1.5420000000000002e-06, 'epoch': 0.0624}
{'loss': 1.5629, 'grad_norm': 1.4422691677523811, 'learning_rate': 1.5520000000000001e-06, 'epoch': 0.0628}
{'loss': 1.5702, 'grad_norm': 1.4418574275776037, 'learning_rate': 1.5620000000000002e-06, 'epoch': 0.0632}
{'loss': 1.5621, 'grad_norm': 1.3180645040721781, 'learning_rate': 1.5720000000000002e-06, 'epoch': 0.0636}
{'loss': 1.5604, 'grad_norm': 1.4335819017819773, 'learning_rate': 1.5820000000000003e-06, 'epoch': 0.064}
{'eval_valid_loss': 1.5390625, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.404, 'eval_valid_steps_per_second': 281.851, 'epoch': 0.064}
{'loss': 1.5564, 'grad_norm': 1.3464477649179327, 'learning_rate': 1.5920000000000002e-06, 'epoch': 0.0644}
{'loss': 1.5576, 'grad_norm': 1.2437305358461135, 'learning_rate': 1.6020000000000003e-06, 'epoch': 0.0648}
{'loss': 1.5676, 'grad_norm': 1.2415875629561717, 'learning_rate': 1.6120000000000002e-06, 'epoch': 0.0652}
{'loss': 1.5667, 'grad_norm': 1.2895103615807295, 'learning_rate': 1.6220000000000003e-06, 'epoch': 0.0656}
{'loss': 1.5573, 'grad_norm': 1.6689528704494996, 'learning_rate': 1.6320000000000002e-06, 'epoch': 0.066}
{'loss': 1.5537, 'grad_norm': 1.232544434947706, 'learning_rate': 1.6420000000000003e-06, 'epoch': 0.0664}
{'loss': 1.5605, 'grad_norm': 1.333287976065926, 'learning_rate': 1.6520000000000002e-06, 'epoch': 0.0668}
{'loss': 1.5636, 'grad_norm': 1.18655241864136, 'learning_rate': 1.662e-06, 'epoch': 0.0672}
{'loss': 1.5554, 'grad_norm': 1.3236013846953456, 'learning_rate': 1.672e-06, 'epoch': 0.0676}
{'loss': 1.5495, 'grad_norm': 1.2178835478730032, 'learning_rate': 1.682e-06, 'epoch': 0.068}
{'eval_valid_loss': 1.53125, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.553, 'eval_valid_steps_per_second': 281.888, 'epoch': 0.068}
{'loss': 1.5554, 'grad_norm': 1.2573969625521624, 'learning_rate': 1.692e-06, 'epoch': 0.0684}
{'loss': 1.558, 'grad_norm': 1.2779406135904732, 'learning_rate': 1.702e-06, 'epoch': 0.0688}
{'loss': 1.5552, 'grad_norm': 1.2371230108715934, 'learning_rate': 1.712e-06, 'epoch': 0.0692}
{'loss': 1.5386, 'grad_norm': 1.3634446136399199, 'learning_rate': 1.722e-06, 'epoch': 0.0696}
{'loss': 1.5458, 'grad_norm': 1.4325921174126552, 'learning_rate': 1.732e-06, 'epoch': 0.07}
{'loss': 1.5427, 'grad_norm': 1.2784224601248844, 'learning_rate': 1.742e-06, 'epoch': 0.0704}
{'loss': 1.5384, 'grad_norm': 1.225619419749997, 'learning_rate': 1.7520000000000001e-06, 'epoch': 0.0708}
{'loss': 1.5455, 'grad_norm': 1.2429562355241108, 'learning_rate': 1.762e-06, 'epoch': 0.0712}
{'loss': 1.5585, 'grad_norm': 1.2660102331534553, 'learning_rate': 1.7720000000000001e-06, 'epoch': 0.0716}
{'loss': 1.5396, 'grad_norm': 1.2141719091916297, 'learning_rate': 1.782e-06, 'epoch': 0.072}
{'eval_valid_loss': 1.525390625, 'eval_valid_runtime': 0.0977, 'eval_valid_samples_per_second': 1023.778, 'eval_valid_steps_per_second': 255.944, 'epoch': 0.072}
{'loss': 1.5519, 'grad_norm': 1.2487714511777763, 'learning_rate': 1.7920000000000002e-06, 'epoch': 0.0724}
{'loss': 1.5486, 'grad_norm': 1.283924413916744, 'learning_rate': 1.802e-06, 'epoch': 0.0728}
{'loss': 1.5386, 'grad_norm': 1.1956289408569476, 'learning_rate': 1.8120000000000002e-06, 'epoch': 0.0732}
{'loss': 1.5403, 'grad_norm': 1.2250610519420277, 'learning_rate': 1.822e-06, 'epoch': 0.0736}
{'loss': 1.5412, 'grad_norm': 1.352883280666802, 'learning_rate': 1.8320000000000002e-06, 'epoch': 0.074}
{'loss': 1.5417, 'grad_norm': 1.227056045612013, 'learning_rate': 1.8420000000000001e-06, 'epoch': 0.0744}
{'loss': 1.5309, 'grad_norm': 1.3003396854180804, 'learning_rate': 1.8520000000000002e-06, 'epoch': 0.0748}
{'loss': 1.5243, 'grad_norm': 1.2167385517489118, 'learning_rate': 1.8620000000000001e-06, 'epoch': 0.0752}
{'loss': 1.542, 'grad_norm': 1.2133094399291346, 'learning_rate': 1.8720000000000002e-06, 'epoch': 0.0756}
{'loss': 1.5294, 'grad_norm': 1.231985690557161, 'learning_rate': 1.8820000000000001e-06, 'epoch': 0.076}
{'eval_valid_loss': 1.51953125, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1110.368, 'eval_valid_steps_per_second': 277.592, 'epoch': 0.076}
{'loss': 1.5428, 'grad_norm': 1.4887283734881547, 'learning_rate': 1.8920000000000003e-06, 'epoch': 0.0764}
{'loss': 1.5324, 'grad_norm': 1.1675050598069094, 'learning_rate': 1.9020000000000002e-06, 'epoch': 0.0768}
{'loss': 1.5425, 'grad_norm': 1.1850269645050493, 'learning_rate': 1.912e-06, 'epoch': 0.0772}
{'loss': 1.5496, 'grad_norm': 1.0858664480813751, 'learning_rate': 1.9220000000000004e-06, 'epoch': 0.0776}
{'loss': 1.5338, 'grad_norm': 1.1385342046896543, 'learning_rate': 1.9320000000000003e-06, 'epoch': 0.078}
{'loss': 1.541, 'grad_norm': 1.3358609501060668, 'learning_rate': 1.942e-06, 'epoch': 0.0784}
{'loss': 1.5305, 'grad_norm': 1.0872581079659964, 'learning_rate': 1.952e-06, 'epoch': 0.0788}
{'loss': 1.5291, 'grad_norm': 1.1134554182974814, 'learning_rate': 1.9620000000000004e-06, 'epoch': 0.0792}
{'loss': 1.5242, 'grad_norm': 1.2719315300734306, 'learning_rate': 1.972e-06, 'epoch': 0.0796}
{'loss': 1.529, 'grad_norm': 1.0855595853898081, 'learning_rate': 1.982e-06, 'epoch': 0.08}
{'eval_valid_loss': 1.513671875, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.535, 'eval_valid_steps_per_second': 279.884, 'epoch': 0.08}
{'loss': 1.5326, 'grad_norm': 1.1412366491055965, 'learning_rate': 1.992e-06, 'epoch': 0.0804}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.5225, 'grad_norm': 1.0914011069048952, 'learning_rate': 2.001e-06, 'epoch': 0.0808}
{'loss': 1.5377, 'grad_norm': 1.3234757657131864, 'learning_rate': 2.0110000000000003e-06, 'epoch': 0.0812}
{'loss': 1.5251, 'grad_norm': 1.3728300523015753, 'learning_rate': 2.0210000000000002e-06, 'epoch': 0.0816}
{'loss': 1.5209, 'grad_norm': 1.149116761254606, 'learning_rate': 2.031e-06, 'epoch': 0.082}
{'loss': 1.5306, 'grad_norm': 1.0541890511810903, 'learning_rate': 2.041e-06, 'epoch': 0.0824}
{'loss': 1.527, 'grad_norm': 1.1422897137882577, 'learning_rate': 2.0510000000000004e-06, 'epoch': 0.0828}
{'loss': 1.5296, 'grad_norm': 1.1670167002637275, 'learning_rate': 2.0610000000000003e-06, 'epoch': 0.0832}
{'loss': 1.5204, 'grad_norm': 1.1020580151227124, 'learning_rate': 2.071e-06, 'epoch': 0.0836}
{'loss': 1.5264, 'grad_norm': 1.2105158533218678, 'learning_rate': 2.081e-06, 'epoch': 0.084}
{'eval_valid_loss': 1.5087890625, 'eval_valid_runtime': 0.0989, 'eval_valid_samples_per_second': 1011.56, 'eval_valid_steps_per_second': 252.89, 'epoch': 0.084}
{'loss': 1.5221, 'grad_norm': 1.1444959358391693, 'learning_rate': 2.0910000000000004e-06, 'epoch': 0.0844}
{'loss': 1.5229, 'grad_norm': 1.168574890365034, 'learning_rate': 2.1010000000000003e-06, 'epoch': 0.0848}
{'loss': 1.5232, 'grad_norm': 1.3110262240029202, 'learning_rate': 2.111e-06, 'epoch': 0.0852}
{'loss': 1.5359, 'grad_norm': 1.0792272321962262, 'learning_rate': 2.121e-06, 'epoch': 0.0856}
{'loss': 1.5188, 'grad_norm': 1.0637178716475209, 'learning_rate': 2.1310000000000004e-06, 'epoch': 0.086}
{'loss': 1.5328, 'grad_norm': 1.1761498364601237, 'learning_rate': 2.1410000000000003e-06, 'epoch': 0.0864}
{'loss': 1.519, 'grad_norm': 1.1221446335550425, 'learning_rate': 2.1510000000000002e-06, 'epoch': 0.0868}
{'loss': 1.5265, 'grad_norm': 1.1525773167301896, 'learning_rate': 2.161e-06, 'epoch': 0.0872}
{'loss': 1.5361, 'grad_norm': 1.1214081908086038, 'learning_rate': 2.171e-06, 'epoch': 0.0876}
{'loss': 1.514, 'grad_norm': 1.0724264651364457, 'learning_rate': 2.181e-06, 'epoch': 0.088}
{'eval_valid_loss': 1.505859375, 'eval_valid_runtime': 0.0882, 'eval_valid_samples_per_second': 1133.323, 'eval_valid_steps_per_second': 283.331, 'epoch': 0.088}
{'loss': 1.5162, 'grad_norm': 1.2270894452465262, 'learning_rate': 2.191e-06, 'epoch': 0.0884}
{'loss': 1.5146, 'grad_norm': 1.1391360300373132, 'learning_rate': 2.201e-06, 'epoch': 0.0888}
{'loss': 1.5159, 'grad_norm': 1.3159875430255623, 'learning_rate': 2.211e-06, 'epoch': 0.0892}
{'loss': 1.536, 'grad_norm': 1.1952174497972263, 'learning_rate': 2.221e-06, 'epoch': 0.0896}
{'loss': 1.5236, 'grad_norm': 1.2114645426283004, 'learning_rate': 2.231e-06, 'epoch': 0.09}
{'loss': 1.5144, 'grad_norm': 1.2081308572038105, 'learning_rate': 2.2410000000000002e-06, 'epoch': 0.0904}
{'loss': 1.5142, 'grad_norm': 1.2928641844587234, 'learning_rate': 2.251e-06, 'epoch': 0.0908}
{'loss': 1.5085, 'grad_norm': 1.2034158169405915, 'learning_rate': 2.261e-06, 'epoch': 0.0912}
{'loss': 1.5174, 'grad_norm': 1.134514482122344, 'learning_rate': 2.271e-06, 'epoch': 0.0916}
{'loss': 1.5176, 'grad_norm': 1.0709886599168192, 'learning_rate': 2.2810000000000003e-06, 'epoch': 0.092}
{'eval_valid_loss': 1.5009765625, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.753, 'eval_valid_steps_per_second': 281.938, 'epoch': 0.092}
{'loss': 1.5092, 'grad_norm': 1.0607163933076318, 'learning_rate': 2.291e-06, 'epoch': 0.0924}
{'loss': 1.508, 'grad_norm': 1.32865912972734, 'learning_rate': 2.301e-06, 'epoch': 0.0928}
{'loss': 1.5106, 'grad_norm': 1.1717496789044062, 'learning_rate': 2.311e-06, 'epoch': 0.0932}
{'loss': 1.5122, 'grad_norm': 1.535330740643457, 'learning_rate': 2.3210000000000003e-06, 'epoch': 0.0936}
{'loss': 1.5104, 'grad_norm': 1.115331645538314, 'learning_rate': 2.3310000000000002e-06, 'epoch': 0.094}
{'loss': 1.5173, 'grad_norm': 1.0908603712084326, 'learning_rate': 2.341e-06, 'epoch': 0.0944}
{'loss': 1.51, 'grad_norm': 1.046292375947777, 'learning_rate': 2.351e-06, 'epoch': 0.0948}
{'loss': 1.5224, 'grad_norm': 1.3244736772089207, 'learning_rate': 2.3610000000000003e-06, 'epoch': 0.0952}
{'loss': 1.512, 'grad_norm': 1.1749364013276118, 'learning_rate': 2.3710000000000003e-06, 'epoch': 0.0956}
{'loss': 1.5225, 'grad_norm': 1.2823443739982379, 'learning_rate': 2.381e-06, 'epoch': 0.096}
{'eval_valid_loss': 1.4970703125, 'eval_valid_runtime': 0.0971, 'eval_valid_samples_per_second': 1029.358, 'eval_valid_steps_per_second': 257.339, 'epoch': 0.096}
{'loss': 1.5072, 'grad_norm': 1.0303917803299794, 'learning_rate': 2.391e-06, 'epoch': 0.0964}
{'loss': 1.5095, 'grad_norm': 1.0239188731137416, 'learning_rate': 2.4010000000000004e-06, 'epoch': 0.0968}
{'loss': 1.5013, 'grad_norm': 1.0905753161937464, 'learning_rate': 2.4110000000000003e-06, 'epoch': 0.0972}
{'loss': 1.5194, 'grad_norm': 1.0457264645735962, 'learning_rate': 2.421e-06, 'epoch': 0.0976}
{'loss': 1.5089, 'grad_norm': 1.0232391811369645, 'learning_rate': 2.431e-06, 'epoch': 0.098}
{'loss': 1.5129, 'grad_norm': 1.0560517697499463, 'learning_rate': 2.4410000000000004e-06, 'epoch': 0.0984}
{'loss': 1.5024, 'grad_norm': 1.0291633944670144, 'learning_rate': 2.4510000000000003e-06, 'epoch': 0.0988}
{'loss': 1.5027, 'grad_norm': 1.0749951140714105, 'learning_rate': 2.4610000000000002e-06, 'epoch': 0.0992}
{'loss': 1.5119, 'grad_norm': 1.2174297970598063, 'learning_rate': 2.471e-06, 'epoch': 0.0996}
{'loss': 1.5053, 'grad_norm': 1.0655473649363452, 'learning_rate': 2.481e-06, 'epoch': 0.1}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'eval_valid_loss': 1.494140625, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.409, 'eval_valid_steps_per_second': 279.102, 'epoch': 0.1}
{'loss': 1.4965, 'grad_norm': 1.1767888868352405, 'learning_rate': 2.491e-06, 'epoch': 0.1004}
{'loss': 1.5157, 'grad_norm': 1.1792947797576598, 'learning_rate': 2.5010000000000003e-06, 'epoch': 0.1008}
{'loss': 1.5003, 'grad_norm': 1.1812897279406445, 'learning_rate': 2.511e-06, 'epoch': 0.1012}
{'loss': 1.5131, 'grad_norm': 1.0342199373562606, 'learning_rate': 2.521e-06, 'epoch': 0.1016}
{'loss': 1.507, 'grad_norm': 1.2648632794970098, 'learning_rate': 2.531e-06, 'epoch': 0.102}
{'loss': 1.5071, 'grad_norm': 1.0750153448983553, 'learning_rate': 2.5410000000000003e-06, 'epoch': 0.1024}
{'loss': 1.5163, 'grad_norm': 0.9663196262387129, 'learning_rate': 2.5510000000000002e-06, 'epoch': 0.1028}
{'loss': 1.5006, 'grad_norm': 1.207636965152112, 'learning_rate': 2.561e-06, 'epoch': 0.1032}
{'loss': 1.5051, 'grad_norm': 1.0439067499801633, 'learning_rate': 2.571e-06, 'epoch': 0.1036}
{'loss': 1.4923, 'grad_norm': 1.018358417411035, 'learning_rate': 2.5810000000000004e-06, 'epoch': 0.104}
{'eval_valid_loss': 1.4921875, 'eval_valid_runtime': 0.0919, 'eval_valid_samples_per_second': 1088.683, 'eval_valid_steps_per_second': 272.171, 'epoch': 0.104}
{'loss': 1.5044, 'grad_norm': 1.0819439439633995, 'learning_rate': 2.5910000000000003e-06, 'epoch': 0.1044}
{'loss': 1.5139, 'grad_norm': 1.0454822722090582, 'learning_rate': 2.601e-06, 'epoch': 0.1048}
{'loss': 1.5014, 'grad_norm': 1.3064968759926048, 'learning_rate': 2.611e-06, 'epoch': 0.1052}
{'loss': 1.5081, 'grad_norm': 0.9481908458242462, 'learning_rate': 2.6210000000000004e-06, 'epoch': 0.1056}
{'loss': 1.5034, 'grad_norm': 0.9921316380807408, 'learning_rate': 2.6310000000000003e-06, 'epoch': 0.106}
{'loss': 1.5011, 'grad_norm': 1.118219349256446, 'learning_rate': 2.641e-06, 'epoch': 0.1064}
{'loss': 1.4885, 'grad_norm': 1.0953435239738039, 'learning_rate': 2.651e-06, 'epoch': 0.1068}
{'loss': 1.5034, 'grad_norm': 1.2105903989409257, 'learning_rate': 2.6610000000000004e-06, 'epoch': 0.1072}
{'loss': 1.4998, 'grad_norm': 1.023635691278606, 'learning_rate': 2.6710000000000003e-06, 'epoch': 0.1076}
{'loss': 1.5085, 'grad_norm': 0.9905736189748356, 'learning_rate': 2.6810000000000003e-06, 'epoch': 0.108}
{'eval_valid_loss': 1.4873046875, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1121.436, 'eval_valid_steps_per_second': 280.359, 'epoch': 0.108}
{'loss': 1.5049, 'grad_norm': 1.1132017023544756, 'learning_rate': 2.691e-06, 'epoch': 0.1084}
{'loss': 1.5026, 'grad_norm': 1.0696171969843196, 'learning_rate': 2.7010000000000005e-06, 'epoch': 0.1088}
{'loss': 1.4991, 'grad_norm': 1.054319265332079, 'learning_rate': 2.7110000000000004e-06, 'epoch': 0.1092}
{'loss': 1.5066, 'grad_norm': 1.1128754009867177, 'learning_rate': 2.7210000000000003e-06, 'epoch': 0.1096}
{'loss': 1.4974, 'grad_norm': 1.0245123844219108, 'learning_rate': 2.731e-06, 'epoch': 0.11}
{'loss': 1.4992, 'grad_norm': 1.0149842478499937, 'learning_rate': 2.7410000000000005e-06, 'epoch': 0.1104}
{'loss': 1.492, 'grad_norm': 1.0876449739079632, 'learning_rate': 2.7510000000000004e-06, 'epoch': 0.1108}
{'loss': 1.5062, 'grad_norm': 1.0835011599449342, 'learning_rate': 2.7610000000000003e-06, 'epoch': 0.1112}
{'loss': 1.4916, 'grad_norm': 1.0914892401302516, 'learning_rate': 2.7710000000000002e-06, 'epoch': 0.1116}
{'loss': 1.4998, 'grad_norm': 1.016239981425968, 'learning_rate': 2.7810000000000006e-06, 'epoch': 0.112}
{'eval_valid_loss': 1.484375, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.938, 'eval_valid_steps_per_second': 279.485, 'epoch': 0.112}
{'loss': 1.5115, 'grad_norm': 1.0148193465096635, 'learning_rate': 2.7910000000000005e-06, 'epoch': 0.1124}
{'loss': 1.5038, 'grad_norm': 1.0562777574830795, 'learning_rate': 2.8010000000000004e-06, 'epoch': 0.1128}
{'loss': 1.5006, 'grad_norm': 1.0657657701609404, 'learning_rate': 2.8110000000000003e-06, 'epoch': 0.1132}
{'loss': 1.4961, 'grad_norm': 0.9973840929648476, 'learning_rate': 2.8210000000000006e-06, 'epoch': 0.1136}
{'loss': 1.4985, 'grad_norm': 0.9474026305627838, 'learning_rate': 2.8310000000000005e-06, 'epoch': 0.114}
{'loss': 1.5028, 'grad_norm': 1.013966213520414, 'learning_rate': 2.8410000000000004e-06, 'epoch': 0.1144}
{'loss': 1.5045, 'grad_norm': 1.0721770442790837, 'learning_rate': 2.8510000000000003e-06, 'epoch': 0.1148}
{'loss': 1.5053, 'grad_norm': 0.9611760004375645, 'learning_rate': 2.8610000000000006e-06, 'epoch': 0.1152}
{'loss': 1.494, 'grad_norm': 1.0217602650803679, 'learning_rate': 2.8710000000000005e-06, 'epoch': 0.1156}
{'loss': 1.4832, 'grad_norm': 1.2634348167921206, 'learning_rate': 2.8810000000000005e-06, 'epoch': 0.116}
{'eval_valid_loss': 1.4814453125, 'eval_valid_runtime': 0.0991, 'eval_valid_samples_per_second': 1009.333, 'eval_valid_steps_per_second': 252.333, 'epoch': 0.116}
{'loss': 1.4975, 'grad_norm': 1.1819457897498507, 'learning_rate': 2.8910000000000004e-06, 'epoch': 0.1164}
{'loss': 1.4991, 'grad_norm': 0.9971451885343656, 'learning_rate': 2.9010000000000007e-06, 'epoch': 0.1168}
{'loss': 1.4822, 'grad_norm': 1.2501279616424277, 'learning_rate': 2.9110000000000006e-06, 'epoch': 0.1172}
{'loss': 1.4858, 'grad_norm': 1.234966091282202, 'learning_rate': 2.9210000000000005e-06, 'epoch': 0.1176}
{'loss': 1.4887, 'grad_norm': 1.1579954482210013, 'learning_rate': 2.9310000000000004e-06, 'epoch': 0.118}
{'loss': 1.4923, 'grad_norm': 1.2458670987010418, 'learning_rate': 2.941e-06, 'epoch': 0.1184}
{'loss': 1.4857, 'grad_norm': 1.0218082538148785, 'learning_rate': 2.9509999999999998e-06, 'epoch': 0.1188}
{'loss': 1.4824, 'grad_norm': 0.9910109971313922, 'learning_rate': 2.961e-06, 'epoch': 0.1192}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 1.4838, 'grad_norm': 1.0490942196571817, 'learning_rate': 2.971e-06, 'epoch': 0.1196}
{'loss': 1.4894, 'grad_norm': 1.1465761953753966, 'learning_rate': 2.981e-06, 'epoch': 0.12}
{'eval_valid_loss': 1.478515625, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1109.375, 'eval_valid_steps_per_second': 277.344, 'epoch': 0.12}
{'loss': 1.4925, 'grad_norm': 1.0371507961457145, 'learning_rate': 2.991e-06, 'epoch': 0.1204}
{'loss': 1.487, 'grad_norm': 1.1477386106174434, 'learning_rate': 3e-06, 'epoch': 0.1208}
{'loss': 1.49, 'grad_norm': 1.0013034156654863, 'learning_rate': 3.01e-06, 'epoch': 0.1212}
{'loss': 1.4835, 'grad_norm': 1.087741317639192, 'learning_rate': 3.0200000000000003e-06, 'epoch': 0.1216}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.4856, 'grad_norm': 1.1765490698966103, 'learning_rate': 3.0300000000000002e-06, 'epoch': 0.122}
{'loss': 1.4925, 'grad_norm': 0.9787800321243474, 'learning_rate': 3.04e-06, 'epoch': 0.1224}
{'loss': 1.493, 'grad_norm': 1.0120594664208231, 'learning_rate': 3.05e-06, 'epoch': 0.1228}
{'loss': 1.4843, 'grad_norm': 1.0492231062161699, 'learning_rate': 3.0600000000000003e-06, 'epoch': 0.1232}
{'loss': 1.483, 'grad_norm': 1.3106197643903101, 'learning_rate': 3.0700000000000003e-06, 'epoch': 0.1236}
{'loss': 1.485, 'grad_norm': 0.9402384479372681, 'learning_rate': 3.08e-06, 'epoch': 0.124}
{'eval_valid_loss': 1.4736328125, 'eval_valid_runtime': 0.0997, 'eval_valid_samples_per_second': 1003.463, 'eval_valid_steps_per_second': 250.866, 'epoch': 0.124}
{'loss': 1.4892, 'grad_norm': 1.1081056535847917, 'learning_rate': 3.09e-06, 'epoch': 0.1244}
{'loss': 1.4891, 'grad_norm': 1.0363107289429754, 'learning_rate': 3.1000000000000004e-06, 'epoch': 0.1248}
{'loss': 1.4909, 'grad_norm': 1.0013243689700613, 'learning_rate': 3.1100000000000003e-06, 'epoch': 0.1252}
{'loss': 1.4794, 'grad_norm': 1.0515523158993072, 'learning_rate': 3.12e-06, 'epoch': 0.1256}
{'loss': 1.4932, 'grad_norm': 1.2018766227603102, 'learning_rate': 3.13e-06, 'epoch': 0.126}
{'loss': 1.4889, 'grad_norm': 0.954150101042953, 'learning_rate': 3.1400000000000004e-06, 'epoch': 0.1264}
{'loss': 1.4861, 'grad_norm': 0.9883911821793849, 'learning_rate': 3.1500000000000003e-06, 'epoch': 0.1268}
{'loss': 1.48, 'grad_norm': 1.0395953257107284, 'learning_rate': 3.1600000000000002e-06, 'epoch': 0.1272}
{'loss': 1.4741, 'grad_norm': 1.1491873265527333, 'learning_rate': 3.17e-06, 'epoch': 0.1276}
{'loss': 1.4947, 'grad_norm': 1.252464450378793, 'learning_rate': 3.1800000000000005e-06, 'epoch': 0.128}
{'eval_valid_loss': 1.470703125, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1128.062, 'eval_valid_steps_per_second': 282.016, 'epoch': 0.128}
{'loss': 1.4746, 'grad_norm': 1.151523581083959, 'learning_rate': 3.1900000000000004e-06, 'epoch': 0.1284}
{'loss': 1.4778, 'grad_norm': 1.4648190483553676, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.1288}
{'loss': 1.4746, 'grad_norm': 0.9853647531736124, 'learning_rate': 3.21e-06, 'epoch': 0.1292}
{'loss': 1.4796, 'grad_norm': 1.0606268717345233, 'learning_rate': 3.2200000000000005e-06, 'epoch': 0.1296}
{'loss': 1.4753, 'grad_norm': 1.238970062540735, 'learning_rate': 3.2300000000000004e-06, 'epoch': 0.13}
{'loss': 1.4683, 'grad_norm': 1.0889014781804105, 'learning_rate': 3.2400000000000003e-06, 'epoch': 0.1304}
{'loss': 1.4804, 'grad_norm': 1.018535981413401, 'learning_rate': 3.2500000000000002e-06, 'epoch': 0.1308}
{'loss': 1.4689, 'grad_norm': 1.2371604085621717, 'learning_rate': 3.2600000000000006e-06, 'epoch': 0.1312}
{'loss': 1.4818, 'grad_norm': 1.066626208191235, 'learning_rate': 3.2700000000000005e-06, 'epoch': 0.1316}
{'loss': 1.4889, 'grad_norm': 1.039338924810699, 'learning_rate': 3.2800000000000004e-06, 'epoch': 0.132}
{'eval_valid_loss': 1.466796875, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1126.744, 'eval_valid_steps_per_second': 281.686, 'epoch': 0.132}
{'loss': 1.481, 'grad_norm': 1.0265456987193462, 'learning_rate': 3.2900000000000003e-06, 'epoch': 0.1324}
{'loss': 1.483, 'grad_norm': 1.2428648784185607, 'learning_rate': 3.3000000000000006e-06, 'epoch': 0.1328}
{'loss': 1.4656, 'grad_norm': 1.01130613359843, 'learning_rate': 3.3100000000000005e-06, 'epoch': 0.1332}
{'loss': 1.4804, 'grad_norm': 1.1852330761918022, 'learning_rate': 3.3200000000000004e-06, 'epoch': 0.1336}
{'loss': 1.4631, 'grad_norm': 1.069479253388296, 'learning_rate': 3.3300000000000003e-06, 'epoch': 0.134}
{'loss': 1.4695, 'grad_norm': 1.1020956609171098, 'learning_rate': 3.3400000000000006e-06, 'epoch': 0.1344}
{'loss': 1.4747, 'grad_norm': 1.1289648367519116, 'learning_rate': 3.3500000000000005e-06, 'epoch': 0.1348}
{'loss': 1.4622, 'grad_norm': 1.1754286639776506, 'learning_rate': 3.3600000000000004e-06, 'epoch': 0.1352}
{'loss': 1.4726, 'grad_norm': 1.1982208504988339, 'learning_rate': 3.3700000000000003e-06, 'epoch': 0.1356}
{'loss': 1.4864, 'grad_norm': 0.9662271901944639, 'learning_rate': 3.3800000000000007e-06, 'epoch': 0.136}
{'eval_valid_loss': 1.4619140625, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.974, 'eval_valid_steps_per_second': 279.994, 'epoch': 0.136}
{'loss': 1.4777, 'grad_norm': 1.2058494516797873, 'learning_rate': 3.3900000000000006e-06, 'epoch': 0.1364}
{'loss': 1.4683, 'grad_norm': 1.182297095490762, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.1368}
{'loss': 1.4822, 'grad_norm': 1.0286720268519995, 'learning_rate': 3.4100000000000004e-06, 'epoch': 0.1372}
{'loss': 1.4619, 'grad_norm': 1.1116237079690494, 'learning_rate': 3.4200000000000007e-06, 'epoch': 0.1376}
{'loss': 1.472, 'grad_norm': 1.185477755949666, 'learning_rate': 3.4300000000000006e-06, 'epoch': 0.138}
{'loss': 1.4837, 'grad_norm': 0.9945185950808343, 'learning_rate': 3.44e-06, 'epoch': 0.1384}
{'loss': 1.4852, 'grad_norm': 1.0081927077632036, 'learning_rate': 3.45e-06, 'epoch': 0.1388}
{'loss': 1.4627, 'grad_norm': 1.2070454717001977, 'learning_rate': 3.46e-06, 'epoch': 0.1392}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.463, 'grad_norm': 1.724650847993689, 'learning_rate': 3.4700000000000002e-06, 'epoch': 0.1396}
{'loss': 1.4701, 'grad_norm': 1.1623102463294592, 'learning_rate': 3.48e-06, 'epoch': 0.14}
{'eval_valid_loss': 1.4580078125, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1129.034, 'eval_valid_steps_per_second': 282.258, 'epoch': 0.14}
{'loss': 1.4637, 'grad_norm': 1.0972892836766428, 'learning_rate': 3.49e-06, 'epoch': 0.1404}
{'loss': 1.4756, 'grad_norm': 1.0221082312214438, 'learning_rate': 3.5e-06, 'epoch': 0.1408}
{'loss': 1.4707, 'grad_norm': 1.0553725860943783, 'learning_rate': 3.5100000000000003e-06, 'epoch': 0.1412}
{'loss': 1.4719, 'grad_norm': 1.1355264316079412, 'learning_rate': 3.52e-06, 'epoch': 0.1416}
{'loss': 1.4676, 'grad_norm': 1.103981701148004, 'learning_rate': 3.53e-06, 'epoch': 0.142}
{'loss': 1.4757, 'grad_norm': 1.1820129217881303, 'learning_rate': 3.54e-06, 'epoch': 0.1424}
{'loss': 1.4625, 'grad_norm': 1.1422087439089144, 'learning_rate': 3.5500000000000003e-06, 'epoch': 0.1428}
{'loss': 1.4731, 'grad_norm': 1.1611565378137758, 'learning_rate': 3.5600000000000002e-06, 'epoch': 0.1432}
{'loss': 1.4574, 'grad_norm': 1.0336594655503897, 'learning_rate': 3.57e-06, 'epoch': 0.1436}
{'loss': 1.4698, 'grad_norm': 1.0976502377627106, 'learning_rate': 3.58e-06, 'epoch': 0.144}
{'eval_valid_loss': 1.4521484375, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1121.04, 'eval_valid_steps_per_second': 280.26, 'epoch': 0.144}
{'loss': 1.4604, 'grad_norm': 1.0702079831556206, 'learning_rate': 3.5900000000000004e-06, 'epoch': 0.1444}
{'loss': 1.4718, 'grad_norm': 1.239371236686264, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.1448}
{'loss': 1.4669, 'grad_norm': 1.1107150148113676, 'learning_rate': 3.61e-06, 'epoch': 0.1452}
{'loss': 1.4589, 'grad_norm': 1.2455681107742245, 'learning_rate': 3.62e-06, 'epoch': 0.1456}
{'loss': 1.4569, 'grad_norm': 1.1181559349997379, 'learning_rate': 3.6300000000000004e-06, 'epoch': 0.146}
{'loss': 1.4596, 'grad_norm': 1.1746060511849976, 'learning_rate': 3.6400000000000003e-06, 'epoch': 0.1464}
{'loss': 1.4679, 'grad_norm': 1.21001674501846, 'learning_rate': 3.65e-06, 'epoch': 0.1468}
{'loss': 1.464, 'grad_norm': 1.6866907810933105, 'learning_rate': 3.66e-06, 'epoch': 0.1472}
{'loss': 1.462, 'grad_norm': 1.1295499962636153, 'learning_rate': 3.6700000000000004e-06, 'epoch': 0.1476}
{'loss': 1.4585, 'grad_norm': 1.0961138198684548, 'learning_rate': 3.6800000000000003e-06, 'epoch': 0.148}
{'eval_valid_loss': 1.4462890625, 'eval_valid_runtime': 0.0906, 'eval_valid_samples_per_second': 1103.157, 'eval_valid_steps_per_second': 275.789, 'epoch': 0.148}
{'loss': 1.456, 'grad_norm': 1.1677236778324152, 'learning_rate': 3.6900000000000002e-06, 'epoch': 0.1484}
{'loss': 1.4579, 'grad_norm': 1.2497912381606977, 'learning_rate': 3.7e-06, 'epoch': 0.1488}
{'loss': 1.4783, 'grad_norm': 1.3316111559417232, 'learning_rate': 3.7100000000000005e-06, 'epoch': 0.1492}
{'loss': 1.4582, 'grad_norm': 1.1317447332675086, 'learning_rate': 3.7200000000000004e-06, 'epoch': 0.1496}
{'loss': 1.4633, 'grad_norm': 1.178020523191951, 'learning_rate': 3.7300000000000003e-06, 'epoch': 0.15}
{'loss': 1.4602, 'grad_norm': 1.2760546154870336, 'learning_rate': 3.74e-06, 'epoch': 0.1504}
{'loss': 1.4727, 'grad_norm': 1.1976801542022895, 'learning_rate': 3.7500000000000005e-06, 'epoch': 0.1508}
{'loss': 1.466, 'grad_norm': 1.3175679042181108, 'learning_rate': 3.7600000000000004e-06, 'epoch': 0.1512}
{'loss': 1.4645, 'grad_norm': 1.2907585113087023, 'learning_rate': 3.7700000000000003e-06, 'epoch': 0.1516}
{'loss': 1.4505, 'grad_norm': 1.193087704541702, 'learning_rate': 3.7800000000000002e-06, 'epoch': 0.152}
{'eval_valid_loss': 1.439453125, 'eval_valid_runtime': 0.0957, 'eval_valid_samples_per_second': 1045.335, 'eval_valid_steps_per_second': 261.334, 'epoch': 0.152}
{'loss': 1.4454, 'grad_norm': 1.4532122995958463, 'learning_rate': 3.79e-06, 'epoch': 0.1524}
{'loss': 1.4455, 'grad_norm': 1.23078658071189, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.1528}
{'loss': 1.4522, 'grad_norm': 1.1432288704989781, 'learning_rate': 3.8100000000000004e-06, 'epoch': 0.1532}
{'loss': 1.4572, 'grad_norm': 1.2435949736933145, 'learning_rate': 3.820000000000001e-06, 'epoch': 0.1536}
{'loss': 1.4566, 'grad_norm': 1.169037513315481, 'learning_rate': 3.830000000000001e-06, 'epoch': 0.154}
{'loss': 1.4419, 'grad_norm': 1.1792474961912331, 'learning_rate': 3.8400000000000005e-06, 'epoch': 0.1544}
{'loss': 1.4572, 'grad_norm': 1.3419069969598203, 'learning_rate': 3.85e-06, 'epoch': 0.1548}
{'loss': 1.4448, 'grad_norm': 1.5126998881596516, 'learning_rate': 3.86e-06, 'epoch': 0.1552}
{'loss': 1.4432, 'grad_norm': 1.286236019526576, 'learning_rate': 3.87e-06, 'epoch': 0.1556}
{'loss': 1.4396, 'grad_norm': 1.4380118816621672, 'learning_rate': 3.88e-06, 'epoch': 0.156}
{'eval_valid_loss': 1.435546875, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1119.188, 'eval_valid_steps_per_second': 279.797, 'epoch': 0.156}
{'loss': 1.4492, 'grad_norm': 1.8150702208341667, 'learning_rate': 3.89e-06, 'epoch': 0.1564}
{'loss': 1.4521, 'grad_norm': 1.6384260882166286, 'learning_rate': 3.900000000000001e-06, 'epoch': 0.1568}
{'loss': 1.4509, 'grad_norm': 1.2287743770489323, 'learning_rate': 3.910000000000001e-06, 'epoch': 0.1572}
{'loss': 1.446, 'grad_norm': 1.164083205429368, 'learning_rate': 3.920000000000001e-06, 'epoch': 0.1576}
{'loss': 1.4471, 'grad_norm': 1.3887072503671707, 'learning_rate': 3.9300000000000005e-06, 'epoch': 0.158}
{'loss': 1.4551, 'grad_norm': 1.298785543102276, 'learning_rate': 3.94e-06, 'epoch': 0.1584}
{'loss': 1.4326, 'grad_norm': 1.2747677045635568, 'learning_rate': 3.95e-06, 'epoch': 0.1588}
{'loss': 1.4439, 'grad_norm': 1.2308861993174807, 'learning_rate': 3.96e-06, 'epoch': 0.1592}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 1.4475, 'grad_norm': 1.284889567454873, 'learning_rate': 3.97e-06, 'epoch': 0.1596}
{'loss': 1.4399, 'grad_norm': 1.2734271324063322, 'learning_rate': 3.980000000000001e-06, 'epoch': 0.16}
{'eval_valid_loss': 1.427734375, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.742, 'eval_valid_steps_per_second': 279.435, 'epoch': 0.16}
{'loss': 1.4495, 'grad_norm': 1.2862017648529527, 'learning_rate': 3.990000000000001e-06, 'epoch': 0.1604}
{'loss': 1.4474, 'grad_norm': 1.5604937786670694, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.1608}
{'loss': 1.4563, 'grad_norm': 1.3073241323515352, 'learning_rate': 4.009e-06, 'epoch': 0.1612}
{'loss': 1.4423, 'grad_norm': 1.3014276662505486, 'learning_rate': 4.019e-06, 'epoch': 0.1616}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.4411, 'grad_norm': 1.7329148770523581, 'learning_rate': 4.029e-06, 'epoch': 0.162}
{'loss': 1.436, 'grad_norm': 1.304739725472459, 'learning_rate': 4.039e-06, 'epoch': 0.1624}
{'loss': 1.4503, 'grad_norm': 1.2760604571561394, 'learning_rate': 4.0490000000000005e-06, 'epoch': 0.1628}
{'loss': 1.4438, 'grad_norm': 1.1980229762491872, 'learning_rate': 4.0590000000000004e-06, 'epoch': 0.1632}
{'loss': 1.4443, 'grad_norm': 1.4973811693441001, 'learning_rate': 4.069e-06, 'epoch': 0.1636}
{'loss': 1.4423, 'grad_norm': 1.6425761246640684, 'learning_rate': 4.079e-06, 'epoch': 0.164}
{'eval_valid_loss': 1.421875, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1126.723, 'eval_valid_steps_per_second': 281.681, 'epoch': 0.164}
{'loss': 1.4296, 'grad_norm': 1.5063700966423241, 'learning_rate': 4.089e-06, 'epoch': 0.1644}
{'loss': 1.4402, 'grad_norm': 1.560362192619806, 'learning_rate': 4.099e-06, 'epoch': 0.1648}
{'loss': 1.437, 'grad_norm': 1.5547039376161447, 'learning_rate': 4.109e-06, 'epoch': 0.1652}
{'loss': 1.4386, 'grad_norm': 1.292331339638514, 'learning_rate': 4.119e-06, 'epoch': 0.1656}
{'loss': 1.4327, 'grad_norm': 1.5679800446563958, 'learning_rate': 4.129000000000001e-06, 'epoch': 0.166}
{'loss': 1.4379, 'grad_norm': 1.2727017061418258, 'learning_rate': 4.1390000000000005e-06, 'epoch': 0.1664}
{'loss': 1.4388, 'grad_norm': 1.196901877564614, 'learning_rate': 4.1490000000000004e-06, 'epoch': 0.1668}
{'loss': 1.4256, 'grad_norm': 1.5859703098154225, 'learning_rate': 4.159e-06, 'epoch': 0.1672}
{'loss': 1.4297, 'grad_norm': 1.407207854210773, 'learning_rate': 4.169e-06, 'epoch': 0.1676}
{'loss': 1.4293, 'grad_norm': 2.115219355676926, 'learning_rate': 4.179e-06, 'epoch': 0.168}
{'eval_valid_loss': 1.4150390625, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1124.919, 'eval_valid_steps_per_second': 281.23, 'epoch': 0.168}
{'loss': 1.4407, 'grad_norm': 1.3564363758075644, 'learning_rate': 4.189e-06, 'epoch': 0.1684}
{'loss': 1.4334, 'grad_norm': 1.540282400157923, 'learning_rate': 4.199e-06, 'epoch': 0.1688}
{'loss': 1.4176, 'grad_norm': 1.4805709481507758, 'learning_rate': 4.209000000000001e-06, 'epoch': 0.1692}
{'loss': 1.4354, 'grad_norm': 1.726420055744709, 'learning_rate': 4.219000000000001e-06, 'epoch': 0.1696}
{'loss': 1.4182, 'grad_norm': 1.4072597163555, 'learning_rate': 4.2290000000000005e-06, 'epoch': 0.17}
{'loss': 1.4345, 'grad_norm': 1.5322472283432724, 'learning_rate': 4.239e-06, 'epoch': 0.1704}
{'loss': 1.413, 'grad_norm': 1.4291065421909828, 'learning_rate': 4.249e-06, 'epoch': 0.1708}
{'loss': 1.4326, 'grad_norm': 1.475905201453728, 'learning_rate': 4.259e-06, 'epoch': 0.1712}
{'loss': 1.4332, 'grad_norm': 1.3396718815822315, 'learning_rate': 4.269e-06, 'epoch': 0.1716}
{'loss': 1.4308, 'grad_norm': 1.707683914209008, 'learning_rate': 4.279e-06, 'epoch': 0.172}
{'eval_valid_loss': 1.4072265625, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.219, 'eval_valid_steps_per_second': 281.805, 'epoch': 0.172}
{'loss': 1.441, 'grad_norm': 1.5761232545862511, 'learning_rate': 4.289000000000001e-06, 'epoch': 0.1724}
{'loss': 1.4237, 'grad_norm': 1.39270690386729, 'learning_rate': 4.299000000000001e-06, 'epoch': 0.1728}
{'loss': 1.4185, 'grad_norm': 1.3975176823022832, 'learning_rate': 4.309000000000001e-06, 'epoch': 0.1732}
{'loss': 1.4252, 'grad_norm': 1.368265963672971, 'learning_rate': 4.3190000000000005e-06, 'epoch': 0.1736}
{'loss': 1.4282, 'grad_norm': 1.3449477219570076, 'learning_rate': 4.329e-06, 'epoch': 0.174}
{'loss': 1.4281, 'grad_norm': 1.4245240351452497, 'learning_rate': 4.339e-06, 'epoch': 0.1744}
{'loss': 1.425, 'grad_norm': 1.4386793771838697, 'learning_rate': 4.349e-06, 'epoch': 0.1748}
{'loss': 1.4129, 'grad_norm': 1.4860297065149104, 'learning_rate': 4.359e-06, 'epoch': 0.1752}
{'loss': 1.4183, 'grad_norm': 1.9338585720393926, 'learning_rate': 4.369000000000001e-06, 'epoch': 0.1756}
{'loss': 1.4142, 'grad_norm': 1.4772586669911332, 'learning_rate': 4.379000000000001e-06, 'epoch': 0.176}
{'eval_valid_loss': 1.40234375, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1116.766, 'eval_valid_steps_per_second': 279.191, 'epoch': 0.176}
{'loss': 1.4186, 'grad_norm': 1.3891521433513683, 'learning_rate': 4.389000000000001e-06, 'epoch': 0.1764}
{'loss': 1.4102, 'grad_norm': 1.3842604943920196, 'learning_rate': 4.3990000000000006e-06, 'epoch': 0.1768}
{'loss': 1.4287, 'grad_norm': 1.3994709996249086, 'learning_rate': 4.4090000000000005e-06, 'epoch': 0.1772}
{'loss': 1.4234, 'grad_norm': 1.5902498228090713, 'learning_rate': 4.419e-06, 'epoch': 0.1776}
{'loss': 1.4095, 'grad_norm': 1.56133666361864, 'learning_rate': 4.429e-06, 'epoch': 0.178}
{'loss': 1.4222, 'grad_norm': 1.3800213819122333, 'learning_rate': 4.439e-06, 'epoch': 0.1784}
{'loss': 1.4208, 'grad_norm': 1.3840107386891136, 'learning_rate': 4.449000000000001e-06, 'epoch': 0.1788}
{'loss': 1.4252, 'grad_norm': 1.4533580572627482, 'learning_rate': 4.459000000000001e-06, 'epoch': 0.1792}
{'loss': 1.4227, 'grad_norm': 1.3278999867282921, 'learning_rate': 4.469000000000001e-06, 'epoch': 0.1796}
{'loss': 1.4106, 'grad_norm': 1.3716582157953598, 'learning_rate': 4.479000000000001e-06, 'epoch': 0.18}
{'eval_valid_loss': 1.3916015625, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1125.112, 'eval_valid_steps_per_second': 281.278, 'epoch': 0.18}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.4148, 'grad_norm': 1.6387977824530264, 'learning_rate': 4.4890000000000006e-06, 'epoch': 0.1804}
{'loss': 1.4283, 'grad_norm': 2.109977042855215, 'learning_rate': 4.4990000000000005e-06, 'epoch': 0.1808}
{'loss': 1.4124, 'grad_norm': 1.6118685561661825, 'learning_rate': 4.509e-06, 'epoch': 0.1812}
{'loss': 1.4078, 'grad_norm': 1.4525715635417489, 'learning_rate': 4.519e-06, 'epoch': 0.1816}
{'loss': 1.4227, 'grad_norm': 1.868685807525778, 'learning_rate': 4.529000000000001e-06, 'epoch': 0.182}
{'loss': 1.4039, 'grad_norm': 1.384830533861661, 'learning_rate': 4.539000000000001e-06, 'epoch': 0.1824}
{'loss': 1.4226, 'grad_norm': 1.566257971597191, 'learning_rate': 4.549000000000001e-06, 'epoch': 0.1828}
{'loss': 1.404, 'grad_norm': 1.4391869256138592, 'learning_rate': 4.559000000000001e-06, 'epoch': 0.1832}
{'loss': 1.4196, 'grad_norm': 1.533258211154618, 'learning_rate': 4.569e-06, 'epoch': 0.1836}
{'loss': 1.3993, 'grad_norm': 1.5590934932202587, 'learning_rate': 4.579e-06, 'epoch': 0.184}
{'eval_valid_loss': 1.388671875, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.623, 'eval_valid_steps_per_second': 278.656, 'epoch': 0.184}
{'loss': 1.4039, 'grad_norm': 1.7686873227731383, 'learning_rate': 4.5890000000000004e-06, 'epoch': 0.1844}
{'loss': 1.4011, 'grad_norm': 1.8165148630791532, 'learning_rate': 4.599e-06, 'epoch': 0.1848}
{'loss': 1.4087, 'grad_norm': 1.5145079364948053, 'learning_rate': 4.609e-06, 'epoch': 0.1852}
{'loss': 1.4125, 'grad_norm': 1.4017370327433423, 'learning_rate': 4.619e-06, 'epoch': 0.1856}
{'loss': 1.4071, 'grad_norm': 1.809799620360321, 'learning_rate': 4.629e-06, 'epoch': 0.186}
{'loss': 1.4117, 'grad_norm': 1.4914688731654733, 'learning_rate': 4.639e-06, 'epoch': 0.1864}
{'loss': 1.3951, 'grad_norm': 1.542283603800522, 'learning_rate': 4.649e-06, 'epoch': 0.1868}
{'loss': 1.4014, 'grad_norm': 1.4427948623374, 'learning_rate': 4.659e-06, 'epoch': 0.1872}
{'loss': 1.4, 'grad_norm': 2.3924431089335845, 'learning_rate': 4.6690000000000005e-06, 'epoch': 0.1876}
{'loss': 1.4197, 'grad_norm': 1.6878086134610335, 'learning_rate': 4.6790000000000004e-06, 'epoch': 0.188}
{'eval_valid_loss': 1.37890625, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1120.522, 'eval_valid_steps_per_second': 280.13, 'epoch': 0.188}
{'loss': 1.4011, 'grad_norm': 1.4526902697012847, 'learning_rate': 4.689e-06, 'epoch': 0.1884}
{'loss': 1.3988, 'grad_norm': 1.5017836392022847, 'learning_rate': 4.699e-06, 'epoch': 0.1888}
{'loss': 1.4024, 'grad_norm': 1.7827974089803733, 'learning_rate': 4.709e-06, 'epoch': 0.1892}
{'loss': 1.3956, 'grad_norm': 1.9498984961501118, 'learning_rate': 4.719e-06, 'epoch': 0.1896}
{'loss': 1.4045, 'grad_norm': 1.8793168045665003, 'learning_rate': 4.729e-06, 'epoch': 0.19}
{'loss': 1.3876, 'grad_norm': 1.6668078650824525, 'learning_rate': 4.739e-06, 'epoch': 0.1904}
{'loss': 1.3884, 'grad_norm': 1.5139828676259262, 'learning_rate': 4.749000000000001e-06, 'epoch': 0.1908}
{'loss': 1.3991, 'grad_norm': 1.9532512356971359, 'learning_rate': 4.7590000000000005e-06, 'epoch': 0.1912}
{'loss': 1.3825, 'grad_norm': 1.855985681904296, 'learning_rate': 4.769e-06, 'epoch': 0.1916}
{'loss': 1.3994, 'grad_norm': 1.8347543251935485, 'learning_rate': 4.779e-06, 'epoch': 0.192}
{'eval_valid_loss': 1.3701171875, 'eval_valid_runtime': 0.0956, 'eval_valid_samples_per_second': 1045.63, 'eval_valid_steps_per_second': 261.407, 'epoch': 0.192}
{'loss': 1.3773, 'grad_norm': 1.6088819882868919, 'learning_rate': 4.789e-06, 'epoch': 0.1924}
{'loss': 1.4034, 'grad_norm': 1.7200159092655294, 'learning_rate': 4.799e-06, 'epoch': 0.1928}
{'loss': 1.3893, 'grad_norm': 1.7512220262761518, 'learning_rate': 4.809e-06, 'epoch': 0.1932}
{'loss': 1.3946, 'grad_norm': 1.7666159355629028, 'learning_rate': 4.819e-06, 'epoch': 0.1936}
{'loss': 1.4027, 'grad_norm': 1.95546031573645, 'learning_rate': 4.829000000000001e-06, 'epoch': 0.194}
{'loss': 1.4003, 'grad_norm': 1.6828939439409585, 'learning_rate': 4.839000000000001e-06, 'epoch': 0.1944}
{'loss': 1.3789, 'grad_norm': 1.8925621417840073, 'learning_rate': 4.8490000000000005e-06, 'epoch': 0.1948}
{'loss': 1.3903, 'grad_norm': 1.6983137570831608, 'learning_rate': 4.859e-06, 'epoch': 0.1952}
{'loss': 1.3904, 'grad_norm': 1.6427886705482584, 'learning_rate': 4.869e-06, 'epoch': 0.1956}
{'loss': 1.3866, 'grad_norm': 1.6293243265045785, 'learning_rate': 4.879e-06, 'epoch': 0.196}
{'eval_valid_loss': 1.3603515625, 'eval_valid_runtime': 0.0918, 'eval_valid_samples_per_second': 1089.181, 'eval_valid_steps_per_second': 272.295, 'epoch': 0.196}
{'loss': 1.3973, 'grad_norm': 1.6432759325674666, 'learning_rate': 4.889e-06, 'epoch': 0.1964}
{'loss': 1.3923, 'grad_norm': 2.054693280055978, 'learning_rate': 4.899e-06, 'epoch': 0.1968}
{'loss': 1.3916, 'grad_norm': 2.074922449460954, 'learning_rate': 4.909000000000001e-06, 'epoch': 0.1972}
{'loss': 1.3851, 'grad_norm': 1.8044838233057772, 'learning_rate': 4.919000000000001e-06, 'epoch': 0.1976}
{'loss': 1.3872, 'grad_norm': 1.6887462504286683, 'learning_rate': 4.929000000000001e-06, 'epoch': 0.198}
{'loss': 1.3795, 'grad_norm': 1.739588314656494, 'learning_rate': 4.9390000000000005e-06, 'epoch': 0.1984}
{'loss': 1.3953, 'grad_norm': 1.8631942536779758, 'learning_rate': 4.949e-06, 'epoch': 0.1988}
{'loss': 1.3783, 'grad_norm': 1.9793417242497497, 'learning_rate': 4.959e-06, 'epoch': 0.1992}
{'loss': 1.3729, 'grad_norm': 2.1391131568522446, 'learning_rate': 4.969e-06, 'epoch': 0.1996}
{'loss': 1.3828, 'grad_norm': 2.0131174132622514, 'learning_rate': 4.979e-06, 'epoch': 0.2}
{'eval_valid_loss': 1.3544921875, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.316, 'eval_valid_steps_per_second': 281.829, 'epoch': 0.2}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.3862, 'grad_norm': 1.8113002012336832, 'learning_rate': 4.989000000000001e-06, 'epoch': 0.2004}
{'loss': 1.3841, 'grad_norm': 1.9409607543068181, 'learning_rate': 4.999000000000001e-06, 'epoch': 0.2008}
{'loss': 1.3825, 'grad_norm': 1.675730644475673, 'learning_rate': 5.008000000000001e-06, 'epoch': 0.2012}
{'loss': 1.3587, 'grad_norm': 1.765082628363159, 'learning_rate': 5.018000000000001e-06, 'epoch': 0.2016}
{'loss': 1.3818, 'grad_norm': 1.885134792245282, 'learning_rate': 5.028000000000001e-06, 'epoch': 0.202}
{'loss': 1.3761, 'grad_norm': 1.962150612636426, 'learning_rate': 5.038000000000001e-06, 'epoch': 0.2024}
{'loss': 1.3829, 'grad_norm': 1.748721724514054, 'learning_rate': 5.048000000000001e-06, 'epoch': 0.2028}
{'loss': 1.3707, 'grad_norm': 1.929437327674185, 'learning_rate': 5.0580000000000005e-06, 'epoch': 0.2032}
{'loss': 1.3737, 'grad_norm': 1.8825459232212827, 'learning_rate': 5.0680000000000004e-06, 'epoch': 0.2036}
{'loss': 1.3737, 'grad_norm': 1.7672137459172796, 'learning_rate': 5.078e-06, 'epoch': 0.204}
{'eval_valid_loss': 1.3427734375, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1120.74, 'eval_valid_steps_per_second': 280.185, 'epoch': 0.204}
{'loss': 1.3708, 'grad_norm': 1.9489720671975368, 'learning_rate': 5.088000000000001e-06, 'epoch': 0.2044}
{'loss': 1.3759, 'grad_norm': 1.7843036953028963, 'learning_rate': 5.098000000000001e-06, 'epoch': 0.2048}
{'loss': 1.3635, 'grad_norm': 2.044297415364108, 'learning_rate': 5.108000000000001e-06, 'epoch': 0.2052}
{'loss': 1.3571, 'grad_norm': 2.020471671192812, 'learning_rate': 5.118000000000001e-06, 'epoch': 0.2056}
{'loss': 1.3666, 'grad_norm': 2.1774114737160115, 'learning_rate': 5.128000000000001e-06, 'epoch': 0.206}
{'loss': 1.3647, 'grad_norm': 2.072431063362576, 'learning_rate': 5.138000000000001e-06, 'epoch': 0.2064}
{'loss': 1.3697, 'grad_norm': 1.8768929820194913, 'learning_rate': 5.1480000000000005e-06, 'epoch': 0.2068}
{'loss': 1.3677, 'grad_norm': 2.2774655095261873, 'learning_rate': 5.158e-06, 'epoch': 0.2072}
{'loss': 1.3657, 'grad_norm': 1.8419721222691583, 'learning_rate': 5.168000000000001e-06, 'epoch': 0.2076}
{'loss': 1.3719, 'grad_norm': 1.9039746486256823, 'learning_rate': 5.178000000000001e-06, 'epoch': 0.208}
{'eval_valid_loss': 1.3330078125, 'eval_valid_runtime': 0.0965, 'eval_valid_samples_per_second': 1035.822, 'eval_valid_steps_per_second': 258.956, 'epoch': 0.208}
{'loss': 1.3653, 'grad_norm': 2.0911594456142133, 'learning_rate': 5.188000000000001e-06, 'epoch': 0.2084}
{'loss': 1.3758, 'grad_norm': 1.9757993566815424, 'learning_rate': 5.198000000000001e-06, 'epoch': 0.2088}
{'loss': 1.3558, 'grad_norm': 2.1135458718852296, 'learning_rate': 5.208000000000001e-06, 'epoch': 0.2092}
{'loss': 1.3562, 'grad_norm': 2.2467454843769685, 'learning_rate': 5.218000000000001e-06, 'epoch': 0.2096}
{'loss': 1.3558, 'grad_norm': 2.358249117782041, 'learning_rate': 5.228000000000001e-06, 'epoch': 0.21}
{'loss': 1.3658, 'grad_norm': 2.032898028932601, 'learning_rate': 5.2380000000000005e-06, 'epoch': 0.2104}
{'loss': 1.3636, 'grad_norm': 2.2527888253584973, 'learning_rate': 5.248000000000001e-06, 'epoch': 0.2108}
{'loss': 1.3631, 'grad_norm': 2.202221252184917, 'learning_rate': 5.258000000000001e-06, 'epoch': 0.2112}
{'loss': 1.3542, 'grad_norm': 2.322612239293651, 'learning_rate': 5.268000000000001e-06, 'epoch': 0.2116}
{'loss': 1.3488, 'grad_norm': 2.2663023758194463, 'learning_rate': 5.278000000000001e-06, 'epoch': 0.212}
{'eval_valid_loss': 1.3271484375, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1125.731, 'eval_valid_steps_per_second': 281.433, 'epoch': 0.212}
{'loss': 1.3591, 'grad_norm': 2.1717140289042343, 'learning_rate': 5.288000000000001e-06, 'epoch': 0.2124}
{'loss': 1.3619, 'grad_norm': 2.158965984864557, 'learning_rate': 5.298000000000001e-06, 'epoch': 0.2128}
{'loss': 1.3621, 'grad_norm': 2.201814674408636, 'learning_rate': 5.308000000000001e-06, 'epoch': 0.2132}
{'loss': 1.346, 'grad_norm': 2.1865326309181206, 'learning_rate': 5.318000000000001e-06, 'epoch': 0.2136}
{'loss': 1.3464, 'grad_norm': 2.0466007529420533, 'learning_rate': 5.328000000000001e-06, 'epoch': 0.214}
{'loss': 1.3582, 'grad_norm': 2.0099021412978706, 'learning_rate': 5.338000000000001e-06, 'epoch': 0.2144}
{'loss': 1.3418, 'grad_norm': 2.120192186881913, 'learning_rate': 5.348000000000001e-06, 'epoch': 0.2148}
{'loss': 1.3575, 'grad_norm': 2.086865147404292, 'learning_rate': 5.358000000000001e-06, 'epoch': 0.2152}
{'loss': 1.3449, 'grad_norm': 2.0681061774325613, 'learning_rate': 5.368000000000001e-06, 'epoch': 0.2156}
{'loss': 1.3473, 'grad_norm': 2.084135087631459, 'learning_rate': 5.378e-06, 'epoch': 0.216}
{'eval_valid_loss': 1.3134765625, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1126.49, 'eval_valid_steps_per_second': 281.622, 'epoch': 0.216}
{'loss': 1.3483, 'grad_norm': 2.483647209904877, 'learning_rate': 5.388e-06, 'epoch': 0.2164}
{'loss': 1.3432, 'grad_norm': 2.228014773522104, 'learning_rate': 5.398e-06, 'epoch': 0.2168}
{'loss': 1.3407, 'grad_norm': 2.269274889177946, 'learning_rate': 5.408e-06, 'epoch': 0.2172}
{'loss': 1.3469, 'grad_norm': 2.0934265655537234, 'learning_rate': 5.418e-06, 'epoch': 0.2176}
{'loss': 1.3467, 'grad_norm': 2.086127940424821, 'learning_rate': 5.4279999999999995e-06, 'epoch': 0.218}
{'loss': 1.3616, 'grad_norm': 2.1535905352730866, 'learning_rate': 5.438e-06, 'epoch': 0.2184}
{'loss': 1.3422, 'grad_norm': 2.57790531031605, 'learning_rate': 5.448e-06, 'epoch': 0.2188}
{'loss': 1.338, 'grad_norm': 2.231632233184415, 'learning_rate': 5.458e-06, 'epoch': 0.2192}
{'loss': 1.3313, 'grad_norm': 2.081262106473611, 'learning_rate': 5.468e-06, 'epoch': 0.2196}
{'loss': 1.3333, 'grad_norm': 2.4496366092686905, 'learning_rate': 5.478e-06, 'epoch': 0.22}
{'eval_valid_loss': 1.3076171875, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1116.816, 'eval_valid_steps_per_second': 279.204, 'epoch': 0.22}
{'loss': 1.3478, 'grad_norm': 2.1936862257031544, 'learning_rate': 5.488e-06, 'epoch': 0.2204}
{'loss': 1.3477, 'grad_norm': 1.9284140326939383, 'learning_rate': 5.498e-06, 'epoch': 0.2208}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.3307, 'grad_norm': 2.387817467059174, 'learning_rate': 5.508e-06, 'epoch': 0.2212}
{'loss': 1.3406, 'grad_norm': 2.275318426206479, 'learning_rate': 5.518e-06, 'epoch': 0.2216}
{'loss': 1.3382, 'grad_norm': 2.53115183439757, 'learning_rate': 5.528e-06, 'epoch': 0.222}
{'loss': 1.3495, 'grad_norm': 2.254279375181885, 'learning_rate': 5.538e-06, 'epoch': 0.2224}
{'loss': 1.3338, 'grad_norm': 2.444393917772304, 'learning_rate': 5.548e-06, 'epoch': 0.2228}
{'loss': 1.3312, 'grad_norm': 2.3720836489833736, 'learning_rate': 5.558e-06, 'epoch': 0.2232}
{'loss': 1.3379, 'grad_norm': 2.452109472891156, 'learning_rate': 5.568e-06, 'epoch': 0.2236}
{'loss': 1.3381, 'grad_norm': 2.064673932554744, 'learning_rate': 5.578e-06, 'epoch': 0.224}
{'eval_valid_loss': 1.2978515625, 'eval_valid_runtime': 0.093, 'eval_valid_samples_per_second': 1075.609, 'eval_valid_steps_per_second': 268.902, 'epoch': 0.224}
{'loss': 1.3297, 'grad_norm': 2.2011694548491176, 'learning_rate': 5.588e-06, 'epoch': 0.2244}
{'loss': 1.339, 'grad_norm': 2.290200100505153, 'learning_rate': 5.5980000000000004e-06, 'epoch': 0.2248}
{'loss': 1.3381, 'grad_norm': 2.085865751739874, 'learning_rate': 5.608e-06, 'epoch': 0.2252}
{'loss': 1.3389, 'grad_norm': 2.4126415082709545, 'learning_rate': 5.618e-06, 'epoch': 0.2256}
{'loss': 1.3341, 'grad_norm': 2.668771151161801, 'learning_rate': 5.628e-06, 'epoch': 0.226}
{'loss': 1.3193, 'grad_norm': 2.3961284251855504, 'learning_rate': 5.638e-06, 'epoch': 0.2264}
{'loss': 1.3272, 'grad_norm': 2.411196742208752, 'learning_rate': 5.648e-06, 'epoch': 0.2268}
{'loss': 1.3206, 'grad_norm': 2.477609395076508, 'learning_rate': 5.658e-06, 'epoch': 0.2272}
{'loss': 1.3378, 'grad_norm': 2.2982971595899726, 'learning_rate': 5.668e-06, 'epoch': 0.2276}
{'loss': 1.3267, 'grad_norm': 2.4616095518385723, 'learning_rate': 5.6780000000000005e-06, 'epoch': 0.228}
{'eval_valid_loss': 1.291015625, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1120.282, 'eval_valid_steps_per_second': 280.071, 'epoch': 0.228}
{'loss': 1.3287, 'grad_norm': 2.046677780114075, 'learning_rate': 5.6880000000000004e-06, 'epoch': 0.2284}
{'loss': 1.3152, 'grad_norm': 2.3532986837731267, 'learning_rate': 5.698e-06, 'epoch': 0.2288}
{'loss': 1.3231, 'grad_norm': 2.3709199015249705, 'learning_rate': 5.708e-06, 'epoch': 0.2292}
{'loss': 1.3246, 'grad_norm': 2.2680709625296926, 'learning_rate': 5.718e-06, 'epoch': 0.2296}
{'loss': 1.3177, 'grad_norm': 2.264227679580329, 'learning_rate': 5.728e-06, 'epoch': 0.23}
{'loss': 1.3169, 'grad_norm': 2.4249976485034512, 'learning_rate': 5.738e-06, 'epoch': 0.2304}
{'loss': 1.3115, 'grad_norm': 2.6477898900266417, 'learning_rate': 5.748e-06, 'epoch': 0.2308}
{'loss': 1.3175, 'grad_norm': 2.252651033667323, 'learning_rate': 5.758000000000001e-06, 'epoch': 0.2312}
{'loss': 1.3276, 'grad_norm': 2.3432974855364552, 'learning_rate': 5.7680000000000005e-06, 'epoch': 0.2316}
{'loss': 1.3266, 'grad_norm': 2.29682408814869, 'learning_rate': 5.778e-06, 'epoch': 0.232}
{'eval_valid_loss': 1.28125, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.624, 'eval_valid_steps_per_second': 278.156, 'epoch': 0.232}
{'loss': 1.3354, 'grad_norm': 2.303397244210477, 'learning_rate': 5.788e-06, 'epoch': 0.2324}
{'loss': 1.3213, 'grad_norm': 2.586555072994522, 'learning_rate': 5.798e-06, 'epoch': 0.2328}
{'loss': 1.3197, 'grad_norm': 2.285108836407575, 'learning_rate': 5.808e-06, 'epoch': 0.2332}
{'loss': 1.3117, 'grad_norm': 2.3984283577561487, 'learning_rate': 5.818e-06, 'epoch': 0.2336}
{'loss': 1.3096, 'grad_norm': 2.199142820629639, 'learning_rate': 5.828e-06, 'epoch': 0.234}
{'loss': 1.3309, 'grad_norm': 2.577532659743672, 'learning_rate': 5.838000000000001e-06, 'epoch': 0.2344}
{'loss': 1.3148, 'grad_norm': 2.711842877693824, 'learning_rate': 5.848000000000001e-06, 'epoch': 0.2348}
{'loss': 1.3022, 'grad_norm': 2.2089395054746928, 'learning_rate': 5.8580000000000005e-06, 'epoch': 0.2352}
{'loss': 1.3121, 'grad_norm': 2.478807226271651, 'learning_rate': 5.868e-06, 'epoch': 0.2356}
{'loss': 1.3091, 'grad_norm': 2.3108924936138058, 'learning_rate': 5.878e-06, 'epoch': 0.236}
{'eval_valid_loss': 1.2734375, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1124.261, 'eval_valid_steps_per_second': 281.065, 'epoch': 0.236}
{'loss': 1.3237, 'grad_norm': 2.4108606048109125, 'learning_rate': 5.888e-06, 'epoch': 0.2364}
{'loss': 1.3285, 'grad_norm': 2.4469018591214167, 'learning_rate': 5.898e-06, 'epoch': 0.2368}
{'loss': 1.3084, 'grad_norm': 2.1592535716251766, 'learning_rate': 5.908e-06, 'epoch': 0.2372}
{'loss': 1.3122, 'grad_norm': 2.4184612857158103, 'learning_rate': 5.918000000000001e-06, 'epoch': 0.2376}
{'loss': 1.3062, 'grad_norm': 2.5561113676705394, 'learning_rate': 5.928000000000001e-06, 'epoch': 0.238}
{'loss': 1.3278, 'grad_norm': 2.226193618333548, 'learning_rate': 5.9380000000000006e-06, 'epoch': 0.2384}
{'loss': 1.3093, 'grad_norm': 2.374596137919667, 'learning_rate': 5.9480000000000005e-06, 'epoch': 0.2388}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.3015, 'grad_norm': 2.3499127269072075, 'learning_rate': 5.958e-06, 'epoch': 0.2392}
{'loss': 1.2998, 'grad_norm': 2.5543889154120976, 'learning_rate': 5.968e-06, 'epoch': 0.2396}
{'loss': 1.3205, 'grad_norm': 2.3265324144230726, 'learning_rate': 5.978e-06, 'epoch': 0.24}
{'eval_valid_loss': 1.265625, 'eval_valid_runtime': 0.0922, 'eval_valid_samples_per_second': 1084.194, 'eval_valid_steps_per_second': 271.049, 'epoch': 0.24}
{'loss': 1.2961, 'grad_norm': 2.358926203023576, 'learning_rate': 5.988e-06, 'epoch': 0.2404}
{'loss': 1.3059, 'grad_norm': 2.5293074383017986, 'learning_rate': 5.998000000000001e-06, 'epoch': 0.2408}
{'loss': 1.3225, 'grad_norm': 2.4321952371181372, 'learning_rate': 6.007e-06, 'epoch': 0.2412}
{'loss': 1.3084, 'grad_norm': 2.4099501176970404, 'learning_rate': 6.017000000000001e-06, 'epoch': 0.2416}
{'loss': 1.291, 'grad_norm': 2.4293388962964184, 'learning_rate': 6.027000000000001e-06, 'epoch': 0.242}
{'loss': 1.3104, 'grad_norm': 2.3134054782521254, 'learning_rate': 6.037000000000001e-06, 'epoch': 0.2424}
{'loss': 1.3017, 'grad_norm': 2.2151691291952913, 'learning_rate': 6.047000000000001e-06, 'epoch': 0.2428}
{'loss': 1.2989, 'grad_norm': 2.498401625604113, 'learning_rate': 6.057000000000001e-06, 'epoch': 0.2432}
{'loss': 1.3028, 'grad_norm': 2.538577347401494, 'learning_rate': 6.0670000000000005e-06, 'epoch': 0.2436}
{'loss': 1.295, 'grad_norm': 2.4052019220919028, 'learning_rate': 6.0770000000000004e-06, 'epoch': 0.244}
{'eval_valid_loss': 1.259765625, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1124.352, 'eval_valid_steps_per_second': 281.088, 'epoch': 0.244}
{'loss': 1.3058, 'grad_norm': 2.4857637914152044, 'learning_rate': 6.087e-06, 'epoch': 0.2444}
{'loss': 1.2876, 'grad_norm': 2.4824451712558937, 'learning_rate': 6.097000000000001e-06, 'epoch': 0.2448}
{'loss': 1.2897, 'grad_norm': 2.536634651860121, 'learning_rate': 6.107000000000001e-06, 'epoch': 0.2452}
{'loss': 1.2962, 'grad_norm': 2.484492245192775, 'learning_rate': 6.117000000000001e-06, 'epoch': 0.2456}
{'loss': 1.2976, 'grad_norm': 2.6628788754602826, 'learning_rate': 6.127000000000001e-06, 'epoch': 0.246}
{'loss': 1.3001, 'grad_norm': 2.564678670315552, 'learning_rate': 6.137000000000001e-06, 'epoch': 0.2464}
{'loss': 1.2813, 'grad_norm': 2.422577928238426, 'learning_rate': 6.147000000000001e-06, 'epoch': 0.2468}
{'loss': 1.2986, 'grad_norm': 2.7307067516327814, 'learning_rate': 6.1570000000000005e-06, 'epoch': 0.2472}
{'loss': 1.299, 'grad_norm': 2.7774680441126263, 'learning_rate': 6.167e-06, 'epoch': 0.2476}
{'loss': 1.292, 'grad_norm': 2.2299347921479202, 'learning_rate': 6.177000000000001e-06, 'epoch': 0.248}
{'eval_valid_loss': 1.2529296875, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1121.64, 'eval_valid_steps_per_second': 280.41, 'epoch': 0.248}
{'loss': 1.2934, 'grad_norm': 2.5132666070693563, 'learning_rate': 6.187000000000001e-06, 'epoch': 0.2484}
{'loss': 1.2912, 'grad_norm': 2.35829583175843, 'learning_rate': 6.197000000000001e-06, 'epoch': 0.2488}
{'loss': 1.2886, 'grad_norm': 2.6260359967685143, 'learning_rate': 6.207000000000001e-06, 'epoch': 0.2492}
{'loss': 1.3002, 'grad_norm': 2.3558023510690203, 'learning_rate': 6.217000000000001e-06, 'epoch': 0.2496}
{'loss': 1.2843, 'grad_norm': 2.502132459968697, 'learning_rate': 6.227000000000001e-06, 'epoch': 0.25}
{'loss': 1.2791, 'grad_norm': 2.695329655026008, 'learning_rate': 6.237000000000001e-06, 'epoch': 0.2504}
{'loss': 1.2827, 'grad_norm': 2.576638706530766, 'learning_rate': 6.2470000000000005e-06, 'epoch': 0.2508}
{'loss': 1.2992, 'grad_norm': 2.6989556058700326, 'learning_rate': 6.257000000000001e-06, 'epoch': 0.2512}
{'loss': 1.2904, 'grad_norm': 2.7179338611404122, 'learning_rate': 6.267000000000001e-06, 'epoch': 0.2516}
{'loss': 1.2799, 'grad_norm': 2.6643157346408244, 'learning_rate': 6.277000000000001e-06, 'epoch': 0.252}
{'eval_valid_loss': 1.2470703125, 'eval_valid_runtime': 0.0906, 'eval_valid_samples_per_second': 1103.607, 'eval_valid_steps_per_second': 275.902, 'epoch': 0.252}
{'loss': 1.2881, 'grad_norm': 2.4720109576658325, 'learning_rate': 6.287000000000001e-06, 'epoch': 0.2524}
{'loss': 1.2895, 'grad_norm': 2.4368771894434844, 'learning_rate': 6.297000000000001e-06, 'epoch': 0.2528}
{'loss': 1.2898, 'grad_norm': 2.4413792601047186, 'learning_rate': 6.307000000000001e-06, 'epoch': 0.2532}
{'loss': 1.2782, 'grad_norm': 2.730817655174557, 'learning_rate': 6.317000000000001e-06, 'epoch': 0.2536}
{'loss': 1.2922, 'grad_norm': 2.793595655816044, 'learning_rate': 6.327000000000001e-06, 'epoch': 0.254}
{'loss': 1.2833, 'grad_norm': 2.7113634264106614, 'learning_rate': 6.337000000000001e-06, 'epoch': 0.2544}
{'loss': 1.2833, 'grad_norm': 2.4445169544137233, 'learning_rate': 6.347000000000001e-06, 'epoch': 0.2548}
{'loss': 1.2814, 'grad_norm': 2.583297909345066, 'learning_rate': 6.357000000000001e-06, 'epoch': 0.2552}
{'loss': 1.2705, 'grad_norm': 2.5861951578630213, 'learning_rate': 6.367000000000001e-06, 'epoch': 0.2556}
{'loss': 1.2887, 'grad_norm': 2.3467246601749867, 'learning_rate': 6.377000000000001e-06, 'epoch': 0.256}
{'eval_valid_loss': 1.2392578125, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.386, 'eval_valid_steps_per_second': 278.347, 'epoch': 0.256}
{'loss': 1.2778, 'grad_norm': 2.9730349887183762, 'learning_rate': 6.387000000000001e-06, 'epoch': 0.2564}
{'loss': 1.277, 'grad_norm': 2.6019994466152085, 'learning_rate': 6.397000000000001e-06, 'epoch': 0.2568}
{'loss': 1.2762, 'grad_norm': 2.754338207592076, 'learning_rate': 6.407000000000001e-06, 'epoch': 0.2572}
{'loss': 1.2726, 'grad_norm': 2.530473716639102, 'learning_rate': 6.417000000000001e-06, 'epoch': 0.2576}
{'loss': 1.2861, 'grad_norm': 2.682232029569773, 'learning_rate': 6.427000000000001e-06, 'epoch': 0.258}
{'loss': 1.2774, 'grad_norm': 2.5903582457457253, 'learning_rate': 6.437000000000001e-06, 'epoch': 0.2584}
{'loss': 1.2659, 'grad_norm': 2.5331048497862847, 'learning_rate': 6.447000000000001e-06, 'epoch': 0.2588}
{'loss': 1.2788, 'grad_norm': 2.6553869079386048, 'learning_rate': 6.457000000000001e-06, 'epoch': 0.2592}
{'loss': 1.2692, 'grad_norm': 2.7933283701037364, 'learning_rate': 6.467000000000001e-06, 'epoch': 0.2596}
{'loss': 1.2809, 'grad_norm': 2.5257525970126955, 'learning_rate': 6.477000000000001e-06, 'epoch': 0.26}
{'eval_valid_loss': 1.23046875, 'eval_valid_runtime': 0.0922, 'eval_valid_samples_per_second': 1084.262, 'eval_valid_steps_per_second': 271.065, 'epoch': 0.26}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.2817, 'grad_norm': 2.442846206211922, 'learning_rate': 6.487000000000001e-06, 'epoch': 0.2604}
{'loss': 1.2727, 'grad_norm': 2.42503766013427, 'learning_rate': 6.4970000000000015e-06, 'epoch': 0.2608}
{'loss': 1.273, 'grad_norm': 2.869859562513182, 'learning_rate': 6.507e-06, 'epoch': 0.2612}
{'loss': 1.2677, 'grad_norm': 2.578530591792125, 'learning_rate': 6.517e-06, 'epoch': 0.2616}
{'loss': 1.2745, 'grad_norm': 2.648474490489721, 'learning_rate': 6.527e-06, 'epoch': 0.262}
{'loss': 1.2881, 'grad_norm': 2.658053318934479, 'learning_rate': 6.537e-06, 'epoch': 0.2624}
{'loss': 1.276, 'grad_norm': 2.4525616873751495, 'learning_rate': 6.547e-06, 'epoch': 0.2628}
{'loss': 1.2656, 'grad_norm': 2.5227879848680694, 'learning_rate': 6.557e-06, 'epoch': 0.2632}
{'loss': 1.2708, 'grad_norm': 2.7314290935101653, 'learning_rate': 6.567e-06, 'epoch': 0.2636}
{'loss': 1.2747, 'grad_norm': 2.7463512110041637, 'learning_rate': 6.577e-06, 'epoch': 0.264}
{'eval_valid_loss': 1.2216796875, 'eval_valid_runtime': 0.0931, 'eval_valid_samples_per_second': 1074.053, 'eval_valid_steps_per_second': 268.513, 'epoch': 0.264}
{'loss': 1.2714, 'grad_norm': 2.6940560660436415, 'learning_rate': 6.587e-06, 'epoch': 0.2644}
{'loss': 1.2678, 'grad_norm': 2.73211565680904, 'learning_rate': 6.597e-06, 'epoch': 0.2648}
{'loss': 1.2787, 'grad_norm': 2.825094337913724, 'learning_rate': 6.6070000000000004e-06, 'epoch': 0.2652}
{'loss': 1.2588, 'grad_norm': 2.718952949666106, 'learning_rate': 6.617e-06, 'epoch': 0.2656}
{'loss': 1.2734, 'grad_norm': 2.5852220530938377, 'learning_rate': 6.627e-06, 'epoch': 0.266}
{'loss': 1.271, 'grad_norm': 2.5841041589078366, 'learning_rate': 6.637e-06, 'epoch': 0.2664}
{'loss': 1.2515, 'grad_norm': 2.7712242590449065, 'learning_rate': 6.647e-06, 'epoch': 0.2668}
{'loss': 1.2548, 'grad_norm': 2.774452620080252, 'learning_rate': 6.657e-06, 'epoch': 0.2672}
{'loss': 1.2556, 'grad_norm': 2.5879850319970474, 'learning_rate': 6.667e-06, 'epoch': 0.2676}
{'loss': 1.2706, 'grad_norm': 2.892073340612802, 'learning_rate': 6.677e-06, 'epoch': 0.268}
{'eval_valid_loss': 1.216796875, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.49, 'eval_valid_steps_per_second': 278.372, 'epoch': 0.268}
{'loss': 1.2618, 'grad_norm': 3.1454172627663275, 'learning_rate': 6.6870000000000005e-06, 'epoch': 0.2684}
{'loss': 1.2636, 'grad_norm': 2.6262685571565956, 'learning_rate': 6.6970000000000004e-06, 'epoch': 0.2688}
{'loss': 1.2634, 'grad_norm': 2.532242318489437, 'learning_rate': 6.707e-06, 'epoch': 0.2692}
{'loss': 1.2546, 'grad_norm': 2.674223057718064, 'learning_rate': 6.717e-06, 'epoch': 0.2696}
{'loss': 1.2598, 'grad_norm': 2.8655413701740815, 'learning_rate': 6.727e-06, 'epoch': 0.27}
{'loss': 1.2534, 'grad_norm': 2.7901035951622455, 'learning_rate': 6.737e-06, 'epoch': 0.2704}
{'loss': 1.2661, 'grad_norm': 2.8283898350761585, 'learning_rate': 6.747e-06, 'epoch': 0.2708}
{'loss': 1.2639, 'grad_norm': 2.8593314120613145, 'learning_rate': 6.757e-06, 'epoch': 0.2712}
{'loss': 1.2494, 'grad_norm': 3.086778323391078, 'learning_rate': 6.767000000000001e-06, 'epoch': 0.2716}
{'loss': 1.2613, 'grad_norm': 2.88453610384639, 'learning_rate': 6.7770000000000005e-06, 'epoch': 0.272}
{'eval_valid_loss': 1.2099609375, 'eval_valid_runtime': 0.0909, 'eval_valid_samples_per_second': 1100.05, 'eval_valid_steps_per_second': 275.013, 'epoch': 0.272}
{'loss': 1.251, 'grad_norm': 2.857440930170652, 'learning_rate': 6.787e-06, 'epoch': 0.2724}
{'loss': 1.2495, 'grad_norm': 2.825800804764488, 'learning_rate': 6.797e-06, 'epoch': 0.2728}
{'loss': 1.2489, 'grad_norm': 3.029223934945781, 'learning_rate': 6.807e-06, 'epoch': 0.2732}
{'loss': 1.2637, 'grad_norm': 2.9709518749819797, 'learning_rate': 6.817e-06, 'epoch': 0.2736}
{'loss': 1.2546, 'grad_norm': 2.673417705638741, 'learning_rate': 6.827e-06, 'epoch': 0.274}
{'loss': 1.2705, 'grad_norm': 2.705324212734464, 'learning_rate': 6.837e-06, 'epoch': 0.2744}
{'loss': 1.2502, 'grad_norm': 2.5106281416343688, 'learning_rate': 6.847000000000001e-06, 'epoch': 0.2748}
{'loss': 1.2509, 'grad_norm': 2.4408833210716803, 'learning_rate': 6.857000000000001e-06, 'epoch': 0.2752}
{'loss': 1.2353, 'grad_norm': 2.6254974331021406, 'learning_rate': 6.8670000000000005e-06, 'epoch': 0.2756}
{'loss': 1.2562, 'grad_norm': 2.923222501383518, 'learning_rate': 6.877e-06, 'epoch': 0.276}
{'eval_valid_loss': 1.201171875, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1122.471, 'eval_valid_steps_per_second': 280.618, 'epoch': 0.276}
{'loss': 1.24, 'grad_norm': 2.666479169629206, 'learning_rate': 6.887e-06, 'epoch': 0.2764}
{'loss': 1.2558, 'grad_norm': 2.8071306314560953, 'learning_rate': 6.897e-06, 'epoch': 0.2768}
{'loss': 1.244, 'grad_norm': 2.977512768622757, 'learning_rate': 6.907e-06, 'epoch': 0.2772}
{'loss': 1.2509, 'grad_norm': 2.5083192449999943, 'learning_rate': 6.917e-06, 'epoch': 0.2776}
{'loss': 1.2523, 'grad_norm': 3.191130300519018, 'learning_rate': 6.927000000000001e-06, 'epoch': 0.278}
{'loss': 1.2465, 'grad_norm': 3.0343177452146572, 'learning_rate': 6.937000000000001e-06, 'epoch': 0.2784}
{'loss': 1.2516, 'grad_norm': 2.8033936525971868, 'learning_rate': 6.9470000000000006e-06, 'epoch': 0.2788}
{'loss': 1.2592, 'grad_norm': 2.8297144285592135, 'learning_rate': 6.9570000000000005e-06, 'epoch': 0.2792}
{'loss': 1.245, 'grad_norm': 2.916967902068071, 'learning_rate': 6.967e-06, 'epoch': 0.2796}
{'loss': 1.2504, 'grad_norm': 2.772134232897825, 'learning_rate': 6.977e-06, 'epoch': 0.28}
{'eval_valid_loss': 1.1953125, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.028, 'eval_valid_steps_per_second': 279.507, 'epoch': 0.28}
{'loss': 1.2427, 'grad_norm': 2.7888334484772463, 'learning_rate': 6.987e-06, 'epoch': 0.2804}
{'loss': 1.2176, 'grad_norm': 2.754810309309631, 'learning_rate': 6.997e-06, 'epoch': 0.2808}
{'loss': 1.2498, 'grad_norm': 2.6998536761247283, 'learning_rate': 7.006e-06, 'epoch': 0.2812}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.2405, 'grad_norm': 2.978234049510743, 'learning_rate': 7.016e-06, 'epoch': 0.2816}
{'loss': 1.246, 'grad_norm': 3.3976132588178576, 'learning_rate': 7.026000000000001e-06, 'epoch': 0.282}
{'loss': 1.2302, 'grad_norm': 2.8169328829461784, 'learning_rate': 7.036000000000001e-06, 'epoch': 0.2824}
{'loss': 1.2545, 'grad_norm': 2.9950672166634003, 'learning_rate': 7.046000000000001e-06, 'epoch': 0.2828}
{'loss': 1.2368, 'grad_norm': 2.7590174055545957, 'learning_rate': 7.056000000000001e-06, 'epoch': 0.2832}
{'loss': 1.2469, 'grad_norm': 2.7405279794829616, 'learning_rate': 7.066000000000001e-06, 'epoch': 0.2836}
{'loss': 1.2468, 'grad_norm': 3.0052732623715004, 'learning_rate': 7.0760000000000005e-06, 'epoch': 0.284}
{'eval_valid_loss': 1.1884765625, 'eval_valid_runtime': 0.0919, 'eval_valid_samples_per_second': 1087.729, 'eval_valid_steps_per_second': 271.932, 'epoch': 0.284}
{'loss': 1.237, 'grad_norm': 3.1411942635072134, 'learning_rate': 7.0860000000000004e-06, 'epoch': 0.2844}
{'loss': 1.25, 'grad_norm': 2.8756422331023543, 'learning_rate': 7.096e-06, 'epoch': 0.2848}
{'loss': 1.2459, 'grad_norm': 2.8425480849259386, 'learning_rate': 7.106000000000001e-06, 'epoch': 0.2852}
{'loss': 1.2494, 'grad_norm': 2.629495404503668, 'learning_rate': 7.116000000000001e-06, 'epoch': 0.2856}
{'loss': 1.2395, 'grad_norm': 2.8379788158599273, 'learning_rate': 7.126000000000001e-06, 'epoch': 0.286}
{'loss': 1.2382, 'grad_norm': 3.2214448304962264, 'learning_rate': 7.136000000000001e-06, 'epoch': 0.2864}
{'loss': 1.2285, 'grad_norm': 3.0282147046827794, 'learning_rate': 7.146000000000001e-06, 'epoch': 0.2868}
{'loss': 1.2267, 'grad_norm': 2.5895422505388077, 'learning_rate': 7.156000000000001e-06, 'epoch': 0.2872}
{'loss': 1.2461, 'grad_norm': 2.762766335883416, 'learning_rate': 7.1660000000000005e-06, 'epoch': 0.2876}
{'loss': 1.2353, 'grad_norm': 2.8385830249164856, 'learning_rate': 7.176e-06, 'epoch': 0.288}
{'eval_valid_loss': 1.181640625, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.717, 'eval_valid_steps_per_second': 279.929, 'epoch': 0.288}
{'loss': 1.2294, 'grad_norm': 2.629963646576746, 'learning_rate': 7.186000000000001e-06, 'epoch': 0.2884}
{'loss': 1.2339, 'grad_norm': 2.7332605434142145, 'learning_rate': 7.196000000000001e-06, 'epoch': 0.2888}
{'loss': 1.2412, 'grad_norm': 2.991048725647973, 'learning_rate': 7.206000000000001e-06, 'epoch': 0.2892}
{'loss': 1.2327, 'grad_norm': 3.1057609420644416, 'learning_rate': 7.216000000000001e-06, 'epoch': 0.2896}
{'loss': 1.2421, 'grad_norm': 2.9776222415064315, 'learning_rate': 7.226000000000001e-06, 'epoch': 0.29}
{'loss': 1.222, 'grad_norm': 2.9450829363334665, 'learning_rate': 7.236000000000001e-06, 'epoch': 0.2904}
{'loss': 1.2085, 'grad_norm': 2.6279023394797174, 'learning_rate': 7.246000000000001e-06, 'epoch': 0.2908}
{'loss': 1.223, 'grad_norm': 2.7935725166388212, 'learning_rate': 7.2560000000000005e-06, 'epoch': 0.2912}
{'loss': 1.2412, 'grad_norm': 3.1586732626509026, 'learning_rate': 7.266000000000001e-06, 'epoch': 0.2916}
{'loss': 1.2171, 'grad_norm': 2.78156034474021, 'learning_rate': 7.276000000000001e-06, 'epoch': 0.292}
{'eval_valid_loss': 1.173828125, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.628, 'eval_valid_steps_per_second': 279.907, 'epoch': 0.292}
{'loss': 1.2206, 'grad_norm': 2.8702277090540917, 'learning_rate': 7.286000000000001e-06, 'epoch': 0.2924}
{'loss': 1.2209, 'grad_norm': 3.233390317288557, 'learning_rate': 7.296000000000001e-06, 'epoch': 0.2928}
{'loss': 1.2168, 'grad_norm': 2.9534729596161573, 'learning_rate': 7.306000000000001e-06, 'epoch': 0.2932}
{'loss': 1.2348, 'grad_norm': 2.9997911181845565, 'learning_rate': 7.316000000000001e-06, 'epoch': 0.2936}
{'loss': 1.2396, 'grad_norm': 3.051890138732686, 'learning_rate': 7.326000000000001e-06, 'epoch': 0.294}
{'loss': 1.2326, 'grad_norm': 2.906411684317364, 'learning_rate': 7.3360000000000006e-06, 'epoch': 0.2944}
{'loss': 1.2362, 'grad_norm': 2.7654983577719947, 'learning_rate': 7.346000000000001e-06, 'epoch': 0.2948}
{'loss': 1.2338, 'grad_norm': 3.1221126044030707, 'learning_rate': 7.356000000000001e-06, 'epoch': 0.2952}
{'loss': 1.2286, 'grad_norm': 2.817439256231611, 'learning_rate': 7.366000000000001e-06, 'epoch': 0.2956}
{'loss': 1.2193, 'grad_norm': 2.946577508782776, 'learning_rate': 7.376000000000001e-06, 'epoch': 0.296}
{'eval_valid_loss': 1.1689453125, 'eval_valid_runtime': 0.0948, 'eval_valid_samples_per_second': 1055.142, 'eval_valid_steps_per_second': 263.785, 'epoch': 0.296}
{'loss': 1.245, 'grad_norm': 2.8145859665132447, 'learning_rate': 7.386000000000001e-06, 'epoch': 0.2964}
{'loss': 1.2133, 'grad_norm': 2.8325098093777092, 'learning_rate': 7.396000000000001e-06, 'epoch': 0.2968}
{'loss': 1.2375, 'grad_norm': 3.1178745239288648, 'learning_rate': 7.406000000000001e-06, 'epoch': 0.2972}
{'loss': 1.2312, 'grad_norm': 2.9540479973594715, 'learning_rate': 7.416000000000001e-06, 'epoch': 0.2976}
{'loss': 1.2168, 'grad_norm': 3.012316123803668, 'learning_rate': 7.426000000000001e-06, 'epoch': 0.298}
{'loss': 1.2107, 'grad_norm': 2.9299465522675914, 'learning_rate': 7.436000000000001e-06, 'epoch': 0.2984}
{'loss': 1.2218, 'grad_norm': 2.9197298171757438, 'learning_rate': 7.446000000000001e-06, 'epoch': 0.2988}
{'loss': 1.2193, 'grad_norm': 3.171687355504403, 'learning_rate': 7.456000000000001e-06, 'epoch': 0.2992}
{'loss': 1.2271, 'grad_norm': 3.2782838084734216, 'learning_rate': 7.466000000000001e-06, 'epoch': 0.2996}
{'loss': 1.2208, 'grad_norm': 3.4310536107767184, 'learning_rate': 7.476000000000001e-06, 'epoch': 0.3}
{'eval_valid_loss': 1.1630859375, 'eval_valid_runtime': 0.0885, 'eval_valid_samples_per_second': 1129.581, 'eval_valid_steps_per_second': 282.395, 'epoch': 0.3}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.2277, 'grad_norm': 2.848412218468193, 'learning_rate': 7.486000000000001e-06, 'epoch': 0.3004}
{'loss': 1.2244, 'grad_norm': 3.03287717071243, 'learning_rate': 7.496000000000001e-06, 'epoch': 0.3008}
{'loss': 1.2101, 'grad_norm': 2.847159804200157, 'learning_rate': 7.506000000000001e-06, 'epoch': 0.3012}
{'loss': 1.2229, 'grad_norm': 3.1457667985831557, 'learning_rate': 7.516000000000001e-06, 'epoch': 0.3016}
{'loss': 1.2028, 'grad_norm': 2.766880725547049, 'learning_rate': 7.526000000000001e-06, 'epoch': 0.302}
{'loss': 1.2181, 'grad_norm': 3.1258864146959406, 'learning_rate': 7.536000000000001e-06, 'epoch': 0.3024}
{'loss': 1.2247, 'grad_norm': 3.224135356515074, 'learning_rate': 7.546000000000001e-06, 'epoch': 0.3028}
{'loss': 1.2171, 'grad_norm': 2.881100277930798, 'learning_rate': 7.556000000000001e-06, 'epoch': 0.3032}
{'loss': 1.213, 'grad_norm': 2.9403269485976526, 'learning_rate': 7.566000000000001e-06, 'epoch': 0.3036}
{'loss': 1.2218, 'grad_norm': 2.946450350111197, 'learning_rate': 7.576000000000001e-06, 'epoch': 0.304}
{'eval_valid_loss': 1.158203125, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.005, 'eval_valid_steps_per_second': 278.001, 'epoch': 0.304}
{'loss': 1.2146, 'grad_norm': 3.0093489104077857, 'learning_rate': 7.586000000000001e-06, 'epoch': 0.3044}
{'loss': 1.2048, 'grad_norm': 2.8738857784609193, 'learning_rate': 7.5960000000000015e-06, 'epoch': 0.3048}
{'loss': 1.2133, 'grad_norm': 3.1123977709454707, 'learning_rate': 7.606000000000001e-06, 'epoch': 0.3052}
{'loss': 1.2295, 'grad_norm': 2.8386521443323947, 'learning_rate': 7.616000000000001e-06, 'epoch': 0.3056}
{'loss': 1.2134, 'grad_norm': 2.6940435269538034, 'learning_rate': 7.626e-06, 'epoch': 0.306}
{'loss': 1.2195, 'grad_norm': 2.87952870300071, 'learning_rate': 7.636e-06, 'epoch': 0.3064}
{'loss': 1.2077, 'grad_norm': 3.2096645217205215, 'learning_rate': 7.646e-06, 'epoch': 0.3068}
{'loss': 1.2184, 'grad_norm': 2.855295778768116, 'learning_rate': 7.656000000000001e-06, 'epoch': 0.3072}
{'loss': 1.2167, 'grad_norm': 3.032087868975459, 'learning_rate': 7.666e-06, 'epoch': 0.3076}
{'loss': 1.2043, 'grad_norm': 2.812030896165981, 'learning_rate': 7.676e-06, 'epoch': 0.308}
{'eval_valid_loss': 1.1533203125, 'eval_valid_runtime': 0.0885, 'eval_valid_samples_per_second': 1130.159, 'eval_valid_steps_per_second': 282.54, 'epoch': 0.308}
{'loss': 1.2118, 'grad_norm': 3.1657893189979363, 'learning_rate': 7.686e-06, 'epoch': 0.3084}
{'loss': 1.1996, 'grad_norm': 3.2681424462514252, 'learning_rate': 7.696e-06, 'epoch': 0.3088}
{'loss': 1.195, 'grad_norm': 3.1017552947651543, 'learning_rate': 7.706e-06, 'epoch': 0.3092}
{'loss': 1.2027, 'grad_norm': 3.3110014656804467, 'learning_rate': 7.716e-06, 'epoch': 0.3096}
{'loss': 1.2073, 'grad_norm': 3.005087899098331, 'learning_rate': 7.726e-06, 'epoch': 0.31}
{'loss': 1.2108, 'grad_norm': 2.9091633632646254, 'learning_rate': 7.736e-06, 'epoch': 0.3104}
{'loss': 1.2173, 'grad_norm': 2.9784468946187674, 'learning_rate': 7.746e-06, 'epoch': 0.3108}
{'loss': 1.2005, 'grad_norm': 3.066172660143074, 'learning_rate': 7.756e-06, 'epoch': 0.3112}
{'loss': 1.2051, 'grad_norm': 2.8963358372203944, 'learning_rate': 7.766e-06, 'epoch': 0.3116}
{'loss': 1.2147, 'grad_norm': 2.7771742376004616, 'learning_rate': 7.776e-06, 'epoch': 0.312}
{'eval_valid_loss': 1.1494140625, 'eval_valid_runtime': 0.096, 'eval_valid_samples_per_second': 1042.156, 'eval_valid_steps_per_second': 260.539, 'epoch': 0.312}
{'loss': 1.1919, 'grad_norm': 3.4463531367117026, 'learning_rate': 7.786e-06, 'epoch': 0.3124}
{'loss': 1.1987, 'grad_norm': 2.877049089062692, 'learning_rate': 7.796e-06, 'epoch': 0.3128}
{'loss': 1.1914, 'grad_norm': 3.486406290155788, 'learning_rate': 7.806e-06, 'epoch': 0.3132}
{'loss': 1.2063, 'grad_norm': 3.4307534162491424, 'learning_rate': 7.816000000000001e-06, 'epoch': 0.3136}
{'loss': 1.1964, 'grad_norm': 2.806160547959027, 'learning_rate': 7.826000000000001e-06, 'epoch': 0.314}
{'loss': 1.2033, 'grad_norm': 2.995918825854714, 'learning_rate': 7.836000000000001e-06, 'epoch': 0.3144}
{'loss': 1.1946, 'grad_norm': 3.4635573260113457, 'learning_rate': 7.846e-06, 'epoch': 0.3148}
{'loss': 1.201, 'grad_norm': 2.9915919097089163, 'learning_rate': 7.856e-06, 'epoch': 0.3152}
{'loss': 1.1944, 'grad_norm': 3.0550591469562893, 'learning_rate': 7.866e-06, 'epoch': 0.3156}
{'loss': 1.1998, 'grad_norm': 3.177496668500977, 'learning_rate': 7.876e-06, 'epoch': 0.316}
{'eval_valid_loss': 1.14453125, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1120.121, 'eval_valid_steps_per_second': 280.03, 'epoch': 0.316}
{'loss': 1.1973, 'grad_norm': 3.0019383873760797, 'learning_rate': 7.886e-06, 'epoch': 0.3164}
{'loss': 1.2006, 'grad_norm': 3.273525118508726, 'learning_rate': 7.896e-06, 'epoch': 0.3168}
{'loss': 1.1947, 'grad_norm': 2.937128302209656, 'learning_rate': 7.906e-06, 'epoch': 0.3172}
{'loss': 1.1993, 'grad_norm': 3.1443619392258735, 'learning_rate': 7.916e-06, 'epoch': 0.3176}
{'loss': 1.1794, 'grad_norm': 2.749437182443669, 'learning_rate': 7.926e-06, 'epoch': 0.318}
{'loss': 1.1926, 'grad_norm': 3.2697597273431, 'learning_rate': 7.936e-06, 'epoch': 0.3184}
{'loss': 1.187, 'grad_norm': 3.124169077076511, 'learning_rate': 7.946e-06, 'epoch': 0.3188}
{'loss': 1.2054, 'grad_norm': 3.034009375579093, 'learning_rate': 7.956e-06, 'epoch': 0.3192}
{'loss': 1.1835, 'grad_norm': 2.943822458513382, 'learning_rate': 7.966e-06, 'epoch': 0.3196}
{'loss': 1.1938, 'grad_norm': 2.928007127146046, 'learning_rate': 7.976000000000001e-06, 'epoch': 0.32}
{'eval_valid_loss': 1.1416015625, 'eval_valid_runtime': 0.1936, 'eval_valid_samples_per_second': 516.527, 'eval_valid_steps_per_second': 129.132, 'epoch': 0.32}
{'loss': 1.1971, 'grad_norm': 3.25716196970453, 'learning_rate': 7.986000000000001e-06, 'epoch': 0.3204}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.1933, 'grad_norm': 3.2672845192090625, 'learning_rate': 7.996000000000001e-06, 'epoch': 0.3208}
{'loss': 1.1767, 'grad_norm': 3.2761170065767495, 'learning_rate': 8.005e-06, 'epoch': 0.3212}
{'loss': 1.1938, 'grad_norm': 3.279205639077224, 'learning_rate': 8.015e-06, 'epoch': 0.3216}
{'loss': 1.1873, 'grad_norm': 2.931539941694217, 'learning_rate': 8.025e-06, 'epoch': 0.322}
{'loss': 1.1903, 'grad_norm': 3.0966120495683045, 'learning_rate': 8.035e-06, 'epoch': 0.3224}
{'loss': 1.1928, 'grad_norm': 3.22154059316521, 'learning_rate': 8.045e-06, 'epoch': 0.3228}
{'loss': 1.2042, 'grad_norm': 2.9350049298422123, 'learning_rate': 8.055e-06, 'epoch': 0.3232}
{'loss': 1.1997, 'grad_norm': 2.9571528844370487, 'learning_rate': 8.065e-06, 'epoch': 0.3236}
{'loss': 1.1781, 'grad_norm': 2.90197391130287, 'learning_rate': 8.075000000000001e-06, 'epoch': 0.324}
{'eval_valid_loss': 1.13671875, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.242, 'eval_valid_steps_per_second': 279.31, 'epoch': 0.324}
{'loss': 1.199, 'grad_norm': 2.9870194008647015, 'learning_rate': 8.085000000000001e-06, 'epoch': 0.3244}
{'loss': 1.183, 'grad_norm': 3.435296089079725, 'learning_rate': 8.095000000000001e-06, 'epoch': 0.3248}
{'loss': 1.1901, 'grad_norm': 3.1068525242304266, 'learning_rate': 8.105000000000001e-06, 'epoch': 0.3252}
{'loss': 1.1788, 'grad_norm': 3.0879645631299777, 'learning_rate': 8.115000000000001e-06, 'epoch': 0.3256}
{'loss': 1.1914, 'grad_norm': 3.368088577322741, 'learning_rate': 8.125000000000001e-06, 'epoch': 0.326}
{'loss': 1.1842, 'grad_norm': 3.433778907548658, 'learning_rate': 8.135000000000001e-06, 'epoch': 0.3264}
{'loss': 1.1819, 'grad_norm': 3.0855905941441035, 'learning_rate': 8.145e-06, 'epoch': 0.3268}
{'loss': 1.1872, 'grad_norm': 2.7772376285671987, 'learning_rate': 8.155e-06, 'epoch': 0.3272}
{'loss': 1.2002, 'grad_norm': 3.1129428962055536, 'learning_rate': 8.165e-06, 'epoch': 0.3276}
{'loss': 1.1853, 'grad_norm': 2.944895292324806, 'learning_rate': 8.175e-06, 'epoch': 0.328}
{'eval_valid_loss': 1.130859375, 'eval_valid_runtime': 0.089, 'eval_valid_samples_per_second': 1123.433, 'eval_valid_steps_per_second': 280.858, 'epoch': 0.328}
{'loss': 1.2039, 'grad_norm': 3.102931856596568, 'learning_rate': 8.185e-06, 'epoch': 0.3284}
{'loss': 1.1923, 'grad_norm': 3.2563503057732492, 'learning_rate': 8.195e-06, 'epoch': 0.3288}
{'loss': 1.1772, 'grad_norm': 3.0696069134760626, 'learning_rate': 8.205e-06, 'epoch': 0.3292}
{'loss': 1.1919, 'grad_norm': 2.918457490775149, 'learning_rate': 8.215e-06, 'epoch': 0.3296}
{'loss': 1.1831, 'grad_norm': 3.139478478941019, 'learning_rate': 8.225e-06, 'epoch': 0.33}
{'loss': 1.1852, 'grad_norm': 3.387097840173469, 'learning_rate': 8.235e-06, 'epoch': 0.3304}
{'loss': 1.1858, 'grad_norm': 3.229224291666708, 'learning_rate': 8.245000000000002e-06, 'epoch': 0.3308}
{'loss': 1.1788, 'grad_norm': 3.152013740141083, 'learning_rate': 8.255000000000001e-06, 'epoch': 0.3312}
{'loss': 1.1592, 'grad_norm': 3.103782546302541, 'learning_rate': 8.265000000000001e-06, 'epoch': 0.3316}
{'loss': 1.1859, 'grad_norm': 2.930352321214645, 'learning_rate': 8.275000000000001e-06, 'epoch': 0.332}
{'eval_valid_loss': 1.12890625, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.478, 'eval_valid_steps_per_second': 279.62, 'epoch': 0.332}
{'loss': 1.178, 'grad_norm': 2.90541248661964, 'learning_rate': 8.285000000000001e-06, 'epoch': 0.3324}
{'loss': 1.1776, 'grad_norm': 3.159678575135104, 'learning_rate': 8.295000000000001e-06, 'epoch': 0.3328}
{'loss': 1.1708, 'grad_norm': 3.1793949873685756, 'learning_rate': 8.305000000000001e-06, 'epoch': 0.3332}
{'loss': 1.171, 'grad_norm': 3.0106767021825465, 'learning_rate': 8.315000000000001e-06, 'epoch': 0.3336}
{'loss': 1.1834, 'grad_norm': 3.3193428957519147, 'learning_rate': 8.325e-06, 'epoch': 0.334}
{'loss': 1.1835, 'grad_norm': 3.4548416444921175, 'learning_rate': 8.335e-06, 'epoch': 0.3344}
{'loss': 1.1763, 'grad_norm': 3.0564499914423706, 'learning_rate': 8.345e-06, 'epoch': 0.3348}
{'loss': 1.1712, 'grad_norm': 3.130547615659119, 'learning_rate': 8.355e-06, 'epoch': 0.3352}
{'loss': 1.1737, 'grad_norm': 3.16106510649618, 'learning_rate': 8.365e-06, 'epoch': 0.3356}
{'loss': 1.1995, 'grad_norm': 3.444205852460687, 'learning_rate': 8.375e-06, 'epoch': 0.336}
{'eval_valid_loss': 1.123046875, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1121.484, 'eval_valid_steps_per_second': 280.371, 'epoch': 0.336}
{'loss': 1.1795, 'grad_norm': 3.043500302939089, 'learning_rate': 8.385e-06, 'epoch': 0.3364}
{'loss': 1.183, 'grad_norm': 3.2243050305131495, 'learning_rate': 8.395e-06, 'epoch': 0.3368}
{'loss': 1.1708, 'grad_norm': 3.166631875139818, 'learning_rate': 8.405000000000002e-06, 'epoch': 0.3372}
{'loss': 1.1749, 'grad_norm': 3.0709521474243413, 'learning_rate': 8.415000000000002e-06, 'epoch': 0.3376}
{'loss': 1.168, 'grad_norm': 2.920876352729832, 'learning_rate': 8.425000000000001e-06, 'epoch': 0.338}
{'loss': 1.1729, 'grad_norm': 2.9245977516396806, 'learning_rate': 8.435000000000001e-06, 'epoch': 0.3384}
{'loss': 1.1722, 'grad_norm': 3.23877369043482, 'learning_rate': 8.445000000000001e-06, 'epoch': 0.3388}
{'loss': 1.1778, 'grad_norm': 3.3371722953443586, 'learning_rate': 8.455000000000001e-06, 'epoch': 0.3392}
{'loss': 1.1825, 'grad_norm': 3.245574184293922, 'learning_rate': 8.465000000000001e-06, 'epoch': 0.3396}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.1757, 'grad_norm': 3.0957223067306088, 'learning_rate': 8.475000000000001e-06, 'epoch': 0.34}
{'eval_valid_loss': 1.12109375, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.031, 'eval_valid_steps_per_second': 278.008, 'epoch': 0.34}
{'loss': 1.1648, 'grad_norm': 3.291090152247526, 'learning_rate': 8.485000000000001e-06, 'epoch': 0.3404}
{'loss': 1.1712, 'grad_norm': 3.2451861842279723, 'learning_rate': 8.495e-06, 'epoch': 0.3408}
{'loss': 1.1714, 'grad_norm': 3.2282370685426813, 'learning_rate': 8.505e-06, 'epoch': 0.3412}
{'loss': 1.1746, 'grad_norm': 3.14019390013209, 'learning_rate': 8.515e-06, 'epoch': 0.3416}
{'loss': 1.1624, 'grad_norm': 3.0563396854152263, 'learning_rate': 8.525e-06, 'epoch': 0.342}
{'loss': 1.1752, 'grad_norm': 3.3689423168256853, 'learning_rate': 8.535e-06, 'epoch': 0.3424}
{'loss': 1.1724, 'grad_norm': 3.34791975197277, 'learning_rate': 8.545e-06, 'epoch': 0.3428}
{'loss': 1.1752, 'grad_norm': 3.3578462271507585, 'learning_rate': 8.555e-06, 'epoch': 0.3432}
{'loss': 1.1645, 'grad_norm': 3.1168908381972824, 'learning_rate': 8.565000000000002e-06, 'epoch': 0.3436}
{'loss': 1.1782, 'grad_norm': 3.0936485379853518, 'learning_rate': 8.575000000000002e-06, 'epoch': 0.344}
{'eval_valid_loss': 1.1123046875, 'eval_valid_runtime': 0.0917, 'eval_valid_samples_per_second': 1090.143, 'eval_valid_steps_per_second': 272.536, 'epoch': 0.344}
{'loss': 1.163, 'grad_norm': 3.312625081921634, 'learning_rate': 8.585000000000002e-06, 'epoch': 0.3444}
{'loss': 1.1668, 'grad_norm': 3.5701897681445613, 'learning_rate': 8.595000000000002e-06, 'epoch': 0.3448}
{'loss': 1.1712, 'grad_norm': 3.242468378240765, 'learning_rate': 8.605000000000001e-06, 'epoch': 0.3452}
{'loss': 1.1783, 'grad_norm': 3.516156626877708, 'learning_rate': 8.615000000000001e-06, 'epoch': 0.3456}
{'loss': 1.1646, 'grad_norm': 3.3389722612867656, 'learning_rate': 8.625000000000001e-06, 'epoch': 0.346}
{'loss': 1.1698, 'grad_norm': 3.1867012958651824, 'learning_rate': 8.635000000000001e-06, 'epoch': 0.3464}
{'loss': 1.1657, 'grad_norm': 3.4582994796444124, 'learning_rate': 8.645000000000001e-06, 'epoch': 0.3468}
{'loss': 1.1761, 'grad_norm': 3.2097427484338454, 'learning_rate': 8.655000000000001e-06, 'epoch': 0.3472}
{'loss': 1.1656, 'grad_norm': 3.1520616483142034, 'learning_rate': 8.665000000000001e-06, 'epoch': 0.3476}
{'loss': 1.1755, 'grad_norm': 3.328939257666046, 'learning_rate': 8.675e-06, 'epoch': 0.348}
{'eval_valid_loss': 1.1064453125, 'eval_valid_runtime': 0.0903, 'eval_valid_samples_per_second': 1106.855, 'eval_valid_steps_per_second': 276.714, 'epoch': 0.348}
{'loss': 1.1818, 'grad_norm': 2.8465304562956057, 'learning_rate': 8.685e-06, 'epoch': 0.3484}
{'loss': 1.1699, 'grad_norm': 3.09533446610815, 'learning_rate': 8.695e-06, 'epoch': 0.3488}
{'loss': 1.1737, 'grad_norm': 2.9072604524983987, 'learning_rate': 8.705e-06, 'epoch': 0.3492}
{'loss': 1.1702, 'grad_norm': 3.148950970336469, 'learning_rate': 8.715e-06, 'epoch': 0.3496}
{'loss': 1.1771, 'grad_norm': 3.091216485304195, 'learning_rate': 8.725000000000002e-06, 'epoch': 0.35}
{'loss': 1.1677, 'grad_norm': 3.144914885629135, 'learning_rate': 8.735000000000002e-06, 'epoch': 0.3504}
{'loss': 1.1677, 'grad_norm': 3.0337066232498255, 'learning_rate': 8.745000000000002e-06, 'epoch': 0.3508}
{'loss': 1.1648, 'grad_norm': 3.3182444066995154, 'learning_rate': 8.755e-06, 'epoch': 0.3512}
{'loss': 1.166, 'grad_norm': 3.243891927521118, 'learning_rate': 8.765e-06, 'epoch': 0.3516}
{'loss': 1.1585, 'grad_norm': 2.8989903984302288, 'learning_rate': 8.775e-06, 'epoch': 0.352}
{'eval_valid_loss': 1.103515625, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.289, 'eval_valid_steps_per_second': 279.322, 'epoch': 0.352}
{'loss': 1.1606, 'grad_norm': 3.9647349375558885, 'learning_rate': 8.785e-06, 'epoch': 0.3524}
{'loss': 1.1584, 'grad_norm': 3.1464666677458597, 'learning_rate': 8.795e-06, 'epoch': 0.3528}
{'loss': 1.1617, 'grad_norm': 3.52898172091352, 'learning_rate': 8.805e-06, 'epoch': 0.3532}
{'loss': 1.1662, 'grad_norm': 3.487097929265598, 'learning_rate': 8.815e-06, 'epoch': 0.3536}
{'loss': 1.1627, 'grad_norm': 3.3590861262829312, 'learning_rate': 8.825000000000001e-06, 'epoch': 0.354}
{'loss': 1.1603, 'grad_norm': 3.027672726384101, 'learning_rate': 8.835000000000001e-06, 'epoch': 0.3544}
{'loss': 1.1621, 'grad_norm': 3.769915277556138, 'learning_rate': 8.845000000000001e-06, 'epoch': 0.3548}
{'loss': 1.151, 'grad_norm': 3.016442030087091, 'learning_rate': 8.855e-06, 'epoch': 0.3552}
{'loss': 1.1598, 'grad_norm': 3.3588673230185293, 'learning_rate': 8.865e-06, 'epoch': 0.3556}
{'loss': 1.1518, 'grad_norm': 3.5702898621624013, 'learning_rate': 8.875e-06, 'epoch': 0.356}
{'eval_valid_loss': 1.10546875, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1114.043, 'eval_valid_steps_per_second': 278.511, 'epoch': 0.356}
{'loss': 1.1653, 'grad_norm': 3.8705519326863813, 'learning_rate': 8.885e-06, 'epoch': 0.3564}
{'loss': 1.1605, 'grad_norm': 3.0365722721025445, 'learning_rate': 8.895e-06, 'epoch': 0.3568}
{'loss': 1.1557, 'grad_norm': 3.2406713640580436, 'learning_rate': 8.905e-06, 'epoch': 0.3572}
{'loss': 1.1562, 'grad_norm': 3.5618738326850417, 'learning_rate': 8.915e-06, 'epoch': 0.3576}
{'loss': 1.1653, 'grad_norm': 3.006090840629178, 'learning_rate': 8.925e-06, 'epoch': 0.358}
{'loss': 1.1662, 'grad_norm': 3.095407576547413, 'learning_rate': 8.935e-06, 'epoch': 0.3584}
{'loss': 1.1478, 'grad_norm': 3.116429066951037, 'learning_rate': 8.945e-06, 'epoch': 0.3588}
{'loss': 1.1453, 'grad_norm': 3.0662445803258858, 'learning_rate': 8.955e-06, 'epoch': 0.3592}
{'loss': 1.1659, 'grad_norm': 3.563396449820664, 'learning_rate': 8.965e-06, 'epoch': 0.3596}
{'loss': 1.1412, 'grad_norm': 3.1037285301396933, 'learning_rate': 8.975e-06, 'epoch': 0.36}
{'eval_valid_loss': 1.09765625, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.904, 'eval_valid_steps_per_second': 278.976, 'epoch': 0.36}
{'loss': 1.1653, 'grad_norm': 3.0932819127941853, 'learning_rate': 8.985000000000001e-06, 'epoch': 0.3604}
{'loss': 1.1436, 'grad_norm': 3.4818121974925558, 'learning_rate': 8.995000000000001e-06, 'epoch': 0.3608}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.1586, 'grad_norm': 3.1183924294179346, 'learning_rate': 9.004e-06, 'epoch': 0.3612}
{'loss': 1.1667, 'grad_norm': 3.4514462027896196, 'learning_rate': 9.014e-06, 'epoch': 0.3616}
{'loss': 1.1608, 'grad_norm': 3.1899661266537462, 'learning_rate': 9.024e-06, 'epoch': 0.362}
{'loss': 1.1449, 'grad_norm': 3.5553664093611204, 'learning_rate': 9.034e-06, 'epoch': 0.3624}
{'loss': 1.1665, 'grad_norm': 3.417019451737962, 'learning_rate': 9.044e-06, 'epoch': 0.3628}
{'loss': 1.158, 'grad_norm': 2.936158655190306, 'learning_rate': 9.054e-06, 'epoch': 0.3632}
{'loss': 1.1379, 'grad_norm': 3.667488115404528, 'learning_rate': 9.064e-06, 'epoch': 0.3636}
{'loss': 1.1566, 'grad_norm': 3.3897624962381747, 'learning_rate': 9.074e-06, 'epoch': 0.364}
{'eval_valid_loss': 1.095703125, 'eval_valid_runtime': 0.0965, 'eval_valid_samples_per_second': 1036.765, 'eval_valid_steps_per_second': 259.191, 'epoch': 0.364}
{'loss': 1.1523, 'grad_norm': 3.191009459464488, 'learning_rate': 9.084e-06, 'epoch': 0.3644}
{'loss': 1.1518, 'grad_norm': 3.2603368698608994, 'learning_rate': 9.094000000000001e-06, 'epoch': 0.3648}
{'loss': 1.1451, 'grad_norm': 3.5657505213904197, 'learning_rate': 9.104000000000001e-06, 'epoch': 0.3652}
{'loss': 1.1418, 'grad_norm': 3.3279183910157326, 'learning_rate': 9.114000000000001e-06, 'epoch': 0.3656}
{'loss': 1.1697, 'grad_norm': 3.471826469226409, 'learning_rate': 9.124000000000001e-06, 'epoch': 0.366}
{'loss': 1.1559, 'grad_norm': 3.376687616927844, 'learning_rate': 9.134000000000001e-06, 'epoch': 0.3664}
{'loss': 1.1514, 'grad_norm': 3.1247531841083918, 'learning_rate': 9.144000000000001e-06, 'epoch': 0.3668}
{'loss': 1.1663, 'grad_norm': 3.6619771460502837, 'learning_rate': 9.154e-06, 'epoch': 0.3672}
{'loss': 1.1482, 'grad_norm': 3.4956540220117467, 'learning_rate': 9.164e-06, 'epoch': 0.3676}
{'loss': 1.1503, 'grad_norm': 3.108631845544245, 'learning_rate': 9.174e-06, 'epoch': 0.368}
{'eval_valid_loss': 1.0888671875, 'eval_valid_runtime': 0.0924, 'eval_valid_samples_per_second': 1082.647, 'eval_valid_steps_per_second': 270.662, 'epoch': 0.368}
{'loss': 1.1451, 'grad_norm': 2.899370038323701, 'learning_rate': 9.184e-06, 'epoch': 0.3684}
{'loss': 1.1454, 'grad_norm': 3.3547858097221357, 'learning_rate': 9.194e-06, 'epoch': 0.3688}
{'loss': 1.1549, 'grad_norm': 3.402095833821797, 'learning_rate': 9.204e-06, 'epoch': 0.3692}
{'loss': 1.1636, 'grad_norm': 2.83270078965868, 'learning_rate': 9.214e-06, 'epoch': 0.3696}
{'loss': 1.1392, 'grad_norm': 3.1766279436191702, 'learning_rate': 9.224e-06, 'epoch': 0.37}
{'loss': 1.147, 'grad_norm': 3.4886646124170797, 'learning_rate': 9.234e-06, 'epoch': 0.3704}
{'loss': 1.1502, 'grad_norm': 3.303361649278007, 'learning_rate': 9.244e-06, 'epoch': 0.3708}
{'loss': 1.1458, 'grad_norm': 3.242274313074671, 'learning_rate': 9.254000000000002e-06, 'epoch': 0.3712}
{'loss': 1.13, 'grad_norm': 3.3857934253073565, 'learning_rate': 9.264000000000001e-06, 'epoch': 0.3716}
{'loss': 1.1489, 'grad_norm': 3.4142663750781987, 'learning_rate': 9.274000000000001e-06, 'epoch': 0.372}
{'eval_valid_loss': 1.0908203125, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.678, 'eval_valid_steps_per_second': 279.92, 'epoch': 0.372}
{'loss': 1.1369, 'grad_norm': 3.313286319109046, 'learning_rate': 9.284000000000001e-06, 'epoch': 0.3724}
{'loss': 1.1546, 'grad_norm': 3.036030686342764, 'learning_rate': 9.294000000000001e-06, 'epoch': 0.3728}
{'loss': 1.1586, 'grad_norm': 3.391357703082245, 'learning_rate': 9.304000000000001e-06, 'epoch': 0.3732}
{'loss': 1.133, 'grad_norm': 3.5094119887810225, 'learning_rate': 9.314000000000001e-06, 'epoch': 0.3736}
{'loss': 1.1496, 'grad_norm': 3.509913972618205, 'learning_rate': 9.324000000000001e-06, 'epoch': 0.374}
{'loss': 1.1399, 'grad_norm': 3.1331567836711596, 'learning_rate': 9.334e-06, 'epoch': 0.3744}
{'loss': 1.1309, 'grad_norm': 3.295273794251659, 'learning_rate': 9.344e-06, 'epoch': 0.3748}
{'loss': 1.1273, 'grad_norm': 3.2400353883941713, 'learning_rate': 9.354e-06, 'epoch': 0.3752}
{'loss': 1.1274, 'grad_norm': 3.1692125016765726, 'learning_rate': 9.364e-06, 'epoch': 0.3756}
{'loss': 1.1519, 'grad_norm': 3.4406897570815778, 'learning_rate': 9.374e-06, 'epoch': 0.376}
{'eval_valid_loss': 1.0869140625, 'eval_valid_runtime': 0.1781, 'eval_valid_samples_per_second': 561.557, 'eval_valid_steps_per_second': 140.389, 'epoch': 0.376}
{'loss': 1.1225, 'grad_norm': 3.103140297551399, 'learning_rate': 9.384e-06, 'epoch': 0.3764}
{'loss': 1.1513, 'grad_norm': 3.4488416143722365, 'learning_rate': 9.394e-06, 'epoch': 0.3768}
{'loss': 1.1471, 'grad_norm': 3.9055620655831866, 'learning_rate': 9.404e-06, 'epoch': 0.3772}
{'loss': 1.1462, 'grad_norm': 3.1813910417449462, 'learning_rate': 9.414000000000002e-06, 'epoch': 0.3776}
{'loss': 1.1347, 'grad_norm': 3.7511531646239646, 'learning_rate': 9.424000000000002e-06, 'epoch': 0.378}
{'loss': 1.1278, 'grad_norm': 3.591312419336954, 'learning_rate': 9.434000000000001e-06, 'epoch': 0.3784}
{'loss': 1.1233, 'grad_norm': 3.7576235844636834, 'learning_rate': 9.444000000000001e-06, 'epoch': 0.3788}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.1466, 'grad_norm': 3.5790360136391848, 'learning_rate': 9.454000000000001e-06, 'epoch': 0.3792}
{'loss': 1.1387, 'grad_norm': 3.2936459316772946, 'learning_rate': 9.464000000000001e-06, 'epoch': 0.3796}
{'loss': 1.1378, 'grad_norm': 3.2548902656293586, 'learning_rate': 9.474000000000001e-06, 'epoch': 0.38}
{'eval_valid_loss': 1.087890625, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1124.961, 'eval_valid_steps_per_second': 281.24, 'epoch': 0.38}
{'loss': 1.1483, 'grad_norm': 3.6277400874707704, 'learning_rate': 9.484000000000001e-06, 'epoch': 0.3804}
{'loss': 1.1328, 'grad_norm': 3.292018024588751, 'learning_rate': 9.494000000000001e-06, 'epoch': 0.3808}
{'loss': 1.128, 'grad_norm': 3.2221883075717908, 'learning_rate': 9.504e-06, 'epoch': 0.3812}
{'loss': 1.125, 'grad_norm': 3.2296532705234373, 'learning_rate': 9.514e-06, 'epoch': 0.3816}
{'loss': 1.1297, 'grad_norm': 3.228983453454808, 'learning_rate': 9.524e-06, 'epoch': 0.382}
{'loss': 1.1342, 'grad_norm': 3.0680048037771765, 'learning_rate': 9.534e-06, 'epoch': 0.3824}
{'loss': 1.1415, 'grad_norm': 3.422846225184743, 'learning_rate': 9.544e-06, 'epoch': 0.3828}
{'loss': 1.1307, 'grad_norm': 4.180352302516343, 'learning_rate': 9.554e-06, 'epoch': 0.3832}
{'loss': 1.1516, 'grad_norm': 3.3989284204648973, 'learning_rate': 9.564e-06, 'epoch': 0.3836}
{'loss': 1.1224, 'grad_norm': 3.365614220363742, 'learning_rate': 9.574000000000002e-06, 'epoch': 0.384}
{'eval_valid_loss': 1.080078125, 'eval_valid_runtime': 0.0984, 'eval_valid_samples_per_second': 1016.601, 'eval_valid_steps_per_second': 254.15, 'epoch': 0.384}
{'loss': 1.1377, 'grad_norm': 3.0945357952744237, 'learning_rate': 9.584000000000002e-06, 'epoch': 0.3844}
{'loss': 1.1316, 'grad_norm': 3.4686332287765533, 'learning_rate': 9.594000000000002e-06, 'epoch': 0.3848}
{'loss': 1.1301, 'grad_norm': 3.405999576917455, 'learning_rate': 9.604000000000002e-06, 'epoch': 0.3852}
{'loss': 1.1352, 'grad_norm': 3.486664178290005, 'learning_rate': 9.614000000000001e-06, 'epoch': 0.3856}
{'loss': 1.1386, 'grad_norm': 3.0549790811524877, 'learning_rate': 9.624000000000001e-06, 'epoch': 0.386}
{'loss': 1.1358, 'grad_norm': 3.3697393089994816, 'learning_rate': 9.634000000000001e-06, 'epoch': 0.3864}
{'loss': 1.1491, 'grad_norm': 3.347561882763251, 'learning_rate': 9.644000000000001e-06, 'epoch': 0.3868}
{'loss': 1.1397, 'grad_norm': 3.737059262931465, 'learning_rate': 9.654000000000001e-06, 'epoch': 0.3872}
{'loss': 1.1316, 'grad_norm': 3.390630757199511, 'learning_rate': 9.664000000000001e-06, 'epoch': 0.3876}
{'loss': 1.125, 'grad_norm': 3.4233313731637063, 'learning_rate': 9.674000000000001e-06, 'epoch': 0.388}
{'eval_valid_loss': 1.076171875, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1120.737, 'eval_valid_steps_per_second': 280.184, 'epoch': 0.388}
{'loss': 1.1303, 'grad_norm': 3.5202353278115015, 'learning_rate': 9.684e-06, 'epoch': 0.3884}
{'loss': 1.147, 'grad_norm': 2.9078533405467577, 'learning_rate': 9.694e-06, 'epoch': 0.3888}
{'loss': 1.1271, 'grad_norm': 3.2587988295735975, 'learning_rate': 9.704e-06, 'epoch': 0.3892}
{'loss': 1.1355, 'grad_norm': 3.5359421335983092, 'learning_rate': 9.714e-06, 'epoch': 0.3896}
{'loss': 1.1193, 'grad_norm': 3.6167504971467546, 'learning_rate': 9.724e-06, 'epoch': 0.39}
{'loss': 1.1419, 'grad_norm': 3.675705547141024, 'learning_rate': 9.734000000000002e-06, 'epoch': 0.3904}
{'loss': 1.1323, 'grad_norm': 3.737493067195931, 'learning_rate': 9.744000000000002e-06, 'epoch': 0.3908}
{'loss': 1.1312, 'grad_norm': 3.2844226757476562, 'learning_rate': 9.754000000000002e-06, 'epoch': 0.3912}
{'loss': 1.1343, 'grad_norm': 3.6721257144475263, 'learning_rate': 9.764000000000002e-06, 'epoch': 0.3916}
{'loss': 1.1227, 'grad_norm': 3.40287685487926, 'learning_rate': 9.774000000000002e-06, 'epoch': 0.392}
{'eval_valid_loss': 1.0771484375, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.922, 'eval_valid_steps_per_second': 278.731, 'epoch': 0.392}
{'loss': 1.1338, 'grad_norm': 3.208318380015482, 'learning_rate': 9.784000000000002e-06, 'epoch': 0.3924}
{'loss': 1.1321, 'grad_norm': 3.1899128831878354, 'learning_rate': 9.794000000000001e-06, 'epoch': 0.3928}
{'loss': 1.1148, 'grad_norm': 3.412207450832829, 'learning_rate': 9.804000000000001e-06, 'epoch': 0.3932}
{'loss': 1.1249, 'grad_norm': 3.623937418191016, 'learning_rate': 9.814000000000001e-06, 'epoch': 0.3936}
{'loss': 1.1314, 'grad_norm': 3.207091030070148, 'learning_rate': 9.824000000000001e-06, 'epoch': 0.394}
{'loss': 1.1255, 'grad_norm': 3.9354783272275506, 'learning_rate': 9.834000000000001e-06, 'epoch': 0.3944}
{'loss': 1.1182, 'grad_norm': 3.7508554595490144, 'learning_rate': 9.844000000000001e-06, 'epoch': 0.3948}
{'loss': 1.1317, 'grad_norm': 3.252681038983732, 'learning_rate': 9.854000000000001e-06, 'epoch': 0.3952}
{'loss': 1.1315, 'grad_norm': 3.2754566183703986, 'learning_rate': 9.864e-06, 'epoch': 0.3956}
{'loss': 1.1294, 'grad_norm': 3.319104852401686, 'learning_rate': 9.874e-06, 'epoch': 0.396}
{'eval_valid_loss': 1.0693359375, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1110.983, 'eval_valid_steps_per_second': 277.746, 'epoch': 0.396}
{'loss': 1.1286, 'grad_norm': 3.5693837383111866, 'learning_rate': 9.884e-06, 'epoch': 0.3964}
{'loss': 1.1288, 'grad_norm': 3.4027080932064657, 'learning_rate': 9.894e-06, 'epoch': 0.3968}
{'loss': 1.1256, 'grad_norm': 3.463893342867138, 'learning_rate': 9.904e-06, 'epoch': 0.3972}
{'loss': 1.1208, 'grad_norm': 3.680725534380707, 'learning_rate': 9.914e-06, 'epoch': 0.3976}
{'loss': 1.1431, 'grad_norm': 3.047921739410191, 'learning_rate': 9.924e-06, 'epoch': 0.398}
{'loss': 1.13, 'grad_norm': 3.769731237611085, 'learning_rate': 9.934e-06, 'epoch': 0.3984}
{'loss': 1.1297, 'grad_norm': 3.8154360144049875, 'learning_rate': 9.944e-06, 'epoch': 0.3988}
{'loss': 1.1312, 'grad_norm': 3.0147157691823545, 'learning_rate': 9.954e-06, 'epoch': 0.3992}
{'loss': 1.1125, 'grad_norm': 3.8302097327124587, 'learning_rate': 9.964e-06, 'epoch': 0.3996}
{'loss': 1.12, 'grad_norm': 3.4194996484381734, 'learning_rate': 9.974e-06, 'epoch': 0.4}
{'eval_valid_loss': 1.0654296875, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1122.549, 'eval_valid_steps_per_second': 280.637, 'epoch': 0.4}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.114, 'grad_norm': 3.789418382743939, 'learning_rate': 9.984e-06, 'epoch': 0.4004}
{'loss': 1.1231, 'grad_norm': 3.5172689451314376, 'learning_rate': 9.994000000000001e-06, 'epoch': 0.4008}
{'loss': 1.1159, 'grad_norm': 3.467201815313877, 'learning_rate': 9.999666666666667e-06, 'epoch': 0.4012}
{'loss': 1.1331, 'grad_norm': 3.6478269258081713, 'learning_rate': 9.998555555555557e-06, 'epoch': 0.4016}
{'loss': 1.1229, 'grad_norm': 3.683640011510067, 'learning_rate': 9.997444444444446e-06, 'epoch': 0.402}
{'loss': 1.1105, 'grad_norm': 3.4361637119076303, 'learning_rate': 9.996333333333335e-06, 'epoch': 0.4024}
{'loss': 1.1313, 'grad_norm': 3.7858097783805134, 'learning_rate': 9.995222222222223e-06, 'epoch': 0.4028}
{'loss': 1.1151, 'grad_norm': 3.5014543491940895, 'learning_rate': 9.994111111111112e-06, 'epoch': 0.4032}
{'loss': 1.1118, 'grad_norm': 3.4675426959631857, 'learning_rate': 9.993e-06, 'epoch': 0.4036}
{'loss': 1.1174, 'grad_norm': 3.1565424339865897, 'learning_rate': 9.99188888888889e-06, 'epoch': 0.404}
{'eval_valid_loss': 1.0634765625, 'eval_valid_runtime': 0.0924, 'eval_valid_samples_per_second': 1081.974, 'eval_valid_steps_per_second': 270.493, 'epoch': 0.404}
{'loss': 1.118, 'grad_norm': 4.033113817607062, 'learning_rate': 9.990777777777778e-06, 'epoch': 0.4044}
{'loss': 1.1174, 'grad_norm': 3.415305219166587, 'learning_rate': 9.989666666666667e-06, 'epoch': 0.4048}
{'loss': 1.112, 'grad_norm': 3.3051904606729163, 'learning_rate': 9.988555555555557e-06, 'epoch': 0.4052}
{'loss': 1.1246, 'grad_norm': 3.3681163878480285, 'learning_rate': 9.987444444444446e-06, 'epoch': 0.4056}
{'loss': 1.1279, 'grad_norm': 3.250709905485529, 'learning_rate': 9.986333333333335e-06, 'epoch': 0.406}
{'loss': 1.1265, 'grad_norm': 3.5027464326047113, 'learning_rate': 9.985222222222223e-06, 'epoch': 0.4064}
{'loss': 1.1201, 'grad_norm': 3.231733035538451, 'learning_rate': 9.984111111111112e-06, 'epoch': 0.4068}
{'loss': 1.1218, 'grad_norm': 3.0363843782338282, 'learning_rate': 9.983e-06, 'epoch': 0.4072}
{'loss': 1.1221, 'grad_norm': 3.8067368504520145, 'learning_rate': 9.98188888888889e-06, 'epoch': 0.4076}
{'loss': 1.1261, 'grad_norm': 3.2985818882813356, 'learning_rate': 9.980777777777778e-06, 'epoch': 0.408}
{'eval_valid_loss': 1.060546875, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1122.246, 'eval_valid_steps_per_second': 280.561, 'epoch': 0.408}
{'loss': 1.1136, 'grad_norm': 3.0958386316335393, 'learning_rate': 9.979666666666667e-06, 'epoch': 0.4084}
{'loss': 1.125, 'grad_norm': 3.260739802028061, 'learning_rate': 9.978555555555557e-06, 'epoch': 0.4088}
{'loss': 1.1084, 'grad_norm': 3.3366562294645936, 'learning_rate': 9.977444444444444e-06, 'epoch': 0.4092}
{'loss': 1.1181, 'grad_norm': 3.320243466725487, 'learning_rate': 9.976333333333335e-06, 'epoch': 0.4096}
{'loss': 1.1229, 'grad_norm': 3.4447890249464628, 'learning_rate': 9.975222222222223e-06, 'epoch': 0.41}
{'loss': 1.1203, 'grad_norm': 3.4001190059535125, 'learning_rate': 9.974111111111112e-06, 'epoch': 0.4104}
{'loss': 1.1001, 'grad_norm': 3.5540648114112954, 'learning_rate': 9.973000000000001e-06, 'epoch': 0.4108}
{'loss': 1.1093, 'grad_norm': 2.86880170197947, 'learning_rate': 9.97188888888889e-06, 'epoch': 0.4112}
{'loss': 1.1263, 'grad_norm': 3.775897170185081, 'learning_rate': 9.970777777777778e-06, 'epoch': 0.4116}
{'loss': 1.1135, 'grad_norm': 3.2901993098513955, 'learning_rate': 9.969666666666667e-06, 'epoch': 0.412}
{'eval_valid_loss': 1.05859375, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.031, 'eval_valid_steps_per_second': 278.008, 'epoch': 0.412}
{'loss': 1.1224, 'grad_norm': 3.7228444983488034, 'learning_rate': 9.968555555555557e-06, 'epoch': 0.4124}
{'loss': 1.1136, 'grad_norm': 3.9121364643989236, 'learning_rate': 9.967444444444444e-06, 'epoch': 0.4128}
{'loss': 1.1163, 'grad_norm': 3.6616555301457554, 'learning_rate': 9.966333333333335e-06, 'epoch': 0.4132}
{'loss': 1.0986, 'grad_norm': 3.7423517195122327, 'learning_rate': 9.965222222222224e-06, 'epoch': 0.4136}
{'loss': 1.1107, 'grad_norm': 3.628904706053308, 'learning_rate': 9.964111111111112e-06, 'epoch': 0.414}
{'loss': 1.124, 'grad_norm': 3.216575387950944, 'learning_rate': 9.963000000000001e-06, 'epoch': 0.4144}
{'loss': 1.106, 'grad_norm': 3.473176786288238, 'learning_rate': 9.96188888888889e-06, 'epoch': 0.4148}
{'loss': 1.112, 'grad_norm': 3.2479005690039715, 'learning_rate': 9.960777777777778e-06, 'epoch': 0.4152}
{'loss': 1.119, 'grad_norm': 3.8624248500414295, 'learning_rate': 9.959666666666667e-06, 'epoch': 0.4156}
{'loss': 1.1119, 'grad_norm': 3.294144887614967, 'learning_rate': 9.958555555555558e-06, 'epoch': 0.416}
{'eval_valid_loss': 1.0595703125, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1120.291, 'eval_valid_steps_per_second': 280.073, 'epoch': 0.416}
{'loss': 1.1161, 'grad_norm': 3.2580053697756077, 'learning_rate': 9.957444444444445e-06, 'epoch': 0.4164}
{'loss': 1.1163, 'grad_norm': 3.7621780938998057, 'learning_rate': 9.956333333333335e-06, 'epoch': 0.4168}
{'loss': 1.0994, 'grad_norm': 3.2999568474000003, 'learning_rate': 9.955222222222222e-06, 'epoch': 0.4172}
{'loss': 1.1044, 'grad_norm': 3.8387092319113676, 'learning_rate': 9.954111111111112e-06, 'epoch': 0.4176}
{'loss': 1.1029, 'grad_norm': 3.727705406373011, 'learning_rate': 9.953000000000001e-06, 'epoch': 0.418}
{'loss': 1.1165, 'grad_norm': 3.343774733050845, 'learning_rate': 9.95188888888889e-06, 'epoch': 0.4184}
{'loss': 1.1157, 'grad_norm': 3.1296749528013463, 'learning_rate': 9.950777777777779e-06, 'epoch': 0.4188}
{'loss': 1.1018, 'grad_norm': 3.8288380328602485, 'learning_rate': 9.949666666666667e-06, 'epoch': 0.4192}
{'loss': 1.1224, 'grad_norm': 3.2846404134434666, 'learning_rate': 9.948555555555558e-06, 'epoch': 0.4196}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 1.127, 'grad_norm': 3.4817607977453107, 'learning_rate': 9.947444444444445e-06, 'epoch': 0.42}
{'eval_valid_loss': 1.0546875, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1111.769, 'eval_valid_steps_per_second': 277.942, 'epoch': 0.42}
{'loss': 1.1165, 'grad_norm': 3.2544734684842447, 'learning_rate': 9.946333333333335e-06, 'epoch': 0.4204}
{'loss': 1.1264, 'grad_norm': 3.7956897863734906, 'learning_rate': 9.945222222222222e-06, 'epoch': 0.4208}
{'loss': 1.1134, 'grad_norm': 3.49888364835761, 'learning_rate': 9.944111111111112e-06, 'epoch': 0.4212}
{'loss': 1.109, 'grad_norm': 3.6793668003810613, 'learning_rate': 9.943000000000001e-06, 'epoch': 0.4216}
{'loss': 1.0995, 'grad_norm': 2.978860313607588, 'learning_rate': 9.94188888888889e-06, 'epoch': 0.422}
{'loss': 1.1029, 'grad_norm': 3.4168602851730854, 'learning_rate': 9.940777777777779e-06, 'epoch': 0.4224}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.0964, 'grad_norm': 3.672492295991362, 'learning_rate': 9.939666666666667e-06, 'epoch': 0.4228}
{'loss': 1.1068, 'grad_norm': 3.54084221088616, 'learning_rate': 9.938555555555556e-06, 'epoch': 0.4232}
{'loss': 1.1129, 'grad_norm': 3.2696584314474064, 'learning_rate': 9.937444444444445e-06, 'epoch': 0.4236}
{'loss': 1.113, 'grad_norm': 3.5753699694778374, 'learning_rate': 9.936333333333335e-06, 'epoch': 0.424}
{'eval_valid_loss': 1.0498046875, 'eval_valid_runtime': 0.0915, 'eval_valid_samples_per_second': 1092.83, 'eval_valid_steps_per_second': 273.208, 'epoch': 0.424}
{'loss': 1.1173, 'grad_norm': 3.5868321468347824, 'learning_rate': 9.935222222222222e-06, 'epoch': 0.4244}
{'loss': 1.1161, 'grad_norm': 3.2705190299004916, 'learning_rate': 9.934111111111113e-06, 'epoch': 0.4248}
{'loss': 1.1122, 'grad_norm': 3.4285159879981757, 'learning_rate': 9.933e-06, 'epoch': 0.4252}
{'loss': 1.1124, 'grad_norm': 3.3854588295683, 'learning_rate': 9.93188888888889e-06, 'epoch': 0.4256}
{'loss': 1.0985, 'grad_norm': 3.4842233411003503, 'learning_rate': 9.930777777777779e-06, 'epoch': 0.426}
{'loss': 1.1083, 'grad_norm': 3.319811255823438, 'learning_rate': 9.929666666666667e-06, 'epoch': 0.4264}
{'loss': 1.1075, 'grad_norm': 3.474867426792747, 'learning_rate': 9.928555555555556e-06, 'epoch': 0.4268}
{'loss': 1.1142, 'grad_norm': 3.4213097074913335, 'learning_rate': 9.927444444444445e-06, 'epoch': 0.4272}
{'loss': 1.0999, 'grad_norm': 3.1712680574202983, 'learning_rate': 9.926333333333335e-06, 'epoch': 0.4276}
{'loss': 1.0925, 'grad_norm': 3.700929159696008, 'learning_rate': 9.925222222222222e-06, 'epoch': 0.428}
{'eval_valid_loss': 1.05078125, 'eval_valid_runtime': 0.096, 'eval_valid_samples_per_second': 1041.954, 'eval_valid_steps_per_second': 260.489, 'epoch': 0.428}
{'loss': 1.1073, 'grad_norm': 3.5438309012703404, 'learning_rate': 9.924111111111113e-06, 'epoch': 0.4284}
{'loss': 1.0965, 'grad_norm': 3.415321885992307, 'learning_rate': 9.923e-06, 'epoch': 0.4288}
{'loss': 1.1173, 'grad_norm': 3.9414625815986937, 'learning_rate': 9.92188888888889e-06, 'epoch': 0.4292}
{'loss': 1.0932, 'grad_norm': 3.7859996962151703, 'learning_rate': 9.920777777777779e-06, 'epoch': 0.4296}
{'loss': 1.1011, 'grad_norm': 3.7422662698821583, 'learning_rate': 9.919666666666668e-06, 'epoch': 0.43}
{'loss': 1.1064, 'grad_norm': 3.093979239399703, 'learning_rate': 9.918555555555556e-06, 'epoch': 0.4304}
{'loss': 1.1135, 'grad_norm': 4.275655167552268, 'learning_rate': 9.917444444444445e-06, 'epoch': 0.4308}
{'loss': 1.1007, 'grad_norm': 3.531175460703892, 'learning_rate': 9.916333333333334e-06, 'epoch': 0.4312}
{'loss': 1.1057, 'grad_norm': 3.5152674175243583, 'learning_rate': 9.915222222222222e-06, 'epoch': 0.4316}
{'loss': 1.113, 'grad_norm': 3.70529960006575, 'learning_rate': 9.914111111111113e-06, 'epoch': 0.432}
{'eval_valid_loss': 1.048828125, 'eval_valid_runtime': 0.1758, 'eval_valid_samples_per_second': 568.705, 'eval_valid_steps_per_second': 142.176, 'epoch': 0.432}
{'loss': 1.1092, 'grad_norm': 3.7612014208794506, 'learning_rate': 9.913e-06, 'epoch': 0.4324}
{'loss': 1.0973, 'grad_norm': 3.821869226795992, 'learning_rate': 9.91188888888889e-06, 'epoch': 0.4328}
{'loss': 1.1026, 'grad_norm': 3.30912382885531, 'learning_rate': 9.910777777777777e-06, 'epoch': 0.4332}
{'loss': 1.0899, 'grad_norm': 3.628408580609259, 'learning_rate': 9.909666666666668e-06, 'epoch': 0.4336}
{'loss': 1.1139, 'grad_norm': 3.4010931841622467, 'learning_rate': 9.908555555555556e-06, 'epoch': 0.434}
{'loss': 1.0989, 'grad_norm': 3.8103148420538315, 'learning_rate': 9.907444444444445e-06, 'epoch': 0.4344}
{'loss': 1.1109, 'grad_norm': 3.2935597261631098, 'learning_rate': 9.906333333333334e-06, 'epoch': 0.4348}
{'loss': 1.099, 'grad_norm': 3.5044511097112765, 'learning_rate': 9.905222222222222e-06, 'epoch': 0.4352}
{'loss': 1.1042, 'grad_norm': 3.620046725537984, 'learning_rate': 9.904111111111113e-06, 'epoch': 0.4356}
{'loss': 1.1083, 'grad_norm': 3.6308156093444692, 'learning_rate': 9.903e-06, 'epoch': 0.436}
{'eval_valid_loss': 1.0419921875, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.451, 'eval_valid_steps_per_second': 279.613, 'epoch': 0.436}
{'loss': 1.1015, 'grad_norm': 3.7253857640077412, 'learning_rate': 9.90188888888889e-06, 'epoch': 0.4364}
{'loss': 1.0948, 'grad_norm': 3.816422039768505, 'learning_rate': 9.900777777777777e-06, 'epoch': 0.4368}
{'loss': 1.1082, 'grad_norm': 3.304683243408348, 'learning_rate': 9.899666666666668e-06, 'epoch': 0.4372}
{'loss': 1.0995, 'grad_norm': 3.001331302172359, 'learning_rate': 9.898555555555556e-06, 'epoch': 0.4376}
{'loss': 1.0856, 'grad_norm': 3.6958515024150724, 'learning_rate': 9.897444444444445e-06, 'epoch': 0.438}
{'loss': 1.1151, 'grad_norm': 3.71358660456293, 'learning_rate': 9.896333333333334e-06, 'epoch': 0.4384}
{'loss': 1.1048, 'grad_norm': 3.9316270717154156, 'learning_rate': 9.895222222222223e-06, 'epoch': 0.4388}
{'loss': 1.1021, 'grad_norm': 3.0587997708170627, 'learning_rate': 9.894111111111111e-06, 'epoch': 0.4392}
{'loss': 1.0945, 'grad_norm': 3.801856513203405, 'learning_rate': 9.893e-06, 'epoch': 0.4396}
{'loss': 1.1094, 'grad_norm': 3.305810543629552, 'learning_rate': 9.89188888888889e-06, 'epoch': 0.44}
{'eval_valid_loss': 1.041015625, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.256, 'eval_valid_steps_per_second': 277.064, 'epoch': 0.44}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.0927, 'grad_norm': 3.5323665760946463, 'learning_rate': 9.890777777777777e-06, 'epoch': 0.4404}
{'loss': 1.1155, 'grad_norm': 3.758475937044536, 'learning_rate': 9.889666666666668e-06, 'epoch': 0.4408}
{'loss': 1.094, 'grad_norm': 3.53918056838453, 'learning_rate': 9.888666666666667e-06, 'epoch': 0.4412}
{'loss': 1.0881, 'grad_norm': 3.3713677782270817, 'learning_rate': 9.887555555555558e-06, 'epoch': 0.4416}
{'loss': 1.0968, 'grad_norm': 3.1462793076203126, 'learning_rate': 9.886444444444445e-06, 'epoch': 0.442}
{'loss': 1.102, 'grad_norm': 3.7010230927771177, 'learning_rate': 9.885333333333335e-06, 'epoch': 0.4424}
{'loss': 1.0909, 'grad_norm': 3.5795371504820794, 'learning_rate': 9.884222222222222e-06, 'epoch': 0.4428}
{'loss': 1.1053, 'grad_norm': 3.562463877310738, 'learning_rate': 9.883111111111113e-06, 'epoch': 0.4432}
{'loss': 1.1056, 'grad_norm': 3.5266471335447624, 'learning_rate': 9.882000000000001e-06, 'epoch': 0.4436}
{'loss': 1.0996, 'grad_norm': 3.590301368278428, 'learning_rate': 9.88088888888889e-06, 'epoch': 0.444}
{'eval_valid_loss': 1.0361328125, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1128.186, 'eval_valid_steps_per_second': 282.047, 'epoch': 0.444}
{'loss': 1.0884, 'grad_norm': 3.1487032207877306, 'learning_rate': 9.879777777777779e-06, 'epoch': 0.4444}
{'loss': 1.0938, 'grad_norm': 3.5290811427373967, 'learning_rate': 9.878666666666667e-06, 'epoch': 0.4448}
{'loss': 1.0915, 'grad_norm': 3.4566037447118645, 'learning_rate': 9.877555555555556e-06, 'epoch': 0.4452}
{'loss': 1.0947, 'grad_norm': 3.104541116996674, 'learning_rate': 9.876444444444445e-06, 'epoch': 0.4456}
{'loss': 1.1034, 'grad_norm': 3.2005635324127972, 'learning_rate': 9.875333333333335e-06, 'epoch': 0.446}
{'loss': 1.0969, 'grad_norm': 3.673294864223486, 'learning_rate': 9.874222222222222e-06, 'epoch': 0.4464}
{'loss': 1.0928, 'grad_norm': 3.3131781369755022, 'learning_rate': 9.873111111111113e-06, 'epoch': 0.4468}
{'loss': 1.0955, 'grad_norm': 3.3466253039680818, 'learning_rate': 9.872e-06, 'epoch': 0.4472}
{'loss': 1.0954, 'grad_norm': 3.4997660865275013, 'learning_rate': 9.87088888888889e-06, 'epoch': 0.4476}
{'loss': 1.0932, 'grad_norm': 3.5857562017216287, 'learning_rate': 9.869777777777779e-06, 'epoch': 0.448}
{'eval_valid_loss': 1.0419921875, 'eval_valid_runtime': 0.089, 'eval_valid_samples_per_second': 1124.038, 'eval_valid_steps_per_second': 281.01, 'epoch': 0.448}
{'loss': 1.1043, 'grad_norm': 3.677029558856601, 'learning_rate': 9.868666666666667e-06, 'epoch': 0.4484}
{'loss': 1.1014, 'grad_norm': 3.44837670940266, 'learning_rate': 9.867555555555556e-06, 'epoch': 0.4488}
{'loss': 1.0861, 'grad_norm': 3.352338607666794, 'learning_rate': 9.866444444444445e-06, 'epoch': 0.4492}
{'loss': 1.0817, 'grad_norm': 3.336658390957773, 'learning_rate': 9.865333333333335e-06, 'epoch': 0.4496}
{'loss': 1.0968, 'grad_norm': 3.6166580834411404, 'learning_rate': 9.864222222222222e-06, 'epoch': 0.45}
{'loss': 1.0902, 'grad_norm': 3.718792770845018, 'learning_rate': 9.863111111111113e-06, 'epoch': 0.4504}
{'loss': 1.1045, 'grad_norm': 3.341742982769828, 'learning_rate': 9.862e-06, 'epoch': 0.4508}
{'loss': 1.0814, 'grad_norm': 3.7908308877403862, 'learning_rate': 9.86088888888889e-06, 'epoch': 0.4512}
{'loss': 1.1063, 'grad_norm': 3.2623079692844854, 'learning_rate': 9.859777777777779e-06, 'epoch': 0.4516}
{'loss': 1.102, 'grad_norm': 3.3895727098029247, 'learning_rate': 9.858666666666668e-06, 'epoch': 0.452}
{'eval_valid_loss': 1.0341796875, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1121.43, 'eval_valid_steps_per_second': 280.357, 'epoch': 0.452}
{'loss': 1.0937, 'grad_norm': 3.0321023568531684, 'learning_rate': 9.857555555555556e-06, 'epoch': 0.4524}
{'loss': 1.1092, 'grad_norm': 3.363082782414452, 'learning_rate': 9.856444444444445e-06, 'epoch': 0.4528}
{'loss': 1.0916, 'grad_norm': 3.35296796127005, 'learning_rate': 9.855333333333334e-06, 'epoch': 0.4532}
{'loss': 1.1025, 'grad_norm': 3.460367099782847, 'learning_rate': 9.854222222222222e-06, 'epoch': 0.4536}
{'loss': 1.0986, 'grad_norm': 3.1027870259241537, 'learning_rate': 9.853111111111113e-06, 'epoch': 0.454}
{'loss': 1.097, 'grad_norm': 3.5164374874942097, 'learning_rate': 9.852e-06, 'epoch': 0.4544}
{'loss': 1.0842, 'grad_norm': 3.46865256060961, 'learning_rate': 9.85088888888889e-06, 'epoch': 0.4548}
{'loss': 1.0909, 'grad_norm': 3.5081793799264136, 'learning_rate': 9.849777777777777e-06, 'epoch': 0.4552}
{'loss': 1.0971, 'grad_norm': 3.2974387767073967, 'learning_rate': 9.848666666666668e-06, 'epoch': 0.4556}
{'loss': 1.0905, 'grad_norm': 3.593930729177767, 'learning_rate': 9.847555555555556e-06, 'epoch': 0.456}
{'eval_valid_loss': 1.0302734375, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.322, 'eval_valid_steps_per_second': 279.331, 'epoch': 0.456}
{'loss': 1.0906, 'grad_norm': 3.1117687883120175, 'learning_rate': 9.846444444444445e-06, 'epoch': 0.4564}
{'loss': 1.0708, 'grad_norm': 3.8126226389811886, 'learning_rate': 9.845333333333334e-06, 'epoch': 0.4568}
{'loss': 1.0931, 'grad_norm': 3.3165092137756678, 'learning_rate': 9.844222222222223e-06, 'epoch': 0.4572}
{'loss': 1.1015, 'grad_norm': 3.262343560410691, 'learning_rate': 9.843111111111113e-06, 'epoch': 0.4576}
{'loss': 1.0764, 'grad_norm': 3.4745554499009206, 'learning_rate': 9.842e-06, 'epoch': 0.458}
{'loss': 1.086, 'grad_norm': 3.328759352355379, 'learning_rate': 9.84088888888889e-06, 'epoch': 0.4584}
{'loss': 1.0896, 'grad_norm': 3.58536249840599, 'learning_rate': 9.839777777777777e-06, 'epoch': 0.4588}
{'loss': 1.0965, 'grad_norm': 3.54915595967873, 'learning_rate': 9.838666666666668e-06, 'epoch': 0.4592}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 1.0855, 'grad_norm': 4.177389336632083, 'learning_rate': 9.837555555555556e-06, 'epoch': 0.4596}
{'loss': 1.0941, 'grad_norm': 4.047123153913733, 'learning_rate': 9.836444444444445e-06, 'epoch': 0.46}
{'eval_valid_loss': 1.0341796875, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1114.084, 'eval_valid_steps_per_second': 278.521, 'epoch': 0.46}
{'loss': 1.089, 'grad_norm': 3.4542437851184475, 'learning_rate': 9.835333333333334e-06, 'epoch': 0.4604}
{'loss': 1.0872, 'grad_norm': 3.60579912980228, 'learning_rate': 9.834222222222223e-06, 'epoch': 0.4608}
{'loss': 1.0841, 'grad_norm': 3.3536019777922994, 'learning_rate': 9.833111111111111e-06, 'epoch': 0.4612}
{'loss': 1.0929, 'grad_norm': 3.494401984890961, 'learning_rate': 9.832e-06, 'epoch': 0.4616}
{'loss': 1.0732, 'grad_norm': 3.3036912809819508, 'learning_rate': 9.83088888888889e-06, 'epoch': 0.462}
{'loss': 1.0747, 'grad_norm': 3.7401012028424456, 'learning_rate': 9.829777777777777e-06, 'epoch': 0.4624}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.0704, 'grad_norm': 3.5239066257830967, 'learning_rate': 9.828666666666668e-06, 'epoch': 0.4628}
{'loss': 1.0802, 'grad_norm': 3.39808104827112, 'learning_rate': 9.827555555555557e-06, 'epoch': 0.4632}
{'loss': 1.0845, 'grad_norm': 3.5396690690941623, 'learning_rate': 9.826444444444445e-06, 'epoch': 0.4636}
{'loss': 1.0902, 'grad_norm': 3.6014815133147406, 'learning_rate': 9.825333333333334e-06, 'epoch': 0.464}
{'eval_valid_loss': 1.029296875, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.374, 'eval_valid_steps_per_second': 279.843, 'epoch': 0.464}
{'loss': 1.0776, 'grad_norm': 3.4395647870231323, 'learning_rate': 9.824222222222223e-06, 'epoch': 0.4644}
{'loss': 1.0782, 'grad_norm': 3.7581268226673674, 'learning_rate': 9.823111111111111e-06, 'epoch': 0.4648}
{'loss': 1.0878, 'grad_norm': 3.6436645769342957, 'learning_rate': 9.822e-06, 'epoch': 0.4652}
{'loss': 1.09, 'grad_norm': 3.5282264610412977, 'learning_rate': 9.82088888888889e-06, 'epoch': 0.4656}
{'loss': 1.0841, 'grad_norm': 3.171954844555058, 'learning_rate': 9.819777777777778e-06, 'epoch': 0.466}
{'loss': 1.0816, 'grad_norm': 3.86613150927188, 'learning_rate': 9.818666666666668e-06, 'epoch': 0.4664}
{'loss': 1.084, 'grad_norm': 3.5153652519061342, 'learning_rate': 9.817555555555557e-06, 'epoch': 0.4668}
{'loss': 1.086, 'grad_norm': 4.1048353379849365, 'learning_rate': 9.816444444444445e-06, 'epoch': 0.4672}
{'loss': 1.0777, 'grad_norm': 3.182603459710488, 'learning_rate': 9.815333333333334e-06, 'epoch': 0.4676}
{'loss': 1.0857, 'grad_norm': 4.246064425510191, 'learning_rate': 9.814222222222223e-06, 'epoch': 0.468}
{'eval_valid_loss': 1.0244140625, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1115.325, 'eval_valid_steps_per_second': 278.831, 'epoch': 0.468}
{'loss': 1.0903, 'grad_norm': 4.754310095504371, 'learning_rate': 9.813111111111112e-06, 'epoch': 0.4684}
{'loss': 1.0709, 'grad_norm': 3.33618449920409, 'learning_rate': 9.812e-06, 'epoch': 0.4688}
{'loss': 1.0943, 'grad_norm': 3.7197262420213497, 'learning_rate': 9.810888888888889e-06, 'epoch': 0.4692}
{'loss': 1.0716, 'grad_norm': 3.6625469546514213, 'learning_rate': 9.809777777777778e-06, 'epoch': 0.4696}
{'loss': 1.0777, 'grad_norm': 4.191376727765467, 'learning_rate': 9.808666666666668e-06, 'epoch': 0.47}
{'loss': 1.0833, 'grad_norm': 3.550563507621966, 'learning_rate': 9.807555555555557e-06, 'epoch': 0.4704}
{'loss': 1.0875, 'grad_norm': 3.0304902392867294, 'learning_rate': 9.806444444444445e-06, 'epoch': 0.4708}
{'loss': 1.0556, 'grad_norm': 3.2244657395533665, 'learning_rate': 9.805333333333334e-06, 'epoch': 0.4712}
{'loss': 1.0845, 'grad_norm': 3.845766499986672, 'learning_rate': 9.804222222222223e-06, 'epoch': 0.4716}
{'loss': 1.068, 'grad_norm': 3.629745993961888, 'learning_rate': 9.803111111111112e-06, 'epoch': 0.472}
{'eval_valid_loss': 1.0244140625, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1111.477, 'eval_valid_steps_per_second': 277.869, 'epoch': 0.472}
{'loss': 1.0901, 'grad_norm': 3.6372738014101595, 'learning_rate': 9.802e-06, 'epoch': 0.4724}
{'loss': 1.0744, 'grad_norm': 3.6853121474033066, 'learning_rate': 9.800888888888889e-06, 'epoch': 0.4728}
{'loss': 1.0926, 'grad_norm': 4.100991083940526, 'learning_rate': 9.799777777777778e-06, 'epoch': 0.4732}
{'loss': 1.0727, 'grad_norm': 3.153732617814541, 'learning_rate': 9.798666666666668e-06, 'epoch': 0.4736}
{'loss': 1.0816, 'grad_norm': 4.405933869296325, 'learning_rate': 9.797555555555557e-06, 'epoch': 0.474}
{'loss': 1.0784, 'grad_norm': 3.1184690417712875, 'learning_rate': 9.796444444444446e-06, 'epoch': 0.4744}
{'loss': 1.0749, 'grad_norm': 3.3223171061945616, 'learning_rate': 9.795333333333334e-06, 'epoch': 0.4748}
{'loss': 1.0753, 'grad_norm': 3.4012236300634013, 'learning_rate': 9.794222222222223e-06, 'epoch': 0.4752}
{'loss': 1.0888, 'grad_norm': 3.6935091356469423, 'learning_rate': 9.793111111111112e-06, 'epoch': 0.4756}
{'loss': 1.0715, 'grad_norm': 3.3484134418657496, 'learning_rate': 9.792e-06, 'epoch': 0.476}
{'eval_valid_loss': 1.0263671875, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.23, 'eval_valid_steps_per_second': 278.307, 'epoch': 0.476}
{'loss': 1.0749, 'grad_norm': 3.4589685992199404, 'learning_rate': 9.790888888888889e-06, 'epoch': 0.4764}
{'loss': 1.0708, 'grad_norm': 3.4301328608858372, 'learning_rate': 9.789777777777778e-06, 'epoch': 0.4768}
{'loss': 1.0778, 'grad_norm': 3.478095550275336, 'learning_rate': 9.788666666666668e-06, 'epoch': 0.4772}
{'loss': 1.0834, 'grad_norm': 3.700956538637063, 'learning_rate': 9.787555555555557e-06, 'epoch': 0.4776}
{'loss': 1.0688, 'grad_norm': 4.3845070675612305, 'learning_rate': 9.786444444444446e-06, 'epoch': 0.478}
{'loss': 1.09, 'grad_norm': 3.2948588041319002, 'learning_rate': 9.785333333333334e-06, 'epoch': 0.4784}
{'loss': 1.0766, 'grad_norm': 4.013221035996288, 'learning_rate': 9.784222222222223e-06, 'epoch': 0.4788}
{'loss': 1.0794, 'grad_norm': 3.3320397469701795, 'learning_rate': 9.783111111111112e-06, 'epoch': 0.4792}
{'loss': 1.0738, 'grad_norm': 3.354819264909099, 'learning_rate': 9.782e-06, 'epoch': 0.4796}
{'loss': 1.0738, 'grad_norm': 3.452531660032853, 'learning_rate': 9.78088888888889e-06, 'epoch': 0.48}
{'eval_valid_loss': 1.025390625, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1111.077, 'eval_valid_steps_per_second': 277.769, 'epoch': 0.48}
{'loss': 1.0664, 'grad_norm': 3.521831493227428, 'learning_rate': 9.779777777777778e-06, 'epoch': 0.4804}
{'loss': 1.0778, 'grad_norm': 3.471974446463049, 'learning_rate': 9.778666666666667e-06, 'epoch': 0.4808}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.0791, 'grad_norm': 3.836975625336281, 'learning_rate': 9.777666666666668e-06, 'epoch': 0.4812}
{'loss': 1.0723, 'grad_norm': 3.522598598107442, 'learning_rate': 9.776555555555557e-06, 'epoch': 0.4816}
{'loss': 1.0675, 'grad_norm': 3.3945672458017593, 'learning_rate': 9.775444444444445e-06, 'epoch': 0.482}
{'loss': 1.0776, 'grad_norm': 4.036544014626372, 'learning_rate': 9.774333333333334e-06, 'epoch': 0.4824}
{'loss': 1.0705, 'grad_norm': 4.042085928980816, 'learning_rate': 9.773222222222223e-06, 'epoch': 0.4828}
{'loss': 1.086, 'grad_norm': 3.742317938074346, 'learning_rate': 9.772111111111111e-06, 'epoch': 0.4832}
{'loss': 1.0658, 'grad_norm': 3.062189251840532, 'learning_rate': 9.771e-06, 'epoch': 0.4836}
{'loss': 1.0666, 'grad_norm': 3.4193943385848278, 'learning_rate': 9.76988888888889e-06, 'epoch': 0.484}
{'eval_valid_loss': 1.0224609375, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.784, 'eval_valid_steps_per_second': 278.196, 'epoch': 0.484}
{'loss': 1.0695, 'grad_norm': 3.6640565868077823, 'learning_rate': 9.768777777777778e-06, 'epoch': 0.4844}
{'loss': 1.0871, 'grad_norm': 3.2642366387502157, 'learning_rate': 9.767666666666668e-06, 'epoch': 0.4848}
{'loss': 1.0809, 'grad_norm': 3.8992725617003723, 'learning_rate': 9.766555555555557e-06, 'epoch': 0.4852}
{'loss': 1.0633, 'grad_norm': 3.3416248591622817, 'learning_rate': 9.765444444444445e-06, 'epoch': 0.4856}
{'loss': 1.069, 'grad_norm': 3.9828849100479826, 'learning_rate': 9.764333333333334e-06, 'epoch': 0.486}
{'loss': 1.0536, 'grad_norm': 3.6806824831199263, 'learning_rate': 9.763222222222223e-06, 'epoch': 0.4864}
{'loss': 1.0694, 'grad_norm': 3.2263825299408384, 'learning_rate': 9.762111111111112e-06, 'epoch': 0.4868}
{'loss': 1.0711, 'grad_norm': 3.5449864993759714, 'learning_rate': 9.761e-06, 'epoch': 0.4872}
{'loss': 1.0792, 'grad_norm': 4.205637127919312, 'learning_rate': 9.75988888888889e-06, 'epoch': 0.4876}
{'loss': 1.0628, 'grad_norm': 3.53651697936985, 'learning_rate': 9.758777777777778e-06, 'epoch': 0.488}
{'eval_valid_loss': 1.017578125, 'eval_valid_runtime': 0.1927, 'eval_valid_samples_per_second': 519.062, 'eval_valid_steps_per_second': 129.766, 'epoch': 0.488}
{'loss': 1.063, 'grad_norm': 3.3662059264869235, 'learning_rate': 9.757666666666668e-06, 'epoch': 0.4884}
{'loss': 1.0721, 'grad_norm': 3.6326138759991533, 'learning_rate': 9.756555555555557e-06, 'epoch': 0.4888}
{'loss': 1.0677, 'grad_norm': 3.4311288572511596, 'learning_rate': 9.755444444444445e-06, 'epoch': 0.4892}
{'loss': 1.0797, 'grad_norm': 3.377374167198356, 'learning_rate': 9.754333333333334e-06, 'epoch': 0.4896}
{'loss': 1.0614, 'grad_norm': 3.652640231741503, 'learning_rate': 9.753222222222223e-06, 'epoch': 0.49}
{'loss': 1.0707, 'grad_norm': 3.5972263025426776, 'learning_rate': 9.752111111111112e-06, 'epoch': 0.4904}
{'loss': 1.0902, 'grad_norm': 3.9737799601521915, 'learning_rate': 9.751e-06, 'epoch': 0.4908}
{'loss': 1.0624, 'grad_norm': 3.859358258539252, 'learning_rate': 9.749888888888889e-06, 'epoch': 0.4912}
{'loss': 1.0824, 'grad_norm': 3.4082097269338596, 'learning_rate': 9.748777777777778e-06, 'epoch': 0.4916}
{'loss': 1.0668, 'grad_norm': 3.306894129292414, 'learning_rate': 9.747666666666668e-06, 'epoch': 0.492}
{'eval_valid_loss': 1.017578125, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.032, 'eval_valid_steps_per_second': 279.008, 'epoch': 0.492}
{'loss': 1.0739, 'grad_norm': 3.8008776159508018, 'learning_rate': 9.746555555555557e-06, 'epoch': 0.4924}
{'loss': 1.08, 'grad_norm': 3.750502934108777, 'learning_rate': 9.745444444444446e-06, 'epoch': 0.4928}
{'loss': 1.0673, 'grad_norm': 3.4000295658789517, 'learning_rate': 9.744333333333334e-06, 'epoch': 0.4932}
{'loss': 1.0655, 'grad_norm': 3.472313116261449, 'learning_rate': 9.743222222222223e-06, 'epoch': 0.4936}
{'loss': 1.0646, 'grad_norm': 3.4908321319159716, 'learning_rate': 9.742111111111112e-06, 'epoch': 0.494}
{'loss': 1.0704, 'grad_norm': 3.826194704697456, 'learning_rate': 9.741e-06, 'epoch': 0.4944}
{'loss': 1.0601, 'grad_norm': 3.6680235447309943, 'learning_rate': 9.739888888888889e-06, 'epoch': 0.4948}
{'loss': 1.0654, 'grad_norm': 3.6325800503431696, 'learning_rate': 9.738777777777778e-06, 'epoch': 0.4952}
{'loss': 1.0662, 'grad_norm': 3.6040931631689217, 'learning_rate': 9.737666666666668e-06, 'epoch': 0.4956}
{'loss': 1.063, 'grad_norm': 3.660217390496319, 'learning_rate': 9.736555555555557e-06, 'epoch': 0.496}
{'eval_valid_loss': 1.0146484375, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.642, 'eval_valid_steps_per_second': 279.661, 'epoch': 0.496}
{'loss': 1.0745, 'grad_norm': 3.4905430380300815, 'learning_rate': 9.735444444444446e-06, 'epoch': 0.4964}
{'loss': 1.0755, 'grad_norm': 3.424615015543272, 'learning_rate': 9.734333333333334e-06, 'epoch': 0.4968}
{'loss': 1.0693, 'grad_norm': 3.903871834667946, 'learning_rate': 9.733222222222223e-06, 'epoch': 0.4972}
{'loss': 1.0819, 'grad_norm': 3.391697753456047, 'learning_rate': 9.732111111111112e-06, 'epoch': 0.4976}
{'loss': 1.0714, 'grad_norm': 4.228789372205893, 'learning_rate': 9.731e-06, 'epoch': 0.498}
{'loss': 1.0626, 'grad_norm': 3.8380438931754632, 'learning_rate': 9.72988888888889e-06, 'epoch': 0.4984}
{'loss': 1.0685, 'grad_norm': 3.3343585742377826, 'learning_rate': 9.728777777777778e-06, 'epoch': 0.4988}
{'loss': 1.0795, 'grad_norm': 3.672431643884043, 'learning_rate': 9.727666666666667e-06, 'epoch': 0.4992}
{'loss': 1.0743, 'grad_norm': 3.1122360916167056, 'learning_rate': 9.726555555555557e-06, 'epoch': 0.4996}
{'loss': 1.0573, 'grad_norm': 3.5068628431757727, 'learning_rate': 9.725444444444446e-06, 'epoch': 0.5}
{'eval_valid_loss': 1.01171875, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1120.235, 'eval_valid_steps_per_second': 280.059, 'epoch': 0.5}
{'loss': 1.0746, 'grad_norm': 3.3280334997359455, 'learning_rate': 9.724333333333334e-06, 'epoch': 0.5004}
{'loss': 1.0564, 'grad_norm': 3.305249231812616, 'learning_rate': 9.723222222222223e-06, 'epoch': 0.5008}
{'loss': 1.0607, 'grad_norm': 3.6149712976395216, 'learning_rate': 9.722111111111112e-06, 'epoch': 0.5012}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.081, 'grad_norm': 3.552404555099637, 'learning_rate': 9.721e-06, 'epoch': 0.5016}
{'loss': 1.0598, 'grad_norm': 3.6309614244128072, 'learning_rate': 9.71988888888889e-06, 'epoch': 0.502}
{'loss': 1.0542, 'grad_norm': 3.831313343053417, 'learning_rate': 9.718777777777778e-06, 'epoch': 0.5024}
{'loss': 1.0674, 'grad_norm': 3.4739924757546006, 'learning_rate': 9.717666666666667e-06, 'epoch': 0.5028}
{'loss': 1.0686, 'grad_norm': 4.185847112769786, 'learning_rate': 9.716555555555557e-06, 'epoch': 0.5032}
{'loss': 1.0654, 'grad_norm': 3.922917626384686, 'learning_rate': 9.715444444444446e-06, 'epoch': 0.5036}
{'loss': 1.0697, 'grad_norm': 3.6262322173131296, 'learning_rate': 9.714333333333335e-06, 'epoch': 0.504}
{'eval_valid_loss': 1.015625, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1125.613, 'eval_valid_steps_per_second': 281.403, 'epoch': 0.504}
{'loss': 1.0543, 'grad_norm': 3.300383585614104, 'learning_rate': 9.713222222222223e-06, 'epoch': 0.5044}
{'loss': 1.0623, 'grad_norm': 3.6131432254307176, 'learning_rate': 9.712111111111112e-06, 'epoch': 0.5048}
{'loss': 1.0759, 'grad_norm': 3.7241732137435712, 'learning_rate': 9.711e-06, 'epoch': 0.5052}
{'loss': 1.056, 'grad_norm': 3.764087727273319, 'learning_rate': 9.70988888888889e-06, 'epoch': 0.5056}
{'loss': 1.075, 'grad_norm': 3.5508537662768407, 'learning_rate': 9.708777777777778e-06, 'epoch': 0.506}
{'loss': 1.0634, 'grad_norm': 3.6411305187094074, 'learning_rate': 9.707666666666667e-06, 'epoch': 0.5064}
{'loss': 1.0675, 'grad_norm': 3.42800072833335, 'learning_rate': 9.706555555555557e-06, 'epoch': 0.5068}
{'loss': 1.062, 'grad_norm': 3.731259694558078, 'learning_rate': 9.705444444444446e-06, 'epoch': 0.5072}
{'loss': 1.0713, 'grad_norm': 3.4281266903590555, 'learning_rate': 9.704333333333335e-06, 'epoch': 0.5076}
{'loss': 1.0547, 'grad_norm': 3.313897791631407, 'learning_rate': 9.703222222222223e-06, 'epoch': 0.508}
{'eval_valid_loss': 1.0087890625, 'eval_valid_runtime': 0.0882, 'eval_valid_samples_per_second': 1133.89, 'eval_valid_steps_per_second': 283.472, 'epoch': 0.508}
{'loss': 1.0608, 'grad_norm': 3.582353051227151, 'learning_rate': 9.702111111111112e-06, 'epoch': 0.5084}
{'loss': 1.0587, 'grad_norm': 3.7006852204948015, 'learning_rate': 9.701e-06, 'epoch': 0.5088}
{'loss': 1.0481, 'grad_norm': 3.5768887578928603, 'learning_rate': 9.69988888888889e-06, 'epoch': 0.5092}
{'loss': 1.0433, 'grad_norm': 4.227870308704062, 'learning_rate': 9.698777777777778e-06, 'epoch': 0.5096}
{'loss': 1.0573, 'grad_norm': 3.4111288722092232, 'learning_rate': 9.697666666666667e-06, 'epoch': 0.51}
{'loss': 1.0636, 'grad_norm': 3.906781007914264, 'learning_rate': 9.696555555555557e-06, 'epoch': 0.5104}
{'loss': 1.0658, 'grad_norm': 3.697627594444066, 'learning_rate': 9.695444444444444e-06, 'epoch': 0.5108}
{'loss': 1.0544, 'grad_norm': 3.850930167824791, 'learning_rate': 9.694333333333335e-06, 'epoch': 0.5112}
{'loss': 1.0574, 'grad_norm': 4.0166786236538865, 'learning_rate': 9.693222222222223e-06, 'epoch': 0.5116}
{'loss': 1.0708, 'grad_norm': 3.6407826598251334, 'learning_rate': 9.692111111111112e-06, 'epoch': 0.512}
{'eval_valid_loss': 1.009765625, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1128.56, 'eval_valid_steps_per_second': 282.14, 'epoch': 0.512}
{'loss': 1.0574, 'grad_norm': 3.7927646994582185, 'learning_rate': 9.691000000000001e-06, 'epoch': 0.5124}
{'loss': 1.0815, 'grad_norm': 3.381410039333161, 'learning_rate': 9.68988888888889e-06, 'epoch': 0.5128}
{'loss': 1.0586, 'grad_norm': 3.319972615516855, 'learning_rate': 9.688777777777778e-06, 'epoch': 0.5132}
{'loss': 1.0772, 'grad_norm': 3.2827520474764578, 'learning_rate': 9.687666666666667e-06, 'epoch': 0.5136}
{'loss': 1.0678, 'grad_norm': 3.4324313595042626, 'learning_rate': 9.686555555555557e-06, 'epoch': 0.514}
{'loss': 1.0626, 'grad_norm': 3.4847172025958013, 'learning_rate': 9.685444444444444e-06, 'epoch': 0.5144}
{'loss': 1.0496, 'grad_norm': 3.477707192208572, 'learning_rate': 9.684333333333335e-06, 'epoch': 0.5148}
{'loss': 1.0723, 'grad_norm': 3.47878314472272, 'learning_rate': 9.683222222222224e-06, 'epoch': 0.5152}
{'loss': 1.0579, 'grad_norm': 3.5487752689000747, 'learning_rate': 9.682111111111112e-06, 'epoch': 0.5156}
{'loss': 1.0522, 'grad_norm': 4.395257793130807, 'learning_rate': 9.681000000000001e-06, 'epoch': 0.516}
{'eval_valid_loss': 1.01171875, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.759, 'eval_valid_steps_per_second': 281.94, 'epoch': 0.516}
{'loss': 1.0536, 'grad_norm': 3.6377035051291973, 'learning_rate': 9.67988888888889e-06, 'epoch': 0.5164}
{'loss': 1.0484, 'grad_norm': 3.332830013581449, 'learning_rate': 9.678777777777778e-06, 'epoch': 0.5168}
{'loss': 1.0626, 'grad_norm': 3.456569300245578, 'learning_rate': 9.677666666666667e-06, 'epoch': 0.5172}
{'loss': 1.0629, 'grad_norm': 3.411322089622031, 'learning_rate': 9.676555555555558e-06, 'epoch': 0.5176}
{'loss': 1.0625, 'grad_norm': 3.445338701075016, 'learning_rate': 9.675444444444445e-06, 'epoch': 0.518}
{'loss': 1.0574, 'grad_norm': 4.042685169834116, 'learning_rate': 9.674333333333335e-06, 'epoch': 0.5184}
{'loss': 1.0572, 'grad_norm': 3.4649228126769054, 'learning_rate': 9.673222222222222e-06, 'epoch': 0.5188}
{'loss': 1.0595, 'grad_norm': 3.551855136816469, 'learning_rate': 9.672111111111112e-06, 'epoch': 0.5192}
{'loss': 1.054, 'grad_norm': 3.5757741160005927, 'learning_rate': 9.671000000000001e-06, 'epoch': 0.5196}
{'loss': 1.0624, 'grad_norm': 3.2851004397338297, 'learning_rate': 9.66988888888889e-06, 'epoch': 0.52}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'eval_valid_loss': 1.0048828125, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1124.774, 'eval_valid_steps_per_second': 281.193, 'epoch': 0.52}
{'loss': 1.0482, 'grad_norm': 3.4019119566816824, 'learning_rate': 9.668777777777779e-06, 'epoch': 0.5204}
{'loss': 1.0502, 'grad_norm': 3.340177775851344, 'learning_rate': 9.667666666666667e-06, 'epoch': 0.5208}
{'loss': 1.0708, 'grad_norm': 3.406157938561815, 'learning_rate': 9.666666666666667e-06, 'epoch': 0.5212}
{'loss': 1.0523, 'grad_norm': 3.5024729234954446, 'learning_rate': 9.665555555555555e-06, 'epoch': 0.5216}
{'loss': 1.0706, 'grad_norm': 3.6838284010192615, 'learning_rate': 9.664444444444446e-06, 'epoch': 0.522}
{'loss': 1.0444, 'grad_norm': 3.5563260755798893, 'learning_rate': 9.663333333333335e-06, 'epoch': 0.5224}
{'loss': 1.0507, 'grad_norm': 3.4336057365477326, 'learning_rate': 9.662222222222223e-06, 'epoch': 0.5228}
{'loss': 1.0545, 'grad_norm': 3.6590832304833576, 'learning_rate': 9.661111111111112e-06, 'epoch': 0.5232}
{'loss': 1.0614, 'grad_norm': 3.902650945140341, 'learning_rate': 9.66e-06, 'epoch': 0.5236}
{'loss': 1.0484, 'grad_norm': 3.3397198302720046, 'learning_rate': 9.65888888888889e-06, 'epoch': 0.524}
{'eval_valid_loss': 1.0048828125, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.943, 'eval_valid_steps_per_second': 278.736, 'epoch': 0.524}
{'loss': 1.0641, 'grad_norm': 3.501208709723919, 'learning_rate': 9.657777777777778e-06, 'epoch': 0.5244}
{'loss': 1.0547, 'grad_norm': 3.7185548683069083, 'learning_rate': 9.656666666666667e-06, 'epoch': 0.5248}
{'loss': 1.0687, 'grad_norm': 3.4630092335896516, 'learning_rate': 9.655555555555556e-06, 'epoch': 0.5252}
{'loss': 1.0796, 'grad_norm': 3.920922912293242, 'learning_rate': 9.654444444444446e-06, 'epoch': 0.5256}
{'loss': 1.0458, 'grad_norm': 3.9537652744862894, 'learning_rate': 9.653333333333335e-06, 'epoch': 0.526}
{'loss': 1.0595, 'grad_norm': 3.3157757091156843, 'learning_rate': 9.652222222222223e-06, 'epoch': 0.5264}
{'loss': 1.0638, 'grad_norm': 3.6988566462079575, 'learning_rate': 9.651111111111112e-06, 'epoch': 0.5268}
{'loss': 1.0698, 'grad_norm': 3.272102657366141, 'learning_rate': 9.65e-06, 'epoch': 0.5272}
{'loss': 1.0546, 'grad_norm': 3.6501501176612035, 'learning_rate': 9.64888888888889e-06, 'epoch': 0.5276}
{'loss': 1.0579, 'grad_norm': 3.5857354981948943, 'learning_rate': 9.647777777777778e-06, 'epoch': 0.528}
{'eval_valid_loss': 1.001953125, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.684, 'eval_valid_steps_per_second': 279.671, 'epoch': 0.528}
{'loss': 1.0637, 'grad_norm': 3.5752244878858566, 'learning_rate': 9.646666666666667e-06, 'epoch': 0.5284}
{'loss': 1.0519, 'grad_norm': 3.518564546620502, 'learning_rate': 9.645555555555557e-06, 'epoch': 0.5288}
{'loss': 1.0378, 'grad_norm': 3.5703927079087387, 'learning_rate': 9.644444444444444e-06, 'epoch': 0.5292}
{'loss': 1.0619, 'grad_norm': 4.059480131577802, 'learning_rate': 9.643333333333335e-06, 'epoch': 0.5296}
{'loss': 1.0631, 'grad_norm': 3.4460920286725605, 'learning_rate': 9.642222222222223e-06, 'epoch': 0.53}
{'loss': 1.0525, 'grad_norm': 3.628684276283909, 'learning_rate': 9.641111111111112e-06, 'epoch': 0.5304}
{'loss': 1.0492, 'grad_norm': 3.3518928418386147, 'learning_rate': 9.640000000000001e-06, 'epoch': 0.5308}
{'loss': 1.0641, 'grad_norm': 3.4267365136168157, 'learning_rate': 9.63888888888889e-06, 'epoch': 0.5312}
{'loss': 1.0534, 'grad_norm': 3.8336299940393466, 'learning_rate': 9.637777777777778e-06, 'epoch': 0.5316}
{'loss': 1.0735, 'grad_norm': 3.155043078519893, 'learning_rate': 9.636666666666667e-06, 'epoch': 0.532}
{'eval_valid_loss': 1.0, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.164, 'eval_valid_steps_per_second': 278.041, 'epoch': 0.532}
{'loss': 1.0455, 'grad_norm': 3.440739154567698, 'learning_rate': 9.635555555555557e-06, 'epoch': 0.5324}
{'loss': 1.0511, 'grad_norm': 3.7594388745494984, 'learning_rate': 9.634444444444444e-06, 'epoch': 0.5328}
{'loss': 1.0607, 'grad_norm': 3.8119758730336994, 'learning_rate': 9.633333333333335e-06, 'epoch': 0.5332}
{'loss': 1.056, 'grad_norm': 3.805753805205104, 'learning_rate': 9.632222222222224e-06, 'epoch': 0.5336}
{'loss': 1.0514, 'grad_norm': 3.553435572569253, 'learning_rate': 9.631111111111112e-06, 'epoch': 0.534}
{'loss': 1.0695, 'grad_norm': 3.8707105445001515, 'learning_rate': 9.630000000000001e-06, 'epoch': 0.5344}
{'loss': 1.0591, 'grad_norm': 3.2529509791900195, 'learning_rate': 9.62888888888889e-06, 'epoch': 0.5348}
{'loss': 1.0526, 'grad_norm': 3.5844672436472593, 'learning_rate': 9.627777777777778e-06, 'epoch': 0.5352}
{'loss': 1.0647, 'grad_norm': 3.780299358825977, 'learning_rate': 9.626666666666667e-06, 'epoch': 0.5356}
{'loss': 1.0551, 'grad_norm': 3.6173316399545943, 'learning_rate': 9.625555555555558e-06, 'epoch': 0.536}
{'eval_valid_loss': 1.0009765625, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1121.436, 'eval_valid_steps_per_second': 280.359, 'epoch': 0.536}
{'loss': 1.0567, 'grad_norm': 3.764257333035296, 'learning_rate': 9.624444444444445e-06, 'epoch': 0.5364}
{'loss': 1.0486, 'grad_norm': 3.1720221818873533, 'learning_rate': 9.623333333333335e-06, 'epoch': 0.5368}
{'loss': 1.04, 'grad_norm': 3.355475162593234, 'learning_rate': 9.622222222222222e-06, 'epoch': 0.5372}
{'loss': 1.0474, 'grad_norm': 3.4889412113604092, 'learning_rate': 9.621111111111112e-06, 'epoch': 0.5376}
{'loss': 1.0512, 'grad_norm': 3.7758771066872248, 'learning_rate': 9.620000000000001e-06, 'epoch': 0.538}
{'loss': 1.0433, 'grad_norm': 3.6136657015436726, 'learning_rate': 9.61888888888889e-06, 'epoch': 0.5384}
{'loss': 1.0478, 'grad_norm': 3.791007559294574, 'learning_rate': 9.617777777777778e-06, 'epoch': 0.5388}
{'loss': 1.0571, 'grad_norm': 3.2748747244745635, 'learning_rate': 9.616666666666667e-06, 'epoch': 0.5392}
{'loss': 1.0492, 'grad_norm': 3.660354958985509, 'learning_rate': 9.615555555555558e-06, 'epoch': 0.5396}
{'loss': 1.0622, 'grad_norm': 3.5282465222009844, 'learning_rate': 9.614444444444445e-06, 'epoch': 0.54}
{'eval_valid_loss': 0.9970703125, 'eval_valid_runtime': 0.0903, 'eval_valid_samples_per_second': 1107.922, 'eval_valid_steps_per_second': 276.98, 'epoch': 0.54}
{'loss': 1.0646, 'grad_norm': 3.9207757419946856, 'learning_rate': 9.613333333333335e-06, 'epoch': 0.5404}
{'loss': 1.0478, 'grad_norm': 3.6524763929790884, 'learning_rate': 9.612222222222222e-06, 'epoch': 0.5408}
{'loss': 1.0515, 'grad_norm': 3.433033468907256, 'learning_rate': 9.611111111111112e-06, 'epoch': 0.5412}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.0486, 'grad_norm': 3.377846753485563, 'learning_rate': 9.610000000000001e-06, 'epoch': 0.5416}
{'loss': 1.0534, 'grad_norm': 3.4398264035236545, 'learning_rate': 9.60888888888889e-06, 'epoch': 0.542}
{'loss': 1.0569, 'grad_norm': 3.903458643168791, 'learning_rate': 9.607777777777779e-06, 'epoch': 0.5424}
{'loss': 1.0507, 'grad_norm': 3.9267629582111194, 'learning_rate': 9.606666666666667e-06, 'epoch': 0.5428}
{'loss': 1.0591, 'grad_norm': 3.7465888721448852, 'learning_rate': 9.605555555555556e-06, 'epoch': 0.5432}
{'loss': 1.0591, 'grad_norm': 3.7018279206301044, 'learning_rate': 9.604444444444445e-06, 'epoch': 0.5436}
{'loss': 1.0451, 'grad_norm': 3.5823898884688057, 'learning_rate': 9.603333333333335e-06, 'epoch': 0.544}
{'eval_valid_loss': 0.99853515625, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1121.421, 'eval_valid_steps_per_second': 280.355, 'epoch': 0.544}
{'loss': 1.0489, 'grad_norm': 3.7319074642549865, 'learning_rate': 9.602222222222222e-06, 'epoch': 0.5444}
{'loss': 1.0318, 'grad_norm': 3.482146512717819, 'learning_rate': 9.601111111111113e-06, 'epoch': 0.5448}
{'loss': 1.04, 'grad_norm': 3.5597726353626844, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.5452}
{'loss': 1.052, 'grad_norm': 3.559002405815351, 'learning_rate': 9.59888888888889e-06, 'epoch': 0.5456}
{'loss': 1.033, 'grad_norm': 3.5563014799391013, 'learning_rate': 9.597777777777779e-06, 'epoch': 0.546}
{'loss': 1.0457, 'grad_norm': 3.5208834168673504, 'learning_rate': 9.596666666666667e-06, 'epoch': 0.5464}
{'loss': 1.0571, 'grad_norm': 3.564654694174901, 'learning_rate': 9.595555555555556e-06, 'epoch': 0.5468}
{'loss': 1.0534, 'grad_norm': 3.2954328465270524, 'learning_rate': 9.594444444444445e-06, 'epoch': 0.5472}
{'loss': 1.0436, 'grad_norm': 3.40607038951811, 'learning_rate': 9.593333333333335e-06, 'epoch': 0.5476}
{'loss': 1.0525, 'grad_norm': 3.7166502177440406, 'learning_rate': 9.592222222222222e-06, 'epoch': 0.548}
{'eval_valid_loss': 0.99365234375, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1116.923, 'eval_valid_steps_per_second': 279.231, 'epoch': 0.548}
{'loss': 1.0407, 'grad_norm': 3.6164001365901677, 'learning_rate': 9.591111111111113e-06, 'epoch': 0.5484}
{'loss': 1.0463, 'grad_norm': 3.403926196684495, 'learning_rate': 9.59e-06, 'epoch': 0.5488}
{'loss': 1.0581, 'grad_norm': 3.7578428588402217, 'learning_rate': 9.58888888888889e-06, 'epoch': 0.5492}
{'loss': 1.044, 'grad_norm': 3.5296387752228084, 'learning_rate': 9.587777777777779e-06, 'epoch': 0.5496}
{'loss': 1.0489, 'grad_norm': 3.756141751168832, 'learning_rate': 9.586666666666667e-06, 'epoch': 0.55}
{'loss': 1.063, 'grad_norm': 3.7042120935395784, 'learning_rate': 9.585555555555556e-06, 'epoch': 0.5504}
{'loss': 1.03, 'grad_norm': 4.5312002771214495, 'learning_rate': 9.584444444444445e-06, 'epoch': 0.5508}
{'loss': 1.0474, 'grad_norm': 3.4544726456351302, 'learning_rate': 9.583333333333335e-06, 'epoch': 0.5512}
{'loss': 1.0458, 'grad_norm': 3.2250449465791085, 'learning_rate': 9.582222222222222e-06, 'epoch': 0.5516}
{'loss': 1.0522, 'grad_norm': 3.603828759789915, 'learning_rate': 9.581111111111113e-06, 'epoch': 0.552}
{'eval_valid_loss': 0.99267578125, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.632, 'eval_valid_steps_per_second': 278.408, 'epoch': 0.552}
{'loss': 1.054, 'grad_norm': 3.7609522306932726, 'learning_rate': 9.58e-06, 'epoch': 0.5524}
{'loss': 1.069, 'grad_norm': 3.5989004711057855, 'learning_rate': 9.57888888888889e-06, 'epoch': 0.5528}
{'loss': 1.0326, 'grad_norm': 3.757725498516764, 'learning_rate': 9.577777777777779e-06, 'epoch': 0.5532}
{'loss': 1.0554, 'grad_norm': 3.557727436092498, 'learning_rate': 9.576666666666668e-06, 'epoch': 0.5536}
{'loss': 1.0514, 'grad_norm': 4.333940214367913, 'learning_rate': 9.575555555555556e-06, 'epoch': 0.554}
{'loss': 1.0405, 'grad_norm': 3.6787349010524495, 'learning_rate': 9.574444444444445e-06, 'epoch': 0.5544}
{'loss': 1.0444, 'grad_norm': 3.3625551297231167, 'learning_rate': 9.573333333333334e-06, 'epoch': 0.5548}
{'loss': 1.0525, 'grad_norm': 3.9200757219338493, 'learning_rate': 9.572222222222222e-06, 'epoch': 0.5552}
{'loss': 1.0589, 'grad_norm': 3.920879693657525, 'learning_rate': 9.571111111111113e-06, 'epoch': 0.5556}
{'loss': 1.041, 'grad_norm': 3.847287155495482, 'learning_rate': 9.57e-06, 'epoch': 0.556}
{'eval_valid_loss': 0.99560546875, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.59, 'eval_valid_steps_per_second': 278.648, 'epoch': 0.556}
{'loss': 1.05, 'grad_norm': 3.87718578956894, 'learning_rate': 9.56888888888889e-06, 'epoch': 0.5564}
{'loss': 1.0447, 'grad_norm': 3.5399036123736605, 'learning_rate': 9.567777777777777e-06, 'epoch': 0.5568}
{'loss': 1.0452, 'grad_norm': 3.5157889179586737, 'learning_rate': 9.566666666666668e-06, 'epoch': 0.5572}
{'loss': 1.0603, 'grad_norm': 3.8100715391571494, 'learning_rate': 9.565555555555556e-06, 'epoch': 0.5576}
{'loss': 1.0409, 'grad_norm': 3.5637412502164008, 'learning_rate': 9.564444444444445e-06, 'epoch': 0.558}
{'loss': 1.0338, 'grad_norm': 3.4503345952666646, 'learning_rate': 9.563333333333334e-06, 'epoch': 0.5584}
{'loss': 1.0407, 'grad_norm': 4.504659862612157, 'learning_rate': 9.562222222222223e-06, 'epoch': 0.5588}
{'loss': 1.0548, 'grad_norm': 3.3810346922658643, 'learning_rate': 9.561111111111113e-06, 'epoch': 0.5592}
{'loss': 1.0457, 'grad_norm': 3.797510918004487, 'learning_rate': 9.56e-06, 'epoch': 0.5596}
{'loss': 1.0623, 'grad_norm': 3.4749133624110207, 'learning_rate': 9.55888888888889e-06, 'epoch': 0.56}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'eval_valid_loss': 0.9931640625, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1111.021, 'eval_valid_steps_per_second': 277.755, 'epoch': 0.56}
{'loss': 1.0493, 'grad_norm': 3.750917537019213, 'learning_rate': 9.557777777777777e-06, 'epoch': 0.5604}
{'loss': 1.0338, 'grad_norm': 3.238695152713454, 'learning_rate': 9.556666666666668e-06, 'epoch': 0.5608}
{'loss': 1.0502, 'grad_norm': 3.6538952474732285, 'learning_rate': 9.555555555555556e-06, 'epoch': 0.5612}
{'loss': 1.0321, 'grad_norm': 3.5888326837866265, 'learning_rate': 9.554555555555556e-06, 'epoch': 0.5616}
{'loss': 1.0514, 'grad_norm': 3.6379138527579924, 'learning_rate': 9.553444444444445e-06, 'epoch': 0.562}
{'loss': 1.0516, 'grad_norm': 3.4051772449451643, 'learning_rate': 9.552333333333335e-06, 'epoch': 0.5624}
{'loss': 1.0556, 'grad_norm': 3.24903568303352, 'learning_rate': 9.551222222222222e-06, 'epoch': 0.5628}
{'loss': 1.034, 'grad_norm': 3.2453287097597716, 'learning_rate': 9.550111111111113e-06, 'epoch': 0.5632}
{'loss': 1.0455, 'grad_norm': 3.4382589282711318, 'learning_rate': 9.549000000000001e-06, 'epoch': 0.5636}
{'loss': 1.0351, 'grad_norm': 3.516902072802969, 'learning_rate': 9.54788888888889e-06, 'epoch': 0.564}
{'eval_valid_loss': 0.9951171875, 'eval_valid_runtime': 0.0908, 'eval_valid_samples_per_second': 1101.052, 'eval_valid_steps_per_second': 275.263, 'epoch': 0.564}
{'loss': 1.0451, 'grad_norm': 3.5345165706650272, 'learning_rate': 9.546777777777779e-06, 'epoch': 0.5644}
{'loss': 1.0397, 'grad_norm': 3.7911996379139734, 'learning_rate': 9.545666666666667e-06, 'epoch': 0.5648}
{'loss': 1.0439, 'grad_norm': 3.967560672524449, 'learning_rate': 9.544555555555556e-06, 'epoch': 0.5652}
{'loss': 1.0497, 'grad_norm': 3.825656904895539, 'learning_rate': 9.543444444444445e-06, 'epoch': 0.5656}
{'loss': 1.0525, 'grad_norm': 3.603510265811564, 'learning_rate': 9.542333333333335e-06, 'epoch': 0.566}
{'loss': 1.0215, 'grad_norm': 3.1357170396222314, 'learning_rate': 9.541222222222222e-06, 'epoch': 0.5664}
{'loss': 1.0379, 'grad_norm': 3.248778342842208, 'learning_rate': 9.540111111111113e-06, 'epoch': 0.5668}
{'loss': 1.0445, 'grad_norm': 3.6585277651616517, 'learning_rate': 9.539e-06, 'epoch': 0.5672}
{'loss': 1.0381, 'grad_norm': 3.3495473783666796, 'learning_rate': 9.53788888888889e-06, 'epoch': 0.5676}
{'loss': 1.0414, 'grad_norm': 3.5830434203230523, 'learning_rate': 9.536777777777779e-06, 'epoch': 0.568}
{'eval_valid_loss': 0.99267578125, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1110.247, 'eval_valid_steps_per_second': 277.562, 'epoch': 0.568}
{'loss': 1.0488, 'grad_norm': 3.229271691162872, 'learning_rate': 9.535666666666667e-06, 'epoch': 0.5684}
{'loss': 1.0597, 'grad_norm': 3.5786739744478, 'learning_rate': 9.534555555555556e-06, 'epoch': 0.5688}
{'loss': 1.0273, 'grad_norm': 3.2454294197425204, 'learning_rate': 9.533444444444445e-06, 'epoch': 0.5692}
{'loss': 1.0204, 'grad_norm': 3.6543854254672885, 'learning_rate': 9.532333333333335e-06, 'epoch': 0.5696}
{'loss': 1.0437, 'grad_norm': 3.8015236987020367, 'learning_rate': 9.531222222222222e-06, 'epoch': 0.57}
{'loss': 1.0531, 'grad_norm': 3.5578837518673163, 'learning_rate': 9.530111111111113e-06, 'epoch': 0.5704}
{'loss': 1.038, 'grad_norm': 3.509756301587288, 'learning_rate': 9.529e-06, 'epoch': 0.5708}
{'loss': 1.0347, 'grad_norm': 4.076737544204418, 'learning_rate': 9.52788888888889e-06, 'epoch': 0.5712}
{'loss': 1.0562, 'grad_norm': 3.433601856764596, 'learning_rate': 9.526777777777779e-06, 'epoch': 0.5716}
{'loss': 1.0555, 'grad_norm': 3.9797130312810194, 'learning_rate': 9.525666666666668e-06, 'epoch': 0.572}
{'eval_valid_loss': 0.990234375, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.527, 'eval_valid_steps_per_second': 279.382, 'epoch': 0.572}
{'loss': 1.0207, 'grad_norm': 3.6539296180250282, 'learning_rate': 9.524555555555556e-06, 'epoch': 0.5724}
{'loss': 1.0406, 'grad_norm': 3.0862629392900374, 'learning_rate': 9.523444444444445e-06, 'epoch': 0.5728}
{'loss': 1.0364, 'grad_norm': 4.022671984095204, 'learning_rate': 9.522333333333334e-06, 'epoch': 0.5732}
{'loss': 1.0314, 'grad_norm': 3.4265632563293154, 'learning_rate': 9.521222222222222e-06, 'epoch': 0.5736}
{'loss': 1.0397, 'grad_norm': 3.510252607268134, 'learning_rate': 9.520111111111113e-06, 'epoch': 0.574}
{'loss': 1.0239, 'grad_norm': 3.5494862073836413, 'learning_rate': 9.519e-06, 'epoch': 0.5744}
{'loss': 1.0345, 'grad_norm': 3.8097402089324386, 'learning_rate': 9.51788888888889e-06, 'epoch': 0.5748}
{'loss': 1.0384, 'grad_norm': 3.8484642768204007, 'learning_rate': 9.516777777777779e-06, 'epoch': 0.5752}
{'loss': 1.0268, 'grad_norm': 3.699797692760732, 'learning_rate': 9.515666666666668e-06, 'epoch': 0.5756}
{'loss': 1.0354, 'grad_norm': 3.4160985832981035, 'learning_rate': 9.514555555555556e-06, 'epoch': 0.576}
{'eval_valid_loss': 0.98876953125, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.047, 'eval_valid_steps_per_second': 279.012, 'epoch': 0.576}
{'loss': 1.0365, 'grad_norm': 3.5290895283963755, 'learning_rate': 9.513444444444445e-06, 'epoch': 0.5764}
{'loss': 1.0427, 'grad_norm': 3.6247071526066414, 'learning_rate': 9.512333333333334e-06, 'epoch': 0.5768}
{'loss': 1.0476, 'grad_norm': 3.3026518447643745, 'learning_rate': 9.511222222222223e-06, 'epoch': 0.5772}
{'loss': 1.0338, 'grad_norm': 4.107586721757413, 'learning_rate': 9.510111111111113e-06, 'epoch': 0.5776}
{'loss': 1.0257, 'grad_norm': 3.7396052615798787, 'learning_rate': 9.509e-06, 'epoch': 0.578}
{'loss': 1.0403, 'grad_norm': 3.327011692292582, 'learning_rate': 9.50788888888889e-06, 'epoch': 0.5784}
{'loss': 1.0425, 'grad_norm': 3.4654963243294663, 'learning_rate': 9.506777777777777e-06, 'epoch': 0.5788}
{'loss': 1.0236, 'grad_norm': 3.427437654786595, 'learning_rate': 9.505666666666668e-06, 'epoch': 0.5792}
{'loss': 1.0582, 'grad_norm': 3.8524403606578637, 'learning_rate': 9.504555555555556e-06, 'epoch': 0.5796}
{'loss': 1.0496, 'grad_norm': 3.590979196261401, 'learning_rate': 9.503444444444445e-06, 'epoch': 0.58}
{'eval_valid_loss': 0.9833984375, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.646, 'eval_valid_steps_per_second': 278.911, 'epoch': 0.58}
{'loss': 1.0334, 'grad_norm': 3.4239639977730434, 'learning_rate': 9.502333333333334e-06, 'epoch': 0.5804}
{'loss': 1.0394, 'grad_norm': 3.346513426240382, 'learning_rate': 9.501222222222223e-06, 'epoch': 0.5808}
{'loss': 1.0467, 'grad_norm': 3.7117192619450243, 'learning_rate': 9.500111111111113e-06, 'epoch': 0.5812}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.0374, 'grad_norm': 3.321456021131014, 'learning_rate': 9.499e-06, 'epoch': 0.5816}
{'loss': 1.0453, 'grad_norm': 3.6013036374219247, 'learning_rate': 9.49788888888889e-06, 'epoch': 0.582}
{'loss': 1.0509, 'grad_norm': 3.495731415966074, 'learning_rate': 9.496777777777777e-06, 'epoch': 0.5824}
{'loss': 1.0235, 'grad_norm': 3.4938402809120452, 'learning_rate': 9.495666666666668e-06, 'epoch': 0.5828}
{'loss': 1.0297, 'grad_norm': 3.6183479457980705, 'learning_rate': 9.494555555555557e-06, 'epoch': 0.5832}
{'loss': 1.0317, 'grad_norm': 3.425429636049808, 'learning_rate': 9.493444444444445e-06, 'epoch': 0.5836}
{'loss': 1.0331, 'grad_norm': 3.1817951106343525, 'learning_rate': 9.492333333333334e-06, 'epoch': 0.584}
{'eval_valid_loss': 0.98828125, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.303, 'eval_valid_steps_per_second': 278.576, 'epoch': 0.584}
{'loss': 1.0528, 'grad_norm': 3.8991952897323188, 'learning_rate': 9.491222222222223e-06, 'epoch': 0.5844}
{'loss': 1.0287, 'grad_norm': 3.6934930463513447, 'learning_rate': 9.490111111111111e-06, 'epoch': 0.5848}
{'loss': 1.0588, 'grad_norm': 3.76779583924359, 'learning_rate': 9.489e-06, 'epoch': 0.5852}
{'loss': 1.0417, 'grad_norm': 3.5796559152995537, 'learning_rate': 9.48788888888889e-06, 'epoch': 0.5856}
{'loss': 1.0345, 'grad_norm': 3.304936329737615, 'learning_rate': 9.486777777777778e-06, 'epoch': 0.586}
{'loss': 1.036, 'grad_norm': 3.487068170385448, 'learning_rate': 9.485666666666668e-06, 'epoch': 0.5864}
{'loss': 1.045, 'grad_norm': 3.8848893043772446, 'learning_rate': 9.484555555555557e-06, 'epoch': 0.5868}
{'loss': 1.0532, 'grad_norm': 3.8982478460686236, 'learning_rate': 9.483444444444445e-06, 'epoch': 0.5872}
{'loss': 1.0315, 'grad_norm': 3.0415150471788097, 'learning_rate': 9.482333333333334e-06, 'epoch': 0.5876}
{'loss': 1.0206, 'grad_norm': 4.084553617323729, 'learning_rate': 9.481222222222223e-06, 'epoch': 0.588}
{'eval_valid_loss': 0.9853515625, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.086, 'eval_valid_steps_per_second': 277.021, 'epoch': 0.588}
{'loss': 1.0467, 'grad_norm': 3.5353125184648477, 'learning_rate': 9.480111111111112e-06, 'epoch': 0.5884}
{'loss': 1.0349, 'grad_norm': 3.3159379841943726, 'learning_rate': 9.479e-06, 'epoch': 0.5888}
{'loss': 1.0412, 'grad_norm': 3.26826221809038, 'learning_rate': 9.47788888888889e-06, 'epoch': 0.5892}
{'loss': 1.0229, 'grad_norm': 3.8566434205117646, 'learning_rate': 9.476777777777778e-06, 'epoch': 0.5896}
{'loss': 1.0178, 'grad_norm': 3.6201492937926654, 'learning_rate': 9.475666666666668e-06, 'epoch': 0.59}
{'loss': 1.0418, 'grad_norm': 3.6412380340882424, 'learning_rate': 9.474555555555557e-06, 'epoch': 0.5904}
{'loss': 1.0398, 'grad_norm': 3.8422954412301586, 'learning_rate': 9.473444444444445e-06, 'epoch': 0.5908}
{'loss': 1.0219, 'grad_norm': 3.712337157662935, 'learning_rate': 9.472333333333334e-06, 'epoch': 0.5912}
{'loss': 1.0355, 'grad_norm': 4.409991589860401, 'learning_rate': 9.471222222222223e-06, 'epoch': 0.5916}
{'loss': 1.0266, 'grad_norm': 3.299058010657984, 'learning_rate': 9.470111111111112e-06, 'epoch': 0.592}
{'eval_valid_loss': 0.982421875, 'eval_valid_runtime': 0.0906, 'eval_valid_samples_per_second': 1103.982, 'eval_valid_steps_per_second': 275.996, 'epoch': 0.592}
{'loss': 1.0385, 'grad_norm': 3.2368881579019533, 'learning_rate': 9.469e-06, 'epoch': 0.5924}
{'loss': 1.048, 'grad_norm': 4.0876010695628056, 'learning_rate': 9.467888888888889e-06, 'epoch': 0.5928}
{'loss': 1.0396, 'grad_norm': 3.6166004668533507, 'learning_rate': 9.466777777777778e-06, 'epoch': 0.5932}
{'loss': 1.0216, 'grad_norm': 4.012783425561999, 'learning_rate': 9.465666666666668e-06, 'epoch': 0.5936}
{'loss': 1.0361, 'grad_norm': 3.6801879040413517, 'learning_rate': 9.464555555555557e-06, 'epoch': 0.594}
{'loss': 1.0473, 'grad_norm': 3.7399557064615827, 'learning_rate': 9.463444444444446e-06, 'epoch': 0.5944}
{'loss': 1.035, 'grad_norm': 3.183065705300441, 'learning_rate': 9.462333333333334e-06, 'epoch': 0.5948}
{'loss': 1.0329, 'grad_norm': 3.661754995874354, 'learning_rate': 9.461222222222223e-06, 'epoch': 0.5952}
{'loss': 1.0462, 'grad_norm': 3.9858814906989424, 'learning_rate': 9.460111111111112e-06, 'epoch': 0.5956}
{'loss': 1.0482, 'grad_norm': 3.8666088942961037, 'learning_rate': 9.459e-06, 'epoch': 0.596}
{'eval_valid_loss': 0.98193359375, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.379, 'eval_valid_steps_per_second': 277.095, 'epoch': 0.596}
{'loss': 1.0297, 'grad_norm': 3.3571879253797308, 'learning_rate': 9.457888888888889e-06, 'epoch': 0.5964}
{'loss': 1.0273, 'grad_norm': 3.801763503675221, 'learning_rate': 9.456777777777778e-06, 'epoch': 0.5968}
{'loss': 1.0274, 'grad_norm': 3.3470517996029905, 'learning_rate': 9.455666666666668e-06, 'epoch': 0.5972}
{'loss': 1.0284, 'grad_norm': 3.5206459727272943, 'learning_rate': 9.454555555555557e-06, 'epoch': 0.5976}
{'loss': 1.0388, 'grad_norm': 3.942662850658899, 'learning_rate': 9.453444444444446e-06, 'epoch': 0.598}
{'loss': 1.0343, 'grad_norm': 3.4134340471646447, 'learning_rate': 9.452333333333334e-06, 'epoch': 0.5984}
{'loss': 1.0345, 'grad_norm': 3.712709987365949, 'learning_rate': 9.451222222222223e-06, 'epoch': 0.5988}
{'loss': 1.045, 'grad_norm': 3.4656995299213524, 'learning_rate': 9.450111111111112e-06, 'epoch': 0.5992}
{'loss': 1.025, 'grad_norm': 3.682404699393752, 'learning_rate': 9.449e-06, 'epoch': 0.5996}
{'loss': 1.0464, 'grad_norm': 3.660307515702021, 'learning_rate': 9.44788888888889e-06, 'epoch': 0.6}
{'eval_valid_loss': 0.98486328125, 'eval_valid_runtime': 0.1782, 'eval_valid_samples_per_second': 561.062, 'eval_valid_steps_per_second': 140.266, 'epoch': 0.6}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.0312, 'grad_norm': 3.614869736676355, 'learning_rate': 9.446777777777778e-06, 'epoch': 0.6004}
{'loss': 1.0339, 'grad_norm': 3.919906000353226, 'learning_rate': 9.445666666666667e-06, 'epoch': 0.6008}
{'loss': 1.0304, 'grad_norm': 3.6349524153280073, 'learning_rate': 9.444555555555557e-06, 'epoch': 0.6012}
{'loss': 1.0371, 'grad_norm': 3.51348035034878, 'learning_rate': 9.443555555555557e-06, 'epoch': 0.6016}
{'loss': 1.04, 'grad_norm': 3.4276179367242507, 'learning_rate': 9.442444444444445e-06, 'epoch': 0.602}
{'loss': 1.0306, 'grad_norm': 3.4934415763014273, 'learning_rate': 9.441333333333334e-06, 'epoch': 0.6024}
{'loss': 1.0283, 'grad_norm': 3.5369090756050388, 'learning_rate': 9.440222222222223e-06, 'epoch': 0.6028}
{'loss': 1.0168, 'grad_norm': 3.694164985091566, 'learning_rate': 9.439111111111111e-06, 'epoch': 0.6032}
{'loss': 1.0162, 'grad_norm': 3.7838940006827646, 'learning_rate': 9.438e-06, 'epoch': 0.6036}
{'loss': 1.0232, 'grad_norm': 3.9115948541066135, 'learning_rate': 9.43688888888889e-06, 'epoch': 0.604}
{'eval_valid_loss': 0.98291015625, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.874, 'eval_valid_steps_per_second': 278.469, 'epoch': 0.604}
{'loss': 1.0307, 'grad_norm': 3.506532992198111, 'learning_rate': 9.435777777777778e-06, 'epoch': 0.6044}
{'loss': 1.0302, 'grad_norm': 3.435859678947543, 'learning_rate': 9.434666666666668e-06, 'epoch': 0.6048}
{'loss': 1.0349, 'grad_norm': 3.514115505403925, 'learning_rate': 9.433555555555557e-06, 'epoch': 0.6052}
{'loss': 1.029, 'grad_norm': 3.56484834335068, 'learning_rate': 9.432444444444445e-06, 'epoch': 0.6056}
{'loss': 1.0263, 'grad_norm': 3.582000582531488, 'learning_rate': 9.431333333333334e-06, 'epoch': 0.606}
{'loss': 1.0312, 'grad_norm': 3.5206771323111035, 'learning_rate': 9.430222222222223e-06, 'epoch': 0.6064}
{'loss': 1.0352, 'grad_norm': 3.1805700815878755, 'learning_rate': 9.429111111111111e-06, 'epoch': 0.6068}
{'loss': 1.0105, 'grad_norm': 3.4282127198755954, 'learning_rate': 9.428e-06, 'epoch': 0.6072}
{'loss': 1.0322, 'grad_norm': 4.145064180358132, 'learning_rate': 9.42688888888889e-06, 'epoch': 0.6076}
{'loss': 1.027, 'grad_norm': 3.23202252182809, 'learning_rate': 9.425777777777778e-06, 'epoch': 0.608}
{'eval_valid_loss': 0.97802734375, 'eval_valid_runtime': 0.0907, 'eval_valid_samples_per_second': 1102.662, 'eval_valid_steps_per_second': 275.665, 'epoch': 0.608}
{'loss': 1.0218, 'grad_norm': 3.193335483473317, 'learning_rate': 9.424666666666668e-06, 'epoch': 0.6084}
{'loss': 1.0149, 'grad_norm': 3.645242423354013, 'learning_rate': 9.423555555555555e-06, 'epoch': 0.6088}
{'loss': 1.0375, 'grad_norm': 3.1871364984621717, 'learning_rate': 9.422444444444445e-06, 'epoch': 0.6092}
{'loss': 1.0205, 'grad_norm': 4.061306425966555, 'learning_rate': 9.421333333333334e-06, 'epoch': 0.6096}
{'loss': 1.0135, 'grad_norm': 3.5554071557265616, 'learning_rate': 9.420222222222223e-06, 'epoch': 0.61}
{'loss': 1.0336, 'grad_norm': 3.608758782963095, 'learning_rate': 9.419111111111112e-06, 'epoch': 0.6104}
{'loss': 1.025, 'grad_norm': 3.638453806200309, 'learning_rate': 9.418e-06, 'epoch': 0.6108}
{'loss': 1.0338, 'grad_norm': 3.5494312703890945, 'learning_rate': 9.41688888888889e-06, 'epoch': 0.6112}
{'loss': 1.0231, 'grad_norm': 3.7170793322763385, 'learning_rate': 9.415777777777778e-06, 'epoch': 0.6116}
{'loss': 1.0304, 'grad_norm': 3.5735604102009852, 'learning_rate': 9.414666666666668e-06, 'epoch': 0.612}
{'eval_valid_loss': 0.9775390625, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1106.738, 'eval_valid_steps_per_second': 276.684, 'epoch': 0.612}
{'loss': 1.0362, 'grad_norm': 4.075769891735861, 'learning_rate': 9.413555555555555e-06, 'epoch': 0.6124}
{'loss': 1.0367, 'grad_norm': 3.460809020677998, 'learning_rate': 9.412444444444446e-06, 'epoch': 0.6128}
{'loss': 1.0294, 'grad_norm': 3.477636561410204, 'learning_rate': 9.411333333333334e-06, 'epoch': 0.6132}
{'loss': 1.034, 'grad_norm': 3.0950175588199063, 'learning_rate': 9.410222222222223e-06, 'epoch': 0.6136}
{'loss': 1.0281, 'grad_norm': 3.483740573482948, 'learning_rate': 9.409111111111112e-06, 'epoch': 0.614}
{'loss': 1.0114, 'grad_norm': 3.1821956068525883, 'learning_rate': 9.408e-06, 'epoch': 0.6144}
{'loss': 1.0235, 'grad_norm': 3.524362886627535, 'learning_rate': 9.406888888888889e-06, 'epoch': 0.6148}
{'loss': 1.0188, 'grad_norm': 4.283743654195751, 'learning_rate': 9.405777777777778e-06, 'epoch': 0.6152}
{'loss': 1.0158, 'grad_norm': 3.6295628769721833, 'learning_rate': 9.404666666666668e-06, 'epoch': 0.6156}
{'loss': 1.0386, 'grad_norm': 3.721606103454429, 'learning_rate': 9.403555555555555e-06, 'epoch': 0.616}
{'eval_valid_loss': 0.97705078125, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1114.019, 'eval_valid_steps_per_second': 278.505, 'epoch': 0.616}
{'loss': 1.0228, 'grad_norm': 3.559555242059147, 'learning_rate': 9.402444444444446e-06, 'epoch': 0.6164}
{'loss': 1.0198, 'grad_norm': 3.363492925808021, 'learning_rate': 9.401333333333334e-06, 'epoch': 0.6168}
{'loss': 1.0282, 'grad_norm': 3.6391306171782043, 'learning_rate': 9.400222222222223e-06, 'epoch': 0.6172}
{'loss': 1.0339, 'grad_norm': 3.5473722201797218, 'learning_rate': 9.399111111111112e-06, 'epoch': 0.6176}
{'loss': 1.0377, 'grad_norm': 3.5913758604180033, 'learning_rate': 9.398e-06, 'epoch': 0.618}
{'loss': 1.0232, 'grad_norm': 4.108428701076659, 'learning_rate': 9.39688888888889e-06, 'epoch': 0.6184}
{'loss': 1.03, 'grad_norm': 3.7260841686429362, 'learning_rate': 9.395777777777778e-06, 'epoch': 0.6188}
{'loss': 1.0252, 'grad_norm': 3.336281528645506, 'learning_rate': 9.394666666666668e-06, 'epoch': 0.6192}
{'loss': 1.0421, 'grad_norm': 4.031124231313528, 'learning_rate': 9.393555555555557e-06, 'epoch': 0.6196}
{'loss': 1.0327, 'grad_norm': 3.7378629890924784, 'learning_rate': 9.392444444444446e-06, 'epoch': 0.62}
{'eval_valid_loss': 0.9736328125, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1128.256, 'eval_valid_steps_per_second': 282.064, 'epoch': 0.62}
{'loss': 1.0282, 'grad_norm': 3.2718569142043052, 'learning_rate': 9.391333333333334e-06, 'epoch': 0.6204}
{'loss': 1.0275, 'grad_norm': 3.3199893390243056, 'learning_rate': 9.390222222222223e-06, 'epoch': 0.6208}
{'loss': 1.0296, 'grad_norm': 3.3984245387739995, 'learning_rate': 9.389111111111112e-06, 'epoch': 0.6212}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.0237, 'grad_norm': 3.5998127756177447, 'learning_rate': 9.388e-06, 'epoch': 0.6216}
{'loss': 1.0152, 'grad_norm': 3.8862636433899005, 'learning_rate': 9.38688888888889e-06, 'epoch': 0.622}
{'loss': 1.0205, 'grad_norm': 3.5523254511512685, 'learning_rate': 9.385777777777778e-06, 'epoch': 0.6224}
{'loss': 1.0287, 'grad_norm': 3.8812122232561337, 'learning_rate': 9.384666666666667e-06, 'epoch': 0.6228}
{'loss': 1.0313, 'grad_norm': 3.170990280263988, 'learning_rate': 9.383555555555557e-06, 'epoch': 0.6232}
{'loss': 1.0133, 'grad_norm': 3.72523580483239, 'learning_rate': 9.382444444444446e-06, 'epoch': 0.6236}
{'loss': 1.0166, 'grad_norm': 3.5637502985927068, 'learning_rate': 9.381333333333335e-06, 'epoch': 0.624}
{'eval_valid_loss': 0.97412109375, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.803, 'eval_valid_steps_per_second': 278.951, 'epoch': 0.624}
{'loss': 1.039, 'grad_norm': 3.4040096772428403, 'learning_rate': 9.380222222222223e-06, 'epoch': 0.6244}
{'loss': 1.0272, 'grad_norm': 4.054954137836754, 'learning_rate': 9.379111111111112e-06, 'epoch': 0.6248}
{'loss': 1.0314, 'grad_norm': 3.76538674008377, 'learning_rate': 9.378e-06, 'epoch': 0.6252}
{'loss': 1.0141, 'grad_norm': 3.4955135695186295, 'learning_rate': 9.37688888888889e-06, 'epoch': 0.6256}
{'loss': 1.0254, 'grad_norm': 4.019089833036056, 'learning_rate': 9.375777777777778e-06, 'epoch': 0.626}
{'loss': 1.0317, 'grad_norm': 3.522635941775783, 'learning_rate': 9.374666666666667e-06, 'epoch': 0.6264}
{'loss': 1.01, 'grad_norm': 3.660553478071775, 'learning_rate': 9.373555555555557e-06, 'epoch': 0.6268}
{'loss': 1.0176, 'grad_norm': 3.8000043523914107, 'learning_rate': 9.372444444444446e-06, 'epoch': 0.6272}
{'loss': 1.0275, 'grad_norm': 3.5520936913586314, 'learning_rate': 9.371333333333335e-06, 'epoch': 0.6276}
{'loss': 1.0145, 'grad_norm': 3.4669780759065936, 'learning_rate': 9.370222222222223e-06, 'epoch': 0.628}
{'eval_valid_loss': 0.97314453125, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.48, 'eval_valid_steps_per_second': 278.87, 'epoch': 0.628}
{'loss': 1.0292, 'grad_norm': 3.418874147661153, 'learning_rate': 9.369111111111112e-06, 'epoch': 0.6284}
{'loss': 1.0329, 'grad_norm': 4.007076099105344, 'learning_rate': 9.368e-06, 'epoch': 0.6288}
{'loss': 1.0432, 'grad_norm': 3.863664367008213, 'learning_rate': 9.36688888888889e-06, 'epoch': 0.6292}
{'loss': 1.0365, 'grad_norm': 3.703367201212183, 'learning_rate': 9.365777777777778e-06, 'epoch': 0.6296}
{'loss': 1.0273, 'grad_norm': 3.522423938866782, 'learning_rate': 9.364666666666667e-06, 'epoch': 0.63}
{'loss': 1.026, 'grad_norm': 3.584686867367437, 'learning_rate': 9.363555555555557e-06, 'epoch': 0.6304}
{'loss': 1.0222, 'grad_norm': 3.304891386122996, 'learning_rate': 9.362444444444444e-06, 'epoch': 0.6308}
{'loss': 1.025, 'grad_norm': 3.5659448548731607, 'learning_rate': 9.361333333333335e-06, 'epoch': 0.6312}
{'loss': 1.0305, 'grad_norm': 3.7526252140652363, 'learning_rate': 9.360222222222223e-06, 'epoch': 0.6316}
{'loss': 1.0143, 'grad_norm': 3.224784314905238, 'learning_rate': 9.359111111111112e-06, 'epoch': 0.632}
{'eval_valid_loss': 0.970703125, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.368, 'eval_valid_steps_per_second': 281.842, 'epoch': 0.632}
{'loss': 1.0183, 'grad_norm': 3.3880806266948436, 'learning_rate': 9.358000000000001e-06, 'epoch': 0.6324}
{'loss': 1.0173, 'grad_norm': 3.5873530965892897, 'learning_rate': 9.35688888888889e-06, 'epoch': 0.6328}
{'loss': 1.0198, 'grad_norm': 3.718824898742891, 'learning_rate': 9.355777777777778e-06, 'epoch': 0.6332}
{'loss': 1.0192, 'grad_norm': 3.7173321128574237, 'learning_rate': 9.354666666666667e-06, 'epoch': 0.6336}
{'loss': 1.021, 'grad_norm': 3.595125880365333, 'learning_rate': 9.353555555555557e-06, 'epoch': 0.634}
{'loss': 1.0106, 'grad_norm': 3.5982380499083773, 'learning_rate': 9.352444444444444e-06, 'epoch': 0.6344}
{'loss': 1.0311, 'grad_norm': 3.822075232362717, 'learning_rate': 9.351333333333335e-06, 'epoch': 0.6348}
{'loss': 1.0156, 'grad_norm': 3.5055080057100905, 'learning_rate': 9.350222222222224e-06, 'epoch': 0.6352}
{'loss': 1.0132, 'grad_norm': 3.508329425607228, 'learning_rate': 9.349111111111112e-06, 'epoch': 0.6356}
{'loss': 1.0297, 'grad_norm': 3.9632336734538196, 'learning_rate': 9.348000000000001e-06, 'epoch': 0.636}
{'eval_valid_loss': 0.97314453125, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1128.193, 'eval_valid_steps_per_second': 282.048, 'epoch': 0.636}
{'loss': 1.01, 'grad_norm': 3.6654170771668655, 'learning_rate': 9.34688888888889e-06, 'epoch': 0.6364}
{'loss': 1.044, 'grad_norm': 3.1834343244267718, 'learning_rate': 9.345777777777778e-06, 'epoch': 0.6368}
{'loss': 1.0365, 'grad_norm': 3.8395018944440666, 'learning_rate': 9.344666666666667e-06, 'epoch': 0.6372}
{'loss': 1.0229, 'grad_norm': 3.6406247134883913, 'learning_rate': 9.343555555555558e-06, 'epoch': 0.6376}
{'loss': 1.0314, 'grad_norm': 3.693722130659019, 'learning_rate': 9.342444444444445e-06, 'epoch': 0.638}
{'loss': 1.0144, 'grad_norm': 3.492234735361454, 'learning_rate': 9.341333333333335e-06, 'epoch': 0.6384}
{'loss': 1.0343, 'grad_norm': 3.746310262618577, 'learning_rate': 9.340222222222222e-06, 'epoch': 0.6388}
{'loss': 1.0237, 'grad_norm': 3.4518130990714613, 'learning_rate': 9.339111111111112e-06, 'epoch': 0.6392}
{'loss': 1.0278, 'grad_norm': 3.553521176665449, 'learning_rate': 9.338000000000001e-06, 'epoch': 0.6396}
{'loss': 1.0363, 'grad_norm': 3.4925834550764603, 'learning_rate': 9.33688888888889e-06, 'epoch': 0.64}
{'eval_valid_loss': 0.97021484375, 'eval_valid_runtime': 0.0908, 'eval_valid_samples_per_second': 1100.775, 'eval_valid_steps_per_second': 275.194, 'epoch': 0.64}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.0223, 'grad_norm': 3.8052045090965323, 'learning_rate': 9.335777777777778e-06, 'epoch': 0.6404}
{'loss': 1.0359, 'grad_norm': 3.8136864598300213, 'learning_rate': 9.334666666666667e-06, 'epoch': 0.6408}
{'loss': 1.0159, 'grad_norm': 3.1131405960148792, 'learning_rate': 9.333555555555558e-06, 'epoch': 0.6412}
{'loss': 1.0242, 'grad_norm': 3.4722057947194065, 'learning_rate': 9.332555555555555e-06, 'epoch': 0.6416}
{'loss': 1.0086, 'grad_norm': 3.699911002429071, 'learning_rate': 9.331444444444446e-06, 'epoch': 0.642}
{'loss': 1.0202, 'grad_norm': 3.1296077137231353, 'learning_rate': 9.330333333333335e-06, 'epoch': 0.6424}
{'loss': 1.0275, 'grad_norm': 3.3857616405296818, 'learning_rate': 9.329222222222223e-06, 'epoch': 0.6428}
{'loss': 1.025, 'grad_norm': 3.0333878656784856, 'learning_rate': 9.328111111111112e-06, 'epoch': 0.6432}
{'loss': 1.0311, 'grad_norm': 3.6455590371989173, 'learning_rate': 9.327e-06, 'epoch': 0.6436}
{'loss': 1.0244, 'grad_norm': 3.656484074721051, 'learning_rate': 9.32588888888889e-06, 'epoch': 0.644}
{'eval_valid_loss': 0.97265625, 'eval_valid_runtime': 0.0903, 'eval_valid_samples_per_second': 1107.852, 'eval_valid_steps_per_second': 276.963, 'epoch': 0.644}
{'loss': 1.0091, 'grad_norm': 3.18500553960034, 'learning_rate': 9.324777777777778e-06, 'epoch': 0.6444}
{'loss': 1.0241, 'grad_norm': 3.5145451391949054, 'learning_rate': 9.323666666666667e-06, 'epoch': 0.6448}
{'loss': 1.0065, 'grad_norm': 3.799148522523558, 'learning_rate': 9.322555555555555e-06, 'epoch': 0.6452}
{'loss': 1.0163, 'grad_norm': 3.0991286714472306, 'learning_rate': 9.321444444444446e-06, 'epoch': 0.6456}
{'loss': 1.008, 'grad_norm': 3.6351037624036904, 'learning_rate': 9.320333333333335e-06, 'epoch': 0.646}
{'loss': 1.0332, 'grad_norm': 3.1229411304695627, 'learning_rate': 9.319222222222223e-06, 'epoch': 0.6464}
{'loss': 1.0229, 'grad_norm': 3.703612074226278, 'learning_rate': 9.318111111111112e-06, 'epoch': 0.6468}
{'loss': 1.0309, 'grad_norm': 3.372794755393996, 'learning_rate': 9.317e-06, 'epoch': 0.6472}
{'loss': 1.0031, 'grad_norm': 3.5456063610698516, 'learning_rate': 9.31588888888889e-06, 'epoch': 0.6476}
{'loss': 1.0211, 'grad_norm': 3.6095166942312362, 'learning_rate': 9.314777777777778e-06, 'epoch': 0.648}
{'eval_valid_loss': 0.96728515625, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.215, 'eval_valid_steps_per_second': 279.804, 'epoch': 0.648}
{'loss': 1.009, 'grad_norm': 3.4752480325755717, 'learning_rate': 9.313666666666667e-06, 'epoch': 0.6484}
{'loss': 1.016, 'grad_norm': 3.5353362738363714, 'learning_rate': 9.312555555555556e-06, 'epoch': 0.6488}
{'loss': 1.0109, 'grad_norm': 3.44657633430789, 'learning_rate': 9.311444444444446e-06, 'epoch': 0.6492}
{'loss': 1.0193, 'grad_norm': 3.4221265691663105, 'learning_rate': 9.310333333333335e-06, 'epoch': 0.6496}
{'loss': 1.0109, 'grad_norm': 3.401544715789299, 'learning_rate': 9.309222222222223e-06, 'epoch': 0.65}
{'loss': 1.0241, 'grad_norm': 3.429935680959677, 'learning_rate': 9.308111111111112e-06, 'epoch': 0.6504}
{'loss': 1.0148, 'grad_norm': 3.59814687525103, 'learning_rate': 9.307e-06, 'epoch': 0.6508}
{'loss': 1.013, 'grad_norm': 3.4235433230460592, 'learning_rate': 9.30588888888889e-06, 'epoch': 0.6512}
{'loss': 1.0205, 'grad_norm': 3.665855465186458, 'learning_rate': 9.304777777777778e-06, 'epoch': 0.6516}
{'loss': 1.025, 'grad_norm': 3.536899082245433, 'learning_rate': 9.303666666666667e-06, 'epoch': 0.652}
{'eval_valid_loss': 0.9716796875, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.845, 'eval_valid_steps_per_second': 278.461, 'epoch': 0.652}
{'loss': 1.0365, 'grad_norm': 3.7595522654051328, 'learning_rate': 9.302555555555556e-06, 'epoch': 0.6524}
{'loss': 1.021, 'grad_norm': 3.5498612809949917, 'learning_rate': 9.301444444444444e-06, 'epoch': 0.6528}
{'loss': 1.0189, 'grad_norm': 3.6096403511567177, 'learning_rate': 9.300333333333335e-06, 'epoch': 0.6532}
{'loss': 1.0174, 'grad_norm': 3.6067338916341902, 'learning_rate': 9.299222222222224e-06, 'epoch': 0.6536}
{'loss': 1.0189, 'grad_norm': 3.442901797300402, 'learning_rate': 9.298111111111112e-06, 'epoch': 0.654}
{'loss': 1.0248, 'grad_norm': 3.4223361902066793, 'learning_rate': 9.297000000000001e-06, 'epoch': 0.6544}
{'loss': 1.0112, 'grad_norm': 3.783453165514835, 'learning_rate': 9.29588888888889e-06, 'epoch': 0.6548}
{'loss': 1.0286, 'grad_norm': 3.578708875901513, 'learning_rate': 9.294777777777778e-06, 'epoch': 0.6552}
{'loss': 1.0315, 'grad_norm': 3.4195577884068706, 'learning_rate': 9.293666666666667e-06, 'epoch': 0.6556}
{'loss': 1.0353, 'grad_norm': 3.478142394359104, 'learning_rate': 9.292555555555556e-06, 'epoch': 0.656}
{'eval_valid_loss': 0.96826171875, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.598, 'eval_valid_steps_per_second': 278.9, 'epoch': 0.656}
{'loss': 1.0211, 'grad_norm': 3.4903012500369535, 'learning_rate': 9.291444444444444e-06, 'epoch': 0.6564}
{'loss': 1.0159, 'grad_norm': 3.6805997313041607, 'learning_rate': 9.290333333333335e-06, 'epoch': 0.6568}
{'loss': 1.0146, 'grad_norm': 3.480174102470232, 'learning_rate': 9.289222222222224e-06, 'epoch': 0.6572}
{'loss': 1.0192, 'grad_norm': 3.609528996538843, 'learning_rate': 9.288111111111112e-06, 'epoch': 0.6576}
{'loss': 1.0218, 'grad_norm': 3.681438766328767, 'learning_rate': 9.287000000000001e-06, 'epoch': 0.658}
{'loss': 1.0248, 'grad_norm': 3.558692821878106, 'learning_rate': 9.28588888888889e-06, 'epoch': 0.6584}
{'loss': 1.0195, 'grad_norm': 3.226956341146989, 'learning_rate': 9.284777777777778e-06, 'epoch': 0.6588}
{'loss': 1.0177, 'grad_norm': 3.7872982519068645, 'learning_rate': 9.283666666666667e-06, 'epoch': 0.6592}
{'loss': 1.0233, 'grad_norm': 3.749141817442268, 'learning_rate': 9.282555555555556e-06, 'epoch': 0.6596}
{'loss': 1.0146, 'grad_norm': 3.6126448447152746, 'learning_rate': 9.281444444444445e-06, 'epoch': 0.66}
{'eval_valid_loss': 0.9638671875, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.246, 'eval_valid_steps_per_second': 279.061, 'epoch': 0.66}
{'loss': 1.0175, 'grad_norm': 3.4014401117262265, 'learning_rate': 9.280333333333335e-06, 'epoch': 0.6604}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.0208, 'grad_norm': 3.72447651634643, 'learning_rate': 9.279222222222222e-06, 'epoch': 0.6608}
{'loss': 1.0152, 'grad_norm': 4.447346267337621, 'learning_rate': 9.278111111111112e-06, 'epoch': 0.6612}
{'loss': 1.0232, 'grad_norm': 3.618962134763209, 'learning_rate': 9.277000000000001e-06, 'epoch': 0.6616}
{'loss': 1.0086, 'grad_norm': 3.744528832405977, 'learning_rate': 9.27588888888889e-06, 'epoch': 0.662}
{'loss': 1.0271, 'grad_norm': 3.536793139100706, 'learning_rate': 9.274777777777779e-06, 'epoch': 0.6624}
{'loss': 1.0252, 'grad_norm': 3.4119222116916212, 'learning_rate': 9.273666666666667e-06, 'epoch': 0.6628}
{'loss': 1.0292, 'grad_norm': 3.224005991019085, 'learning_rate': 9.272555555555558e-06, 'epoch': 0.6632}
{'loss': 1.0104, 'grad_norm': 3.397305916245387, 'learning_rate': 9.271444444444445e-06, 'epoch': 0.6636}
{'loss': 1.0021, 'grad_norm': 3.0707499554019244, 'learning_rate': 9.270333333333335e-06, 'epoch': 0.664}
{'eval_valid_loss': 0.96533203125, 'eval_valid_runtime': 0.0909, 'eval_valid_samples_per_second': 1099.713, 'eval_valid_steps_per_second': 274.928, 'epoch': 0.664}
{'loss': 1.026, 'grad_norm': 3.223862227002172, 'learning_rate': 9.269222222222222e-06, 'epoch': 0.6644}
{'loss': 1.0061, 'grad_norm': 3.872164380926033, 'learning_rate': 9.268111111111113e-06, 'epoch': 0.6648}
{'loss': 1.0139, 'grad_norm': 3.5262347449777622, 'learning_rate': 9.267000000000001e-06, 'epoch': 0.6652}
{'loss': 1.0192, 'grad_norm': 3.202961028949638, 'learning_rate': 9.26588888888889e-06, 'epoch': 0.6656}
{'loss': 1.0223, 'grad_norm': 3.424094033673565, 'learning_rate': 9.264777777777779e-06, 'epoch': 0.666}
{'loss': 1.0107, 'grad_norm': 3.3736209347754946, 'learning_rate': 9.263666666666667e-06, 'epoch': 0.6664}
{'loss': 1.0119, 'grad_norm': 3.921748079472719, 'learning_rate': 9.262555555555556e-06, 'epoch': 0.6668}
{'loss': 1.0127, 'grad_norm': 3.4243183644570765, 'learning_rate': 9.261444444444445e-06, 'epoch': 0.6672}
{'loss': 1.0168, 'grad_norm': 4.111856951194866, 'learning_rate': 9.260333333333335e-06, 'epoch': 0.6676}
{'loss': 1.0149, 'grad_norm': 3.50800866680858, 'learning_rate': 9.259222222222222e-06, 'epoch': 0.668}
{'eval_valid_loss': 0.96630859375, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.428, 'eval_valid_steps_per_second': 278.357, 'epoch': 0.668}
{'loss': 1.0035, 'grad_norm': 3.5434390999722853, 'learning_rate': 9.258111111111113e-06, 'epoch': 0.6684}
{'loss': 1.0185, 'grad_norm': 3.227823911438587, 'learning_rate': 9.257e-06, 'epoch': 0.6688}
{'loss': 1.0123, 'grad_norm': 3.7674080991769014, 'learning_rate': 9.25588888888889e-06, 'epoch': 0.6692}
{'loss': 1.0004, 'grad_norm': 3.505779067795944, 'learning_rate': 9.254777777777779e-06, 'epoch': 0.6696}
{'loss': 1.0113, 'grad_norm': 3.371759757758606, 'learning_rate': 9.253666666666667e-06, 'epoch': 0.67}
{'loss': 1.033, 'grad_norm': 3.6097580842673223, 'learning_rate': 9.252555555555556e-06, 'epoch': 0.6704}
{'loss': 1.0072, 'grad_norm': 3.826839308970222, 'learning_rate': 9.251444444444445e-06, 'epoch': 0.6708}
{'loss': 1.0188, 'grad_norm': 3.4682459593904507, 'learning_rate': 9.250333333333335e-06, 'epoch': 0.6712}
{'loss': 1.0098, 'grad_norm': 4.0813409979169935, 'learning_rate': 9.249222222222222e-06, 'epoch': 0.6716}
{'loss': 1.0042, 'grad_norm': 3.2837922647295295, 'learning_rate': 9.248111111111113e-06, 'epoch': 0.672}
{'eval_valid_loss': 0.96484375, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1121.697, 'eval_valid_steps_per_second': 280.424, 'epoch': 0.672}
{'loss': 1.0108, 'grad_norm': 3.4755170900536303, 'learning_rate': 9.247e-06, 'epoch': 0.6724}
{'loss': 1.0115, 'grad_norm': 3.943198440237892, 'learning_rate': 9.24588888888889e-06, 'epoch': 0.6728}
{'loss': 1.0112, 'grad_norm': 3.606437999181824, 'learning_rate': 9.244777777777779e-06, 'epoch': 0.6732}
{'loss': 1.0214, 'grad_norm': 3.331901297321489, 'learning_rate': 9.243666666666668e-06, 'epoch': 0.6736}
{'loss': 1.009, 'grad_norm': 3.726784761466443, 'learning_rate': 9.242555555555556e-06, 'epoch': 0.674}
{'loss': 1.0169, 'grad_norm': 3.2817940578820473, 'learning_rate': 9.241444444444445e-06, 'epoch': 0.6744}
{'loss': 1.0038, 'grad_norm': 3.249387619205367, 'learning_rate': 9.240333333333334e-06, 'epoch': 0.6748}
{'loss': 1.0186, 'grad_norm': 3.4375629679374606, 'learning_rate': 9.239222222222222e-06, 'epoch': 0.6752}
{'loss': 1.0082, 'grad_norm': 3.543815713438001, 'learning_rate': 9.238111111111113e-06, 'epoch': 0.6756}
{'loss': 1.0083, 'grad_norm': 3.637762811021738, 'learning_rate': 9.237e-06, 'epoch': 0.676}
{'eval_valid_loss': 0.9638671875, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1121.67, 'eval_valid_steps_per_second': 280.417, 'epoch': 0.676}
{'loss': 1.0087, 'grad_norm': 3.3736015089132296, 'learning_rate': 9.23588888888889e-06, 'epoch': 0.6764}
{'loss': 1.0063, 'grad_norm': 3.6129542682233575, 'learning_rate': 9.234777777777777e-06, 'epoch': 0.6768}
{'loss': 1.013, 'grad_norm': 3.414888155545277, 'learning_rate': 9.233666666666668e-06, 'epoch': 0.6772}
{'loss': 1.0172, 'grad_norm': 3.3705631072736626, 'learning_rate': 9.232555555555556e-06, 'epoch': 0.6776}
{'loss': 1.0188, 'grad_norm': 3.8105211437926645, 'learning_rate': 9.231444444444445e-06, 'epoch': 0.678}
{'loss': 1.0134, 'grad_norm': 3.3884676033476344, 'learning_rate': 9.230333333333334e-06, 'epoch': 0.6784}
{'loss': 1.0099, 'grad_norm': 3.6847580397580493, 'learning_rate': 9.229222222222222e-06, 'epoch': 0.6788}
{'loss': 1.0042, 'grad_norm': 3.2830482641548078, 'learning_rate': 9.228111111111113e-06, 'epoch': 0.6792}
{'loss': 1.0036, 'grad_norm': 3.8618863309615907, 'learning_rate': 9.227e-06, 'epoch': 0.6796}
{'loss': 1.0111, 'grad_norm': 3.6252762097328755, 'learning_rate': 9.22588888888889e-06, 'epoch': 0.68}
{'eval_valid_loss': 0.96142578125, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.714, 'eval_valid_steps_per_second': 278.929, 'epoch': 0.68}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.0041, 'grad_norm': 3.518244712488412, 'learning_rate': 9.224777777777777e-06, 'epoch': 0.6804}
{'loss': 1.0092, 'grad_norm': 3.734708763115015, 'learning_rate': 9.223666666666668e-06, 'epoch': 0.6808}
{'loss': 1.0282, 'grad_norm': 3.597406136333373, 'learning_rate': 9.222555555555556e-06, 'epoch': 0.6812}
{'loss': 1.012, 'grad_norm': 3.447720929301314, 'learning_rate': 9.221555555555556e-06, 'epoch': 0.6816}
{'loss': 1.0033, 'grad_norm': 3.447777270853928, 'learning_rate': 9.220444444444445e-06, 'epoch': 0.682}
{'loss': 1.0088, 'grad_norm': 3.3152097289166096, 'learning_rate': 9.219333333333335e-06, 'epoch': 0.6824}
{'loss': 1.0088, 'grad_norm': 3.4145746261612895, 'learning_rate': 9.218222222222222e-06, 'epoch': 0.6828}
{'loss': 1.0185, 'grad_norm': 3.908512741856706, 'learning_rate': 9.217111111111112e-06, 'epoch': 0.6832}
{'loss': 1.0258, 'grad_norm': 3.960732629658092, 'learning_rate': 9.216000000000001e-06, 'epoch': 0.6836}
{'loss': 1.0089, 'grad_norm': 3.773269286266463, 'learning_rate': 9.21488888888889e-06, 'epoch': 0.684}
{'eval_valid_loss': 0.9619140625, 'eval_valid_runtime': 0.0903, 'eval_valid_samples_per_second': 1107.299, 'eval_valid_steps_per_second': 276.825, 'epoch': 0.684}
{'loss': 1.0082, 'grad_norm': 3.750327223170436, 'learning_rate': 9.213777777777779e-06, 'epoch': 0.6844}
{'loss': 1.0037, 'grad_norm': 3.8084768424064386, 'learning_rate': 9.212666666666667e-06, 'epoch': 0.6848}
{'loss': 1.0036, 'grad_norm': 3.35972615558001, 'learning_rate': 9.211555555555556e-06, 'epoch': 0.6852}
{'loss': 1.0095, 'grad_norm': 4.0018246333586776, 'learning_rate': 9.210444444444445e-06, 'epoch': 0.6856}
{'loss': 1.0209, 'grad_norm': 3.6710744593848763, 'learning_rate': 9.209333333333335e-06, 'epoch': 0.686}
{'loss': 1.0154, 'grad_norm': 3.753263547501532, 'learning_rate': 9.208222222222222e-06, 'epoch': 0.6864}
{'loss': 1.0155, 'grad_norm': 3.646847478506129, 'learning_rate': 9.207111111111113e-06, 'epoch': 0.6868}
{'loss': 1.0104, 'grad_norm': 3.4119566963506758, 'learning_rate': 9.206000000000001e-06, 'epoch': 0.6872}
{'loss': 1.0056, 'grad_norm': 3.3931615104966513, 'learning_rate': 9.20488888888889e-06, 'epoch': 0.6876}
{'loss': 1.0073, 'grad_norm': 3.9335906133857046, 'learning_rate': 9.203777777777779e-06, 'epoch': 0.688}
{'eval_valid_loss': 0.95751953125, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1105.819, 'eval_valid_steps_per_second': 276.455, 'epoch': 0.688}
{'loss': 0.9936, 'grad_norm': 3.7148364174306163, 'learning_rate': 9.202666666666667e-06, 'epoch': 0.6884}
{'loss': 1.0104, 'grad_norm': 3.5015227121920884, 'learning_rate': 9.201555555555556e-06, 'epoch': 0.6888}
{'loss': 1.0179, 'grad_norm': 3.3753799118686043, 'learning_rate': 9.200444444444445e-06, 'epoch': 0.6892}
{'loss': 1.0186, 'grad_norm': 3.4553024452065246, 'learning_rate': 9.199333333333335e-06, 'epoch': 0.6896}
{'loss': 1.015, 'grad_norm': 3.232566733442208, 'learning_rate': 9.198222222222222e-06, 'epoch': 0.69}
{'loss': 1.0193, 'grad_norm': 3.7061836551948732, 'learning_rate': 9.197111111111113e-06, 'epoch': 0.6904}
{'loss': 1.0184, 'grad_norm': 3.31203586997543, 'learning_rate': 9.196e-06, 'epoch': 0.6908}
{'loss': 1.015, 'grad_norm': 3.3965073552377016, 'learning_rate': 9.19488888888889e-06, 'epoch': 0.6912}
{'loss': 1.0095, 'grad_norm': 3.8905907594943536, 'learning_rate': 9.193777777777779e-06, 'epoch': 0.6916}
{'loss': 1.014, 'grad_norm': 3.747591564518033, 'learning_rate': 9.192666666666668e-06, 'epoch': 0.692}
{'eval_valid_loss': 0.9580078125, 'eval_valid_runtime': 0.0907, 'eval_valid_samples_per_second': 1102.088, 'eval_valid_steps_per_second': 275.522, 'epoch': 0.692}
{'loss': 0.9964, 'grad_norm': 3.27597723060073, 'learning_rate': 9.191555555555556e-06, 'epoch': 0.6924}
{'loss': 1.0218, 'grad_norm': 3.9770555921212845, 'learning_rate': 9.190444444444445e-06, 'epoch': 0.6928}
{'loss': 0.9988, 'grad_norm': 3.290883011226022, 'learning_rate': 9.189333333333335e-06, 'epoch': 0.6932}
{'loss': 0.9955, 'grad_norm': 3.5080756532646453, 'learning_rate': 9.188222222222222e-06, 'epoch': 0.6936}
{'loss': 1.0038, 'grad_norm': 3.105161342160784, 'learning_rate': 9.187111111111113e-06, 'epoch': 0.694}
{'loss': 1.008, 'grad_norm': 3.3393682397898568, 'learning_rate': 9.186e-06, 'epoch': 0.6944}
{'loss': 1.0131, 'grad_norm': 3.086744294752405, 'learning_rate': 9.18488888888889e-06, 'epoch': 0.6948}
{'loss': 0.9985, 'grad_norm': 3.8028964423003417, 'learning_rate': 9.183777777777779e-06, 'epoch': 0.6952}
{'loss': 1.0184, 'grad_norm': 3.407054316200528, 'learning_rate': 9.182666666666668e-06, 'epoch': 0.6956}
{'loss': 1.0152, 'grad_norm': 3.7166235317555385, 'learning_rate': 9.181555555555556e-06, 'epoch': 0.696}
{'eval_valid_loss': 0.9599609375, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1122.399, 'eval_valid_steps_per_second': 280.6, 'epoch': 0.696}
{'loss': 1.0151, 'grad_norm': 3.5965012343728198, 'learning_rate': 9.180444444444445e-06, 'epoch': 0.6964}
{'loss': 1.0234, 'grad_norm': 3.3043712437584722, 'learning_rate': 9.179333333333334e-06, 'epoch': 0.6968}
{'loss': 1.0167, 'grad_norm': 3.419367860268202, 'learning_rate': 9.178222222222222e-06, 'epoch': 0.6972}
{'loss': 1.0078, 'grad_norm': 3.5949229772537667, 'learning_rate': 9.177111111111113e-06, 'epoch': 0.6976}
{'loss': 1.0237, 'grad_norm': 3.2176103518753423, 'learning_rate': 9.176e-06, 'epoch': 0.698}
{'loss': 1.0024, 'grad_norm': 3.78056504217006, 'learning_rate': 9.17488888888889e-06, 'epoch': 0.6984}
{'loss': 1.0094, 'grad_norm': 3.9758233543200934, 'learning_rate': 9.173777777777777e-06, 'epoch': 0.6988}
{'loss': 1.0062, 'grad_norm': 3.5368844123468373, 'learning_rate': 9.172666666666668e-06, 'epoch': 0.6992}
{'loss': 1.0143, 'grad_norm': 3.4090481735209606, 'learning_rate': 9.171555555555556e-06, 'epoch': 0.6996}
{'loss': 1.0007, 'grad_norm': 3.526565144765637, 'learning_rate': 9.170444444444445e-06, 'epoch': 0.7}
{'eval_valid_loss': 0.95703125, 'eval_valid_runtime': 0.0907, 'eval_valid_samples_per_second': 1101.966, 'eval_valid_steps_per_second': 275.492, 'epoch': 0.7}
{'loss': 1.0084, 'grad_norm': 3.4622619281959697, 'learning_rate': 9.169333333333334e-06, 'epoch': 0.7004}
{'loss': 1.006, 'grad_norm': 2.9405878759912962, 'learning_rate': 9.168222222222223e-06, 'epoch': 0.7008}
{'loss': 0.9992, 'grad_norm': 3.5231718737164055, 'learning_rate': 9.167111111111113e-06, 'epoch': 0.7012}
{'loss': 1.0067, 'grad_norm': 3.251555996493756, 'learning_rate': 9.166e-06, 'epoch': 0.7016}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.0, 'grad_norm': 3.6140668272791587, 'learning_rate': 9.16488888888889e-06, 'epoch': 0.702}
{'loss': 1.0063, 'grad_norm': 3.191098855574931, 'learning_rate': 9.163777777777777e-06, 'epoch': 0.7024}
{'loss': 1.0038, 'grad_norm': 3.5485635174875245, 'learning_rate': 9.162666666666668e-06, 'epoch': 0.7028}
{'loss': 1.0084, 'grad_norm': 3.065779477348299, 'learning_rate': 9.161555555555557e-06, 'epoch': 0.7032}
{'loss': 1.0005, 'grad_norm': 3.408511530959759, 'learning_rate': 9.160444444444445e-06, 'epoch': 0.7036}
{'loss': 1.002, 'grad_norm': 3.779961697558456, 'learning_rate': 9.159333333333334e-06, 'epoch': 0.704}
{'eval_valid_loss': 0.958984375, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1114.06, 'eval_valid_steps_per_second': 278.515, 'epoch': 0.704}
{'loss': 0.9951, 'grad_norm': 3.3551989126009034, 'learning_rate': 9.158222222222223e-06, 'epoch': 0.7044}
{'loss': 1.0087, 'grad_norm': 3.8474378649705656, 'learning_rate': 9.157111111111111e-06, 'epoch': 0.7048}
{'loss': 1.0198, 'grad_norm': 3.768537858904185, 'learning_rate': 9.156e-06, 'epoch': 0.7052}
{'loss': 0.9941, 'grad_norm': 3.5648939220702496, 'learning_rate': 9.15488888888889e-06, 'epoch': 0.7056}
{'loss': 1.0101, 'grad_norm': 3.506445790500467, 'learning_rate': 9.153777777777778e-06, 'epoch': 0.706}
{'loss': 1.0204, 'grad_norm': 3.571391003274873, 'learning_rate': 9.152666666666668e-06, 'epoch': 0.7064}
{'loss': 1.0054, 'grad_norm': 3.2399620875300923, 'learning_rate': 9.151555555555557e-06, 'epoch': 0.7068}
{'loss': 0.9986, 'grad_norm': 3.62562488234345, 'learning_rate': 9.150444444444445e-06, 'epoch': 0.7072}
{'loss': 1.0188, 'grad_norm': 3.584880614933091, 'learning_rate': 9.149333333333334e-06, 'epoch': 0.7076}
{'loss': 1.0195, 'grad_norm': 3.5741394367694244, 'learning_rate': 9.148222222222223e-06, 'epoch': 0.708}
{'eval_valid_loss': 0.9619140625, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1114.075, 'eval_valid_steps_per_second': 278.519, 'epoch': 0.708}
{'loss': 1.0068, 'grad_norm': 3.9268116979309933, 'learning_rate': 9.147111111111111e-06, 'epoch': 0.7084}
{'loss': 1.0063, 'grad_norm': 3.400382228711678, 'learning_rate': 9.146e-06, 'epoch': 0.7088}
{'loss': 1.0001, 'grad_norm': 3.417978855663074, 'learning_rate': 9.14488888888889e-06, 'epoch': 0.7092}
{'loss': 0.9953, 'grad_norm': 3.9838244936740628, 'learning_rate': 9.143777777777778e-06, 'epoch': 0.7096}
{'loss': 1.0082, 'grad_norm': 3.6308969677337943, 'learning_rate': 9.142666666666668e-06, 'epoch': 0.71}
{'loss': 1.0052, 'grad_norm': 3.463716764493678, 'learning_rate': 9.141555555555557e-06, 'epoch': 0.7104}
{'loss': 0.9884, 'grad_norm': 3.3612687473050937, 'learning_rate': 9.140444444444445e-06, 'epoch': 0.7108}
{'loss': 1.0133, 'grad_norm': 3.5837985289407737, 'learning_rate': 9.139333333333334e-06, 'epoch': 0.7112}
{'loss': 1.007, 'grad_norm': 3.415942915965456, 'learning_rate': 9.138222222222223e-06, 'epoch': 0.7116}
{'loss': 0.9987, 'grad_norm': 3.349813445077965, 'learning_rate': 9.137111111111112e-06, 'epoch': 0.712}
{'eval_valid_loss': 0.95556640625, 'eval_valid_runtime': 0.1809, 'eval_valid_samples_per_second': 552.727, 'eval_valid_steps_per_second': 138.182, 'epoch': 0.712}
{'loss': 1.0073, 'grad_norm': 3.1786942113855257, 'learning_rate': 9.136e-06, 'epoch': 0.7124}
{'loss': 1.0025, 'grad_norm': 3.231249658374289, 'learning_rate': 9.134888888888889e-06, 'epoch': 0.7128}
{'loss': 1.0071, 'grad_norm': 3.458614620389629, 'learning_rate': 9.133777777777778e-06, 'epoch': 0.7132}
{'loss': 1.0072, 'grad_norm': 3.668132498014128, 'learning_rate': 9.132666666666668e-06, 'epoch': 0.7136}
{'loss': 1.0139, 'grad_norm': 3.5211219368350877, 'learning_rate': 9.131555555555557e-06, 'epoch': 0.714}
{'loss': 1.0077, 'grad_norm': 3.3115348849468536, 'learning_rate': 9.130444444444446e-06, 'epoch': 0.7144}
{'loss': 1.0003, 'grad_norm': 3.8760414262276197, 'learning_rate': 9.129333333333334e-06, 'epoch': 0.7148}
{'loss': 1.0092, 'grad_norm': 3.831505711222298, 'learning_rate': 9.128222222222223e-06, 'epoch': 0.7152}
{'loss': 1.003, 'grad_norm': 3.917752991171362, 'learning_rate': 9.127111111111112e-06, 'epoch': 0.7156}
{'loss': 0.9995, 'grad_norm': 3.0506465652569528, 'learning_rate': 9.126e-06, 'epoch': 0.716}
{'eval_valid_loss': 0.95703125, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1109.114, 'eval_valid_steps_per_second': 277.279, 'epoch': 0.716}
{'loss': 1.0043, 'grad_norm': 3.5126198279823715, 'learning_rate': 9.124888888888889e-06, 'epoch': 0.7164}
{'loss': 0.9963, 'grad_norm': 3.336016705396846, 'learning_rate': 9.123777777777778e-06, 'epoch': 0.7168}
{'loss': 0.999, 'grad_norm': 3.448543521373069, 'learning_rate': 9.122666666666668e-06, 'epoch': 0.7172}
{'loss': 1.0028, 'grad_norm': 3.3267327114151066, 'learning_rate': 9.121555555555557e-06, 'epoch': 0.7176}
{'loss': 0.9954, 'grad_norm': 3.721676116036769, 'learning_rate': 9.120444444444446e-06, 'epoch': 0.718}
{'loss': 1.0103, 'grad_norm': 3.2044664876789315, 'learning_rate': 9.119333333333334e-06, 'epoch': 0.7184}
{'loss': 0.9864, 'grad_norm': 3.619000723986568, 'learning_rate': 9.118222222222223e-06, 'epoch': 0.7188}
{'loss': 1.0021, 'grad_norm': 3.2066143754629852, 'learning_rate': 9.117111111111112e-06, 'epoch': 0.7192}
{'loss': 0.9983, 'grad_norm': 3.3219222441334435, 'learning_rate': 9.116e-06, 'epoch': 0.7196}
{'loss': 1.0019, 'grad_norm': 3.6491983669971884, 'learning_rate': 9.11488888888889e-06, 'epoch': 0.72}
{'eval_valid_loss': 0.9521484375, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1114.188, 'eval_valid_steps_per_second': 278.547, 'epoch': 0.72}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.0076, 'grad_norm': 3.3203301911724235, 'learning_rate': 9.113777777777778e-06, 'epoch': 0.7204}
{'loss': 1.0102, 'grad_norm': 4.025056294823516, 'learning_rate': 9.112666666666667e-06, 'epoch': 0.7208}
{'loss': 0.9892, 'grad_norm': 3.4230194268306398, 'learning_rate': 9.111555555555557e-06, 'epoch': 0.7212}
{'loss': 0.9995, 'grad_norm': 3.2968667739837856, 'learning_rate': 9.110555555555557e-06, 'epoch': 0.7216}
{'loss': 1.0003, 'grad_norm': 3.3500864473262966, 'learning_rate': 9.109444444444445e-06, 'epoch': 0.722}
{'loss': 1.0042, 'grad_norm': 3.5991578918571463, 'learning_rate': 9.108333333333334e-06, 'epoch': 0.7224}
{'loss': 1.005, 'grad_norm': 4.231623498161319, 'learning_rate': 9.107222222222223e-06, 'epoch': 0.7228}
{'loss': 0.9975, 'grad_norm': 3.426239061376402, 'learning_rate': 9.106111111111113e-06, 'epoch': 0.7232}
{'loss': 1.0026, 'grad_norm': 3.5195470676214864, 'learning_rate': 9.105e-06, 'epoch': 0.7236}
{'loss': 1.0068, 'grad_norm': 3.4095523819553266, 'learning_rate': 9.10388888888889e-06, 'epoch': 0.724}
{'eval_valid_loss': 0.9580078125, 'eval_valid_runtime': 0.0961, 'eval_valid_samples_per_second': 1040.151, 'eval_valid_steps_per_second': 260.038, 'epoch': 0.724}
{'loss': 1.0108, 'grad_norm': 3.7095982915811017, 'learning_rate': 9.102777777777777e-06, 'epoch': 0.7244}
{'loss': 0.9931, 'grad_norm': 3.188328766135737, 'learning_rate': 9.101666666666668e-06, 'epoch': 0.7248}
{'loss': 1.0071, 'grad_norm': 3.4869766444262877, 'learning_rate': 9.100555555555557e-06, 'epoch': 0.7252}
{'loss': 0.9872, 'grad_norm': 3.9002717926001442, 'learning_rate': 9.099444444444445e-06, 'epoch': 0.7256}
{'loss': 1.0009, 'grad_norm': 4.088484413410234, 'learning_rate': 9.098333333333334e-06, 'epoch': 0.726}
{'loss': 1.018, 'grad_norm': 3.418745499770081, 'learning_rate': 9.097222222222223e-06, 'epoch': 0.7264}
{'loss': 1.0027, 'grad_norm': 4.249806343603186, 'learning_rate': 9.096111111111111e-06, 'epoch': 0.7268}
{'loss': 0.9994, 'grad_norm': 3.1863599215613116, 'learning_rate': 9.095e-06, 'epoch': 0.7272}
{'loss': 1.0071, 'grad_norm': 3.1247354300083208, 'learning_rate': 9.09388888888889e-06, 'epoch': 0.7276}
{'loss': 1.0008, 'grad_norm': 3.2883899028189663, 'learning_rate': 9.092777777777778e-06, 'epoch': 0.728}
{'eval_valid_loss': 0.95458984375, 'eval_valid_runtime': 0.0905, 'eval_valid_samples_per_second': 1104.849, 'eval_valid_steps_per_second': 276.212, 'epoch': 0.728}
{'loss': 0.9836, 'grad_norm': 3.367287362160119, 'learning_rate': 9.091666666666668e-06, 'epoch': 0.7284}
{'loss': 0.9895, 'grad_norm': 3.454046307726606, 'learning_rate': 9.090555555555555e-06, 'epoch': 0.7288}
{'loss': 0.9933, 'grad_norm': 3.3957233333128953, 'learning_rate': 9.089444444444445e-06, 'epoch': 0.7292}
{'loss': 1.0128, 'grad_norm': 3.4368187576106837, 'learning_rate': 9.088333333333334e-06, 'epoch': 0.7296}
{'loss': 1.0007, 'grad_norm': 3.157913147512944, 'learning_rate': 9.087222222222223e-06, 'epoch': 0.73}
{'loss': 0.992, 'grad_norm': 4.122217786184759, 'learning_rate': 9.086111111111112e-06, 'epoch': 0.7304}
{'loss': 0.9989, 'grad_norm': 3.5892730358851503, 'learning_rate': 9.085e-06, 'epoch': 0.7308}
{'loss': 1.0049, 'grad_norm': 3.669140077223565, 'learning_rate': 9.08388888888889e-06, 'epoch': 0.7312}
{'loss': 1.0029, 'grad_norm': 3.3989910595684534, 'learning_rate': 9.082777777777778e-06, 'epoch': 0.7316}
{'loss': 0.9941, 'grad_norm': 3.4441388611617563, 'learning_rate': 9.081666666666668e-06, 'epoch': 0.732}
{'eval_valid_loss': 0.955078125, 'eval_valid_runtime': 0.0905, 'eval_valid_samples_per_second': 1105.186, 'eval_valid_steps_per_second': 276.297, 'epoch': 0.732}
{'loss': 1.0098, 'grad_norm': 3.458067399260028, 'learning_rate': 9.080555555555555e-06, 'epoch': 0.7324}
{'loss': 0.9962, 'grad_norm': 3.5860255610804406, 'learning_rate': 9.079444444444446e-06, 'epoch': 0.7328}
{'loss': 1.0025, 'grad_norm': 3.4379149793350194, 'learning_rate': 9.078333333333334e-06, 'epoch': 0.7332}
{'loss': 0.9968, 'grad_norm': 3.6058192056958482, 'learning_rate': 9.077222222222223e-06, 'epoch': 0.7336}
{'loss': 1.0115, 'grad_norm': 3.7042637615258465, 'learning_rate': 9.076111111111112e-06, 'epoch': 0.734}
{'loss': 1.0002, 'grad_norm': 3.0034103083369423, 'learning_rate': 9.075e-06, 'epoch': 0.7344}
{'loss': 1.0068, 'grad_norm': 3.4250242295069473, 'learning_rate': 9.073888888888889e-06, 'epoch': 0.7348}
{'loss': 0.9974, 'grad_norm': 3.4220800294809446, 'learning_rate': 9.072777777777778e-06, 'epoch': 0.7352}
{'loss': 0.9981, 'grad_norm': 3.1539165633672925, 'learning_rate': 9.071666666666668e-06, 'epoch': 0.7356}
{'loss': 0.9969, 'grad_norm': 3.191680803010343, 'learning_rate': 9.070555555555555e-06, 'epoch': 0.736}
{'eval_valid_loss': 0.94873046875, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1111.047, 'eval_valid_steps_per_second': 277.762, 'epoch': 0.736}
{'loss': 1.0079, 'grad_norm': 3.580675873330917, 'learning_rate': 9.069444444444446e-06, 'epoch': 0.7364}
{'loss': 1.0006, 'grad_norm': 3.3989112523791394, 'learning_rate': 9.068333333333334e-06, 'epoch': 0.7368}
{'loss': 0.9785, 'grad_norm': 3.2277655863946584, 'learning_rate': 9.067222222222223e-06, 'epoch': 0.7372}
{'loss': 1.0017, 'grad_norm': 3.7023182404623975, 'learning_rate': 9.066111111111112e-06, 'epoch': 0.7376}
{'loss': 1.0011, 'grad_norm': 3.519924840395644, 'learning_rate': 9.065e-06, 'epoch': 0.738}
{'loss': 0.9968, 'grad_norm': 3.534694822767711, 'learning_rate': 9.06388888888889e-06, 'epoch': 0.7384}
{'loss': 0.9896, 'grad_norm': 3.3453664972187047, 'learning_rate': 9.062777777777778e-06, 'epoch': 0.7388}
{'loss': 0.9982, 'grad_norm': 3.550851567309663, 'learning_rate': 9.061666666666668e-06, 'epoch': 0.7392}
{'loss': 0.9776, 'grad_norm': 3.4481139093012905, 'learning_rate': 9.060555555555555e-06, 'epoch': 0.7396}
{'loss': 1.0004, 'grad_norm': 3.2463427653551853, 'learning_rate': 9.059444444444446e-06, 'epoch': 0.74}
{'eval_valid_loss': 0.94970703125, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.179, 'eval_valid_steps_per_second': 278.295, 'epoch': 0.74}
{'loss': 1.0028, 'grad_norm': 3.730361345752205, 'learning_rate': 9.058333333333334e-06, 'epoch': 0.7404}
{'loss': 1.0175, 'grad_norm': 3.806313952329968, 'learning_rate': 9.057222222222223e-06, 'epoch': 0.7408}
{'loss': 0.994, 'grad_norm': 3.555554185786745, 'learning_rate': 9.056111111111112e-06, 'epoch': 0.7412}
{'loss': 0.9902, 'grad_norm': 3.2048217466330966, 'learning_rate': 9.055e-06, 'epoch': 0.7416}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9982, 'grad_norm': 3.526104325940294, 'learning_rate': 9.05388888888889e-06, 'epoch': 0.742}
{'loss': 0.9904, 'grad_norm': 3.2641958550018253, 'learning_rate': 9.052777777777778e-06, 'epoch': 0.7424}
{'loss': 0.9859, 'grad_norm': 3.1859509967781863, 'learning_rate': 9.051666666666667e-06, 'epoch': 0.7428}
{'loss': 1.0101, 'grad_norm': 3.525558264806927, 'learning_rate': 9.050555555555555e-06, 'epoch': 0.7432}
{'loss': 1.0104, 'grad_norm': 3.9084491642757855, 'learning_rate': 9.049444444444446e-06, 'epoch': 0.7436}
{'loss': 1.0122, 'grad_norm': 3.38832256704466, 'learning_rate': 9.048333333333335e-06, 'epoch': 0.744}
{'eval_valid_loss': 0.951171875, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1128.997, 'eval_valid_steps_per_second': 282.249, 'epoch': 0.744}
{'loss': 1.0085, 'grad_norm': 3.325748660837681, 'learning_rate': 9.047222222222223e-06, 'epoch': 0.7444}
{'loss': 0.9852, 'grad_norm': 3.5287342393599506, 'learning_rate': 9.046111111111112e-06, 'epoch': 0.7448}
{'loss': 1.0129, 'grad_norm': 3.3270959472098647, 'learning_rate': 9.045e-06, 'epoch': 0.7452}
{'loss': 1.0, 'grad_norm': 3.318880629617723, 'learning_rate': 9.04388888888889e-06, 'epoch': 0.7456}
{'loss': 0.9996, 'grad_norm': 4.230314806728801, 'learning_rate': 9.042777777777778e-06, 'epoch': 0.746}
{'loss': 1.0122, 'grad_norm': 3.4266196326603855, 'learning_rate': 9.041666666666667e-06, 'epoch': 0.7464}
{'loss': 0.9992, 'grad_norm': 3.895054237810774, 'learning_rate': 9.040555555555555e-06, 'epoch': 0.7468}
{'loss': 1.0094, 'grad_norm': 3.4682168207131676, 'learning_rate': 9.039444444444446e-06, 'epoch': 0.7472}
{'loss': 0.9998, 'grad_norm': 3.4276657575819836, 'learning_rate': 9.038333333333335e-06, 'epoch': 0.7476}
{'loss': 1.0058, 'grad_norm': 3.4020978135787554, 'learning_rate': 9.037222222222223e-06, 'epoch': 0.748}
{'eval_valid_loss': 0.95458984375, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1111.071, 'eval_valid_steps_per_second': 277.768, 'epoch': 0.748}
{'loss': 1.0016, 'grad_norm': 3.564991899296571, 'learning_rate': 9.036111111111112e-06, 'epoch': 0.7484}
{'loss': 0.9875, 'grad_norm': 3.5909774866219024, 'learning_rate': 9.035e-06, 'epoch': 0.7488}
{'loss': 1.0046, 'grad_norm': 3.3882950367118223, 'learning_rate': 9.03388888888889e-06, 'epoch': 0.7492}
{'loss': 1.007, 'grad_norm': 3.57457326996488, 'learning_rate': 9.032777777777778e-06, 'epoch': 0.7496}
{'loss': 1.01, 'grad_norm': 3.482858832604336, 'learning_rate': 9.031666666666667e-06, 'epoch': 0.75}
{'loss': 0.993, 'grad_norm': 3.903396388103012, 'learning_rate': 9.030555555555556e-06, 'epoch': 0.7504}
{'loss': 0.9996, 'grad_norm': 3.5659980916404863, 'learning_rate': 9.029444444444444e-06, 'epoch': 0.7508}
{'loss': 0.9956, 'grad_norm': 3.56270024924728, 'learning_rate': 9.028333333333335e-06, 'epoch': 0.7512}
{'loss': 0.9949, 'grad_norm': 3.059912240275232, 'learning_rate': 9.027222222222223e-06, 'epoch': 0.7516}
{'loss': 0.9954, 'grad_norm': 3.4203346304005455, 'learning_rate': 9.026111111111112e-06, 'epoch': 0.752}
{'eval_valid_loss': 0.95263671875, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.671, 'eval_valid_steps_per_second': 278.168, 'epoch': 0.752}
{'loss': 1.0002, 'grad_norm': 3.006572884094748, 'learning_rate': 9.025e-06, 'epoch': 0.7524}
{'loss': 0.9875, 'grad_norm': 3.9809060198162567, 'learning_rate': 9.02388888888889e-06, 'epoch': 0.7528}
{'loss': 1.0031, 'grad_norm': 3.1823886767063194, 'learning_rate': 9.022777777777778e-06, 'epoch': 0.7532}
{'loss': 1.0046, 'grad_norm': 3.381495195055127, 'learning_rate': 9.021666666666667e-06, 'epoch': 0.7536}
{'loss': 0.9828, 'grad_norm': 3.2521998223102186, 'learning_rate': 9.020555555555557e-06, 'epoch': 0.754}
{'loss': 0.9925, 'grad_norm': 4.227810701810708, 'learning_rate': 9.019444444444444e-06, 'epoch': 0.7544}
{'loss': 0.9931, 'grad_norm': 3.318771426532207, 'learning_rate': 9.018333333333335e-06, 'epoch': 0.7548}
{'loss': 0.9817, 'grad_norm': 3.9293584666898633, 'learning_rate': 9.017222222222224e-06, 'epoch': 0.7552}
{'loss': 0.9895, 'grad_norm': 4.455424160293579, 'learning_rate': 9.016111111111112e-06, 'epoch': 0.7556}
{'loss': 0.9986, 'grad_norm': 3.4146187545183575, 'learning_rate': 9.015000000000001e-06, 'epoch': 0.756}
{'eval_valid_loss': 0.947265625, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.385, 'eval_valid_steps_per_second': 279.346, 'epoch': 0.756}
{'loss': 1.0061, 'grad_norm': 3.7326351645793623, 'learning_rate': 9.01388888888889e-06, 'epoch': 0.7564}
{'loss': 0.9986, 'grad_norm': 3.757799461802112, 'learning_rate': 9.012777777777778e-06, 'epoch': 0.7568}
{'loss': 0.9977, 'grad_norm': 3.6239070560123996, 'learning_rate': 9.011666666666667e-06, 'epoch': 0.7572}
{'loss': 0.9979, 'grad_norm': 3.76024534657674, 'learning_rate': 9.010555555555557e-06, 'epoch': 0.7576}
{'loss': 1.0064, 'grad_norm': 4.0709387211480434, 'learning_rate': 9.009444444444444e-06, 'epoch': 0.758}
{'loss': 0.9931, 'grad_norm': 3.5761306235300436, 'learning_rate': 9.008333333333335e-06, 'epoch': 0.7584}
{'loss': 0.9918, 'grad_norm': 3.558664876015602, 'learning_rate': 9.007222222222222e-06, 'epoch': 0.7588}
{'loss': 1.0019, 'grad_norm': 3.8106017468058777, 'learning_rate': 9.006111111111112e-06, 'epoch': 0.7592}
{'loss': 0.9903, 'grad_norm': 3.223012509161061, 'learning_rate': 9.005000000000001e-06, 'epoch': 0.7596}
{'loss': 0.9952, 'grad_norm': 3.351855036277578, 'learning_rate': 9.00388888888889e-06, 'epoch': 0.76}
{'eval_valid_loss': 0.94677734375, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1110.486, 'eval_valid_steps_per_second': 277.621, 'epoch': 0.76}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.986, 'grad_norm': 3.797267893490419, 'learning_rate': 9.002777777777778e-06, 'epoch': 0.7604}
{'loss': 0.9979, 'grad_norm': 3.097888932983653, 'learning_rate': 9.001666666666667e-06, 'epoch': 0.7608}
{'loss': 0.9968, 'grad_norm': 3.2285468347498267, 'learning_rate': 9.000555555555558e-06, 'epoch': 0.7612}
{'loss': 1.0016, 'grad_norm': 3.613929401722516, 'learning_rate': 8.999555555555555e-06, 'epoch': 0.7616}
{'loss': 0.9851, 'grad_norm': 3.303586907332107, 'learning_rate': 8.998444444444446e-06, 'epoch': 0.762}
{'loss': 1.0023, 'grad_norm': 3.083648334771008, 'learning_rate': 8.997333333333334e-06, 'epoch': 0.7624}
{'loss': 0.9971, 'grad_norm': 3.4937005147005538, 'learning_rate': 8.996222222222223e-06, 'epoch': 0.7628}
{'loss': 1.0112, 'grad_norm': 3.789119963849255, 'learning_rate': 8.995111111111112e-06, 'epoch': 0.7632}
{'loss': 0.9973, 'grad_norm': 3.3312504290639517, 'learning_rate': 8.994e-06, 'epoch': 0.7636}
{'loss': 1.0049, 'grad_norm': 3.0197306440086606, 'learning_rate': 8.99288888888889e-06, 'epoch': 0.764}
{'eval_valid_loss': 0.94921875, 'eval_valid_runtime': 0.0974, 'eval_valid_samples_per_second': 1027.089, 'eval_valid_steps_per_second': 256.772, 'epoch': 0.764}
{'loss': 0.9838, 'grad_norm': 3.5357383122391575, 'learning_rate': 8.991777777777778e-06, 'epoch': 0.7644}
{'loss': 0.983, 'grad_norm': 3.793972208720622, 'learning_rate': 8.990666666666667e-06, 'epoch': 0.7648}
{'loss': 0.9917, 'grad_norm': 3.4501022382459214, 'learning_rate': 8.989555555555555e-06, 'epoch': 0.7652}
{'loss': 1.003, 'grad_norm': 3.3620218805642703, 'learning_rate': 8.988444444444446e-06, 'epoch': 0.7656}
{'loss': 0.9888, 'grad_norm': 3.509232673238629, 'learning_rate': 8.987333333333335e-06, 'epoch': 0.766}
{'loss': 1.0021, 'grad_norm': 3.4740984810731934, 'learning_rate': 8.986222222222223e-06, 'epoch': 0.7664}
{'loss': 0.9829, 'grad_norm': 3.633978986607977, 'learning_rate': 8.985111111111112e-06, 'epoch': 0.7668}
{'loss': 1.0082, 'grad_norm': 3.900812942800924, 'learning_rate': 8.984e-06, 'epoch': 0.7672}
{'loss': 0.9934, 'grad_norm': 3.49079343206702, 'learning_rate': 8.98288888888889e-06, 'epoch': 0.7676}
{'loss': 0.9951, 'grad_norm': 4.16967841337721, 'learning_rate': 8.981777777777778e-06, 'epoch': 0.768}
{'eval_valid_loss': 0.94775390625, 'eval_valid_runtime': 0.2287, 'eval_valid_samples_per_second': 437.312, 'eval_valid_steps_per_second': 109.328, 'epoch': 0.768}
{'loss': 0.9729, 'grad_norm': 3.2878687452340833, 'learning_rate': 8.980666666666667e-06, 'epoch': 0.7684}
{'loss': 1.0036, 'grad_norm': 3.0019969453014617, 'learning_rate': 8.979555555555556e-06, 'epoch': 0.7688}
{'loss': 0.9876, 'grad_norm': 3.7176833506341795, 'learning_rate': 8.978444444444446e-06, 'epoch': 0.7692}
{'loss': 0.9965, 'grad_norm': 3.338106285704133, 'learning_rate': 8.977333333333335e-06, 'epoch': 0.7696}
{'loss': 0.9846, 'grad_norm': 3.919225504497938, 'learning_rate': 8.976222222222223e-06, 'epoch': 0.77}
{'loss': 0.9928, 'grad_norm': 3.3485035308151687, 'learning_rate': 8.975111111111112e-06, 'epoch': 0.7704}
{'loss': 1.0036, 'grad_norm': 3.4611867380060954, 'learning_rate': 8.974e-06, 'epoch': 0.7708}
{'loss': 1.0088, 'grad_norm': 3.969667914263494, 'learning_rate': 8.97288888888889e-06, 'epoch': 0.7712}
{'loss': 1.0016, 'grad_norm': 3.4730534020691746, 'learning_rate': 8.971777777777778e-06, 'epoch': 0.7716}
{'loss': 0.9854, 'grad_norm': 3.562490446512898, 'learning_rate': 8.970666666666667e-06, 'epoch': 0.772}
{'eval_valid_loss': 0.9453125, 'eval_valid_runtime': 0.089, 'eval_valid_samples_per_second': 1123.129, 'eval_valid_steps_per_second': 280.782, 'epoch': 0.772}
{'loss': 1.0004, 'grad_norm': 3.6625712355238096, 'learning_rate': 8.969555555555556e-06, 'epoch': 0.7724}
{'loss': 0.9978, 'grad_norm': 3.690294112344551, 'learning_rate': 8.968444444444444e-06, 'epoch': 0.7728}
{'loss': 0.9962, 'grad_norm': 3.2206897353616313, 'learning_rate': 8.967333333333335e-06, 'epoch': 0.7732}
{'loss': 0.9853, 'grad_norm': 3.859561004480736, 'learning_rate': 8.966222222222223e-06, 'epoch': 0.7736}
{'loss': 0.9923, 'grad_norm': 3.7070636968931185, 'learning_rate': 8.965111111111112e-06, 'epoch': 0.774}
{'loss': 0.9854, 'grad_norm': 3.4170379330346745, 'learning_rate': 8.964000000000001e-06, 'epoch': 0.7744}
{'loss': 1.0056, 'grad_norm': 3.3619878942650647, 'learning_rate': 8.96288888888889e-06, 'epoch': 0.7748}
{'loss': 0.9782, 'grad_norm': 3.4241613213964763, 'learning_rate': 8.961777777777778e-06, 'epoch': 0.7752}
{'loss': 0.9879, 'grad_norm': 3.172369218479899, 'learning_rate': 8.960666666666667e-06, 'epoch': 0.7756}
{'loss': 1.0069, 'grad_norm': 3.2266985423524064, 'learning_rate': 8.959555555555556e-06, 'epoch': 0.776}
{'eval_valid_loss': 0.94873046875, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.537, 'eval_valid_steps_per_second': 278.384, 'epoch': 0.776}
{'loss': 1.0036, 'grad_norm': 3.694688191814304, 'learning_rate': 8.958444444444444e-06, 'epoch': 0.7764}
{'loss': 0.9939, 'grad_norm': 3.5674860010226634, 'learning_rate': 8.957333333333335e-06, 'epoch': 0.7768}
{'loss': 1.0034, 'grad_norm': 3.3393084359179173, 'learning_rate': 8.956222222222224e-06, 'epoch': 0.7772}
{'loss': 0.9934, 'grad_norm': 3.285424909821214, 'learning_rate': 8.955111111111112e-06, 'epoch': 0.7776}
{'loss': 0.9968, 'grad_norm': 3.5790889557620322, 'learning_rate': 8.954000000000001e-06, 'epoch': 0.778}
{'loss': 0.9981, 'grad_norm': 3.972514597299979, 'learning_rate': 8.95288888888889e-06, 'epoch': 0.7784}
{'loss': 0.9987, 'grad_norm': 3.4612944959189242, 'learning_rate': 8.951777777777778e-06, 'epoch': 0.7788}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.9939, 'grad_norm': 3.211481284992577, 'learning_rate': 8.950666666666667e-06, 'epoch': 0.7792}
{'loss': 0.9958, 'grad_norm': 3.735332110983872, 'learning_rate': 8.949555555555556e-06, 'epoch': 0.7796}
{'loss': 0.9947, 'grad_norm': 3.8563511079365442, 'learning_rate': 8.948444444444445e-06, 'epoch': 0.78}
{'eval_valid_loss': 0.94970703125, 'eval_valid_runtime': 0.0906, 'eval_valid_samples_per_second': 1103.799, 'eval_valid_steps_per_second': 275.95, 'epoch': 0.78}
{'loss': 0.9899, 'grad_norm': 3.622205289998261, 'learning_rate': 8.947333333333335e-06, 'epoch': 0.7804}
{'loss': 0.9957, 'grad_norm': 3.3298440353137817, 'learning_rate': 8.946222222222222e-06, 'epoch': 0.7808}
{'loss': 0.9887, 'grad_norm': 3.6795914104898433, 'learning_rate': 8.945111111111112e-06, 'epoch': 0.7812}
{'loss': 0.9939, 'grad_norm': 3.3807847110078075, 'learning_rate': 8.944000000000001e-06, 'epoch': 0.7816}
{'loss': 0.9954, 'grad_norm': 3.4875528871634804, 'learning_rate': 8.94288888888889e-06, 'epoch': 0.782}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 1.009, 'grad_norm': 3.5004446479503293, 'learning_rate': 8.941777777777779e-06, 'epoch': 0.7824}
{'loss': 1.01, 'grad_norm': 3.7523080716491, 'learning_rate': 8.940666666666667e-06, 'epoch': 0.7828}
{'loss': 0.996, 'grad_norm': 3.4422500563109044, 'learning_rate': 8.939555555555556e-06, 'epoch': 0.7832}
{'loss': 1.0018, 'grad_norm': 3.2913328385785094, 'learning_rate': 8.938444444444445e-06, 'epoch': 0.7836}
{'loss': 0.9838, 'grad_norm': 3.883217036398181, 'learning_rate': 8.937333333333335e-06, 'epoch': 0.784}
{'eval_valid_loss': 0.9462890625, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.83, 'eval_valid_steps_per_second': 278.457, 'epoch': 0.784}
{'loss': 0.9759, 'grad_norm': 3.3134775248873813, 'learning_rate': 8.936222222222222e-06, 'epoch': 0.7844}
{'loss': 0.9829, 'grad_norm': 3.26897089268354, 'learning_rate': 8.935111111111112e-06, 'epoch': 0.7848}
{'loss': 0.9883, 'grad_norm': 3.454459066443037, 'learning_rate': 8.934000000000001e-06, 'epoch': 0.7852}
{'loss': 1.0026, 'grad_norm': 3.4012358533599403, 'learning_rate': 8.93288888888889e-06, 'epoch': 0.7856}
{'loss': 0.9886, 'grad_norm': 3.5625796309154434, 'learning_rate': 8.931777777777779e-06, 'epoch': 0.786}
{'loss': 0.9925, 'grad_norm': 3.279975498270771, 'learning_rate': 8.930666666666667e-06, 'epoch': 0.7864}
{'loss': 0.9901, 'grad_norm': 3.8184825984652733, 'learning_rate': 8.929555555555556e-06, 'epoch': 0.7868}
{'loss': 0.9923, 'grad_norm': 3.6933260653409357, 'learning_rate': 8.928444444444445e-06, 'epoch': 0.7872}
{'loss': 0.9793, 'grad_norm': 3.0384524696622415, 'learning_rate': 8.927333333333335e-06, 'epoch': 0.7876}
{'loss': 0.9893, 'grad_norm': 3.9313618030023973, 'learning_rate': 8.926222222222222e-06, 'epoch': 0.788}
{'eval_valid_loss': 0.94140625, 'eval_valid_runtime': 0.0909, 'eval_valid_samples_per_second': 1100.307, 'eval_valid_steps_per_second': 275.077, 'epoch': 0.788}
{'loss': 0.9868, 'grad_norm': 3.493275389738286, 'learning_rate': 8.925111111111113e-06, 'epoch': 0.7884}
{'loss': 0.9928, 'grad_norm': 3.657350358281114, 'learning_rate': 8.924e-06, 'epoch': 0.7888}
{'loss': 0.9956, 'grad_norm': 3.2559807931540896, 'learning_rate': 8.92288888888889e-06, 'epoch': 0.7892}
{'loss': 0.9722, 'grad_norm': 3.199573725381971, 'learning_rate': 8.921777777777779e-06, 'epoch': 0.7896}
{'loss': 0.9625, 'grad_norm': 2.875026567999774, 'learning_rate': 8.920666666666667e-06, 'epoch': 0.79}
{'loss': 0.9972, 'grad_norm': 3.118313540571376, 'learning_rate': 8.919555555555556e-06, 'epoch': 0.7904}
{'loss': 0.9966, 'grad_norm': 3.529971262555493, 'learning_rate': 8.918444444444445e-06, 'epoch': 0.7908}
{'loss': 0.9892, 'grad_norm': 3.3179909432542245, 'learning_rate': 8.917333333333335e-06, 'epoch': 0.7912}
{'loss': 1.0149, 'grad_norm': 3.2319450188028673, 'learning_rate': 8.916222222222222e-06, 'epoch': 0.7916}
{'loss': 0.984, 'grad_norm': 3.901340642082035, 'learning_rate': 8.915111111111113e-06, 'epoch': 0.792}
{'eval_valid_loss': 0.94677734375, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1110.688, 'eval_valid_steps_per_second': 277.672, 'epoch': 0.792}
{'loss': 0.9832, 'grad_norm': 3.300028653092691, 'learning_rate': 8.914e-06, 'epoch': 0.7924}
{'loss': 0.9959, 'grad_norm': 3.774389458856178, 'learning_rate': 8.91288888888889e-06, 'epoch': 0.7928}
{'loss': 0.9778, 'grad_norm': 3.8750618037555755, 'learning_rate': 8.911777777777779e-06, 'epoch': 0.7932}
{'loss': 0.9909, 'grad_norm': 3.639442137083061, 'learning_rate': 8.910666666666668e-06, 'epoch': 0.7936}
{'loss': 0.995, 'grad_norm': 3.3048805829720287, 'learning_rate': 8.909555555555556e-06, 'epoch': 0.794}
{'loss': 0.9868, 'grad_norm': 3.2108870453594673, 'learning_rate': 8.908444444444445e-06, 'epoch': 0.7944}
{'loss': 0.9899, 'grad_norm': 3.854691803013225, 'learning_rate': 8.907333333333334e-06, 'epoch': 0.7948}
{'loss': 0.9825, 'grad_norm': 3.229648499790865, 'learning_rate': 8.906222222222222e-06, 'epoch': 0.7952}
{'loss': 0.9832, 'grad_norm': 2.980924938882072, 'learning_rate': 8.905111111111113e-06, 'epoch': 0.7956}
{'loss': 0.9835, 'grad_norm': 3.696320458786533, 'learning_rate': 8.904e-06, 'epoch': 0.796}
{'eval_valid_loss': 0.94775390625, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.741, 'eval_valid_steps_per_second': 278.935, 'epoch': 0.796}
{'loss': 0.9951, 'grad_norm': 4.009343098538056, 'learning_rate': 8.90288888888889e-06, 'epoch': 0.7964}
{'loss': 0.9941, 'grad_norm': 3.2709470393022704, 'learning_rate': 8.901777777777779e-06, 'epoch': 0.7968}
{'loss': 0.9959, 'grad_norm': 3.4787448162844843, 'learning_rate': 8.900666666666668e-06, 'epoch': 0.7972}
{'loss': 0.9726, 'grad_norm': 3.583318007051974, 'learning_rate': 8.899555555555556e-06, 'epoch': 0.7976}
{'loss': 0.9972, 'grad_norm': 4.07429240993138, 'learning_rate': 8.898444444444445e-06, 'epoch': 0.798}
{'loss': 0.9928, 'grad_norm': 3.5033344665571486, 'learning_rate': 8.897333333333334e-06, 'epoch': 0.7984}
{'loss': 1.0083, 'grad_norm': 3.2133805292783615, 'learning_rate': 8.896222222222222e-06, 'epoch': 0.7988}
{'loss': 0.9834, 'grad_norm': 3.222205890051423, 'learning_rate': 8.895111111111113e-06, 'epoch': 0.7992}
{'loss': 0.9861, 'grad_norm': 3.121185711972758, 'learning_rate': 8.894e-06, 'epoch': 0.7996}
{'loss': 0.9928, 'grad_norm': 3.435963373923797, 'learning_rate': 8.89288888888889e-06, 'epoch': 0.8}
{'eval_valid_loss': 0.943359375, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.781, 'eval_valid_steps_per_second': 278.195, 'epoch': 0.8}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9854, 'grad_norm': 3.311687216818956, 'learning_rate': 8.891777777777777e-06, 'epoch': 0.8004}
{'loss': 0.9881, 'grad_norm': 3.8736480077098228, 'learning_rate': 8.890666666666668e-06, 'epoch': 0.8008}
{'loss': 0.9768, 'grad_norm': 3.2801923318769215, 'learning_rate': 8.889555555555556e-06, 'epoch': 0.8012}
{'loss': 0.9767, 'grad_norm': 3.3753478436120155, 'learning_rate': 8.888555555555556e-06, 'epoch': 0.8016}
{'loss': 0.9981, 'grad_norm': 3.641954072977965, 'learning_rate': 8.887444444444445e-06, 'epoch': 0.802}
{'loss': 0.9995, 'grad_norm': 3.411377363724323, 'learning_rate': 8.886333333333335e-06, 'epoch': 0.8024}
{'loss': 0.984, 'grad_norm': 3.2630764996325983, 'learning_rate': 8.885222222222222e-06, 'epoch': 0.8028}
{'loss': 0.9818, 'grad_norm': 3.75780399028108, 'learning_rate': 8.884111111111112e-06, 'epoch': 0.8032}
{'loss': 0.9927, 'grad_norm': 3.1931033343873514, 'learning_rate': 8.883000000000001e-06, 'epoch': 0.8036}
{'loss': 0.9815, 'grad_norm': 3.410503828550169, 'learning_rate': 8.88188888888889e-06, 'epoch': 0.804}
{'eval_valid_loss': 0.94189453125, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1120.606, 'eval_valid_steps_per_second': 280.151, 'epoch': 0.804}
{'loss': 0.9859, 'grad_norm': 3.450729868573042, 'learning_rate': 8.880777777777779e-06, 'epoch': 0.8044}
{'loss': 0.9953, 'grad_norm': 3.209687948151688, 'learning_rate': 8.879666666666667e-06, 'epoch': 0.8048}
{'loss': 0.9919, 'grad_norm': 3.3290440181504546, 'learning_rate': 8.878555555555556e-06, 'epoch': 0.8052}
{'loss': 0.9703, 'grad_norm': 3.725829830160921, 'learning_rate': 8.877444444444445e-06, 'epoch': 0.8056}
{'loss': 0.9894, 'grad_norm': 3.163660593762804, 'learning_rate': 8.876333333333335e-06, 'epoch': 0.806}
{'loss': 0.9848, 'grad_norm': 3.6050060467133624, 'learning_rate': 8.875222222222222e-06, 'epoch': 0.8064}
{'loss': 0.9676, 'grad_norm': 3.757921118167742, 'learning_rate': 8.874111111111113e-06, 'epoch': 0.8068}
{'loss': 0.9821, 'grad_norm': 3.592344639524043, 'learning_rate': 8.873000000000001e-06, 'epoch': 0.8072}
{'loss': 0.9786, 'grad_norm': 3.4531253106993107, 'learning_rate': 8.87188888888889e-06, 'epoch': 0.8076}
{'loss': 0.9831, 'grad_norm': 3.2571906632949066, 'learning_rate': 8.870777777777779e-06, 'epoch': 0.808}
{'eval_valid_loss': 0.94140625, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1111.28, 'eval_valid_steps_per_second': 277.82, 'epoch': 0.808}
{'loss': 0.9768, 'grad_norm': 3.9351744217611144, 'learning_rate': 8.869666666666667e-06, 'epoch': 0.8084}
{'loss': 0.9758, 'grad_norm': 3.4868166968528884, 'learning_rate': 8.868555555555556e-06, 'epoch': 0.8088}
{'loss': 0.9865, 'grad_norm': 3.0732126977000833, 'learning_rate': 8.867444444444445e-06, 'epoch': 0.8092}
{'loss': 0.991, 'grad_norm': 3.455704937840117, 'learning_rate': 8.866333333333335e-06, 'epoch': 0.8096}
{'loss': 0.9846, 'grad_norm': 3.29621388826057, 'learning_rate': 8.865222222222222e-06, 'epoch': 0.81}
{'loss': 0.9849, 'grad_norm': 3.7029291334543664, 'learning_rate': 8.864111111111113e-06, 'epoch': 0.8104}
{'loss': 0.9943, 'grad_norm': 3.0796311658316067, 'learning_rate': 8.863e-06, 'epoch': 0.8108}
{'loss': 0.9796, 'grad_norm': 3.74964167154485, 'learning_rate': 8.86188888888889e-06, 'epoch': 0.8112}
{'loss': 1.0031, 'grad_norm': 3.7489278532235772, 'learning_rate': 8.860777777777779e-06, 'epoch': 0.8116}
{'loss': 0.9758, 'grad_norm': 3.4823572192391805, 'learning_rate': 8.859666666666668e-06, 'epoch': 0.812}
{'eval_valid_loss': 0.939453125, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1114.161, 'eval_valid_steps_per_second': 278.54, 'epoch': 0.812}
{'loss': 0.9964, 'grad_norm': 3.7635698210614015, 'learning_rate': 8.858555555555556e-06, 'epoch': 0.8124}
{'loss': 0.9901, 'grad_norm': 3.1890581380467578, 'learning_rate': 8.857444444444445e-06, 'epoch': 0.8128}
{'loss': 1.0, 'grad_norm': 3.6460373721475174, 'learning_rate': 8.856333333333335e-06, 'epoch': 0.8132}
{'loss': 0.991, 'grad_norm': 3.522040553293269, 'learning_rate': 8.855222222222222e-06, 'epoch': 0.8136}
{'loss': 0.9787, 'grad_norm': 3.4137883458171627, 'learning_rate': 8.854111111111113e-06, 'epoch': 0.814}
{'loss': 0.9888, 'grad_norm': 3.2146624841956224, 'learning_rate': 8.853e-06, 'epoch': 0.8144}
{'loss': 0.978, 'grad_norm': 3.498558335051117, 'learning_rate': 8.85188888888889e-06, 'epoch': 0.8148}
{'loss': 0.9804, 'grad_norm': 3.577409572791348, 'learning_rate': 8.850777777777779e-06, 'epoch': 0.8152}
{'loss': 0.9875, 'grad_norm': 3.8569836640431125, 'learning_rate': 8.849666666666668e-06, 'epoch': 0.8156}
{'loss': 0.97, 'grad_norm': 3.854096575262904, 'learning_rate': 8.848555555555556e-06, 'epoch': 0.816}
{'eval_valid_loss': 0.94140625, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.123, 'eval_valid_steps_per_second': 278.031, 'epoch': 0.816}
{'loss': 0.9891, 'grad_norm': 3.705364451507872, 'learning_rate': 8.847444444444445e-06, 'epoch': 0.8164}
{'loss': 0.9776, 'grad_norm': 3.2848549703549645, 'learning_rate': 8.846333333333334e-06, 'epoch': 0.8168}
{'loss': 0.9919, 'grad_norm': 3.3994556085841663, 'learning_rate': 8.845222222222222e-06, 'epoch': 0.8172}
{'loss': 0.9809, 'grad_norm': 3.490168574587623, 'learning_rate': 8.844111111111113e-06, 'epoch': 0.8176}
{'loss': 0.9875, 'grad_norm': 3.3154877108411025, 'learning_rate': 8.843e-06, 'epoch': 0.818}
{'loss': 0.9749, 'grad_norm': 3.6033748278726825, 'learning_rate': 8.84188888888889e-06, 'epoch': 0.8184}
{'loss': 1.0016, 'grad_norm': 3.070234768492613, 'learning_rate': 8.840777777777777e-06, 'epoch': 0.8188}
{'loss': 0.9947, 'grad_norm': 3.4984487178157515, 'learning_rate': 8.839666666666668e-06, 'epoch': 0.8192}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9908, 'grad_norm': 3.4344749925546827, 'learning_rate': 8.838555555555556e-06, 'epoch': 0.8196}
{'loss': 0.9829, 'grad_norm': 3.6487004024039833, 'learning_rate': 8.837444444444445e-06, 'epoch': 0.82}
{'eval_valid_loss': 0.9384765625, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.807, 'eval_valid_steps_per_second': 278.202, 'epoch': 0.82}
{'loss': 0.9812, 'grad_norm': 3.1798249011146282, 'learning_rate': 8.836333333333334e-06, 'epoch': 0.8204}
{'loss': 0.9935, 'grad_norm': 3.437292014679068, 'learning_rate': 8.835222222222223e-06, 'epoch': 0.8208}
{'loss': 0.9737, 'grad_norm': 3.201121601596732, 'learning_rate': 8.834111111111113e-06, 'epoch': 0.8212}
{'loss': 0.9758, 'grad_norm': 3.032347981698691, 'learning_rate': 8.833e-06, 'epoch': 0.8216}
{'loss': 0.989, 'grad_norm': 3.405026837437831, 'learning_rate': 8.83188888888889e-06, 'epoch': 0.822}
{'loss': 0.9942, 'grad_norm': 3.309584900392477, 'learning_rate': 8.830777777777777e-06, 'epoch': 0.8224}
{'loss': 0.9847, 'grad_norm': 3.3922864209849304, 'learning_rate': 8.829666666666668e-06, 'epoch': 0.8228}
{'loss': 0.9902, 'grad_norm': 3.0427859818265905, 'learning_rate': 8.828555555555557e-06, 'epoch': 0.8232}
{'loss': 0.9772, 'grad_norm': 3.723724900334267, 'learning_rate': 8.827444444444445e-06, 'epoch': 0.8236}
{'loss': 0.9862, 'grad_norm': 3.310120186554128, 'learning_rate': 8.826333333333334e-06, 'epoch': 0.824}
{'eval_valid_loss': 0.9375, 'eval_valid_runtime': 0.1718, 'eval_valid_samples_per_second': 582.165, 'eval_valid_steps_per_second': 145.541, 'epoch': 0.824}
{'loss': 0.9744, 'grad_norm': 3.2542575861741394, 'learning_rate': 8.825222222222223e-06, 'epoch': 0.8244}
{'loss': 0.994, 'grad_norm': 3.4402262194127387, 'learning_rate': 8.824111111111111e-06, 'epoch': 0.8248}
{'loss': 0.9876, 'grad_norm': 3.3132415607200705, 'learning_rate': 8.823e-06, 'epoch': 0.8252}
{'loss': 0.9733, 'grad_norm': 3.2219817224359453, 'learning_rate': 8.82188888888889e-06, 'epoch': 0.8256}
{'loss': 0.9976, 'grad_norm': 3.322111723854403, 'learning_rate': 8.820777777777777e-06, 'epoch': 0.826}
{'loss': 0.9771, 'grad_norm': 3.432290846612781, 'learning_rate': 8.819666666666668e-06, 'epoch': 0.8264}
{'loss': 0.9822, 'grad_norm': 3.7314321023027825, 'learning_rate': 8.818555555555557e-06, 'epoch': 0.8268}
{'loss': 0.9947, 'grad_norm': 3.0782951753310175, 'learning_rate': 8.817444444444445e-06, 'epoch': 0.8272}
{'loss': 0.9731, 'grad_norm': 3.6993146197132774, 'learning_rate': 8.816333333333334e-06, 'epoch': 0.8276}
{'loss': 0.9741, 'grad_norm': 3.3527265185722204, 'learning_rate': 8.815222222222223e-06, 'epoch': 0.828}
{'eval_valid_loss': 0.93798828125, 'eval_valid_runtime': 0.0905, 'eval_valid_samples_per_second': 1104.497, 'eval_valid_steps_per_second': 276.124, 'epoch': 0.828}
{'loss': 0.9853, 'grad_norm': 3.3037280770944517, 'learning_rate': 8.814111111111111e-06, 'epoch': 0.8284}
{'loss': 0.978, 'grad_norm': 3.3448422154389066, 'learning_rate': 8.813e-06, 'epoch': 0.8288}
{'loss': 0.9909, 'grad_norm': 3.2960920172463233, 'learning_rate': 8.81188888888889e-06, 'epoch': 0.8292}
{'loss': 0.9763, 'grad_norm': 3.355260440635368, 'learning_rate': 8.810777777777778e-06, 'epoch': 0.8296}
{'loss': 0.9803, 'grad_norm': 3.370089968292581, 'learning_rate': 8.809666666666668e-06, 'epoch': 0.83}
{'loss': 0.9833, 'grad_norm': 3.391346112051794, 'learning_rate': 8.808555555555555e-06, 'epoch': 0.8304}
{'loss': 0.9944, 'grad_norm': 3.846948564385372, 'learning_rate': 8.807444444444445e-06, 'epoch': 0.8308}
{'loss': 0.9821, 'grad_norm': 3.3712942211884496, 'learning_rate': 8.806333333333334e-06, 'epoch': 0.8312}
{'loss': 0.9905, 'grad_norm': 3.500369188728164, 'learning_rate': 8.805222222222223e-06, 'epoch': 0.8316}
{'loss': 0.9806, 'grad_norm': 3.3212194842055442, 'learning_rate': 8.804111111111112e-06, 'epoch': 0.832}
{'eval_valid_loss': 0.93798828125, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.853, 'eval_valid_steps_per_second': 278.463, 'epoch': 0.832}
{'loss': 0.9925, 'grad_norm': 3.822390001209989, 'learning_rate': 8.803e-06, 'epoch': 0.8324}
{'loss': 0.9819, 'grad_norm': 3.670162150428476, 'learning_rate': 8.80188888888889e-06, 'epoch': 0.8328}
{'loss': 0.9883, 'grad_norm': 3.4965123868946777, 'learning_rate': 8.800777777777778e-06, 'epoch': 0.8332}
{'loss': 0.9795, 'grad_norm': 3.3046014743006062, 'learning_rate': 8.799666666666668e-06, 'epoch': 0.8336}
{'loss': 0.9836, 'grad_norm': 3.4660025360881774, 'learning_rate': 8.798555555555555e-06, 'epoch': 0.834}
{'loss': 0.9906, 'grad_norm': 3.1554790442823704, 'learning_rate': 8.797444444444446e-06, 'epoch': 0.8344}
{'loss': 0.9901, 'grad_norm': 3.487920240493734, 'learning_rate': 8.796333333333334e-06, 'epoch': 0.8348}
{'loss': 0.9672, 'grad_norm': 3.4632687951158077, 'learning_rate': 8.795222222222223e-06, 'epoch': 0.8352}
{'loss': 0.982, 'grad_norm': 3.7377256261538467, 'learning_rate': 8.794111111111112e-06, 'epoch': 0.8356}
{'loss': 0.983, 'grad_norm': 3.1639234818424433, 'learning_rate': 8.793e-06, 'epoch': 0.836}
{'eval_valid_loss': 0.9404296875, 'eval_valid_runtime': 0.0908, 'eval_valid_samples_per_second': 1101.7, 'eval_valid_steps_per_second': 275.425, 'epoch': 0.836}
{'loss': 0.9923, 'grad_norm': 3.501783240261775, 'learning_rate': 8.791888888888889e-06, 'epoch': 0.8364}
{'loss': 0.9781, 'grad_norm': 3.7803635464539447, 'learning_rate': 8.790777777777778e-06, 'epoch': 0.8368}
{'loss': 0.9918, 'grad_norm': 3.3656776742877645, 'learning_rate': 8.789666666666668e-06, 'epoch': 0.8372}
{'loss': 0.9671, 'grad_norm': 3.088791302675369, 'learning_rate': 8.788555555555555e-06, 'epoch': 0.8376}
{'loss': 0.9973, 'grad_norm': 3.4308895012037643, 'learning_rate': 8.787444444444446e-06, 'epoch': 0.838}
{'loss': 0.9789, 'grad_norm': 3.3688078873567147, 'learning_rate': 8.786333333333334e-06, 'epoch': 0.8384}
{'loss': 0.9836, 'grad_norm': 3.4140677026495148, 'learning_rate': 8.785222222222223e-06, 'epoch': 0.8388}
{'loss': 0.9952, 'grad_norm': 3.5026288718258702, 'learning_rate': 8.784111111111112e-06, 'epoch': 0.8392}
{'loss': 0.9689, 'grad_norm': 3.3189537948421886, 'learning_rate': 8.783e-06, 'epoch': 0.8396}
{'loss': 0.9851, 'grad_norm': 3.6636829627003795, 'learning_rate': 8.78188888888889e-06, 'epoch': 0.84}
{'eval_valid_loss': 0.9365234375, 'eval_valid_runtime': 0.0903, 'eval_valid_samples_per_second': 1107.211, 'eval_valid_steps_per_second': 276.803, 'epoch': 0.84}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9719, 'grad_norm': 3.2521594007899606, 'learning_rate': 8.780777777777778e-06, 'epoch': 0.8404}
{'loss': 0.9817, 'grad_norm': 3.224461968586134, 'learning_rate': 8.779666666666668e-06, 'epoch': 0.8408}
{'loss': 0.9803, 'grad_norm': 3.286221743056276, 'learning_rate': 8.778555555555555e-06, 'epoch': 0.8412}
{'loss': 0.9828, 'grad_norm': 3.7280804243724397, 'learning_rate': 8.777555555555556e-06, 'epoch': 0.8416}
{'loss': 0.983, 'grad_norm': 3.5226359079348493, 'learning_rate': 8.776444444444445e-06, 'epoch': 0.842}
{'loss': 0.9857, 'grad_norm': 3.531723969645584, 'learning_rate': 8.775333333333334e-06, 'epoch': 0.8424}
{'loss': 0.9941, 'grad_norm': 3.413812265893332, 'learning_rate': 8.774222222222223e-06, 'epoch': 0.8428}
{'loss': 0.982, 'grad_norm': 3.2760321410282733, 'learning_rate': 8.773111111111113e-06, 'epoch': 0.8432}
{'loss': 0.9874, 'grad_norm': 3.580431765141203, 'learning_rate': 8.772e-06, 'epoch': 0.8436}
{'loss': 0.9962, 'grad_norm': 3.6379126976651857, 'learning_rate': 8.77088888888889e-06, 'epoch': 0.844}
{'eval_valid_loss': 0.9375, 'eval_valid_runtime': 0.0885, 'eval_valid_samples_per_second': 1129.462, 'eval_valid_steps_per_second': 282.366, 'epoch': 0.844}
{'loss': 0.9898, 'grad_norm': 3.801997492966485, 'learning_rate': 8.769777777777777e-06, 'epoch': 0.8444}
{'loss': 0.9844, 'grad_norm': 3.2762160969900838, 'learning_rate': 8.768666666666668e-06, 'epoch': 0.8448}
{'loss': 0.9802, 'grad_norm': 3.6667389031721713, 'learning_rate': 8.767555555555557e-06, 'epoch': 0.8452}
{'loss': 0.9912, 'grad_norm': 3.3267198918683603, 'learning_rate': 8.766444444444445e-06, 'epoch': 0.8456}
{'loss': 0.9716, 'grad_norm': 3.2652415547826483, 'learning_rate': 8.765333333333334e-06, 'epoch': 0.846}
{'loss': 0.9769, 'grad_norm': 3.5512983996789993, 'learning_rate': 8.764222222222223e-06, 'epoch': 0.8464}
{'loss': 0.9777, 'grad_norm': 3.2304047436352477, 'learning_rate': 8.763111111111111e-06, 'epoch': 0.8468}
{'loss': 0.9851, 'grad_norm': 3.368218620303051, 'learning_rate': 8.762e-06, 'epoch': 0.8472}
{'loss': 0.9865, 'grad_norm': 3.1503848639268224, 'learning_rate': 8.76088888888889e-06, 'epoch': 0.8476}
{'loss': 0.9783, 'grad_norm': 3.333023594333539, 'learning_rate': 8.759777777777778e-06, 'epoch': 0.848}
{'eval_valid_loss': 0.93603515625, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.898, 'eval_valid_steps_per_second': 278.474, 'epoch': 0.848}
{'loss': 0.9835, 'grad_norm': 3.349940683423317, 'learning_rate': 8.758666666666668e-06, 'epoch': 0.8484}
{'loss': 0.9831, 'grad_norm': 3.330939513188963, 'learning_rate': 8.757555555555555e-06, 'epoch': 0.8488}
{'loss': 0.962, 'grad_norm': 3.7380450262773546, 'learning_rate': 8.756444444444445e-06, 'epoch': 0.8492}
{'loss': 0.9672, 'grad_norm': 3.5086635755081614, 'learning_rate': 8.755333333333334e-06, 'epoch': 0.8496}
{'loss': 0.9898, 'grad_norm': 3.1678747405415706, 'learning_rate': 8.754222222222223e-06, 'epoch': 0.85}
{'loss': 0.9798, 'grad_norm': 3.2896680840721664, 'learning_rate': 8.753111111111112e-06, 'epoch': 0.8504}
{'loss': 0.9823, 'grad_norm': 3.9300875753977826, 'learning_rate': 8.752e-06, 'epoch': 0.8508}
{'loss': 1.0014, 'grad_norm': 3.3556172401337077, 'learning_rate': 8.75088888888889e-06, 'epoch': 0.8512}
{'loss': 0.9775, 'grad_norm': 2.9597884039463587, 'learning_rate': 8.749777777777778e-06, 'epoch': 0.8516}
{'loss': 0.9714, 'grad_norm': 3.2463160873636805, 'learning_rate': 8.748666666666668e-06, 'epoch': 0.852}
{'eval_valid_loss': 0.9365234375, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.783, 'eval_valid_steps_per_second': 279.946, 'epoch': 0.852}
{'loss': 0.9868, 'grad_norm': 3.202761838841505, 'learning_rate': 8.747555555555555e-06, 'epoch': 0.8524}
{'loss': 0.9716, 'grad_norm': 3.1943865602657664, 'learning_rate': 8.746444444444445e-06, 'epoch': 0.8528}
{'loss': 0.9812, 'grad_norm': 3.455272101978348, 'learning_rate': 8.745333333333334e-06, 'epoch': 0.8532}
{'loss': 0.9737, 'grad_norm': 3.130402548927519, 'learning_rate': 8.744222222222223e-06, 'epoch': 0.8536}
{'loss': 0.9768, 'grad_norm': 3.3458962014555858, 'learning_rate': 8.743111111111112e-06, 'epoch': 0.854}
{'loss': 0.9792, 'grad_norm': 3.4034005942378274, 'learning_rate': 8.742e-06, 'epoch': 0.8544}
{'loss': 0.9942, 'grad_norm': 3.666219487159193, 'learning_rate': 8.740888888888889e-06, 'epoch': 0.8548}
{'loss': 0.987, 'grad_norm': 3.5657052294358285, 'learning_rate': 8.739777777777778e-06, 'epoch': 0.8552}
{'loss': 0.9744, 'grad_norm': 3.945045099544553, 'learning_rate': 8.738666666666668e-06, 'epoch': 0.8556}
{'loss': 0.975, 'grad_norm': 3.591882104158055, 'learning_rate': 8.737555555555555e-06, 'epoch': 0.856}
{'eval_valid_loss': 0.93408203125, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1110.377, 'eval_valid_steps_per_second': 277.594, 'epoch': 0.856}
{'loss': 0.9807, 'grad_norm': 3.216944113849226, 'learning_rate': 8.736444444444446e-06, 'epoch': 0.8564}
{'loss': 0.9711, 'grad_norm': 3.4911617761098337, 'learning_rate': 8.735333333333334e-06, 'epoch': 0.8568}
{'loss': 0.9699, 'grad_norm': 3.089206663635483, 'learning_rate': 8.734222222222223e-06, 'epoch': 0.8572}
{'loss': 0.98, 'grad_norm': 3.6309584121371112, 'learning_rate': 8.733111111111112e-06, 'epoch': 0.8576}
{'loss': 0.9793, 'grad_norm': 3.0605193175223633, 'learning_rate': 8.732e-06, 'epoch': 0.858}
{'loss': 0.981, 'grad_norm': 3.511357833086489, 'learning_rate': 8.730888888888889e-06, 'epoch': 0.8584}
{'loss': 0.9776, 'grad_norm': 3.2624305176917905, 'learning_rate': 8.729777777777778e-06, 'epoch': 0.8588}
{'loss': 0.9761, 'grad_norm': 3.0406127153170006, 'learning_rate': 8.728666666666668e-06, 'epoch': 0.8592}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.9761, 'grad_norm': 3.3467964038471427, 'learning_rate': 8.727555555555555e-06, 'epoch': 0.8596}
{'loss': 0.965, 'grad_norm': 3.1028341861996274, 'learning_rate': 8.726444444444446e-06, 'epoch': 0.86}
{'eval_valid_loss': 0.9365234375, 'eval_valid_runtime': 0.0911, 'eval_valid_samples_per_second': 1097.373, 'eval_valid_steps_per_second': 274.343, 'epoch': 0.86}
{'loss': 0.9799, 'grad_norm': 3.4440132597807582, 'learning_rate': 8.725333333333334e-06, 'epoch': 0.8604}
{'loss': 0.9779, 'grad_norm': 3.38026286327577, 'learning_rate': 8.724222222222223e-06, 'epoch': 0.8608}
{'loss': 0.9735, 'grad_norm': 3.3016967677532363, 'learning_rate': 8.723111111111112e-06, 'epoch': 0.8612}
{'loss': 0.9706, 'grad_norm': 3.4328884576477376, 'learning_rate': 8.722e-06, 'epoch': 0.8616}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9925, 'grad_norm': 3.56927832519125, 'learning_rate': 8.72088888888889e-06, 'epoch': 0.862}
{'loss': 0.979, 'grad_norm': 3.5439100098185277, 'learning_rate': 8.719777777777778e-06, 'epoch': 0.8624}
{'loss': 0.9783, 'grad_norm': 3.409033215713714, 'learning_rate': 8.718666666666668e-06, 'epoch': 0.8628}
{'loss': 0.9749, 'grad_norm': 3.079822730468316, 'learning_rate': 8.717555555555555e-06, 'epoch': 0.8632}
{'loss': 0.9704, 'grad_norm': 3.4779750140429466, 'learning_rate': 8.716444444444446e-06, 'epoch': 0.8636}
{'loss': 0.9864, 'grad_norm': 3.5354555287844986, 'learning_rate': 8.715333333333334e-06, 'epoch': 0.864}
{'eval_valid_loss': 0.9326171875, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1109.722, 'eval_valid_steps_per_second': 277.43, 'epoch': 0.864}
{'loss': 0.9733, 'grad_norm': 3.453510055822277, 'learning_rate': 8.714222222222223e-06, 'epoch': 0.8644}
{'loss': 0.9701, 'grad_norm': 3.2859358525872917, 'learning_rate': 8.713111111111112e-06, 'epoch': 0.8648}
{'loss': 0.9661, 'grad_norm': 3.2123075286561353, 'learning_rate': 8.712e-06, 'epoch': 0.8652}
{'loss': 0.9557, 'grad_norm': 3.320769433388354, 'learning_rate': 8.71088888888889e-06, 'epoch': 0.8656}
{'loss': 0.9789, 'grad_norm': 3.0269378882268456, 'learning_rate': 8.709777777777778e-06, 'epoch': 0.866}
{'loss': 0.9771, 'grad_norm': 3.228864758232082, 'learning_rate': 8.708666666666667e-06, 'epoch': 0.8664}
{'loss': 0.9768, 'grad_norm': 3.457491517542697, 'learning_rate': 8.707555555555555e-06, 'epoch': 0.8668}
{'loss': 0.9693, 'grad_norm': 3.2890522428882707, 'learning_rate': 8.706444444444446e-06, 'epoch': 0.8672}
{'loss': 0.9778, 'grad_norm': 3.013636630254556, 'learning_rate': 8.705333333333335e-06, 'epoch': 0.8676}
{'loss': 0.9678, 'grad_norm': 3.038892412696979, 'learning_rate': 8.704222222222223e-06, 'epoch': 0.868}
{'eval_valid_loss': 0.9365234375, 'eval_valid_runtime': 0.0967, 'eval_valid_samples_per_second': 1034.361, 'eval_valid_steps_per_second': 258.59, 'epoch': 0.868}
{'loss': 0.978, 'grad_norm': 3.4435504204714595, 'learning_rate': 8.703111111111112e-06, 'epoch': 0.8684}
{'loss': 0.9765, 'grad_norm': 3.460845515490508, 'learning_rate': 8.702e-06, 'epoch': 0.8688}
{'loss': 0.9679, 'grad_norm': 3.1540068879889853, 'learning_rate': 8.70088888888889e-06, 'epoch': 0.8692}
{'loss': 0.966, 'grad_norm': 3.019565300792655, 'learning_rate': 8.699777777777778e-06, 'epoch': 0.8696}
{'loss': 0.9755, 'grad_norm': 3.112511150692495, 'learning_rate': 8.698666666666667e-06, 'epoch': 0.87}
{'loss': 0.9768, 'grad_norm': 3.6339979801130378, 'learning_rate': 8.697555555555556e-06, 'epoch': 0.8704}
{'loss': 0.9818, 'grad_norm': 3.181153945746791, 'learning_rate': 8.696444444444446e-06, 'epoch': 0.8708}
{'loss': 0.9759, 'grad_norm': 3.5608208865278415, 'learning_rate': 8.695333333333335e-06, 'epoch': 0.8712}
{'loss': 0.9685, 'grad_norm': 3.1292376682050396, 'learning_rate': 8.694222222222223e-06, 'epoch': 0.8716}
{'loss': 0.9777, 'grad_norm': 3.3973291716389813, 'learning_rate': 8.693111111111112e-06, 'epoch': 0.872}
{'eval_valid_loss': 0.93359375, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1122.928, 'eval_valid_steps_per_second': 280.732, 'epoch': 0.872}
{'loss': 0.9689, 'grad_norm': 3.141731674540361, 'learning_rate': 8.692e-06, 'epoch': 0.8724}
{'loss': 0.9903, 'grad_norm': 3.2668369818504672, 'learning_rate': 8.69088888888889e-06, 'epoch': 0.8728}
{'loss': 0.9714, 'grad_norm': 3.3792128760066777, 'learning_rate': 8.689777777777778e-06, 'epoch': 0.8732}
{'loss': 0.9712, 'grad_norm': 3.348823210431089, 'learning_rate': 8.688666666666667e-06, 'epoch': 0.8736}
{'loss': 0.9593, 'grad_norm': 3.0663998063135174, 'learning_rate': 8.687555555555556e-06, 'epoch': 0.874}
{'loss': 0.9762, 'grad_norm': 3.058807209697401, 'learning_rate': 8.686444444444444e-06, 'epoch': 0.8744}
{'loss': 0.9793, 'grad_norm': 3.0785980926066396, 'learning_rate': 8.685333333333335e-06, 'epoch': 0.8748}
{'loss': 0.969, 'grad_norm': 3.609972140635302, 'learning_rate': 8.684222222222223e-06, 'epoch': 0.8752}
{'loss': 0.9801, 'grad_norm': 3.074145218043699, 'learning_rate': 8.683111111111112e-06, 'epoch': 0.8756}
{'loss': 0.984, 'grad_norm': 3.5517200869679857, 'learning_rate': 8.682000000000001e-06, 'epoch': 0.876}
{'eval_valid_loss': 0.93359375, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.02, 'eval_valid_steps_per_second': 279.005, 'epoch': 0.876}
{'loss': 0.9665, 'grad_norm': 3.365100249076767, 'learning_rate': 8.68088888888889e-06, 'epoch': 0.8764}
{'loss': 0.9705, 'grad_norm': 3.5134661764311272, 'learning_rate': 8.679777777777778e-06, 'epoch': 0.8768}
{'loss': 0.9795, 'grad_norm': 3.0403742472177147, 'learning_rate': 8.678666666666667e-06, 'epoch': 0.8772}
{'loss': 0.9703, 'grad_norm': 3.3769417316339436, 'learning_rate': 8.677555555555556e-06, 'epoch': 0.8776}
{'loss': 0.9674, 'grad_norm': 3.4866496047425493, 'learning_rate': 8.676444444444444e-06, 'epoch': 0.878}
{'loss': 0.9808, 'grad_norm': 3.132489299512338, 'learning_rate': 8.675333333333335e-06, 'epoch': 0.8784}
{'loss': 0.9835, 'grad_norm': 3.3414439868379198, 'learning_rate': 8.674222222222224e-06, 'epoch': 0.8788}
{'loss': 0.9746, 'grad_norm': 3.56931471289966, 'learning_rate': 8.673111111111112e-06, 'epoch': 0.8792}
{'loss': 0.9798, 'grad_norm': 3.058518195650219, 'learning_rate': 8.672000000000001e-06, 'epoch': 0.8796}
{'loss': 0.9825, 'grad_norm': 3.6825476219790994, 'learning_rate': 8.67088888888889e-06, 'epoch': 0.88}
{'eval_valid_loss': 0.9326171875, 'eval_valid_runtime': 0.0905, 'eval_valid_samples_per_second': 1104.575, 'eval_valid_steps_per_second': 276.144, 'epoch': 0.88}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9696, 'grad_norm': 3.705977246828633, 'learning_rate': 8.669777777777778e-06, 'epoch': 0.8804}
{'loss': 0.9698, 'grad_norm': 3.2700644802303978, 'learning_rate': 8.668666666666667e-06, 'epoch': 0.8808}
{'loss': 0.9776, 'grad_norm': 3.3305033124275267, 'learning_rate': 8.667555555555556e-06, 'epoch': 0.8812}
{'loss': 0.9719, 'grad_norm': 3.2797806583457185, 'learning_rate': 8.666555555555555e-06, 'epoch': 0.8816}
{'loss': 0.9731, 'grad_norm': 3.4593834326416455, 'learning_rate': 8.665444444444446e-06, 'epoch': 0.882}
{'loss': 0.9711, 'grad_norm': 3.3450308245078255, 'learning_rate': 8.664333333333334e-06, 'epoch': 0.8824}
{'loss': 0.9774, 'grad_norm': 3.2935635175520424, 'learning_rate': 8.663222222222223e-06, 'epoch': 0.8828}
{'loss': 0.9762, 'grad_norm': 3.587705039435282, 'learning_rate': 8.662111111111112e-06, 'epoch': 0.8832}
{'loss': 0.9777, 'grad_norm': 3.5720946290317284, 'learning_rate': 8.661e-06, 'epoch': 0.8836}
{'loss': 0.977, 'grad_norm': 3.426534467687718, 'learning_rate': 8.65988888888889e-06, 'epoch': 0.884}
{'eval_valid_loss': 0.931640625, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1109.149, 'eval_valid_steps_per_second': 277.287, 'epoch': 0.884}
{'loss': 0.9789, 'grad_norm': 3.0730624611530697, 'learning_rate': 8.658777777777778e-06, 'epoch': 0.8844}
{'loss': 0.9859, 'grad_norm': 3.894007610292056, 'learning_rate': 8.657666666666667e-06, 'epoch': 0.8848}
{'loss': 0.982, 'grad_norm': 3.5955586401791644, 'learning_rate': 8.656555555555555e-06, 'epoch': 0.8852}
{'loss': 0.9829, 'grad_norm': 3.104602092994619, 'learning_rate': 8.655444444444446e-06, 'epoch': 0.8856}
{'loss': 0.9786, 'grad_norm': 3.3549555074012756, 'learning_rate': 8.654333333333335e-06, 'epoch': 0.886}
{'loss': 0.9749, 'grad_norm': 3.4604445681211695, 'learning_rate': 8.653222222222223e-06, 'epoch': 0.8864}
{'loss': 0.9679, 'grad_norm': 3.467174636246214, 'learning_rate': 8.652111111111112e-06, 'epoch': 0.8868}
{'loss': 0.9797, 'grad_norm': 3.3293108014132478, 'learning_rate': 8.651e-06, 'epoch': 0.8872}
{'loss': 0.9629, 'grad_norm': 3.5245590708627663, 'learning_rate': 8.64988888888889e-06, 'epoch': 0.8876}
{'loss': 0.9785, 'grad_norm': 3.509642210899065, 'learning_rate': 8.648777777777778e-06, 'epoch': 0.888}
{'eval_valid_loss': 0.93115234375, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.552, 'eval_valid_steps_per_second': 278.638, 'epoch': 0.888}
{'loss': 0.9854, 'grad_norm': 3.148436051739911, 'learning_rate': 8.647666666666667e-06, 'epoch': 0.8884}
{'loss': 0.9773, 'grad_norm': 3.332898356945399, 'learning_rate': 8.646555555555556e-06, 'epoch': 0.8888}
{'loss': 0.9577, 'grad_norm': 3.733745968883217, 'learning_rate': 8.645444444444446e-06, 'epoch': 0.8892}
{'loss': 0.9705, 'grad_norm': 3.6577185347007184, 'learning_rate': 8.644333333333335e-06, 'epoch': 0.8896}
{'loss': 0.9911, 'grad_norm': 3.4103168742486427, 'learning_rate': 8.643222222222223e-06, 'epoch': 0.89}
{'loss': 0.9836, 'grad_norm': 3.7814244947288485, 'learning_rate': 8.642111111111112e-06, 'epoch': 0.8904}
{'loss': 0.9718, 'grad_norm': 3.5608520209651346, 'learning_rate': 8.641e-06, 'epoch': 0.8908}
{'loss': 0.9732, 'grad_norm': 3.2646078140876185, 'learning_rate': 8.63988888888889e-06, 'epoch': 0.8912}
{'loss': 0.9764, 'grad_norm': 4.05699766642104, 'learning_rate': 8.638777777777778e-06, 'epoch': 0.8916}
{'loss': 0.9649, 'grad_norm': 3.590296188584392, 'learning_rate': 8.637666666666667e-06, 'epoch': 0.892}
{'eval_valid_loss': 0.93017578125, 'eval_valid_runtime': 0.0909, 'eval_valid_samples_per_second': 1100.37, 'eval_valid_steps_per_second': 275.093, 'epoch': 0.892}
{'loss': 0.9733, 'grad_norm': 3.295116488911302, 'learning_rate': 8.636555555555556e-06, 'epoch': 0.8924}
{'loss': 0.9742, 'grad_norm': 3.5489901986952823, 'learning_rate': 8.635444444444444e-06, 'epoch': 0.8928}
{'loss': 0.9629, 'grad_norm': 3.7025620881721095, 'learning_rate': 8.634333333333335e-06, 'epoch': 0.8932}
{'loss': 0.9641, 'grad_norm': 3.4739471971065496, 'learning_rate': 8.633222222222223e-06, 'epoch': 0.8936}
{'loss': 0.9687, 'grad_norm': 3.4908417022293925, 'learning_rate': 8.632111111111112e-06, 'epoch': 0.894}
{'loss': 0.9658, 'grad_norm': 3.258404849422891, 'learning_rate': 8.631000000000001e-06, 'epoch': 0.8944}
{'loss': 0.97, 'grad_norm': 3.404199753129306, 'learning_rate': 8.62988888888889e-06, 'epoch': 0.8948}
{'loss': 0.976, 'grad_norm': 3.6153518599963377, 'learning_rate': 8.628777777777778e-06, 'epoch': 0.8952}
{'loss': 0.9747, 'grad_norm': 3.424598054550604, 'learning_rate': 8.627666666666667e-06, 'epoch': 0.8956}
{'loss': 0.962, 'grad_norm': 3.4446066394982013, 'learning_rate': 8.626555555555556e-06, 'epoch': 0.896}
{'eval_valid_loss': 0.9345703125, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.554, 'eval_valid_steps_per_second': 278.888, 'epoch': 0.896}
{'loss': 0.9665, 'grad_norm': 3.08451094670795, 'learning_rate': 8.625444444444444e-06, 'epoch': 0.8964}
{'loss': 0.9798, 'grad_norm': 3.2222353018754646, 'learning_rate': 8.624333333333335e-06, 'epoch': 0.8968}
{'loss': 0.9772, 'grad_norm': 3.3633893446437595, 'learning_rate': 8.623222222222224e-06, 'epoch': 0.8972}
{'loss': 0.9765, 'grad_norm': 3.6305841235911527, 'learning_rate': 8.622111111111112e-06, 'epoch': 0.8976}
{'loss': 0.9851, 'grad_norm': 3.0384059186936283, 'learning_rate': 8.621000000000001e-06, 'epoch': 0.898}
{'loss': 0.9772, 'grad_norm': 3.6701672417684255, 'learning_rate': 8.61988888888889e-06, 'epoch': 0.8984}
{'loss': 0.9723, 'grad_norm': 3.3961155467960658, 'learning_rate': 8.618777777777778e-06, 'epoch': 0.8988}
{'loss': 0.9777, 'grad_norm': 3.0840445205206137, 'learning_rate': 8.617666666666667e-06, 'epoch': 0.8992}
{'loss': 0.9661, 'grad_norm': 3.725186587787324, 'learning_rate': 8.616555555555556e-06, 'epoch': 0.8996}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.9666, 'grad_norm': 3.4134564069046016, 'learning_rate': 8.615444444444445e-06, 'epoch': 0.9}
{'eval_valid_loss': 0.931640625, 'eval_valid_runtime': 0.0908, 'eval_valid_samples_per_second': 1101.674, 'eval_valid_steps_per_second': 275.418, 'epoch': 0.9}
{'loss': 0.9818, 'grad_norm': 3.396189522736533, 'learning_rate': 8.614333333333335e-06, 'epoch': 0.9004}
{'loss': 0.9672, 'grad_norm': 3.3221833646660826, 'learning_rate': 8.613222222222224e-06, 'epoch': 0.9008}
{'loss': 0.9815, 'grad_norm': 3.7528843912949212, 'learning_rate': 8.612111111111112e-06, 'epoch': 0.9012}
{'loss': 0.9728, 'grad_norm': 3.347451327271335, 'learning_rate': 8.611000000000001e-06, 'epoch': 0.9016}
{'loss': 0.959, 'grad_norm': 2.890037157238936, 'learning_rate': 8.60988888888889e-06, 'epoch': 0.902}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9768, 'grad_norm': 2.987921440054381, 'learning_rate': 8.608777777777778e-06, 'epoch': 0.9024}
{'loss': 0.9793, 'grad_norm': 3.695883902311659, 'learning_rate': 8.607666666666667e-06, 'epoch': 0.9028}
{'loss': 0.9791, 'grad_norm': 3.3905123239244577, 'learning_rate': 8.606555555555556e-06, 'epoch': 0.9032}
{'loss': 0.9805, 'grad_norm': 3.2652376757451176, 'learning_rate': 8.605444444444445e-06, 'epoch': 0.9036}
{'loss': 0.9765, 'grad_norm': 3.5837720844478476, 'learning_rate': 8.604333333333335e-06, 'epoch': 0.904}
{'eval_valid_loss': 0.927734375, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1106.037, 'eval_valid_steps_per_second': 276.509, 'epoch': 0.904}
{'loss': 0.9738, 'grad_norm': 3.419316959965785, 'learning_rate': 8.603222222222222e-06, 'epoch': 0.9044}
{'loss': 0.972, 'grad_norm': 3.578791560425704, 'learning_rate': 8.602111111111112e-06, 'epoch': 0.9048}
{'loss': 0.9888, 'grad_norm': 3.6053421973669924, 'learning_rate': 8.601000000000001e-06, 'epoch': 0.9052}
{'loss': 0.9737, 'grad_norm': 3.5676200280317762, 'learning_rate': 8.59988888888889e-06, 'epoch': 0.9056}
{'loss': 0.973, 'grad_norm': 3.235997175608582, 'learning_rate': 8.598777777777779e-06, 'epoch': 0.906}
{'loss': 0.9746, 'grad_norm': 4.235073102434967, 'learning_rate': 8.597666666666667e-06, 'epoch': 0.9064}
{'loss': 0.9642, 'grad_norm': 3.493788699855302, 'learning_rate': 8.596555555555556e-06, 'epoch': 0.9068}
{'loss': 0.9712, 'grad_norm': 3.626167602602295, 'learning_rate': 8.595444444444445e-06, 'epoch': 0.9072}
{'loss': 0.982, 'grad_norm': 3.4191892439363114, 'learning_rate': 8.594333333333335e-06, 'epoch': 0.9076}
{'loss': 0.9643, 'grad_norm': 3.7005354442303515, 'learning_rate': 8.593222222222222e-06, 'epoch': 0.908}
{'eval_valid_loss': 0.93017578125, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1111.686, 'eval_valid_steps_per_second': 277.922, 'epoch': 0.908}
{'loss': 0.9434, 'grad_norm': 3.183114335173143, 'learning_rate': 8.592111111111113e-06, 'epoch': 0.9084}
{'loss': 0.973, 'grad_norm': 3.509796753810594, 'learning_rate': 8.591000000000001e-06, 'epoch': 0.9088}
{'loss': 0.983, 'grad_norm': 3.253243652510513, 'learning_rate': 8.58988888888889e-06, 'epoch': 0.9092}
{'loss': 0.9822, 'grad_norm': 3.199067611757327, 'learning_rate': 8.588777777777779e-06, 'epoch': 0.9096}
{'loss': 0.9686, 'grad_norm': 3.3497662743837076, 'learning_rate': 8.587666666666667e-06, 'epoch': 0.91}
{'loss': 0.9773, 'grad_norm': 3.463120686849026, 'learning_rate': 8.586555555555556e-06, 'epoch': 0.9104}
{'loss': 0.9723, 'grad_norm': 3.667835115348554, 'learning_rate': 8.585444444444445e-06, 'epoch': 0.9108}
{'loss': 0.9758, 'grad_norm': 3.078664083765014, 'learning_rate': 8.584333333333335e-06, 'epoch': 0.9112}
{'loss': 0.9745, 'grad_norm': 3.3250106244885487, 'learning_rate': 8.583222222222222e-06, 'epoch': 0.9116}
{'loss': 0.9793, 'grad_norm': 3.8234192788921852, 'learning_rate': 8.582111111111113e-06, 'epoch': 0.912}
{'eval_valid_loss': 0.9296875, 'eval_valid_runtime': 0.0907, 'eval_valid_samples_per_second': 1102.954, 'eval_valid_steps_per_second': 275.739, 'epoch': 0.912}
{'loss': 0.9764, 'grad_norm': 3.525925208478305, 'learning_rate': 8.581e-06, 'epoch': 0.9124}
{'loss': 0.9608, 'grad_norm': 3.433982462573829, 'learning_rate': 8.57988888888889e-06, 'epoch': 0.9128}
{'loss': 0.9781, 'grad_norm': 3.1107391451295445, 'learning_rate': 8.578777777777779e-06, 'epoch': 0.9132}
{'loss': 0.9577, 'grad_norm': 3.2230472027005557, 'learning_rate': 8.577666666666667e-06, 'epoch': 0.9136}
{'loss': 0.976, 'grad_norm': 3.450911317239093, 'learning_rate': 8.576555555555556e-06, 'epoch': 0.914}
{'loss': 0.9752, 'grad_norm': 3.392274156655774, 'learning_rate': 8.575444444444445e-06, 'epoch': 0.9144}
{'loss': 0.9635, 'grad_norm': 3.3676470821264495, 'learning_rate': 8.574333333333335e-06, 'epoch': 0.9148}
{'loss': 0.9812, 'grad_norm': 3.648630108538005, 'learning_rate': 8.573222222222222e-06, 'epoch': 0.9152}
{'loss': 0.9754, 'grad_norm': 3.5919269913653773, 'learning_rate': 8.572111111111113e-06, 'epoch': 0.9156}
{'loss': 0.9718, 'grad_norm': 3.0348240634979082, 'learning_rate': 8.571e-06, 'epoch': 0.916}
{'eval_valid_loss': 0.9306640625, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.356, 'eval_valid_steps_per_second': 278.089, 'epoch': 0.916}
{'loss': 0.9721, 'grad_norm': 3.9441215908350262, 'learning_rate': 8.56988888888889e-06, 'epoch': 0.9164}
{'loss': 0.9723, 'grad_norm': 3.2367569081651197, 'learning_rate': 8.568777777777779e-06, 'epoch': 0.9168}
{'loss': 0.9861, 'grad_norm': 3.272290349718694, 'learning_rate': 8.567666666666668e-06, 'epoch': 0.9172}
{'loss': 0.9756, 'grad_norm': 3.103233185310387, 'learning_rate': 8.566555555555556e-06, 'epoch': 0.9176}
{'loss': 0.9686, 'grad_norm': 3.223031603599341, 'learning_rate': 8.565444444444445e-06, 'epoch': 0.918}
{'loss': 0.972, 'grad_norm': 3.1652959609298668, 'learning_rate': 8.564333333333334e-06, 'epoch': 0.9184}
{'loss': 0.9774, 'grad_norm': 3.7305776849789254, 'learning_rate': 8.563222222222222e-06, 'epoch': 0.9188}
{'loss': 0.9735, 'grad_norm': 3.340054966008874, 'learning_rate': 8.562111111111113e-06, 'epoch': 0.9192}
{'loss': 0.97, 'grad_norm': 3.527769365117079, 'learning_rate': 8.561e-06, 'epoch': 0.9196}
{'loss': 0.963, 'grad_norm': 3.532152929743503, 'learning_rate': 8.55988888888889e-06, 'epoch': 0.92}
{'eval_valid_loss': 0.9287109375, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.141, 'eval_valid_steps_per_second': 277.035, 'epoch': 0.92}
{'loss': 0.9742, 'grad_norm': 3.2463001594149525, 'learning_rate': 8.558777777777777e-06, 'epoch': 0.9204}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9696, 'grad_norm': 3.3789827547618296, 'learning_rate': 8.557666666666668e-06, 'epoch': 0.9208}
{'loss': 0.9873, 'grad_norm': 3.36611646182663, 'learning_rate': 8.556555555555556e-06, 'epoch': 0.9212}
{'loss': 0.9464, 'grad_norm': 3.139235981713599, 'learning_rate': 8.555555555555556e-06, 'epoch': 0.9216}
{'loss': 0.9643, 'grad_norm': 3.268634148946229, 'learning_rate': 8.554444444444445e-06, 'epoch': 0.922}
{'loss': 0.9579, 'grad_norm': 2.930882619597578, 'learning_rate': 8.553333333333333e-06, 'epoch': 0.9224}
{'loss': 0.9692, 'grad_norm': 3.7609913123623047, 'learning_rate': 8.552222222222222e-06, 'epoch': 0.9228}
{'loss': 0.9653, 'grad_norm': 3.4456548023370814, 'learning_rate': 8.551111111111112e-06, 'epoch': 0.9232}
{'loss': 0.9708, 'grad_norm': 3.064301768467921, 'learning_rate': 8.550000000000001e-06, 'epoch': 0.9236}
{'loss': 0.9683, 'grad_norm': 3.305965510137092, 'learning_rate': 8.54888888888889e-06, 'epoch': 0.924}
{'eval_valid_loss': 0.927734375, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.108, 'eval_valid_steps_per_second': 278.027, 'epoch': 0.924}
{'loss': 0.963, 'grad_norm': 3.3481415315625465, 'learning_rate': 8.547777777777779e-06, 'epoch': 0.9244}
{'loss': 0.974, 'grad_norm': 3.4026101727415377, 'learning_rate': 8.546666666666667e-06, 'epoch': 0.9248}
{'loss': 0.9566, 'grad_norm': 3.113714841278918, 'learning_rate': 8.545555555555556e-06, 'epoch': 0.9252}
{'loss': 0.9798, 'grad_norm': 3.0646093169475948, 'learning_rate': 8.544444444444445e-06, 'epoch': 0.9256}
{'loss': 0.9644, 'grad_norm': 3.3673785392221243, 'learning_rate': 8.543333333333333e-06, 'epoch': 0.926}
{'loss': 0.9792, 'grad_norm': 3.0332063575427726, 'learning_rate': 8.542222222222222e-06, 'epoch': 0.9264}
{'loss': 0.9495, 'grad_norm': 3.2064943967776505, 'learning_rate': 8.541111111111113e-06, 'epoch': 0.9268}
{'loss': 0.9743, 'grad_norm': 3.2542539413087166, 'learning_rate': 8.540000000000001e-06, 'epoch': 0.9272}
{'loss': 0.975, 'grad_norm': 3.266655745577235, 'learning_rate': 8.53888888888889e-06, 'epoch': 0.9276}
{'loss': 0.9636, 'grad_norm': 3.6430538654303275, 'learning_rate': 8.537777777777779e-06, 'epoch': 0.928}
{'eval_valid_loss': 0.92919921875, 'eval_valid_runtime': 0.091, 'eval_valid_samples_per_second': 1098.756, 'eval_valid_steps_per_second': 274.689, 'epoch': 0.928}
{'loss': 0.9629, 'grad_norm': 3.6637707006415643, 'learning_rate': 8.536666666666667e-06, 'epoch': 0.9284}
{'loss': 0.9757, 'grad_norm': 3.4615888822176437, 'learning_rate': 8.535555555555556e-06, 'epoch': 0.9288}
{'loss': 0.9557, 'grad_norm': 3.78201703890609, 'learning_rate': 8.534444444444445e-06, 'epoch': 0.9292}
{'loss': 0.9646, 'grad_norm': 3.499772830811549, 'learning_rate': 8.533333333333335e-06, 'epoch': 0.9296}
{'loss': 0.9752, 'grad_norm': 3.0958875149948186, 'learning_rate': 8.532222222222222e-06, 'epoch': 0.93}
{'loss': 0.981, 'grad_norm': 3.7333555306402006, 'learning_rate': 8.531111111111113e-06, 'epoch': 0.9304}
{'loss': 0.9688, 'grad_norm': 3.5006633708902988, 'learning_rate': 8.530000000000001e-06, 'epoch': 0.9308}
{'loss': 0.9732, 'grad_norm': 3.3211134539163027, 'learning_rate': 8.52888888888889e-06, 'epoch': 0.9312}
{'loss': 0.9617, 'grad_norm': 3.5055696075464793, 'learning_rate': 8.527777777777779e-06, 'epoch': 0.9316}
{'loss': 0.9605, 'grad_norm': 3.689354963395221, 'learning_rate': 8.526666666666667e-06, 'epoch': 0.932}
{'eval_valid_loss': 0.92626953125, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1110.309, 'eval_valid_steps_per_second': 277.577, 'epoch': 0.932}
{'loss': 0.9651, 'grad_norm': 3.2426851005632815, 'learning_rate': 8.525555555555556e-06, 'epoch': 0.9324}
{'loss': 0.9688, 'grad_norm': 3.6477990418879265, 'learning_rate': 8.524444444444445e-06, 'epoch': 0.9328}
{'loss': 0.9567, 'grad_norm': 3.1412701110256784, 'learning_rate': 8.523333333333335e-06, 'epoch': 0.9332}
{'loss': 0.9503, 'grad_norm': 3.2080456039083955, 'learning_rate': 8.522222222222222e-06, 'epoch': 0.9336}
{'loss': 0.9684, 'grad_norm': 3.824122598158213, 'learning_rate': 8.521111111111113e-06, 'epoch': 0.934}
{'loss': 0.9584, 'grad_norm': 3.4926105729432426, 'learning_rate': 8.52e-06, 'epoch': 0.9344}
{'loss': 0.9741, 'grad_norm': 3.391247951395869, 'learning_rate': 8.51888888888889e-06, 'epoch': 0.9348}
{'loss': 0.9572, 'grad_norm': 3.252726484105661, 'learning_rate': 8.517777777777779e-06, 'epoch': 0.9352}
{'loss': 0.9637, 'grad_norm': 2.9939784773127545, 'learning_rate': 8.516666666666668e-06, 'epoch': 0.9356}
{'loss': 0.9752, 'grad_norm': 3.240706286978867, 'learning_rate': 8.515555555555556e-06, 'epoch': 0.936}
{'eval_valid_loss': 0.9287109375, 'eval_valid_runtime': 0.1686, 'eval_valid_samples_per_second': 593.11, 'eval_valid_steps_per_second': 148.278, 'epoch': 0.936}
{'loss': 0.9784, 'grad_norm': 2.9990492744786197, 'learning_rate': 8.514444444444445e-06, 'epoch': 0.9364}
{'loss': 0.974, 'grad_norm': 3.0977494465108055, 'learning_rate': 8.513333333333335e-06, 'epoch': 0.9368}
{'loss': 0.9622, 'grad_norm': 3.535385166575622, 'learning_rate': 8.512222222222222e-06, 'epoch': 0.9372}
{'loss': 0.978, 'grad_norm': 3.479436171062304, 'learning_rate': 8.511111111111113e-06, 'epoch': 0.9376}
{'loss': 0.9659, 'grad_norm': 3.2873939939458077, 'learning_rate': 8.51e-06, 'epoch': 0.938}
{'loss': 0.9763, 'grad_norm': 3.366455795196163, 'learning_rate': 8.50888888888889e-06, 'epoch': 0.9384}
{'loss': 0.9764, 'grad_norm': 3.3731335052869293, 'learning_rate': 8.507777777777779e-06, 'epoch': 0.9388}
{'loss': 0.9647, 'grad_norm': 3.0166772973064737, 'learning_rate': 8.506666666666668e-06, 'epoch': 0.9392}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.9607, 'grad_norm': 3.2377520452060975, 'learning_rate': 8.505555555555556e-06, 'epoch': 0.9396}
{'loss': 0.9645, 'grad_norm': 3.5921694389642536, 'learning_rate': 8.504444444444445e-06, 'epoch': 0.94}
{'eval_valid_loss': 0.92529296875, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1111.828, 'eval_valid_steps_per_second': 277.957, 'epoch': 0.94}
{'loss': 0.9578, 'grad_norm': 3.0721619837716663, 'learning_rate': 8.503333333333334e-06, 'epoch': 0.9404}
{'loss': 0.9715, 'grad_norm': 3.148423149890238, 'learning_rate': 8.502222222222223e-06, 'epoch': 0.9408}
{'loss': 0.9424, 'grad_norm': 3.320786879815664, 'learning_rate': 8.501111111111113e-06, 'epoch': 0.9412}
{'loss': 0.9664, 'grad_norm': 3.438920325841855, 'learning_rate': 8.5e-06, 'epoch': 0.9416}
{'loss': 0.9494, 'grad_norm': 3.1843085019072435, 'learning_rate': 8.49888888888889e-06, 'epoch': 0.942}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9709, 'grad_norm': 3.263118265477837, 'learning_rate': 8.497777777777777e-06, 'epoch': 0.9424}
{'loss': 0.9812, 'grad_norm': 3.2694359130814687, 'learning_rate': 8.496666666666668e-06, 'epoch': 0.9428}
{'loss': 0.9624, 'grad_norm': 3.19276439478548, 'learning_rate': 8.495555555555556e-06, 'epoch': 0.9432}
{'loss': 0.9652, 'grad_norm': 3.2921236340701387, 'learning_rate': 8.494444444444445e-06, 'epoch': 0.9436}
{'loss': 0.9652, 'grad_norm': 3.3080214238487855, 'learning_rate': 8.493333333333334e-06, 'epoch': 0.944}
{'eval_valid_loss': 0.92578125, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.581, 'eval_valid_steps_per_second': 277.145, 'epoch': 0.944}
{'loss': 0.9743, 'grad_norm': 3.2342940057758236, 'learning_rate': 8.492222222222223e-06, 'epoch': 0.9444}
{'loss': 0.9585, 'grad_norm': 3.5854097447540005, 'learning_rate': 8.491111111111113e-06, 'epoch': 0.9448}
{'loss': 0.964, 'grad_norm': 3.1634395881004567, 'learning_rate': 8.49e-06, 'epoch': 0.9452}
{'loss': 0.9715, 'grad_norm': 3.4917513513120673, 'learning_rate': 8.48888888888889e-06, 'epoch': 0.9456}
{'loss': 0.9624, 'grad_norm': 3.3986515372388397, 'learning_rate': 8.487777777777777e-06, 'epoch': 0.946}
{'loss': 0.9702, 'grad_norm': 3.3997554459695474, 'learning_rate': 8.486666666666668e-06, 'epoch': 0.9464}
{'loss': 0.9687, 'grad_norm': 3.2325598004467535, 'learning_rate': 8.485555555555557e-06, 'epoch': 0.9468}
{'loss': 0.964, 'grad_norm': 3.1768306767210235, 'learning_rate': 8.484444444444445e-06, 'epoch': 0.9472}
{'loss': 0.9793, 'grad_norm': 3.546911970441614, 'learning_rate': 8.483333333333334e-06, 'epoch': 0.9476}
{'loss': 0.9629, 'grad_norm': 3.364021538796457, 'learning_rate': 8.482222222222223e-06, 'epoch': 0.948}
{'eval_valid_loss': 0.92529296875, 'eval_valid_runtime': 0.0907, 'eval_valid_samples_per_second': 1103.036, 'eval_valid_steps_per_second': 275.759, 'epoch': 0.948}
{'loss': 0.9788, 'grad_norm': 3.3003715305992802, 'learning_rate': 8.481111111111111e-06, 'epoch': 0.9484}
{'loss': 0.9783, 'grad_norm': 3.2984279030678776, 'learning_rate': 8.48e-06, 'epoch': 0.9488}
{'loss': 0.9667, 'grad_norm': 3.2615324869480937, 'learning_rate': 8.47888888888889e-06, 'epoch': 0.9492}
{'loss': 0.9655, 'grad_norm': 3.277887101536889, 'learning_rate': 8.477777777777778e-06, 'epoch': 0.9496}
{'loss': 0.9668, 'grad_norm': 3.2159606458138494, 'learning_rate': 8.476666666666668e-06, 'epoch': 0.95}
{'loss': 0.9683, 'grad_norm': 3.250285191127358, 'learning_rate': 8.475555555555555e-06, 'epoch': 0.9504}
{'loss': 0.9805, 'grad_norm': 3.264093614925727, 'learning_rate': 8.474444444444445e-06, 'epoch': 0.9508}
{'loss': 0.9627, 'grad_norm': 3.39205913713756, 'learning_rate': 8.473333333333334e-06, 'epoch': 0.9512}
{'loss': 0.9662, 'grad_norm': 3.9372238940359847, 'learning_rate': 8.472222222222223e-06, 'epoch': 0.9516}
{'loss': 0.9679, 'grad_norm': 3.5115330427413873, 'learning_rate': 8.471111111111112e-06, 'epoch': 0.952}
{'eval_valid_loss': 0.9287109375, 'eval_valid_runtime': 0.0911, 'eval_valid_samples_per_second': 1098.083, 'eval_valid_steps_per_second': 274.521, 'epoch': 0.952}
{'loss': 0.9641, 'grad_norm': 3.9102986526091645, 'learning_rate': 8.47e-06, 'epoch': 0.9524}
{'loss': 0.9594, 'grad_norm': 3.106626417198774, 'learning_rate': 8.46888888888889e-06, 'epoch': 0.9528}
{'loss': 0.9644, 'grad_norm': 3.679673099692123, 'learning_rate': 8.467777777777778e-06, 'epoch': 0.9532}
{'loss': 0.9665, 'grad_norm': 3.6173765161946703, 'learning_rate': 8.466666666666668e-06, 'epoch': 0.9536}
{'loss': 0.9589, 'grad_norm': 3.2610032444656736, 'learning_rate': 8.465555555555555e-06, 'epoch': 0.954}
{'loss': 0.9629, 'grad_norm': 3.1267878853898603, 'learning_rate': 8.464444444444445e-06, 'epoch': 0.9544}
{'loss': 0.9602, 'grad_norm': 3.5939944971920252, 'learning_rate': 8.463333333333334e-06, 'epoch': 0.9548}
{'loss': 0.9773, 'grad_norm': 3.6514258715394448, 'learning_rate': 8.462222222222223e-06, 'epoch': 0.9552}
{'loss': 0.9684, 'grad_norm': 2.9751279182341235, 'learning_rate': 8.461111111111112e-06, 'epoch': 0.9556}
{'loss': 0.9682, 'grad_norm': 3.233442918568863, 'learning_rate': 8.46e-06, 'epoch': 0.956}
{'eval_valid_loss': 0.92578125, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.929, 'eval_valid_steps_per_second': 279.732, 'epoch': 0.956}
{'loss': 0.9535, 'grad_norm': 3.1366378374697095, 'learning_rate': 8.458888888888889e-06, 'epoch': 0.9564}
{'loss': 0.9699, 'grad_norm': 3.198026385998258, 'learning_rate': 8.457777777777778e-06, 'epoch': 0.9568}
{'loss': 0.9792, 'grad_norm': 2.9585605834036692, 'learning_rate': 8.456666666666668e-06, 'epoch': 0.9572}
{'loss': 0.9769, 'grad_norm': 3.2788017403537366, 'learning_rate': 8.455555555555555e-06, 'epoch': 0.9576}
{'loss': 0.9516, 'grad_norm': 3.132051047160039, 'learning_rate': 8.454444444444446e-06, 'epoch': 0.958}
{'loss': 0.971, 'grad_norm': 3.2967866460884245, 'learning_rate': 8.453333333333334e-06, 'epoch': 0.9584}
{'loss': 0.9652, 'grad_norm': 3.5856160370925982, 'learning_rate': 8.452222222222223e-06, 'epoch': 0.9588}
{'loss': 0.9623, 'grad_norm': 3.6234928977320844, 'learning_rate': 8.451111111111112e-06, 'epoch': 0.9592}
{'loss': 0.9616, 'grad_norm': 3.073311687661844, 'learning_rate': 8.45e-06, 'epoch': 0.9596}
{'loss': 0.9701, 'grad_norm': 3.235881380914319, 'learning_rate': 8.448888888888889e-06, 'epoch': 0.96}
{'eval_valid_loss': 0.923828125, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1106.046, 'eval_valid_steps_per_second': 276.512, 'epoch': 0.96}
{'loss': 0.9791, 'grad_norm': 3.657079292866807, 'learning_rate': 8.447777777777778e-06, 'epoch': 0.9604}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9708, 'grad_norm': 3.4937039012290225, 'learning_rate': 8.446666666666668e-06, 'epoch': 0.9608}
{'loss': 0.9646, 'grad_norm': 3.195794206791724, 'learning_rate': 8.445555555555555e-06, 'epoch': 0.9612}
{'loss': 0.9526, 'grad_norm': 3.2165173500040214, 'learning_rate': 8.444444444444446e-06, 'epoch': 0.9616}
{'loss': 0.968, 'grad_norm': 3.449171936477822, 'learning_rate': 8.443444444444445e-06, 'epoch': 0.962}
{'loss': 0.9522, 'grad_norm': 3.5631386201227175, 'learning_rate': 8.442333333333334e-06, 'epoch': 0.9624}
{'loss': 0.9696, 'grad_norm': 3.28547312220945, 'learning_rate': 8.441222222222223e-06, 'epoch': 0.9628}
{'loss': 0.9556, 'grad_norm': 3.103876639051359, 'learning_rate': 8.440111111111113e-06, 'epoch': 0.9632}
{'loss': 0.9696, 'grad_norm': 3.057037483752958, 'learning_rate': 8.439e-06, 'epoch': 0.9636}
{'loss': 0.9648, 'grad_norm': 3.402660447106823, 'learning_rate': 8.43788888888889e-06, 'epoch': 0.964}
{'eval_valid_loss': 0.9267578125, 'eval_valid_runtime': 0.0913, 'eval_valid_samples_per_second': 1095.099, 'eval_valid_steps_per_second': 273.775, 'epoch': 0.964}
{'loss': 0.955, 'grad_norm': 3.2213581406926792, 'learning_rate': 8.436777777777777e-06, 'epoch': 0.9644}
{'loss': 0.9618, 'grad_norm': 3.2902285939373814, 'learning_rate': 8.435666666666668e-06, 'epoch': 0.9648}
{'loss': 0.9569, 'grad_norm': 2.8990945713892278, 'learning_rate': 8.434555555555557e-06, 'epoch': 0.9652}
{'loss': 0.9649, 'grad_norm': 3.5774155792202924, 'learning_rate': 8.433444444444445e-06, 'epoch': 0.9656}
{'loss': 0.9804, 'grad_norm': 3.488209790974829, 'learning_rate': 8.432333333333334e-06, 'epoch': 0.966}
{'loss': 0.9621, 'grad_norm': 3.1534224393532404, 'learning_rate': 8.431222222222223e-06, 'epoch': 0.9664}
{'loss': 0.9586, 'grad_norm': 3.736896700951575, 'learning_rate': 8.430111111111113e-06, 'epoch': 0.9668}
{'loss': 0.9721, 'grad_norm': 3.4481444019506777, 'learning_rate': 8.429e-06, 'epoch': 0.9672}
{'loss': 0.9657, 'grad_norm': 3.2573286744688974, 'learning_rate': 8.42788888888889e-06, 'epoch': 0.9676}
{'loss': 0.957, 'grad_norm': 3.3478186622490984, 'learning_rate': 8.426777777777778e-06, 'epoch': 0.968}
{'eval_valid_loss': 0.92333984375, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1125.713, 'eval_valid_steps_per_second': 281.428, 'epoch': 0.968}
{'loss': 0.9572, 'grad_norm': 3.6580827146348107, 'learning_rate': 8.425666666666668e-06, 'epoch': 0.9684}
{'loss': 0.9728, 'grad_norm': 3.4166763129136912, 'learning_rate': 8.424555555555557e-06, 'epoch': 0.9688}
{'loss': 0.9656, 'grad_norm': 3.301586319295423, 'learning_rate': 8.423444444444445e-06, 'epoch': 0.9692}
{'loss': 0.9683, 'grad_norm': 3.2682970422431694, 'learning_rate': 8.422333333333334e-06, 'epoch': 0.9696}
{'loss': 0.9557, 'grad_norm': 3.0743858456793536, 'learning_rate': 8.421222222222223e-06, 'epoch': 0.97}
{'loss': 0.9684, 'grad_norm': 3.021489365912016, 'learning_rate': 8.420111111111111e-06, 'epoch': 0.9704}
{'loss': 0.965, 'grad_norm': 3.5445412843968715, 'learning_rate': 8.419e-06, 'epoch': 0.9708}
{'loss': 0.9663, 'grad_norm': 3.236907216523295, 'learning_rate': 8.41788888888889e-06, 'epoch': 0.9712}
{'loss': 0.9722, 'grad_norm': 3.2246771654550654, 'learning_rate': 8.416777777777778e-06, 'epoch': 0.9716}
{'loss': 0.9493, 'grad_norm': 3.2884023642717897, 'learning_rate': 8.415666666666668e-06, 'epoch': 0.972}
{'eval_valid_loss': 0.921875, 'eval_valid_runtime': 0.089, 'eval_valid_samples_per_second': 1123.066, 'eval_valid_steps_per_second': 280.767, 'epoch': 0.972}
{'loss': 0.9758, 'grad_norm': 3.480692709621938, 'learning_rate': 8.414555555555555e-06, 'epoch': 0.9724}
{'loss': 0.964, 'grad_norm': 3.2623805031505104, 'learning_rate': 8.413444444444445e-06, 'epoch': 0.9728}
{'loss': 0.9555, 'grad_norm': 3.3198397849675043, 'learning_rate': 8.412333333333334e-06, 'epoch': 0.9732}
{'loss': 0.9605, 'grad_norm': 3.4540504751614773, 'learning_rate': 8.411222222222223e-06, 'epoch': 0.9736}
{'loss': 0.9767, 'grad_norm': 3.1407125565199916, 'learning_rate': 8.410111111111112e-06, 'epoch': 0.974}
{'loss': 0.9565, 'grad_norm': 3.0750516759205455, 'learning_rate': 8.409e-06, 'epoch': 0.9744}
{'loss': 0.9576, 'grad_norm': 3.2721130769055313, 'learning_rate': 8.40788888888889e-06, 'epoch': 0.9748}
{'loss': 0.9583, 'grad_norm': 3.3992158108143777, 'learning_rate': 8.406777777777778e-06, 'epoch': 0.9752}
{'loss': 0.956, 'grad_norm': 3.2521866814766356, 'learning_rate': 8.405666666666668e-06, 'epoch': 0.9756}
{'loss': 0.9653, 'grad_norm': 3.723479589175571, 'learning_rate': 8.404555555555555e-06, 'epoch': 0.976}
{'eval_valid_loss': 0.921875, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1124.934, 'eval_valid_steps_per_second': 281.233, 'epoch': 0.976}
{'loss': 0.9573, 'grad_norm': 3.4166671018290047, 'learning_rate': 8.403444444444446e-06, 'epoch': 0.9764}
{'loss': 0.9639, 'grad_norm': 3.2075626801857755, 'learning_rate': 8.402333333333334e-06, 'epoch': 0.9768}
{'loss': 0.9625, 'grad_norm': 3.3485012345649956, 'learning_rate': 8.401222222222223e-06, 'epoch': 0.9772}
{'loss': 0.9577, 'grad_norm': 2.925825528674326, 'learning_rate': 8.400111111111112e-06, 'epoch': 0.9776}
{'loss': 0.966, 'grad_norm': 3.041668252857439, 'learning_rate': 8.399e-06, 'epoch': 0.978}
{'loss': 0.9607, 'grad_norm': 3.2905793758319617, 'learning_rate': 8.397888888888889e-06, 'epoch': 0.9784}
{'loss': 0.9727, 'grad_norm': 3.166564997287399, 'learning_rate': 8.396777777777778e-06, 'epoch': 0.9788}
{'loss': 0.967, 'grad_norm': 3.435189129288374, 'learning_rate': 8.395666666666668e-06, 'epoch': 0.9792}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.9637, 'grad_norm': 3.1438314760999404, 'learning_rate': 8.394555555555555e-06, 'epoch': 0.9796}
{'loss': 0.9693, 'grad_norm': 3.4832423554560203, 'learning_rate': 8.393444444444446e-06, 'epoch': 0.98}
{'eval_valid_loss': 0.91943359375, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.408, 'eval_valid_steps_per_second': 277.102, 'epoch': 0.98}
{'loss': 0.956, 'grad_norm': 3.1114771077513734, 'learning_rate': 8.392333333333334e-06, 'epoch': 0.9804}
{'loss': 0.961, 'grad_norm': 3.2128242833671212, 'learning_rate': 8.391222222222223e-06, 'epoch': 0.9808}
{'loss': 0.9461, 'grad_norm': 3.035495311989806, 'learning_rate': 8.390111111111112e-06, 'epoch': 0.9812}
{'loss': 0.966, 'grad_norm': 3.3801380624309587, 'learning_rate': 8.389e-06, 'epoch': 0.9816}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9553, 'grad_norm': 3.4496516024165125, 'learning_rate': 8.38788888888889e-06, 'epoch': 0.982}
{'loss': 0.9559, 'grad_norm': 3.543390124917926, 'learning_rate': 8.386777777777778e-06, 'epoch': 0.9824}
{'loss': 0.9717, 'grad_norm': 3.9124487102668906, 'learning_rate': 8.385666666666668e-06, 'epoch': 0.9828}
{'loss': 0.9657, 'grad_norm': 3.1744022730380332, 'learning_rate': 8.384555555555555e-06, 'epoch': 0.9832}
{'loss': 0.9668, 'grad_norm': 3.3665686833094615, 'learning_rate': 8.383444444444446e-06, 'epoch': 0.9836}
{'loss': 0.9658, 'grad_norm': 2.9621493192341593, 'learning_rate': 8.382333333333334e-06, 'epoch': 0.984}
{'eval_valid_loss': 0.9208984375, 'eval_valid_runtime': 0.0907, 'eval_valid_samples_per_second': 1102.534, 'eval_valid_steps_per_second': 275.634, 'epoch': 0.984}
{'loss': 0.96, 'grad_norm': 3.3953429152719083, 'learning_rate': 8.381222222222223e-06, 'epoch': 0.9844}
{'loss': 0.9644, 'grad_norm': 3.4460152149440186, 'learning_rate': 8.380111111111112e-06, 'epoch': 0.9848}
{'loss': 0.9515, 'grad_norm': 3.110594313893443, 'learning_rate': 8.379e-06, 'epoch': 0.9852}
{'loss': 0.9625, 'grad_norm': 3.115433651281464, 'learning_rate': 8.37788888888889e-06, 'epoch': 0.9856}
{'loss': 0.9596, 'grad_norm': 3.6255774695732117, 'learning_rate': 8.376777777777778e-06, 'epoch': 0.986}
{'loss': 0.9438, 'grad_norm': 3.0550927090976536, 'learning_rate': 8.375666666666667e-06, 'epoch': 0.9864}
{'loss': 0.9475, 'grad_norm': 3.1939564464605734, 'learning_rate': 8.374555555555555e-06, 'epoch': 0.9868}
{'loss': 0.9536, 'grad_norm': 3.1870198542668016, 'learning_rate': 8.373444444444446e-06, 'epoch': 0.9872}
{'loss': 0.973, 'grad_norm': 3.5970572802426712, 'learning_rate': 8.372333333333335e-06, 'epoch': 0.9876}
{'loss': 0.9668, 'grad_norm': 3.335995157712857, 'learning_rate': 8.371222222222223e-06, 'epoch': 0.988}
{'eval_valid_loss': 0.919921875, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1115.305, 'eval_valid_steps_per_second': 278.826, 'epoch': 0.988}
{'loss': 0.965, 'grad_norm': 3.085304600966233, 'learning_rate': 8.370111111111112e-06, 'epoch': 0.9884}
{'loss': 0.9665, 'grad_norm': 3.410075969996439, 'learning_rate': 8.369e-06, 'epoch': 0.9888}
{'loss': 0.9593, 'grad_norm': 3.5183400076181366, 'learning_rate': 8.36788888888889e-06, 'epoch': 0.9892}
{'loss': 0.9654, 'grad_norm': 3.3724946772206055, 'learning_rate': 8.366777777777778e-06, 'epoch': 0.9896}
{'loss': 0.9497, 'grad_norm': 3.020439297638709, 'learning_rate': 8.365666666666667e-06, 'epoch': 0.99}
{'loss': 0.9669, 'grad_norm': 3.326600141836395, 'learning_rate': 8.364555555555556e-06, 'epoch': 0.9904}
{'loss': 0.9611, 'grad_norm': 3.3759228186485433, 'learning_rate': 8.363444444444446e-06, 'epoch': 0.9908}
{'loss': 0.9616, 'grad_norm': 3.120074701393053, 'learning_rate': 8.362333333333335e-06, 'epoch': 0.9912}
{'loss': 0.9585, 'grad_norm': 3.3249841833096956, 'learning_rate': 8.361222222222223e-06, 'epoch': 0.9916}
{'loss': 0.958, 'grad_norm': 3.437715150862472, 'learning_rate': 8.360111111111112e-06, 'epoch': 0.992}
{'eval_valid_loss': 0.92138671875, 'eval_valid_runtime': 0.1893, 'eval_valid_samples_per_second': 528.337, 'eval_valid_steps_per_second': 132.084, 'epoch': 0.992}
{'loss': 0.9669, 'grad_norm': 3.141235168929027, 'learning_rate': 8.359e-06, 'epoch': 0.9924}
{'loss': 0.9646, 'grad_norm': 3.4197023278770464, 'learning_rate': 8.35788888888889e-06, 'epoch': 0.9928}
{'loss': 0.9617, 'grad_norm': 3.461590285555166, 'learning_rate': 8.356777777777778e-06, 'epoch': 0.9932}
{'loss': 0.9742, 'grad_norm': 3.4008397786778013, 'learning_rate': 8.355666666666667e-06, 'epoch': 0.9936}
{'loss': 0.9566, 'grad_norm': 3.2301550716342087, 'learning_rate': 8.354555555555556e-06, 'epoch': 0.994}
{'loss': 0.9564, 'grad_norm': 3.0568216288541676, 'learning_rate': 8.353444444444444e-06, 'epoch': 0.9944}
{'loss': 0.9473, 'grad_norm': 3.4803169294440264, 'learning_rate': 8.352333333333335e-06, 'epoch': 0.9948}
{'loss': 0.9629, 'grad_norm': 3.2209277794690254, 'learning_rate': 8.351222222222223e-06, 'epoch': 0.9952}
{'loss': 0.956, 'grad_norm': 3.2742218953260656, 'learning_rate': 8.350111111111112e-06, 'epoch': 0.9956}
{'loss': 0.9519, 'grad_norm': 3.2081432946800303, 'learning_rate': 8.349000000000001e-06, 'epoch': 0.996}
{'eval_valid_loss': 0.92138671875, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.167, 'eval_valid_steps_per_second': 279.292, 'epoch': 0.996}
{'loss': 0.9553, 'grad_norm': 3.3210115841195718, 'learning_rate': 8.34788888888889e-06, 'epoch': 0.9964}
{'loss': 0.9709, 'grad_norm': 3.5394625406084237, 'learning_rate': 8.346777777777778e-06, 'epoch': 0.9968}
{'loss': 0.9625, 'grad_norm': 3.325807382320997, 'learning_rate': 8.345666666666667e-06, 'epoch': 0.9972}
{'loss': 0.9533, 'grad_norm': 3.3636401048142717, 'learning_rate': 8.344555555555556e-06, 'epoch': 0.9976}
{'loss': 0.9551, 'grad_norm': 3.333837111471721, 'learning_rate': 8.343444444444444e-06, 'epoch': 0.998}
{'loss': 0.9434, 'grad_norm': 3.465748622536566, 'learning_rate': 8.342333333333335e-06, 'epoch': 0.9984}
{'loss': 0.9612, 'grad_norm': 3.7337703614370965, 'learning_rate': 8.341222222222224e-06, 'epoch': 0.9988}
{'loss': 0.9585, 'grad_norm': 3.176029735195377, 'learning_rate': 8.340111111111112e-06, 'epoch': 0.9992}
{'loss': 0.9699, 'grad_norm': 3.224597193823761, 'learning_rate': 8.339000000000001e-06, 'epoch': 0.9996}
{'loss': 0.9587, 'grad_norm': 3.4954414483722536, 'learning_rate': 8.33788888888889e-06, 'epoch': 1.0}
{'eval_valid_loss': 0.91796875, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.206, 'eval_valid_steps_per_second': 277.051, 'epoch': 1.0}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9589, 'grad_norm': 3.2273964069829173, 'learning_rate': 8.336777777777778e-06, 'epoch': 1.0004}
{'loss': 0.9711, 'grad_norm': 3.6710968248119413, 'learning_rate': 8.335666666666667e-06, 'epoch': 1.0008}
{'loss': 0.9452, 'grad_norm': 3.186788189639494, 'learning_rate': 8.334555555555556e-06, 'epoch': 1.0012}
{'loss': 0.9463, 'grad_norm': 3.717731240115464, 'learning_rate': 8.333444444444445e-06, 'epoch': 1.0016}
{'loss': 0.9546, 'grad_norm': 3.510734954953868, 'learning_rate': 8.332444444444446e-06, 'epoch': 1.002}
{'loss': 0.9571, 'grad_norm': 3.2391450509842743, 'learning_rate': 8.331333333333333e-06, 'epoch': 1.0024}
{'loss': 0.963, 'grad_norm': 3.2582406053169835, 'learning_rate': 8.330222222222223e-06, 'epoch': 1.0028}
{'loss': 0.9572, 'grad_norm': 3.2004502829009738, 'learning_rate': 8.329111111111112e-06, 'epoch': 1.0032}
{'loss': 0.959, 'grad_norm': 3.354683567679117, 'learning_rate': 8.328e-06, 'epoch': 1.0036}
{'loss': 0.9539, 'grad_norm': 3.4832150448711436, 'learning_rate': 8.32688888888889e-06, 'epoch': 1.004}
{'eval_valid_loss': 0.9189453125, 'eval_valid_runtime': 0.0909, 'eval_valid_samples_per_second': 1100.102, 'eval_valid_steps_per_second': 275.026, 'epoch': 1.004}
{'loss': 0.9554, 'grad_norm': 3.1990561903943218, 'learning_rate': 8.325777777777778e-06, 'epoch': 1.0044}
{'loss': 0.9643, 'grad_norm': 2.9818703891620157, 'learning_rate': 8.324666666666668e-06, 'epoch': 1.0048}
{'loss': 0.9676, 'grad_norm': 3.2666840820837626, 'learning_rate': 8.323555555555555e-06, 'epoch': 1.0052}
{'loss': 0.9514, 'grad_norm': 2.818153030127514, 'learning_rate': 8.322444444444446e-06, 'epoch': 1.0056}
{'loss': 0.9749, 'grad_norm': 3.248226020905335, 'learning_rate': 8.321333333333333e-06, 'epoch': 1.006}
{'loss': 0.9487, 'grad_norm': 3.337439812964044, 'learning_rate': 8.320222222222223e-06, 'epoch': 1.0064}
{'loss': 0.9688, 'grad_norm': 3.2698406041228125, 'learning_rate': 8.319111111111112e-06, 'epoch': 1.0068}
{'loss': 0.9579, 'grad_norm': 3.1042358830221524, 'learning_rate': 8.318e-06, 'epoch': 1.0072}
{'loss': 0.962, 'grad_norm': 3.258547793685987, 'learning_rate': 8.31688888888889e-06, 'epoch': 1.0076}
{'loss': 0.9449, 'grad_norm': 3.2737428527293746, 'learning_rate': 8.315777777777778e-06, 'epoch': 1.008}
{'eval_valid_loss': 0.91943359375, 'eval_valid_runtime': 0.091, 'eval_valid_samples_per_second': 1099.485, 'eval_valid_steps_per_second': 274.871, 'epoch': 1.008}
{'loss': 0.9568, 'grad_norm': 3.3143762457073933, 'learning_rate': 8.314666666666667e-06, 'epoch': 1.0084}
{'loss': 0.9676, 'grad_norm': 3.2938987079470867, 'learning_rate': 8.313555555555556e-06, 'epoch': 1.0088}
{'loss': 0.9607, 'grad_norm': 3.501524729356891, 'learning_rate': 8.312444444444446e-06, 'epoch': 1.0092}
{'loss': 0.9646, 'grad_norm': 3.3949426417077184, 'learning_rate': 8.311333333333333e-06, 'epoch': 1.0096}
{'loss': 0.9661, 'grad_norm': 3.243864540326954, 'learning_rate': 8.310222222222223e-06, 'epoch': 1.01}
{'loss': 0.9633, 'grad_norm': 3.67458004433382, 'learning_rate': 8.309111111111112e-06, 'epoch': 1.0104}
{'loss': 0.954, 'grad_norm': 3.5076240539908397, 'learning_rate': 8.308e-06, 'epoch': 1.0108}
{'loss': 0.9635, 'grad_norm': 3.5195360935190556, 'learning_rate': 8.30688888888889e-06, 'epoch': 1.0112}
{'loss': 0.9476, 'grad_norm': 3.396231634773411, 'learning_rate': 8.305777777777778e-06, 'epoch': 1.0116}
{'loss': 0.9656, 'grad_norm': 3.246910947172961, 'learning_rate': 8.304666666666667e-06, 'epoch': 1.012}
{'eval_valid_loss': 0.919921875, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.43, 'eval_valid_steps_per_second': 279.107, 'epoch': 1.012}
{'loss': 0.9557, 'grad_norm': 3.554069666565546, 'learning_rate': 8.303555555555556e-06, 'epoch': 1.0124}
{'loss': 0.9511, 'grad_norm': 3.636128870473269, 'learning_rate': 8.302444444444446e-06, 'epoch': 1.0128}
{'loss': 0.9599, 'grad_norm': 3.186848368139735, 'learning_rate': 8.301333333333333e-06, 'epoch': 1.0132}
{'loss': 0.951, 'grad_norm': 3.01364892245024, 'learning_rate': 8.300222222222223e-06, 'epoch': 1.0136}
{'loss': 0.9686, 'grad_norm': 3.2406048324384127, 'learning_rate': 8.299111111111112e-06, 'epoch': 1.014}
{'loss': 0.9587, 'grad_norm': 3.0517748144057646, 'learning_rate': 8.298000000000001e-06, 'epoch': 1.0144}
{'loss': 0.9583, 'grad_norm': 3.457583048258215, 'learning_rate': 8.29688888888889e-06, 'epoch': 1.0148}
{'loss': 0.961, 'grad_norm': 3.3359986060663327, 'learning_rate': 8.295777777777778e-06, 'epoch': 1.0152}
{'loss': 0.959, 'grad_norm': 3.853643632745439, 'learning_rate': 8.294666666666667e-06, 'epoch': 1.0156}
{'loss': 0.9641, 'grad_norm': 3.697619663543528, 'learning_rate': 8.293555555555556e-06, 'epoch': 1.016}
{'eval_valid_loss': 0.91943359375, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.925, 'eval_valid_steps_per_second': 278.981, 'epoch': 1.016}
{'loss': 0.9367, 'grad_norm': 2.986484311437345, 'learning_rate': 8.292444444444444e-06, 'epoch': 1.0164}
{'loss': 0.9482, 'grad_norm': 3.1474304008363676, 'learning_rate': 8.291333333333335e-06, 'epoch': 1.0168}
{'loss': 0.9534, 'grad_norm': 3.0198502168553096, 'learning_rate': 8.290222222222224e-06, 'epoch': 1.0172}
{'loss': 0.9719, 'grad_norm': 3.1491137173587225, 'learning_rate': 8.289111111111112e-06, 'epoch': 1.0176}
{'loss': 0.9515, 'grad_norm': 3.2716811664662897, 'learning_rate': 8.288000000000001e-06, 'epoch': 1.018}
{'loss': 0.956, 'grad_norm': 3.411465300730028, 'learning_rate': 8.28688888888889e-06, 'epoch': 1.0184}
{'loss': 0.9448, 'grad_norm': 3.7667388218464266, 'learning_rate': 8.285777777777778e-06, 'epoch': 1.0188}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.9518, 'grad_norm': 3.2157591748210894, 'learning_rate': 8.284666666666667e-06, 'epoch': 1.0192}
{'loss': 0.9589, 'grad_norm': 3.337541243837666, 'learning_rate': 8.283555555555556e-06, 'epoch': 1.0196}
{'loss': 0.9568, 'grad_norm': 3.31986270326217, 'learning_rate': 8.282444444444445e-06, 'epoch': 1.02}
{'eval_valid_loss': 0.91748046875, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.563, 'eval_valid_steps_per_second': 278.891, 'epoch': 1.02}
{'loss': 0.9542, 'grad_norm': 3.303365781267346, 'learning_rate': 8.281333333333335e-06, 'epoch': 1.0204}
{'loss': 0.9502, 'grad_norm': 3.0479166646676226, 'learning_rate': 8.280222222222224e-06, 'epoch': 1.0208}
{'loss': 0.9658, 'grad_norm': 3.0217343148715763, 'learning_rate': 8.279111111111112e-06, 'epoch': 1.0212}
{'loss': 0.9574, 'grad_norm': 3.1719966451686084, 'learning_rate': 8.278000000000001e-06, 'epoch': 1.0216}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9575, 'grad_norm': 3.2719662439545063, 'learning_rate': 8.27688888888889e-06, 'epoch': 1.022}
{'loss': 0.9504, 'grad_norm': 3.299991770575117, 'learning_rate': 8.275777777777778e-06, 'epoch': 1.0224}
{'loss': 0.9564, 'grad_norm': 3.3513502485028486, 'learning_rate': 8.274666666666667e-06, 'epoch': 1.0228}
{'loss': 0.9489, 'grad_norm': 3.5530975978200634, 'learning_rate': 8.273555555555556e-06, 'epoch': 1.0232}
{'loss': 0.9453, 'grad_norm': 3.150984422568336, 'learning_rate': 8.272444444444445e-06, 'epoch': 1.0236}
{'loss': 0.9708, 'grad_norm': 3.33443999794672, 'learning_rate': 8.271333333333335e-06, 'epoch': 1.024}
{'eval_valid_loss': 0.9208984375, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.468, 'eval_valid_steps_per_second': 278.117, 'epoch': 1.024}
{'loss': 0.9536, 'grad_norm': 3.754870668944725, 'learning_rate': 8.270222222222222e-06, 'epoch': 1.0244}
{'loss': 0.9444, 'grad_norm': 3.3981586210332195, 'learning_rate': 8.269111111111112e-06, 'epoch': 1.0248}
{'loss': 0.96, 'grad_norm': 3.166203459733855, 'learning_rate': 8.268000000000001e-06, 'epoch': 1.0252}
{'loss': 0.9508, 'grad_norm': 2.9110042181472595, 'learning_rate': 8.26688888888889e-06, 'epoch': 1.0256}
{'loss': 0.9495, 'grad_norm': 3.457989894903475, 'learning_rate': 8.265777777777779e-06, 'epoch': 1.026}
{'loss': 0.9497, 'grad_norm': 3.354487834189088, 'learning_rate': 8.264666666666667e-06, 'epoch': 1.0264}
{'loss': 0.9607, 'grad_norm': 3.1337853250692254, 'learning_rate': 8.263555555555556e-06, 'epoch': 1.0268}
{'loss': 0.9649, 'grad_norm': 3.498590142828856, 'learning_rate': 8.262444444444445e-06, 'epoch': 1.0272}
{'loss': 0.9635, 'grad_norm': 2.983652846357167, 'learning_rate': 8.261333333333335e-06, 'epoch': 1.0276}
{'loss': 0.9555, 'grad_norm': 3.405295124427958, 'learning_rate': 8.260222222222222e-06, 'epoch': 1.028}
{'eval_valid_loss': 0.91748046875, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1115.139, 'eval_valid_steps_per_second': 278.785, 'epoch': 1.028}
{'loss': 0.9657, 'grad_norm': 3.2698256110578106, 'learning_rate': 8.259111111111113e-06, 'epoch': 1.0284}
{'loss': 0.9594, 'grad_norm': 3.429342448054168, 'learning_rate': 8.258000000000001e-06, 'epoch': 1.0288}
{'loss': 0.9724, 'grad_norm': 3.281908723014626, 'learning_rate': 8.25688888888889e-06, 'epoch': 1.0292}
{'loss': 0.9639, 'grad_norm': 3.4177301029270444, 'learning_rate': 8.255777777777779e-06, 'epoch': 1.0296}
{'loss': 0.9625, 'grad_norm': 3.5465250556831944, 'learning_rate': 8.254666666666667e-06, 'epoch': 1.03}
{'loss': 0.9527, 'grad_norm': 3.185126458731157, 'learning_rate': 8.253555555555556e-06, 'epoch': 1.0304}
{'loss': 0.9682, 'grad_norm': 3.210499216132788, 'learning_rate': 8.252444444444445e-06, 'epoch': 1.0308}
{'loss': 0.9517, 'grad_norm': 3.3475823588990945, 'learning_rate': 8.251333333333335e-06, 'epoch': 1.0312}
{'loss': 0.9525, 'grad_norm': 3.119181031387589, 'learning_rate': 8.250222222222222e-06, 'epoch': 1.0316}
{'loss': 0.9532, 'grad_norm': 3.2644627432997018, 'learning_rate': 8.249111111111113e-06, 'epoch': 1.032}
{'eval_valid_loss': 0.91748046875, 'eval_valid_runtime': 0.0906, 'eval_valid_samples_per_second': 1103.184, 'eval_valid_steps_per_second': 275.796, 'epoch': 1.032}
{'loss': 0.9691, 'grad_norm': 3.2761868422969016, 'learning_rate': 8.248e-06, 'epoch': 1.0324}
{'loss': 0.9576, 'grad_norm': 3.2062086846290456, 'learning_rate': 8.24688888888889e-06, 'epoch': 1.0328}
{'loss': 0.9554, 'grad_norm': 3.2735680073872158, 'learning_rate': 8.245777777777779e-06, 'epoch': 1.0332}
{'loss': 0.9571, 'grad_norm': 3.1992661934899074, 'learning_rate': 8.244666666666667e-06, 'epoch': 1.0336}
{'loss': 0.949, 'grad_norm': 3.288804857397494, 'learning_rate': 8.243555555555556e-06, 'epoch': 1.034}
{'loss': 0.947, 'grad_norm': 3.672594998158684, 'learning_rate': 8.242444444444445e-06, 'epoch': 1.0344}
{'loss': 0.9508, 'grad_norm': 3.2176142698061403, 'learning_rate': 8.241333333333335e-06, 'epoch': 1.0348}
{'loss': 0.9526, 'grad_norm': 3.2864413928498606, 'learning_rate': 8.240222222222222e-06, 'epoch': 1.0352}
{'loss': 0.9586, 'grad_norm': 2.823656366475235, 'learning_rate': 8.239111111111113e-06, 'epoch': 1.0356}
{'loss': 0.9681, 'grad_norm': 3.284053957753166, 'learning_rate': 8.238e-06, 'epoch': 1.036}
{'eval_valid_loss': 0.9189453125, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.685, 'eval_valid_steps_per_second': 279.421, 'epoch': 1.036}
{'loss': 0.9595, 'grad_norm': 3.315707812771654, 'learning_rate': 8.23688888888889e-06, 'epoch': 1.0364}
{'loss': 0.96, 'grad_norm': 3.430227762826215, 'learning_rate': 8.235777777777779e-06, 'epoch': 1.0368}
{'loss': 0.9535, 'grad_norm': 3.4032875792846142, 'learning_rate': 8.234666666666668e-06, 'epoch': 1.0372}
{'loss': 0.9585, 'grad_norm': 3.2077538423150256, 'learning_rate': 8.233555555555556e-06, 'epoch': 1.0376}
{'loss': 0.9456, 'grad_norm': 3.328576017095349, 'learning_rate': 8.232444444444445e-06, 'epoch': 1.038}
{'loss': 0.9581, 'grad_norm': 3.4561769194921736, 'learning_rate': 8.231333333333334e-06, 'epoch': 1.0384}
{'loss': 0.9494, 'grad_norm': 3.357497466846911, 'learning_rate': 8.230222222222222e-06, 'epoch': 1.0388}
{'loss': 0.9467, 'grad_norm': 3.329282371330962, 'learning_rate': 8.229111111111113e-06, 'epoch': 1.0392}
{'loss': 0.9626, 'grad_norm': 3.4241928193600755, 'learning_rate': 8.228e-06, 'epoch': 1.0396}
{'loss': 0.9627, 'grad_norm': 3.6125841117755324, 'learning_rate': 8.22688888888889e-06, 'epoch': 1.04}
{'eval_valid_loss': 0.91796875, 'eval_valid_runtime': 0.0903, 'eval_valid_samples_per_second': 1107.989, 'eval_valid_steps_per_second': 276.997, 'epoch': 1.04}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9536, 'grad_norm': 3.273159434086608, 'learning_rate': 8.225777777777777e-06, 'epoch': 1.0404}
{'loss': 0.9539, 'grad_norm': 3.148569288879547, 'learning_rate': 8.224666666666668e-06, 'epoch': 1.0408}
{'loss': 0.9534, 'grad_norm': 2.9787071191484227, 'learning_rate': 8.223555555555556e-06, 'epoch': 1.0412}
{'loss': 0.9522, 'grad_norm': 3.4000348601237103, 'learning_rate': 8.222444444444445e-06, 'epoch': 1.0416}
{'loss': 0.9534, 'grad_norm': 3.199254959143739, 'learning_rate': 8.221444444444445e-06, 'epoch': 1.042}
{'loss': 0.9655, 'grad_norm': 3.131143053783016, 'learning_rate': 8.220333333333333e-06, 'epoch': 1.0424}
{'loss': 0.9583, 'grad_norm': 3.2486084838462608, 'learning_rate': 8.219222222222224e-06, 'epoch': 1.0428}
{'loss': 0.9553, 'grad_norm': 3.5140144219338665, 'learning_rate': 8.218111111111112e-06, 'epoch': 1.0432}
{'loss': 0.9452, 'grad_norm': 3.2716148600959896, 'learning_rate': 8.217000000000001e-06, 'epoch': 1.0436}
{'loss': 0.9622, 'grad_norm': 3.2127699901809943, 'learning_rate': 8.21588888888889e-06, 'epoch': 1.044}
{'eval_valid_loss': 0.92041015625, 'eval_valid_runtime': 0.0948, 'eval_valid_samples_per_second': 1054.571, 'eval_valid_steps_per_second': 263.643, 'epoch': 1.044}
{'loss': 0.9582, 'grad_norm': 3.279725338161914, 'learning_rate': 8.214777777777779e-06, 'epoch': 1.0444}
{'loss': 0.9619, 'grad_norm': 3.0928081369576423, 'learning_rate': 8.213666666666667e-06, 'epoch': 1.0448}
{'loss': 0.9434, 'grad_norm': 3.630119047156949, 'learning_rate': 8.212555555555556e-06, 'epoch': 1.0452}
{'loss': 0.9563, 'grad_norm': 3.1917977530665316, 'learning_rate': 8.211444444444445e-06, 'epoch': 1.0456}
{'loss': 0.9442, 'grad_norm': 2.9059453978935252, 'learning_rate': 8.210333333333333e-06, 'epoch': 1.046}
{'loss': 0.9542, 'grad_norm': 3.1907298703197196, 'learning_rate': 8.209222222222222e-06, 'epoch': 1.0464}
{'loss': 0.9577, 'grad_norm': 3.4054690436940462, 'learning_rate': 8.208111111111113e-06, 'epoch': 1.0468}
{'loss': 0.9507, 'grad_norm': 3.1861746500982173, 'learning_rate': 8.207000000000001e-06, 'epoch': 1.0472}
{'loss': 0.9591, 'grad_norm': 3.0818775752715735, 'learning_rate': 8.20588888888889e-06, 'epoch': 1.0476}
{'loss': 0.9544, 'grad_norm': 2.9403114358415765, 'learning_rate': 8.204777777777779e-06, 'epoch': 1.048}
{'eval_valid_loss': 0.91650390625, 'eval_valid_runtime': 0.1332, 'eval_valid_samples_per_second': 750.984, 'eval_valid_steps_per_second': 187.746, 'epoch': 1.048}
{'loss': 0.954, 'grad_norm': 3.321954998585418, 'learning_rate': 8.203666666666667e-06, 'epoch': 1.0484}
{'loss': 0.9513, 'grad_norm': 3.5572840335404736, 'learning_rate': 8.202555555555556e-06, 'epoch': 1.0488}
{'loss': 0.9533, 'grad_norm': 3.2686338024748105, 'learning_rate': 8.201444444444445e-06, 'epoch': 1.0492}
{'loss': 0.9447, 'grad_norm': 3.039462006892616, 'learning_rate': 8.200333333333333e-06, 'epoch': 1.0496}
{'loss': 0.9401, 'grad_norm': 3.119316358876852, 'learning_rate': 8.199222222222222e-06, 'epoch': 1.05}
{'loss': 0.9542, 'grad_norm': 3.0215743581026473, 'learning_rate': 8.198111111111113e-06, 'epoch': 1.0504}
{'loss': 0.9574, 'grad_norm': 3.1709229210183554, 'learning_rate': 8.197000000000001e-06, 'epoch': 1.0508}
{'loss': 0.9498, 'grad_norm': 3.52869625078995, 'learning_rate': 8.19588888888889e-06, 'epoch': 1.0512}
{'loss': 0.9513, 'grad_norm': 3.2242041737009983, 'learning_rate': 8.194777777777779e-06, 'epoch': 1.0516}
{'loss': 0.9488, 'grad_norm': 3.2336325230915532, 'learning_rate': 8.193666666666667e-06, 'epoch': 1.052}
{'eval_valid_loss': 0.91845703125, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.842, 'eval_valid_steps_per_second': 278.96, 'epoch': 1.052}
{'loss': 0.956, 'grad_norm': 3.2749040818910093, 'learning_rate': 8.192555555555556e-06, 'epoch': 1.0524}
{'loss': 0.9496, 'grad_norm': 3.5908566645031534, 'learning_rate': 8.191444444444445e-06, 'epoch': 1.0528}
{'loss': 0.9397, 'grad_norm': 3.8001338489700855, 'learning_rate': 8.190333333333334e-06, 'epoch': 1.0532}
{'loss': 0.9631, 'grad_norm': 3.057114488232277, 'learning_rate': 8.189222222222222e-06, 'epoch': 1.0536}
{'loss': 0.9602, 'grad_norm': 3.056813215069283, 'learning_rate': 8.188111111111113e-06, 'epoch': 1.054}
{'loss': 0.9674, 'grad_norm': 3.5078204097531183, 'learning_rate': 8.187e-06, 'epoch': 1.0544}
{'loss': 0.9464, 'grad_norm': 3.471784278377912, 'learning_rate': 8.18588888888889e-06, 'epoch': 1.0548}
{'loss': 0.9593, 'grad_norm': 3.1666205344269724, 'learning_rate': 8.184777777777779e-06, 'epoch': 1.0552}
{'loss': 0.9474, 'grad_norm': 3.3724101514279137, 'learning_rate': 8.183666666666668e-06, 'epoch': 1.0556}
{'loss': 0.9532, 'grad_norm': 3.782726551262822, 'learning_rate': 8.182555555555556e-06, 'epoch': 1.056}
{'eval_valid_loss': 0.91650390625, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.49, 'eval_valid_steps_per_second': 278.372, 'epoch': 1.056}
{'loss': 0.959, 'grad_norm': 3.1362136776228318, 'learning_rate': 8.181444444444445e-06, 'epoch': 1.0564}
{'loss': 0.9495, 'grad_norm': 3.0926681129630076, 'learning_rate': 8.180333333333334e-06, 'epoch': 1.0568}
{'loss': 0.9449, 'grad_norm': 3.461203003079024, 'learning_rate': 8.179222222222222e-06, 'epoch': 1.0572}
{'loss': 0.9665, 'grad_norm': 3.2134881388669556, 'learning_rate': 8.178111111111113e-06, 'epoch': 1.0576}
{'loss': 0.951, 'grad_norm': 3.4281335234210064, 'learning_rate': 8.177e-06, 'epoch': 1.058}
{'loss': 0.9449, 'grad_norm': 3.4776029937103807, 'learning_rate': 8.17588888888889e-06, 'epoch': 1.0584}
{'loss': 0.9423, 'grad_norm': 3.2892889456824066, 'learning_rate': 8.174777777777779e-06, 'epoch': 1.0588}
{'loss': 0.972, 'grad_norm': 3.18085543578273, 'learning_rate': 8.173666666666668e-06, 'epoch': 1.0592}
{'loss': 0.9469, 'grad_norm': 3.1252984953418634, 'learning_rate': 8.172555555555556e-06, 'epoch': 1.0596}
{'loss': 0.9598, 'grad_norm': 2.988326228183513, 'learning_rate': 8.171444444444445e-06, 'epoch': 1.06}
{'eval_valid_loss': 0.91943359375, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.745, 'eval_valid_steps_per_second': 278.186, 'epoch': 1.06}
{'loss': 0.9563, 'grad_norm': 3.3383692380958485, 'learning_rate': 8.170333333333334e-06, 'epoch': 1.0604}
{'loss': 0.9357, 'grad_norm': 3.4366276327664296, 'learning_rate': 8.169222222222222e-06, 'epoch': 1.0608}
{'loss': 0.9461, 'grad_norm': 3.3181651273547534, 'learning_rate': 8.168111111111113e-06, 'epoch': 1.0612}
{'loss': 0.9589, 'grad_norm': 3.5057587760582045, 'learning_rate': 8.167e-06, 'epoch': 1.0616}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9588, 'grad_norm': 3.1723595516869287, 'learning_rate': 8.16588888888889e-06, 'epoch': 1.062}
{'loss': 0.9431, 'grad_norm': 3.207564138914417, 'learning_rate': 8.164777777777777e-06, 'epoch': 1.0624}
{'loss': 0.9532, 'grad_norm': 3.139745968835896, 'learning_rate': 8.163666666666668e-06, 'epoch': 1.0628}
{'loss': 0.9645, 'grad_norm': 3.1766839333969283, 'learning_rate': 8.162555555555556e-06, 'epoch': 1.0632}
{'loss': 0.9499, 'grad_norm': 3.5236251848733593, 'learning_rate': 8.161444444444445e-06, 'epoch': 1.0636}
{'loss': 0.9455, 'grad_norm': 3.086192716837102, 'learning_rate': 8.160333333333334e-06, 'epoch': 1.064}
{'eval_valid_loss': 0.91455078125, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1122.54, 'eval_valid_steps_per_second': 280.635, 'epoch': 1.064}
{'loss': 0.944, 'grad_norm': 3.703408306889705, 'learning_rate': 8.159222222222223e-06, 'epoch': 1.0644}
{'loss': 0.9656, 'grad_norm': 3.215716460236854, 'learning_rate': 8.158111111111113e-06, 'epoch': 1.0648}
{'loss': 0.9705, 'grad_norm': 2.967952229573581, 'learning_rate': 8.157e-06, 'epoch': 1.0652}
{'loss': 0.9571, 'grad_norm': 3.526830084523336, 'learning_rate': 8.15588888888889e-06, 'epoch': 1.0656}
{'loss': 0.9505, 'grad_norm': 3.625694726274485, 'learning_rate': 8.154777777777777e-06, 'epoch': 1.066}
{'loss': 0.9508, 'grad_norm': 3.587996164097644, 'learning_rate': 8.153666666666668e-06, 'epoch': 1.0664}
{'loss': 0.9378, 'grad_norm': 3.2072201021558113, 'learning_rate': 8.152555555555557e-06, 'epoch': 1.0668}
{'loss': 0.9469, 'grad_norm': 3.1458823482324387, 'learning_rate': 8.151444444444445e-06, 'epoch': 1.0672}
{'loss': 0.9602, 'grad_norm': 3.1926399843866404, 'learning_rate': 8.150333333333334e-06, 'epoch': 1.0676}
{'loss': 0.9437, 'grad_norm': 3.512042886713288, 'learning_rate': 8.149222222222223e-06, 'epoch': 1.068}
{'eval_valid_loss': 0.9169921875, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.389, 'eval_valid_steps_per_second': 281.847, 'epoch': 1.068}
{'loss': 0.9606, 'grad_norm': 3.452106049316539, 'learning_rate': 8.148111111111111e-06, 'epoch': 1.0684}
{'loss': 0.9482, 'grad_norm': 3.478504504315069, 'learning_rate': 8.147e-06, 'epoch': 1.0688}
{'loss': 0.9632, 'grad_norm': 3.4090974350346612, 'learning_rate': 8.14588888888889e-06, 'epoch': 1.0692}
{'loss': 0.9554, 'grad_norm': 3.8218007767488817, 'learning_rate': 8.144777777777778e-06, 'epoch': 1.0695999999999999}
{'loss': 0.9466, 'grad_norm': 3.3555808797981386, 'learning_rate': 8.143666666666668e-06, 'epoch': 1.07}
{'loss': 0.9603, 'grad_norm': 3.319377722705837, 'learning_rate': 8.142555555555555e-06, 'epoch': 1.0704}
{'loss': 0.9477, 'grad_norm': 3.2472278803404535, 'learning_rate': 8.141444444444445e-06, 'epoch': 1.0708}
{'loss': 0.9581, 'grad_norm': 3.51547103544697, 'learning_rate': 8.140333333333334e-06, 'epoch': 1.0712}
{'loss': 0.9562, 'grad_norm': 3.308394956415197, 'learning_rate': 8.139222222222223e-06, 'epoch': 1.0716}
{'loss': 0.9554, 'grad_norm': 3.4302309774390274, 'learning_rate': 8.138111111111111e-06, 'epoch': 1.072}
{'eval_valid_loss': 0.91552734375, 'eval_valid_runtime': 0.0915, 'eval_valid_samples_per_second': 1093.124, 'eval_valid_steps_per_second': 273.281, 'epoch': 1.072}
{'loss': 0.9725, 'grad_norm': 3.19876740081574, 'learning_rate': 8.137e-06, 'epoch': 1.0724}
{'loss': 0.9498, 'grad_norm': 2.9802198093588466, 'learning_rate': 8.13588888888889e-06, 'epoch': 1.0728}
{'loss': 0.9419, 'grad_norm': 3.221274691284666, 'learning_rate': 8.134777777777778e-06, 'epoch': 1.0732}
{'loss': 0.9523, 'grad_norm': 3.4263756909395644, 'learning_rate': 8.133666666666668e-06, 'epoch': 1.0735999999999999}
{'loss': 0.961, 'grad_norm': 3.47398194967971, 'learning_rate': 8.132555555555555e-06, 'epoch': 1.074}
{'loss': 0.9545, 'grad_norm': 3.15680269557673, 'learning_rate': 8.131444444444445e-06, 'epoch': 1.0744}
{'loss': 0.9442, 'grad_norm': 3.0990797911278256, 'learning_rate': 8.130333333333334e-06, 'epoch': 1.0748}
{'loss': 0.9426, 'grad_norm': 3.151861785007711, 'learning_rate': 8.129222222222223e-06, 'epoch': 1.0752}
{'loss': 0.9601, 'grad_norm': 2.922526924312756, 'learning_rate': 8.128111111111112e-06, 'epoch': 1.0756000000000001}
{'loss': 0.9628, 'grad_norm': 3.4796122857586558, 'learning_rate': 8.127e-06, 'epoch': 1.076}
{'eval_valid_loss': 0.91357421875, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.993, 'eval_valid_steps_per_second': 278.748, 'epoch': 1.076}
{'loss': 0.9617, 'grad_norm': 3.331512813328207, 'learning_rate': 8.125888888888889e-06, 'epoch': 1.0764}
{'loss': 0.9592, 'grad_norm': 3.757105913568553, 'learning_rate': 8.124777777777778e-06, 'epoch': 1.0768}
{'loss': 0.9603, 'grad_norm': 2.950507128897767, 'learning_rate': 8.123666666666668e-06, 'epoch': 1.0772}
{'loss': 0.9403, 'grad_norm': 3.120486939300676, 'learning_rate': 8.122555555555555e-06, 'epoch': 1.0776}
{'loss': 0.9489, 'grad_norm': 3.4566723739643734, 'learning_rate': 8.121444444444446e-06, 'epoch': 1.078}
{'loss': 0.9641, 'grad_norm': 3.4391990970727515, 'learning_rate': 8.120333333333334e-06, 'epoch': 1.0784}
{'loss': 0.9599, 'grad_norm': 3.117685158759512, 'learning_rate': 8.119222222222223e-06, 'epoch': 1.0788}
{'loss': 0.9703, 'grad_norm': 3.2318649134850146, 'learning_rate': 8.118111111111112e-06, 'epoch': 1.0792}
{'loss': 0.9665, 'grad_norm': 2.978138283762972, 'learning_rate': 8.117e-06, 'epoch': 1.0796000000000001}
{'loss': 0.9555, 'grad_norm': 3.1041237179560692, 'learning_rate': 8.115888888888889e-06, 'epoch': 1.08}
{'eval_valid_loss': 0.9150390625, 'eval_valid_runtime': 0.0883, 'eval_valid_samples_per_second': 1132.803, 'eval_valid_steps_per_second': 283.201, 'epoch': 1.08}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9567, 'grad_norm': 3.3772241451661102, 'learning_rate': 8.114777777777778e-06, 'epoch': 1.0804}
{'loss': 0.9547, 'grad_norm': 3.55511584373511, 'learning_rate': 8.113666666666668e-06, 'epoch': 1.0808}
{'loss': 0.9398, 'grad_norm': 3.3008302493098247, 'learning_rate': 8.112555555555555e-06, 'epoch': 1.0812}
{'loss': 0.9594, 'grad_norm': 2.992837272590112, 'learning_rate': 8.111444444444446e-06, 'epoch': 1.0816}
{'loss': 0.9448, 'grad_norm': 3.72891845445376, 'learning_rate': 8.110444444444445e-06, 'epoch': 1.082}
{'loss': 0.9502, 'grad_norm': 3.1876196651809763, 'learning_rate': 8.109333333333334e-06, 'epoch': 1.0824}
{'loss': 0.9546, 'grad_norm': 3.1468412835020545, 'learning_rate': 8.108222222222223e-06, 'epoch': 1.0828}
{'loss': 0.9439, 'grad_norm': 3.2373185423587616, 'learning_rate': 8.107111111111113e-06, 'epoch': 1.0832}
{'loss': 0.956, 'grad_norm': 3.5510183995854394, 'learning_rate': 8.106e-06, 'epoch': 1.0836}
{'loss': 0.9509, 'grad_norm': 3.4264076208344725, 'learning_rate': 8.10488888888889e-06, 'epoch': 1.084}
{'eval_valid_loss': 0.91357421875, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.201, 'eval_valid_steps_per_second': 279.55, 'epoch': 1.084}
{'loss': 0.9593, 'grad_norm': 3.0711492218543, 'learning_rate': 8.103777777777777e-06, 'epoch': 1.0844}
{'loss': 0.9441, 'grad_norm': 3.2723284825197125, 'learning_rate': 8.102666666666668e-06, 'epoch': 1.0848}
{'loss': 0.9587, 'grad_norm': 3.113802513221855, 'learning_rate': 8.101555555555557e-06, 'epoch': 1.0852}
{'loss': 0.9659, 'grad_norm': 3.4387502477403813, 'learning_rate': 8.100444444444445e-06, 'epoch': 1.0856}
{'loss': 0.9558, 'grad_norm': 2.8598096861074405, 'learning_rate': 8.099333333333334e-06, 'epoch': 1.086}
{'loss': 0.95, 'grad_norm': 3.552609936934541, 'learning_rate': 8.098222222222223e-06, 'epoch': 1.0864}
{'loss': 0.962, 'grad_norm': 2.9824499942076, 'learning_rate': 8.097111111111113e-06, 'epoch': 1.0868}
{'loss': 0.9598, 'grad_norm': 3.1687388019251044, 'learning_rate': 8.096e-06, 'epoch': 1.0872}
{'loss': 0.9541, 'grad_norm': 3.1343995162750513, 'learning_rate': 8.09488888888889e-06, 'epoch': 1.0876}
{'loss': 0.959, 'grad_norm': 3.31333596099606, 'learning_rate': 8.093777777777777e-06, 'epoch': 1.088}
{'eval_valid_loss': 0.91064453125, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1106.802, 'eval_valid_steps_per_second': 276.701, 'epoch': 1.088}
{'loss': 0.9595, 'grad_norm': 3.1450933676659045, 'learning_rate': 8.092666666666668e-06, 'epoch': 1.0884}
{'loss': 0.9497, 'grad_norm': 3.346806172323024, 'learning_rate': 8.091555555555557e-06, 'epoch': 1.0888}
{'loss': 0.9473, 'grad_norm': 3.3280372608063584, 'learning_rate': 8.090444444444445e-06, 'epoch': 1.0892}
{'loss': 0.9548, 'grad_norm': 3.2579400657359616, 'learning_rate': 8.089333333333334e-06, 'epoch': 1.0896}
{'loss': 0.9556, 'grad_norm': 3.4249535042328563, 'learning_rate': 8.088222222222223e-06, 'epoch': 1.09}
{'loss': 0.9482, 'grad_norm': 3.204117328171338, 'learning_rate': 8.087111111111111e-06, 'epoch': 1.0904}
{'loss': 0.9445, 'grad_norm': 3.398421083607979, 'learning_rate': 8.086e-06, 'epoch': 1.0908}
{'loss': 0.949, 'grad_norm': 3.388994607205798, 'learning_rate': 8.08488888888889e-06, 'epoch': 1.0912}
{'loss': 0.9417, 'grad_norm': 3.109610045477322, 'learning_rate': 8.083777777777778e-06, 'epoch': 1.0916}
{'loss': 0.9435, 'grad_norm': 3.5652405086032823, 'learning_rate': 8.082666666666668e-06, 'epoch': 1.092}
{'eval_valid_loss': 0.91455078125, 'eval_valid_runtime': 0.0912, 'eval_valid_samples_per_second': 1096.129, 'eval_valid_steps_per_second': 274.032, 'epoch': 1.092}
{'loss': 0.9701, 'grad_norm': 3.561097588482866, 'learning_rate': 8.081555555555555e-06, 'epoch': 1.0924}
{'loss': 0.9558, 'grad_norm': 3.320048045678236, 'learning_rate': 8.080444444444445e-06, 'epoch': 1.0928}
{'loss': 0.9634, 'grad_norm': 3.4642136268531383, 'learning_rate': 8.079333333333334e-06, 'epoch': 1.0932}
{'loss': 0.95, 'grad_norm': 3.2593583571977423, 'learning_rate': 8.078222222222223e-06, 'epoch': 1.0936}
{'loss': 0.9488, 'grad_norm': 2.9816536804017346, 'learning_rate': 8.077111111111112e-06, 'epoch': 1.094}
{'loss': 0.9518, 'grad_norm': 3.8031166013877247, 'learning_rate': 8.076e-06, 'epoch': 1.0944}
{'loss': 0.9453, 'grad_norm': 3.218863633381403, 'learning_rate': 8.07488888888889e-06, 'epoch': 1.0948}
{'loss': 0.9469, 'grad_norm': 3.227027249982768, 'learning_rate': 8.073777777777778e-06, 'epoch': 1.0952}
{'loss': 0.94, 'grad_norm': 3.5958820984334356, 'learning_rate': 8.072666666666668e-06, 'epoch': 1.0956}
{'loss': 0.9461, 'grad_norm': 3.2167059324434133, 'learning_rate': 8.071555555555555e-06, 'epoch': 1.096}
{'eval_valid_loss': 0.91259765625, 'eval_valid_runtime': 0.0915, 'eval_valid_samples_per_second': 1092.335, 'eval_valid_steps_per_second': 273.084, 'epoch': 1.096}
{'loss': 0.9426, 'grad_norm': 3.1219326033065347, 'learning_rate': 8.070444444444446e-06, 'epoch': 1.0964}
{'loss': 0.9357, 'grad_norm': 3.1115753113951357, 'learning_rate': 8.069333333333334e-06, 'epoch': 1.0968}
{'loss': 0.9459, 'grad_norm': 2.9018790077051984, 'learning_rate': 8.068222222222223e-06, 'epoch': 1.0972}
{'loss': 0.9564, 'grad_norm': 3.556805853485781, 'learning_rate': 8.067111111111112e-06, 'epoch': 1.0976}
{'loss': 0.9668, 'grad_norm': 3.740516942757238, 'learning_rate': 8.066e-06, 'epoch': 1.098}
{'loss': 0.9427, 'grad_norm': 3.347308377827491, 'learning_rate': 8.064888888888889e-06, 'epoch': 1.0984}
{'loss': 0.951, 'grad_norm': 3.6456771308497853, 'learning_rate': 8.063777777777778e-06, 'epoch': 1.0988}
{'loss': 0.9261, 'grad_norm': 2.9120527091803763, 'learning_rate': 8.062666666666668e-06, 'epoch': 1.0992}
{'loss': 0.9466, 'grad_norm': 3.087739548676057, 'learning_rate': 8.061555555555555e-06, 'epoch': 1.0996}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.9585, 'grad_norm': 3.188779592791688, 'learning_rate': 8.060444444444446e-06, 'epoch': 1.1}
{'eval_valid_loss': 0.90966796875, 'eval_valid_runtime': 0.0917, 'eval_valid_samples_per_second': 1090.211, 'eval_valid_steps_per_second': 272.553, 'epoch': 1.1}
{'loss': 0.9382, 'grad_norm': 2.9870733874270647, 'learning_rate': 8.059333333333333e-06, 'epoch': 1.1004}
{'loss': 0.9522, 'grad_norm': 3.931300626589491, 'learning_rate': 8.058222222222223e-06, 'epoch': 1.1008}
{'loss': 0.948, 'grad_norm': 4.027558358928128, 'learning_rate': 8.057111111111112e-06, 'epoch': 1.1012}
{'loss': 0.9358, 'grad_norm': 2.9371806843671755, 'learning_rate': 8.056e-06, 'epoch': 1.1016}
{'loss': 0.9496, 'grad_norm': 3.228713503183457, 'learning_rate': 8.05488888888889e-06, 'epoch': 1.102}
{'loss': 0.936, 'grad_norm': 3.2188341351966354, 'learning_rate': 8.053777777777778e-06, 'epoch': 1.1024}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9561, 'grad_norm': 3.340015393581437, 'learning_rate': 8.052666666666668e-06, 'epoch': 1.1028}
{'loss': 0.9515, 'grad_norm': 3.3533303726741264, 'learning_rate': 8.051555555555555e-06, 'epoch': 1.1032}
{'loss': 0.9453, 'grad_norm': 3.292905189199395, 'learning_rate': 8.050444444444446e-06, 'epoch': 1.1036}
{'loss': 0.9537, 'grad_norm': 3.013215273312111, 'learning_rate': 8.049333333333333e-06, 'epoch': 1.104}
{'eval_valid_loss': 0.91259765625, 'eval_valid_runtime': 0.0953, 'eval_valid_samples_per_second': 1049.35, 'eval_valid_steps_per_second': 262.337, 'epoch': 1.104}
{'loss': 0.9398, 'grad_norm': 3.0387970286236916, 'learning_rate': 8.048222222222223e-06, 'epoch': 1.1044}
{'loss': 0.9362, 'grad_norm': 3.020235135054085, 'learning_rate': 8.047111111111112e-06, 'epoch': 1.1048}
{'loss': 0.9467, 'grad_norm': 3.160569153394717, 'learning_rate': 8.046e-06, 'epoch': 1.1052}
{'loss': 0.9538, 'grad_norm': 3.130795340736917, 'learning_rate': 8.04488888888889e-06, 'epoch': 1.1056}
{'loss': 0.9471, 'grad_norm': 2.9326375934552704, 'learning_rate': 8.043777777777778e-06, 'epoch': 1.106}
{'loss': 0.9545, 'grad_norm': 2.9801413379984725, 'learning_rate': 8.042666666666667e-06, 'epoch': 1.1064}
{'loss': 0.9582, 'grad_norm': 3.009062519665757, 'learning_rate': 8.041555555555555e-06, 'epoch': 1.1068}
{'loss': 0.9499, 'grad_norm': 3.1311703609001365, 'learning_rate': 8.040444444444446e-06, 'epoch': 1.1072}
{'loss': 0.9599, 'grad_norm': 3.3741784243725452, 'learning_rate': 8.039333333333335e-06, 'epoch': 1.1076}
{'loss': 0.9515, 'grad_norm': 3.1927759880045934, 'learning_rate': 8.038222222222223e-06, 'epoch': 1.108}
{'eval_valid_loss': 0.91064453125, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.929, 'eval_valid_steps_per_second': 279.732, 'epoch': 1.108}
{'loss': 0.9528, 'grad_norm': 3.11092880378094, 'learning_rate': 8.037111111111112e-06, 'epoch': 1.1084}
{'loss': 0.9613, 'grad_norm': 2.954727242786004, 'learning_rate': 8.036e-06, 'epoch': 1.1088}
{'loss': 0.9452, 'grad_norm': 3.230659156363408, 'learning_rate': 8.03488888888889e-06, 'epoch': 1.1092}
{'loss': 0.9513, 'grad_norm': 3.0550058496547203, 'learning_rate': 8.033777777777778e-06, 'epoch': 1.1096}
{'loss': 0.9498, 'grad_norm': 3.217495618297021, 'learning_rate': 8.032666666666667e-06, 'epoch': 1.11}
{'loss': 0.947, 'grad_norm': 3.24077103707871, 'learning_rate': 8.031555555555555e-06, 'epoch': 1.1104}
{'loss': 0.9389, 'grad_norm': 2.9395797144603737, 'learning_rate': 8.030444444444446e-06, 'epoch': 1.1108}
{'loss': 0.9549, 'grad_norm': 2.9228088192247186, 'learning_rate': 8.029333333333335e-06, 'epoch': 1.1112}
{'loss': 0.9596, 'grad_norm': 3.171906175131346, 'learning_rate': 8.028222222222223e-06, 'epoch': 1.1116}
{'loss': 0.9585, 'grad_norm': 3.4021817857489864, 'learning_rate': 8.027111111111112e-06, 'epoch': 1.112}
{'eval_valid_loss': 0.91162109375, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.71, 'eval_valid_steps_per_second': 277.177, 'epoch': 1.112}
{'loss': 0.9408, 'grad_norm': 3.328212253340176, 'learning_rate': 8.026e-06, 'epoch': 1.1124}
{'loss': 0.9483, 'grad_norm': 3.3012511150296113, 'learning_rate': 8.02488888888889e-06, 'epoch': 1.1128}
{'loss': 0.9474, 'grad_norm': 3.7376179360757074, 'learning_rate': 8.023777777777778e-06, 'epoch': 1.1132}
{'loss': 0.947, 'grad_norm': 3.215941537193409, 'learning_rate': 8.022666666666667e-06, 'epoch': 1.1136}
{'loss': 0.942, 'grad_norm': 3.7961515828508587, 'learning_rate': 8.021555555555556e-06, 'epoch': 1.114}
{'loss': 0.9399, 'grad_norm': 3.206896752384262, 'learning_rate': 8.020444444444444e-06, 'epoch': 1.1144}
{'loss': 0.9439, 'grad_norm': 3.426569309741001, 'learning_rate': 8.019333333333335e-06, 'epoch': 1.1148}
{'loss': 0.9491, 'grad_norm': 3.540689528063279, 'learning_rate': 8.018222222222223e-06, 'epoch': 1.1152}
{'loss': 0.9488, 'grad_norm': 3.4343729447835063, 'learning_rate': 8.017111111111112e-06, 'epoch': 1.1156}
{'loss': 0.9403, 'grad_norm': 3.0604778250358495, 'learning_rate': 8.016e-06, 'epoch': 1.116}
{'eval_valid_loss': 0.91015625, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.227, 'eval_valid_steps_per_second': 279.307, 'epoch': 1.116}
{'loss': 0.9515, 'grad_norm': 3.6633633107117873, 'learning_rate': 8.01488888888889e-06, 'epoch': 1.1164}
{'loss': 0.948, 'grad_norm': 3.008714367299325, 'learning_rate': 8.013777777777778e-06, 'epoch': 1.1168}
{'loss': 0.9441, 'grad_norm': 2.9012679189303685, 'learning_rate': 8.012666666666667e-06, 'epoch': 1.1172}
{'loss': 0.9534, 'grad_norm': 3.587158211433059, 'learning_rate': 8.011555555555556e-06, 'epoch': 1.1176}
{'loss': 0.9393, 'grad_norm': 3.273579487418347, 'learning_rate': 8.010444444444444e-06, 'epoch': 1.1179999999999999}
{'loss': 0.9385, 'grad_norm': 3.4780462463456483, 'learning_rate': 8.009333333333335e-06, 'epoch': 1.1184}
{'loss': 0.9448, 'grad_norm': 3.2284905720573542, 'learning_rate': 8.008222222222224e-06, 'epoch': 1.1188}
{'loss': 0.96, 'grad_norm': 3.7772338914330166, 'learning_rate': 8.007111111111112e-06, 'epoch': 1.1192}
{'loss': 0.9533, 'grad_norm': 3.4060600997765427, 'learning_rate': 8.006000000000001e-06, 'epoch': 1.1196}
{'loss': 0.9465, 'grad_norm': 3.6664765138881794, 'learning_rate': 8.00488888888889e-06, 'epoch': 1.12}
{'eval_valid_loss': 0.91357421875, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1120.845, 'eval_valid_steps_per_second': 280.211, 'epoch': 1.12}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9515, 'grad_norm': 3.208081527629454, 'learning_rate': 8.003777777777778e-06, 'epoch': 1.1204}
{'loss': 0.9618, 'grad_norm': 3.266569849610169, 'learning_rate': 8.002666666666667e-06, 'epoch': 1.1208}
{'loss': 0.933, 'grad_norm': 2.9834348985407524, 'learning_rate': 8.001555555555556e-06, 'epoch': 1.1212}
{'loss': 0.9458, 'grad_norm': 3.6181053826107834, 'learning_rate': 8.000444444444444e-06, 'epoch': 1.1216}
{'loss': 0.9377, 'grad_norm': 3.480982690064575, 'learning_rate': 7.999444444444446e-06, 'epoch': 1.1219999999999999}
{'loss': 0.9406, 'grad_norm': 3.2123411595747733, 'learning_rate': 7.998333333333333e-06, 'epoch': 1.1224}
{'loss': 0.9665, 'grad_norm': 3.025624669419152, 'learning_rate': 7.997222222222223e-06, 'epoch': 1.1228}
{'loss': 0.9586, 'grad_norm': 3.046970747396946, 'learning_rate': 7.996111111111112e-06, 'epoch': 1.1232}
{'loss': 0.9359, 'grad_norm': 3.073219107706969, 'learning_rate': 7.995e-06, 'epoch': 1.1236}
{'loss': 0.9556, 'grad_norm': 3.3613683508638803, 'learning_rate': 7.99388888888889e-06, 'epoch': 1.124}
{'eval_valid_loss': 0.90869140625, 'eval_valid_runtime': 0.0913, 'eval_valid_samples_per_second': 1095.176, 'eval_valid_steps_per_second': 273.794, 'epoch': 1.124}
{'loss': 0.9437, 'grad_norm': 2.9974829326649615, 'learning_rate': 7.992777777777778e-06, 'epoch': 1.1244}
{'loss': 0.9521, 'grad_norm': 3.083993304132795, 'learning_rate': 7.991666666666668e-06, 'epoch': 1.1248}
{'loss': 0.9501, 'grad_norm': 3.04588445554449, 'learning_rate': 7.990555555555555e-06, 'epoch': 1.1252}
{'loss': 0.9335, 'grad_norm': 3.1872923166271163, 'learning_rate': 7.989444444444446e-06, 'epoch': 1.1256}
{'loss': 0.9394, 'grad_norm': 3.416718207334979, 'learning_rate': 7.988333333333333e-06, 'epoch': 1.126}
{'loss': 0.9519, 'grad_norm': 3.0700249088314395, 'learning_rate': 7.987222222222223e-06, 'epoch': 1.1264}
{'loss': 0.9321, 'grad_norm': 3.395675905163105, 'learning_rate': 7.986111111111112e-06, 'epoch': 1.1268}
{'loss': 0.9375, 'grad_norm': 3.251883080446966, 'learning_rate': 7.985e-06, 'epoch': 1.1272}
{'loss': 0.9488, 'grad_norm': 3.0140307466802994, 'learning_rate': 7.98388888888889e-06, 'epoch': 1.1276}
{'loss': 0.9511, 'grad_norm': 3.6052733726920976, 'learning_rate': 7.982777777777778e-06, 'epoch': 1.1280000000000001}
{'eval_valid_loss': 0.90966796875, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.094, 'eval_valid_steps_per_second': 279.024, 'epoch': 1.1280000000000001}
{'loss': 0.9361, 'grad_norm': 3.4216190573372764, 'learning_rate': 7.981666666666667e-06, 'epoch': 1.1284}
{'loss': 0.9369, 'grad_norm': 3.4149712895908153, 'learning_rate': 7.980555555555555e-06, 'epoch': 1.1288}
{'loss': 0.9476, 'grad_norm': 3.0176610743902277, 'learning_rate': 7.979444444444446e-06, 'epoch': 1.1292}
{'loss': 0.9442, 'grad_norm': 3.0501014547984213, 'learning_rate': 7.978333333333333e-06, 'epoch': 1.1296}
{'loss': 0.9536, 'grad_norm': 3.6925001350253006, 'learning_rate': 7.977222222222223e-06, 'epoch': 1.13}
{'loss': 0.9407, 'grad_norm': 3.069844877629508, 'learning_rate': 7.976111111111112e-06, 'epoch': 1.1304}
{'loss': 0.9454, 'grad_norm': 3.044544815687146, 'learning_rate': 7.975e-06, 'epoch': 1.1308}
{'loss': 0.9517, 'grad_norm': 3.632547742158396, 'learning_rate': 7.97388888888889e-06, 'epoch': 1.1312}
{'loss': 0.9432, 'grad_norm': 3.110724669015732, 'learning_rate': 7.972777777777778e-06, 'epoch': 1.1316}
{'loss': 0.9407, 'grad_norm': 3.333405411457561, 'learning_rate': 7.971666666666667e-06, 'epoch': 1.1320000000000001}
{'eval_valid_loss': 0.90869140625, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1115.207, 'eval_valid_steps_per_second': 278.802, 'epoch': 1.1320000000000001}
{'loss': 0.9418, 'grad_norm': 3.0312545127441792, 'learning_rate': 7.970555555555556e-06, 'epoch': 1.1324}
{'loss': 0.9416, 'grad_norm': 3.586982342127017, 'learning_rate': 7.969444444444446e-06, 'epoch': 1.1328}
{'loss': 0.9383, 'grad_norm': 3.256752399705779, 'learning_rate': 7.968333333333333e-06, 'epoch': 1.1332}
{'loss': 0.9521, 'grad_norm': 3.358086582889189, 'learning_rate': 7.967222222222223e-06, 'epoch': 1.1336}
{'loss': 0.954, 'grad_norm': 3.399532781783741, 'learning_rate': 7.966111111111112e-06, 'epoch': 1.134}
{'loss': 0.9593, 'grad_norm': 3.077314192588079, 'learning_rate': 7.965e-06, 'epoch': 1.1344}
{'loss': 0.9525, 'grad_norm': 3.0160040938314183, 'learning_rate': 7.96388888888889e-06, 'epoch': 1.1348}
{'loss': 0.9483, 'grad_norm': 3.3819776830839254, 'learning_rate': 7.962777777777778e-06, 'epoch': 1.1352}
{'loss': 0.9331, 'grad_norm': 3.033833621805506, 'learning_rate': 7.961666666666667e-06, 'epoch': 1.1356}
{'loss': 0.9468, 'grad_norm': 3.2826456008455867, 'learning_rate': 7.960555555555556e-06, 'epoch': 1.1360000000000001}
{'eval_valid_loss': 0.91064453125, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1128.839, 'eval_valid_steps_per_second': 282.21, 'epoch': 1.1360000000000001}
{'loss': 0.9469, 'grad_norm': 3.3387208712959224, 'learning_rate': 7.959444444444444e-06, 'epoch': 1.1364}
{'loss': 0.9317, 'grad_norm': 3.37123603564727, 'learning_rate': 7.958333333333333e-06, 'epoch': 1.1368}
{'loss': 0.9376, 'grad_norm': 3.1243492974881737, 'learning_rate': 7.957222222222223e-06, 'epoch': 1.1372}
{'loss': 0.9366, 'grad_norm': 3.0962525755587023, 'learning_rate': 7.956111111111112e-06, 'epoch': 1.1376}
{'loss': 0.9466, 'grad_norm': 2.9010305398742497, 'learning_rate': 7.955000000000001e-06, 'epoch': 1.138}
{'loss': 0.9364, 'grad_norm': 3.1186717426824937, 'learning_rate': 7.95388888888889e-06, 'epoch': 1.1384}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.9401, 'grad_norm': 3.4815789024542565, 'learning_rate': 7.952777777777778e-06, 'epoch': 1.1388}
{'loss': 0.9567, 'grad_norm': 3.0822467027594413, 'learning_rate': 7.951666666666667e-06, 'epoch': 1.1392}
{'loss': 0.9385, 'grad_norm': 3.2613528109187446, 'learning_rate': 7.950555555555556e-06, 'epoch': 1.1396}
{'loss': 0.9552, 'grad_norm': 3.195463564036942, 'learning_rate': 7.949444444444444e-06, 'epoch': 1.1400000000000001}
{'eval_valid_loss': 0.90673828125, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.809, 'eval_valid_steps_per_second': 279.702, 'epoch': 1.1400000000000001}
{'loss': 0.941, 'grad_norm': 3.4700367491230013, 'learning_rate': 7.948333333333333e-06, 'epoch': 1.1404}
{'loss': 0.9486, 'grad_norm': 3.4689570786121036, 'learning_rate': 7.947222222222224e-06, 'epoch': 1.1408}
{'loss': 0.9436, 'grad_norm': 3.4038617922453915, 'learning_rate': 7.946111111111112e-06, 'epoch': 1.1412}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9276, 'grad_norm': 3.3395998949404526, 'learning_rate': 7.945000000000001e-06, 'epoch': 1.1416}
{'loss': 0.9396, 'grad_norm': 3.474640904541372, 'learning_rate': 7.94388888888889e-06, 'epoch': 1.142}
{'loss': 0.9483, 'grad_norm': 2.9391261226210785, 'learning_rate': 7.942777777777778e-06, 'epoch': 1.1424}
{'loss': 0.9361, 'grad_norm': 2.8923821856211918, 'learning_rate': 7.941666666666667e-06, 'epoch': 1.1428}
{'loss': 0.9457, 'grad_norm': 3.2040171334520537, 'learning_rate': 7.940555555555556e-06, 'epoch': 1.1432}
{'loss': 0.9464, 'grad_norm': 2.981215897051172, 'learning_rate': 7.939444444444445e-06, 'epoch': 1.1436}
{'loss': 0.9509, 'grad_norm': 3.052615172535499, 'learning_rate': 7.938333333333333e-06, 'epoch': 1.144}
{'eval_valid_loss': 0.90869140625, 'eval_valid_runtime': 0.0974, 'eval_valid_samples_per_second': 1026.544, 'eval_valid_steps_per_second': 256.636, 'epoch': 1.144}
{'loss': 0.9516, 'grad_norm': 3.3952778038081353, 'learning_rate': 7.937222222222222e-06, 'epoch': 1.1444}
{'loss': 0.9421, 'grad_norm': 3.010716945803459, 'learning_rate': 7.936111111111112e-06, 'epoch': 1.1448}
{'loss': 0.943, 'grad_norm': 2.986487225323033, 'learning_rate': 7.935000000000001e-06, 'epoch': 1.1452}
{'loss': 0.9441, 'grad_norm': 2.9865818851452923, 'learning_rate': 7.93388888888889e-06, 'epoch': 1.1456}
{'loss': 0.9538, 'grad_norm': 3.3173832437168547, 'learning_rate': 7.932777777777779e-06, 'epoch': 1.146}
{'loss': 0.9479, 'grad_norm': 3.144917027284096, 'learning_rate': 7.931666666666667e-06, 'epoch': 1.1464}
{'loss': 0.9498, 'grad_norm': 3.322849103585548, 'learning_rate': 7.930555555555556e-06, 'epoch': 1.1468}
{'loss': 0.9515, 'grad_norm': 2.9539490211132517, 'learning_rate': 7.929444444444445e-06, 'epoch': 1.1472}
{'loss': 0.9518, 'grad_norm': 3.3806712397387293, 'learning_rate': 7.928333333333333e-06, 'epoch': 1.1476}
{'loss': 0.9477, 'grad_norm': 2.859158721415429, 'learning_rate': 7.927222222222222e-06, 'epoch': 1.148}
{'eval_valid_loss': 0.90771484375, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1110.824, 'eval_valid_steps_per_second': 277.706, 'epoch': 1.148}
{'loss': 0.949, 'grad_norm': 3.2336603379472897, 'learning_rate': 7.926111111111112e-06, 'epoch': 1.1484}
{'loss': 0.9415, 'grad_norm': 3.0446278232511728, 'learning_rate': 7.925000000000001e-06, 'epoch': 1.1488}
{'loss': 0.949, 'grad_norm': 3.3213186826341325, 'learning_rate': 7.92388888888889e-06, 'epoch': 1.1492}
{'loss': 0.9453, 'grad_norm': 3.2682090373641524, 'learning_rate': 7.922777777777779e-06, 'epoch': 1.1496}
{'loss': 0.9295, 'grad_norm': 3.6021050729237136, 'learning_rate': 7.921666666666667e-06, 'epoch': 1.15}
{'loss': 0.9459, 'grad_norm': 3.2305284007975663, 'learning_rate': 7.920555555555556e-06, 'epoch': 1.1504}
{'loss': 0.9423, 'grad_norm': 3.3197934272281864, 'learning_rate': 7.919444444444445e-06, 'epoch': 1.1508}
{'loss': 0.9491, 'grad_norm': 3.3838353625896476, 'learning_rate': 7.918333333333333e-06, 'epoch': 1.1512}
{'loss': 0.9521, 'grad_norm': 3.6177790345542684, 'learning_rate': 7.917222222222222e-06, 'epoch': 1.1516}
{'loss': 0.9432, 'grad_norm': 3.416554159351211, 'learning_rate': 7.916111111111113e-06, 'epoch': 1.152}
{'eval_valid_loss': 0.90966796875, 'eval_valid_runtime': 0.0911, 'eval_valid_samples_per_second': 1097.348, 'eval_valid_steps_per_second': 274.337, 'epoch': 1.152}
{'loss': 0.9311, 'grad_norm': 3.3715530471605604, 'learning_rate': 7.915000000000001e-06, 'epoch': 1.1524}
{'loss': 0.9489, 'grad_norm': 3.6248357423201996, 'learning_rate': 7.91388888888889e-06, 'epoch': 1.1528}
{'loss': 0.9396, 'grad_norm': 3.342516805396848, 'learning_rate': 7.912777777777779e-06, 'epoch': 1.1532}
{'loss': 0.957, 'grad_norm': 3.1032215840962376, 'learning_rate': 7.911666666666667e-06, 'epoch': 1.1536}
{'loss': 0.9476, 'grad_norm': 2.9771393444771044, 'learning_rate': 7.910555555555556e-06, 'epoch': 1.154}
{'loss': 0.9365, 'grad_norm': 3.1235778719802028, 'learning_rate': 7.909444444444445e-06, 'epoch': 1.1544}
{'loss': 0.9509, 'grad_norm': 3.3339121385558004, 'learning_rate': 7.908333333333335e-06, 'epoch': 1.1548}
{'loss': 0.9424, 'grad_norm': 3.1556254326043196, 'learning_rate': 7.907222222222222e-06, 'epoch': 1.1552}
{'loss': 0.9418, 'grad_norm': 3.337769070326604, 'learning_rate': 7.906111111111113e-06, 'epoch': 1.1556}
{'loss': 0.9538, 'grad_norm': 3.157462633052274, 'learning_rate': 7.905e-06, 'epoch': 1.156}
{'eval_valid_loss': 0.9072265625, 'eval_valid_runtime': 0.0903, 'eval_valid_samples_per_second': 1107.03, 'eval_valid_steps_per_second': 276.757, 'epoch': 1.156}
{'loss': 0.9377, 'grad_norm': 2.831911586995756, 'learning_rate': 7.90388888888889e-06, 'epoch': 1.1564}
{'loss': 0.9477, 'grad_norm': 3.272926436340268, 'learning_rate': 7.902777777777779e-06, 'epoch': 1.1568}
{'loss': 0.9332, 'grad_norm': 3.4364814723217623, 'learning_rate': 7.901666666666668e-06, 'epoch': 1.1572}
{'loss': 0.9365, 'grad_norm': 3.0230301180862, 'learning_rate': 7.900555555555556e-06, 'epoch': 1.1576}
{'loss': 0.957, 'grad_norm': 3.274124819901661, 'learning_rate': 7.899444444444445e-06, 'epoch': 1.158}
{'loss': 0.94, 'grad_norm': 3.4155510247849983, 'learning_rate': 7.898333333333335e-06, 'epoch': 1.1584}
{'loss': 0.9432, 'grad_norm': 3.0047963365193513, 'learning_rate': 7.897222222222222e-06, 'epoch': 1.1588}
{'loss': 0.9473, 'grad_norm': 3.139173399968053, 'learning_rate': 7.896111111111113e-06, 'epoch': 1.1592}
{'loss': 0.9404, 'grad_norm': 3.103202607182246, 'learning_rate': 7.895e-06, 'epoch': 1.1596}
{'loss': 0.9243, 'grad_norm': 3.1561811552943126, 'learning_rate': 7.89388888888889e-06, 'epoch': 1.16}
{'eval_valid_loss': 0.908203125, 'eval_valid_runtime': 0.0903, 'eval_valid_samples_per_second': 1107.732, 'eval_valid_steps_per_second': 276.933, 'epoch': 1.16}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9386, 'grad_norm': 3.348752646806503, 'learning_rate': 7.892777777777779e-06, 'epoch': 1.1604}
{'loss': 0.96, 'grad_norm': 3.374249198195026, 'learning_rate': 7.891666666666668e-06, 'epoch': 1.1608}
{'loss': 0.9408, 'grad_norm': 2.9306152197312, 'learning_rate': 7.890555555555556e-06, 'epoch': 1.1612}
{'loss': 0.952, 'grad_norm': 3.0679122142473627, 'learning_rate': 7.889444444444445e-06, 'epoch': 1.1616}
{'loss': 0.9481, 'grad_norm': 3.230609923003017, 'learning_rate': 7.888444444444445e-06, 'epoch': 1.162}
{'loss': 0.9275, 'grad_norm': 2.939902712889343, 'learning_rate': 7.887333333333333e-06, 'epoch': 1.1623999999999999}
{'loss': 0.9485, 'grad_norm': 3.290080957097859, 'learning_rate': 7.886222222222224e-06, 'epoch': 1.1628}
{'loss': 0.9363, 'grad_norm': 3.466304888134222, 'learning_rate': 7.885111111111112e-06, 'epoch': 1.1632}
{'loss': 0.9464, 'grad_norm': 3.3985281395060785, 'learning_rate': 7.884000000000001e-06, 'epoch': 1.1636}
{'loss': 0.9302, 'grad_norm': 3.035876862395984, 'learning_rate': 7.88288888888889e-06, 'epoch': 1.164}
{'eval_valid_loss': 0.90869140625, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.203, 'eval_valid_steps_per_second': 279.301, 'epoch': 1.164}
{'loss': 0.9449, 'grad_norm': 3.152780608924196, 'learning_rate': 7.881777777777779e-06, 'epoch': 1.1644}
{'loss': 0.9467, 'grad_norm': 3.445875454883142, 'learning_rate': 7.880666666666667e-06, 'epoch': 1.1648}
{'loss': 0.9473, 'grad_norm': 3.358263000259789, 'learning_rate': 7.879555555555556e-06, 'epoch': 1.1652}
{'loss': 0.9472, 'grad_norm': 3.132832524718406, 'learning_rate': 7.878444444444445e-06, 'epoch': 1.1656}
{'loss': 0.9527, 'grad_norm': 3.0310442353245426, 'learning_rate': 7.877333333333333e-06, 'epoch': 1.166}
{'loss': 0.9372, 'grad_norm': 3.5489579692930335, 'learning_rate': 7.876222222222222e-06, 'epoch': 1.1663999999999999}
{'loss': 0.9223, 'grad_norm': 3.291362130688617, 'learning_rate': 7.875111111111112e-06, 'epoch': 1.1668}
{'loss': 0.9487, 'grad_norm': 3.653617661200533, 'learning_rate': 7.874000000000001e-06, 'epoch': 1.1672}
{'loss': 0.9544, 'grad_norm': 3.1855811532969494, 'learning_rate': 7.87288888888889e-06, 'epoch': 1.1676}
{'loss': 0.9471, 'grad_norm': 3.041874072872449, 'learning_rate': 7.871777777777779e-06, 'epoch': 1.168}
{'eval_valid_loss': 0.9052734375, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.176, 'eval_valid_steps_per_second': 278.294, 'epoch': 1.168}
{'loss': 0.9366, 'grad_norm': 3.256145993290517, 'learning_rate': 7.870666666666667e-06, 'epoch': 1.1684}
{'loss': 0.9361, 'grad_norm': 3.179697444440958, 'learning_rate': 7.869555555555556e-06, 'epoch': 1.1688}
{'loss': 0.94, 'grad_norm': 3.0993160885448505, 'learning_rate': 7.868444444444445e-06, 'epoch': 1.1692}
{'loss': 0.9355, 'grad_norm': 3.4708477498900345, 'learning_rate': 7.867333333333333e-06, 'epoch': 1.1696}
{'loss': 0.9381, 'grad_norm': 3.243716410277398, 'learning_rate': 7.866222222222222e-06, 'epoch': 1.17}
{'loss': 0.9514, 'grad_norm': 2.771361511604045, 'learning_rate': 7.865111111111113e-06, 'epoch': 1.1703999999999999}
{'loss': 0.9341, 'grad_norm': 3.274096875481332, 'learning_rate': 7.864000000000001e-06, 'epoch': 1.1708}
{'loss': 0.9385, 'grad_norm': 3.1120483082175796, 'learning_rate': 7.86288888888889e-06, 'epoch': 1.1712}
{'loss': 0.9364, 'grad_norm': 3.134952698826815, 'learning_rate': 7.861777777777779e-06, 'epoch': 1.1716}
{'loss': 0.9334, 'grad_norm': 3.2790484626838197, 'learning_rate': 7.860666666666667e-06, 'epoch': 1.172}
{'eval_valid_loss': 0.9052734375, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.552, 'eval_valid_steps_per_second': 279.138, 'epoch': 1.172}
{'loss': 0.9493, 'grad_norm': 3.253505301988713, 'learning_rate': 7.859555555555556e-06, 'epoch': 1.1724}
{'loss': 0.9363, 'grad_norm': 3.3497070120324937, 'learning_rate': 7.858444444444445e-06, 'epoch': 1.1728}
{'loss': 0.9485, 'grad_norm': 3.256295726989552, 'learning_rate': 7.857333333333334e-06, 'epoch': 1.1732}
{'loss': 0.9478, 'grad_norm': 2.928332175515311, 'learning_rate': 7.856222222222222e-06, 'epoch': 1.1736}
{'loss': 0.9413, 'grad_norm': 3.420246816869682, 'learning_rate': 7.855111111111113e-06, 'epoch': 1.174}
{'loss': 0.9458, 'grad_norm': 3.2481403532357156, 'learning_rate': 7.854e-06, 'epoch': 1.1743999999999999}
{'loss': 0.945, 'grad_norm': 3.675227902929764, 'learning_rate': 7.85288888888889e-06, 'epoch': 1.1748}
{'loss': 0.9396, 'grad_norm': 3.257551104992995, 'learning_rate': 7.851777777777779e-06, 'epoch': 1.1752}
{'loss': 0.9343, 'grad_norm': 3.0726116201426725, 'learning_rate': 7.850666666666668e-06, 'epoch': 1.1756}
{'loss': 0.9383, 'grad_norm': 3.0382321554598617, 'learning_rate': 7.849555555555556e-06, 'epoch': 1.176}
{'eval_valid_loss': 0.90576171875, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1126.832, 'eval_valid_steps_per_second': 281.708, 'epoch': 1.176}
{'loss': 0.9363, 'grad_norm': 3.1728536759049835, 'learning_rate': 7.848444444444445e-06, 'epoch': 1.1764000000000001}
{'loss': 0.9437, 'grad_norm': 3.259925358391685, 'learning_rate': 7.847333333333334e-06, 'epoch': 1.1768}
{'loss': 0.9396, 'grad_norm': 3.01938847923547, 'learning_rate': 7.846222222222222e-06, 'epoch': 1.1772}
{'loss': 0.942, 'grad_norm': 3.283912659178846, 'learning_rate': 7.845111111111113e-06, 'epoch': 1.1776}
{'loss': 0.9422, 'grad_norm': 3.0337091774192033, 'learning_rate': 7.844e-06, 'epoch': 1.178}
{'loss': 0.9421, 'grad_norm': 3.377605536271138, 'learning_rate': 7.84288888888889e-06, 'epoch': 1.1784}
{'loss': 0.9432, 'grad_norm': 3.0740505981990496, 'learning_rate': 7.841777777777779e-06, 'epoch': 1.1788}
{'loss': 0.9482, 'grad_norm': 3.1070706536449224, 'learning_rate': 7.840666666666668e-06, 'epoch': 1.1792}
{'loss': 0.9459, 'grad_norm': 3.3435613899870162, 'learning_rate': 7.839555555555556e-06, 'epoch': 1.1796}
{'loss': 0.9497, 'grad_norm': 3.2498885740839385, 'learning_rate': 7.838444444444445e-06, 'epoch': 1.18}
{'eval_valid_loss': 0.90673828125, 'eval_valid_runtime': 0.0884, 'eval_valid_samples_per_second': 1130.927, 'eval_valid_steps_per_second': 282.732, 'epoch': 1.18}
{'loss': 0.9374, 'grad_norm': 3.055036754106112, 'learning_rate': 7.837333333333334e-06, 'epoch': 1.1804000000000001}
{'loss': 0.9427, 'grad_norm': 3.3169201563925315, 'learning_rate': 7.836222222222222e-06, 'epoch': 1.1808}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9504, 'grad_norm': 3.09575328124417, 'learning_rate': 7.835111111111113e-06, 'epoch': 1.1812}
{'loss': 0.9446, 'grad_norm': 3.2481562813358544, 'learning_rate': 7.834e-06, 'epoch': 1.1816}
{'loss': 0.9526, 'grad_norm': 2.86902085814501, 'learning_rate': 7.83288888888889e-06, 'epoch': 1.182}
{'loss': 0.949, 'grad_norm': 3.232485260586867, 'learning_rate': 7.831777777777779e-06, 'epoch': 1.1824}
{'loss': 0.9319, 'grad_norm': 3.065843907200036, 'learning_rate': 7.830666666666668e-06, 'epoch': 1.1828}
{'loss': 0.946, 'grad_norm': 3.244904209509977, 'learning_rate': 7.829555555555556e-06, 'epoch': 1.1832}
{'loss': 0.9424, 'grad_norm': 3.373628973639844, 'learning_rate': 7.828444444444445e-06, 'epoch': 1.1836}
{'loss': 0.9382, 'grad_norm': 3.4388260451144865, 'learning_rate': 7.827333333333334e-06, 'epoch': 1.184}
{'eval_valid_loss': 0.90478515625, 'eval_valid_runtime': 0.097, 'eval_valid_samples_per_second': 1031.101, 'eval_valid_steps_per_second': 257.775, 'epoch': 1.184}
{'loss': 0.9377, 'grad_norm': 3.0707602720398604, 'learning_rate': 7.826222222222223e-06, 'epoch': 1.1844000000000001}
{'loss': 0.9458, 'grad_norm': 3.299106231584613, 'learning_rate': 7.825111111111113e-06, 'epoch': 1.1848}
{'loss': 0.9446, 'grad_norm': 2.888792725382857, 'learning_rate': 7.824e-06, 'epoch': 1.1852}
{'loss': 0.9513, 'grad_norm': 3.775650560140642, 'learning_rate': 7.82288888888889e-06, 'epoch': 1.1856}
{'loss': 0.9343, 'grad_norm': 3.3056861414658014, 'learning_rate': 7.821777777777777e-06, 'epoch': 1.186}
{'loss': 0.9406, 'grad_norm': 3.1300561913421974, 'learning_rate': 7.820666666666668e-06, 'epoch': 1.1864}
{'loss': 0.9344, 'grad_norm': 3.25854805891681, 'learning_rate': 7.819555555555557e-06, 'epoch': 1.1868}
{'loss': 0.9385, 'grad_norm': 3.4704551973164572, 'learning_rate': 7.818444444444445e-06, 'epoch': 1.1872}
{'loss': 0.9379, 'grad_norm': 3.514220029048713, 'learning_rate': 7.817333333333334e-06, 'epoch': 1.1876}
{'loss': 0.9573, 'grad_norm': 3.1411801127542716, 'learning_rate': 7.816222222222223e-06, 'epoch': 1.188}
{'eval_valid_loss': 0.90673828125, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.916, 'eval_valid_steps_per_second': 281.979, 'epoch': 1.188}
{'loss': 0.9307, 'grad_norm': 3.389873773002733, 'learning_rate': 7.815111111111113e-06, 'epoch': 1.1884000000000001}
{'loss': 0.9468, 'grad_norm': 3.2137811325268624, 'learning_rate': 7.814e-06, 'epoch': 1.1888}
{'loss': 0.9412, 'grad_norm': 3.0886061030716077, 'learning_rate': 7.81288888888889e-06, 'epoch': 1.1892}
{'loss': 0.9418, 'grad_norm': 3.127927820048518, 'learning_rate': 7.811777777777777e-06, 'epoch': 1.1896}
{'loss': 0.9375, 'grad_norm': 3.0684842206298426, 'learning_rate': 7.810666666666668e-06, 'epoch': 1.19}
{'loss': 0.9428, 'grad_norm': 3.462281373072937, 'learning_rate': 7.809555555555557e-06, 'epoch': 1.1904}
{'loss': 0.9291, 'grad_norm': 3.2142860883757964, 'learning_rate': 7.808444444444445e-06, 'epoch': 1.1908}
{'loss': 0.9325, 'grad_norm': 3.375040725179728, 'learning_rate': 7.807333333333334e-06, 'epoch': 1.1912}
{'loss': 0.954, 'grad_norm': 3.3659483539100656, 'learning_rate': 7.806222222222223e-06, 'epoch': 1.1916}
{'loss': 0.9447, 'grad_norm': 3.013203424423406, 'learning_rate': 7.805111111111111e-06, 'epoch': 1.192}
{'eval_valid_loss': 0.90380859375, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.918, 'eval_valid_steps_per_second': 279.979, 'epoch': 1.192}
{'loss': 0.9508, 'grad_norm': 2.978844126111847, 'learning_rate': 7.804e-06, 'epoch': 1.1924}
{'loss': 0.9192, 'grad_norm': 3.1011174552881364, 'learning_rate': 7.80288888888889e-06, 'epoch': 1.1928}
{'loss': 0.9406, 'grad_norm': 3.5046150230638236, 'learning_rate': 7.801777777777778e-06, 'epoch': 1.1932}
{'loss': 0.9324, 'grad_norm': 3.3527736387294858, 'learning_rate': 7.800666666666668e-06, 'epoch': 1.1936}
{'loss': 0.9354, 'grad_norm': 3.404824212223786, 'learning_rate': 7.799555555555555e-06, 'epoch': 1.194}
{'loss': 0.9462, 'grad_norm': 2.9731866711709203, 'learning_rate': 7.798444444444445e-06, 'epoch': 1.1944}
{'loss': 0.932, 'grad_norm': 3.0980098662031517, 'learning_rate': 7.797333333333334e-06, 'epoch': 1.1948}
{'loss': 0.9517, 'grad_norm': 3.2366983481649716, 'learning_rate': 7.796222222222223e-06, 'epoch': 1.1952}
{'loss': 0.9409, 'grad_norm': 3.2088359350326243, 'learning_rate': 7.795111111111112e-06, 'epoch': 1.1956}
{'loss': 0.9519, 'grad_norm': 3.3960775665887484, 'learning_rate': 7.794e-06, 'epoch': 1.196}
{'eval_valid_loss': 0.90625, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.168, 'eval_valid_steps_per_second': 279.042, 'epoch': 1.196}
{'loss': 0.9442, 'grad_norm': 3.226023695988786, 'learning_rate': 7.79288888888889e-06, 'epoch': 1.1964}
{'loss': 0.9339, 'grad_norm': 3.236810706929593, 'learning_rate': 7.791777777777778e-06, 'epoch': 1.1968}
{'loss': 0.945, 'grad_norm': 3.2587928120354612, 'learning_rate': 7.790666666666668e-06, 'epoch': 1.1972}
{'loss': 0.9499, 'grad_norm': 3.2926888126292257, 'learning_rate': 7.789555555555555e-06, 'epoch': 1.1976}
{'loss': 0.9372, 'grad_norm': 2.9867775817712547, 'learning_rate': 7.788444444444446e-06, 'epoch': 1.198}
{'loss': 0.9459, 'grad_norm': 3.3831831484536172, 'learning_rate': 7.787333333333334e-06, 'epoch': 1.1984}
{'loss': 0.9427, 'grad_norm': 3.280047895849413, 'learning_rate': 7.786222222222223e-06, 'epoch': 1.1988}
{'loss': 0.9329, 'grad_norm': 3.095686085169734, 'learning_rate': 7.785111111111112e-06, 'epoch': 1.1992}
{'loss': 0.9363, 'grad_norm': 3.085976515897544, 'learning_rate': 7.784e-06, 'epoch': 1.1996}
{'loss': 0.9461, 'grad_norm': 3.2895979909749475, 'learning_rate': 7.782888888888889e-06, 'epoch': 1.2}
{'eval_valid_loss': 0.9052734375, 'eval_valid_runtime': 0.1081, 'eval_valid_samples_per_second': 924.664, 'eval_valid_steps_per_second': 231.166, 'epoch': 1.2}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.9375, 'grad_norm': 3.144796485850856, 'learning_rate': 7.781777777777778e-06, 'epoch': 1.2004}
{'loss': 0.9445, 'grad_norm': 2.9597233318535823, 'learning_rate': 7.780666666666668e-06, 'epoch': 1.2008}
{'loss': 0.947, 'grad_norm': 3.1020248030108113, 'learning_rate': 7.779555555555555e-06, 'epoch': 1.2012}
{'loss': 0.9403, 'grad_norm': 3.2902385665901277, 'learning_rate': 7.778444444444446e-06, 'epoch': 1.2016}
{'loss': 0.9328, 'grad_norm': 3.660077903340588, 'learning_rate': 7.777444444444445e-06, 'epoch': 1.202}
{'loss': 0.9475, 'grad_norm': 3.1729327538112324, 'learning_rate': 7.776333333333334e-06, 'epoch': 1.2024}
{'loss': 0.9344, 'grad_norm': 3.0321850170293208, 'learning_rate': 7.775222222222223e-06, 'epoch': 1.2028}
{'loss': 0.9353, 'grad_norm': 3.2695734985247076, 'learning_rate': 7.774111111111113e-06, 'epoch': 1.2032}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.936, 'grad_norm': 3.155947311919469, 'learning_rate': 7.773e-06, 'epoch': 1.2036}
{'loss': 0.9455, 'grad_norm': 3.264565318718636, 'learning_rate': 7.77188888888889e-06, 'epoch': 1.204}
{'eval_valid_loss': 0.90380859375, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1111.136, 'eval_valid_steps_per_second': 277.784, 'epoch': 1.204}
{'loss': 0.9437, 'grad_norm': 2.9572337495713854, 'learning_rate': 7.770777777777777e-06, 'epoch': 1.2044}
{'loss': 0.9522, 'grad_norm': 3.4322910897346595, 'learning_rate': 7.769666666666668e-06, 'epoch': 1.2048}
{'loss': 0.9362, 'grad_norm': 3.0004248516930137, 'learning_rate': 7.768555555555556e-06, 'epoch': 1.2052}
{'loss': 0.9361, 'grad_norm': 3.2523652419865305, 'learning_rate': 7.767444444444445e-06, 'epoch': 1.2056}
{'loss': 0.9459, 'grad_norm': 2.9338150245732195, 'learning_rate': 7.766333333333334e-06, 'epoch': 1.206}
{'loss': 0.9447, 'grad_norm': 3.4775024514974677, 'learning_rate': 7.765222222222223e-06, 'epoch': 1.2064}
{'loss': 0.9437, 'grad_norm': 3.217993804841512, 'learning_rate': 7.764111111111113e-06, 'epoch': 1.2068}
{'loss': 0.9527, 'grad_norm': 3.234793179886068, 'learning_rate': 7.763e-06, 'epoch': 1.2072}
{'loss': 0.9574, 'grad_norm': 2.9538334395367305, 'learning_rate': 7.76188888888889e-06, 'epoch': 1.2076}
{'loss': 0.9544, 'grad_norm': 2.9742648149935658, 'learning_rate': 7.760777777777777e-06, 'epoch': 1.208}
{'eval_valid_loss': 0.90234375, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1116.763, 'eval_valid_steps_per_second': 279.191, 'epoch': 1.208}
{'loss': 0.9385, 'grad_norm': 2.82898972921978, 'learning_rate': 7.759666666666668e-06, 'epoch': 1.2084}
{'loss': 0.9312, 'grad_norm': 3.056312509299246, 'learning_rate': 7.758555555555557e-06, 'epoch': 1.2088}
{'loss': 0.9485, 'grad_norm': 3.0932818164488673, 'learning_rate': 7.757444444444445e-06, 'epoch': 1.2092}
{'loss': 0.9335, 'grad_norm': 3.6960397694221263, 'learning_rate': 7.756333333333334e-06, 'epoch': 1.2096}
{'loss': 0.9268, 'grad_norm': 3.0034390050169657, 'learning_rate': 7.755222222222223e-06, 'epoch': 1.21}
{'loss': 0.9415, 'grad_norm': 2.875058339397793, 'learning_rate': 7.754111111111111e-06, 'epoch': 1.2104}
{'loss': 0.9391, 'grad_norm': 3.1525113180594144, 'learning_rate': 7.753e-06, 'epoch': 1.2107999999999999}
{'loss': 0.9427, 'grad_norm': 2.812534946648391, 'learning_rate': 7.75188888888889e-06, 'epoch': 1.2112}
{'loss': 0.9326, 'grad_norm': 3.2075922820269427, 'learning_rate': 7.750777777777778e-06, 'epoch': 1.2116}
{'loss': 0.9331, 'grad_norm': 3.254182966356733, 'learning_rate': 7.749666666666668e-06, 'epoch': 1.212}
{'eval_valid_loss': 0.9033203125, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.765, 'eval_valid_steps_per_second': 278.941, 'epoch': 1.212}
{'loss': 0.949, 'grad_norm': 3.643500948028839, 'learning_rate': 7.748555555555555e-06, 'epoch': 1.2124}
{'loss': 0.9292, 'grad_norm': 3.055512797292491, 'learning_rate': 7.747444444444445e-06, 'epoch': 1.2128}
{'loss': 0.9402, 'grad_norm': 3.524956149646913, 'learning_rate': 7.746333333333334e-06, 'epoch': 1.2132}
{'loss': 0.9311, 'grad_norm': 3.0114642479775897, 'learning_rate': 7.745222222222223e-06, 'epoch': 1.2136}
{'loss': 0.9387, 'grad_norm': 3.5384699681231075, 'learning_rate': 7.744111111111112e-06, 'epoch': 1.214}
{'loss': 0.9281, 'grad_norm': 2.9097836147064116, 'learning_rate': 7.743e-06, 'epoch': 1.2144}
{'loss': 0.9317, 'grad_norm': 3.0520147645731925, 'learning_rate': 7.74188888888889e-06, 'epoch': 1.2147999999999999}
{'loss': 0.9431, 'grad_norm': 2.903320230380717, 'learning_rate': 7.740777777777778e-06, 'epoch': 1.2152}
{'loss': 0.9382, 'grad_norm': 3.3225853086101176, 'learning_rate': 7.739666666666668e-06, 'epoch': 1.2156}
{'loss': 0.9425, 'grad_norm': 3.4302120546208195, 'learning_rate': 7.738555555555555e-06, 'epoch': 1.216}
{'eval_valid_loss': 0.9052734375, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1109.657, 'eval_valid_steps_per_second': 277.414, 'epoch': 1.216}
{'loss': 0.943, 'grad_norm': 3.0701970668379874, 'learning_rate': 7.737444444444445e-06, 'epoch': 1.2164}
{'loss': 0.9414, 'grad_norm': 3.1238625840670005, 'learning_rate': 7.736333333333334e-06, 'epoch': 1.2168}
{'loss': 0.9479, 'grad_norm': 2.954881589698778, 'learning_rate': 7.735222222222223e-06, 'epoch': 1.2172}
{'loss': 0.9303, 'grad_norm': 2.8499989024377683, 'learning_rate': 7.734111111111112e-06, 'epoch': 1.2176}
{'loss': 0.9368, 'grad_norm': 3.3012662271709634, 'learning_rate': 7.733e-06, 'epoch': 1.218}
{'loss': 0.9342, 'grad_norm': 3.100986532930608, 'learning_rate': 7.731888888888889e-06, 'epoch': 1.2184}
{'loss': 0.9458, 'grad_norm': 3.1719429309504807, 'learning_rate': 7.730777777777778e-06, 'epoch': 1.2187999999999999}
{'loss': 0.9308, 'grad_norm': 3.1479471075596153, 'learning_rate': 7.729666666666668e-06, 'epoch': 1.2192}
{'loss': 0.9366, 'grad_norm': 3.1500021688514384, 'learning_rate': 7.728555555555555e-06, 'epoch': 1.2196}
{'loss': 0.943, 'grad_norm': 3.288571037851677, 'learning_rate': 7.727444444444446e-06, 'epoch': 1.22}
{'eval_valid_loss': 0.90380859375, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1105.626, 'eval_valid_steps_per_second': 276.407, 'epoch': 1.22}
{'loss': 0.9346, 'grad_norm': 3.01334509372702, 'learning_rate': 7.726333333333334e-06, 'epoch': 1.2204}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9459, 'grad_norm': 3.183122106146719, 'learning_rate': 7.725222222222223e-06, 'epoch': 1.2208}
{'loss': 0.9396, 'grad_norm': 3.2477788028897767, 'learning_rate': 7.724111111111112e-06, 'epoch': 1.2212}
{'loss': 0.9202, 'grad_norm': 3.207446007619787, 'learning_rate': 7.723e-06, 'epoch': 1.2216}
{'loss': 0.9376, 'grad_norm': 2.8920013038057206, 'learning_rate': 7.721888888888889e-06, 'epoch': 1.222}
{'loss': 0.9308, 'grad_norm': 3.1869148764516027, 'learning_rate': 7.720777777777778e-06, 'epoch': 1.2224}
{'loss': 0.9332, 'grad_norm': 3.0219749540664487, 'learning_rate': 7.719666666666668e-06, 'epoch': 1.2227999999999999}
{'loss': 0.9365, 'grad_norm': 3.2445011662007737, 'learning_rate': 7.718555555555555e-06, 'epoch': 1.2232}
{'loss': 0.9432, 'grad_norm': 3.2300965026635295, 'learning_rate': 7.717444444444446e-06, 'epoch': 1.2236}
{'loss': 0.9348, 'grad_norm': 2.8705217375517806, 'learning_rate': 7.716333333333333e-06, 'epoch': 1.224}
{'eval_valid_loss': 0.90380859375, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1115.082, 'eval_valid_steps_per_second': 278.771, 'epoch': 1.224}
{'loss': 0.9379, 'grad_norm': 3.394036374382733, 'learning_rate': 7.715222222222223e-06, 'epoch': 1.2244}
{'loss': 0.9319, 'grad_norm': 3.2079730120532037, 'learning_rate': 7.714111111111112e-06, 'epoch': 1.2248}
{'loss': 0.9417, 'grad_norm': 3.0377945890547093, 'learning_rate': 7.713e-06, 'epoch': 1.2252}
{'loss': 0.944, 'grad_norm': 3.0923543729855516, 'learning_rate': 7.71188888888889e-06, 'epoch': 1.2256}
{'loss': 0.9395, 'grad_norm': 2.799456582384905, 'learning_rate': 7.710777777777778e-06, 'epoch': 1.226}
{'loss': 0.9433, 'grad_norm': 2.99616067306797, 'learning_rate': 7.709666666666668e-06, 'epoch': 1.2264}
{'loss': 0.9428, 'grad_norm': 3.7282667281781388, 'learning_rate': 7.708555555555555e-06, 'epoch': 1.2268}
{'loss': 0.9294, 'grad_norm': 3.3386329731628033, 'learning_rate': 7.707444444444446e-06, 'epoch': 1.2272}
{'loss': 0.9349, 'grad_norm': 3.260736612256343, 'learning_rate': 7.706333333333333e-06, 'epoch': 1.2276}
{'loss': 0.9464, 'grad_norm': 3.2311499268402883, 'learning_rate': 7.705222222222223e-06, 'epoch': 1.228}
{'eval_valid_loss': 0.90478515625, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.531, 'eval_valid_steps_per_second': 279.133, 'epoch': 1.228}
{'loss': 0.9474, 'grad_norm': 2.9466132168072026, 'learning_rate': 7.704111111111112e-06, 'epoch': 1.2284}
{'loss': 0.9323, 'grad_norm': 3.363150121101967, 'learning_rate': 7.703e-06, 'epoch': 1.2288000000000001}
{'loss': 0.9497, 'grad_norm': 3.298054488662485, 'learning_rate': 7.70188888888889e-06, 'epoch': 1.2292}
{'loss': 0.9313, 'grad_norm': 3.1769826383215367, 'learning_rate': 7.700777777777778e-06, 'epoch': 1.2296}
{'loss': 0.9373, 'grad_norm': 3.4351330844635073, 'learning_rate': 7.699666666666667e-06, 'epoch': 1.23}
{'loss': 0.9365, 'grad_norm': 2.970627000624651, 'learning_rate': 7.698555555555555e-06, 'epoch': 1.2304}
{'loss': 0.9287, 'grad_norm': 3.473876060910921, 'learning_rate': 7.697444444444446e-06, 'epoch': 1.2308}
{'loss': 0.943, 'grad_norm': 3.181790062080328, 'learning_rate': 7.696333333333333e-06, 'epoch': 1.2312}
{'loss': 0.932, 'grad_norm': 3.1387745069567186, 'learning_rate': 7.695222222222223e-06, 'epoch': 1.2316}
{'loss': 0.938, 'grad_norm': 3.152028820846439, 'learning_rate': 7.694111111111112e-06, 'epoch': 1.232}
{'eval_valid_loss': 0.90380859375, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1120.714, 'eval_valid_steps_per_second': 280.178, 'epoch': 1.232}
{'loss': 0.9287, 'grad_norm': 3.1669881034304823, 'learning_rate': 7.693e-06, 'epoch': 1.2324}
{'loss': 0.9523, 'grad_norm': 2.971212941501532, 'learning_rate': 7.69188888888889e-06, 'epoch': 1.2328000000000001}
{'loss': 0.9402, 'grad_norm': 3.523839737289776, 'learning_rate': 7.690777777777778e-06, 'epoch': 1.2332}
{'loss': 0.9471, 'grad_norm': 3.2806160087266014, 'learning_rate': 7.689666666666667e-06, 'epoch': 1.2336}
{'loss': 0.9396, 'grad_norm': 3.60040145331045, 'learning_rate': 7.688555555555556e-06, 'epoch': 1.234}
{'loss': 0.9461, 'grad_norm': 3.042093613894614, 'learning_rate': 7.687444444444446e-06, 'epoch': 1.2344}
{'loss': 0.9426, 'grad_norm': 3.2106493614093843, 'learning_rate': 7.686333333333333e-06, 'epoch': 1.2348}
{'loss': 0.9398, 'grad_norm': 3.2010588332134966, 'learning_rate': 7.685222222222223e-06, 'epoch': 1.2352}
{'loss': 0.9401, 'grad_norm': 3.095415360707256, 'learning_rate': 7.684111111111112e-06, 'epoch': 1.2356}
{'loss': 0.934, 'grad_norm': 3.444945196870204, 'learning_rate': 7.683e-06, 'epoch': 1.236}
{'eval_valid_loss': 0.9033203125, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1119.15, 'eval_valid_steps_per_second': 279.787, 'epoch': 1.236}
{'loss': 0.9427, 'grad_norm': 3.110402239471996, 'learning_rate': 7.68188888888889e-06, 'epoch': 1.2364}
{'loss': 0.9379, 'grad_norm': 3.2784939135602147, 'learning_rate': 7.680777777777778e-06, 'epoch': 1.2368000000000001}
{'loss': 0.9399, 'grad_norm': 3.1660391673131154, 'learning_rate': 7.679666666666667e-06, 'epoch': 1.2372}
{'loss': 0.9427, 'grad_norm': 3.4488084577145903, 'learning_rate': 7.678555555555556e-06, 'epoch': 1.2376}
{'loss': 0.9352, 'grad_norm': 3.199124037909148, 'learning_rate': 7.677444444444444e-06, 'epoch': 1.238}
{'loss': 0.9364, 'grad_norm': 3.262324010918181, 'learning_rate': 7.676333333333333e-06, 'epoch': 1.2384}
{'loss': 0.9402, 'grad_norm': 2.9266091717258726, 'learning_rate': 7.675222222222223e-06, 'epoch': 1.2388}
{'loss': 0.9358, 'grad_norm': 3.0849681828804103, 'learning_rate': 7.674111111111112e-06, 'epoch': 1.2392}
{'loss': 0.94, 'grad_norm': 3.349092378005197, 'learning_rate': 7.673000000000001e-06, 'epoch': 1.2396}
{'loss': 0.9324, 'grad_norm': 2.993680646168687, 'learning_rate': 7.67188888888889e-06, 'epoch': 1.24}
{'eval_valid_loss': 0.90380859375, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.138, 'eval_valid_steps_per_second': 279.284, 'epoch': 1.24}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.945, 'grad_norm': 2.646789544447612, 'learning_rate': 7.670777777777778e-06, 'epoch': 1.2404}
{'loss': 0.9298, 'grad_norm': 3.112602897076875, 'learning_rate': 7.669666666666667e-06, 'epoch': 1.2408}
{'loss': 0.9326, 'grad_norm': 3.018587840998731, 'learning_rate': 7.668555555555556e-06, 'epoch': 1.2412}
{'loss': 0.9375, 'grad_norm': 3.2821698216508186, 'learning_rate': 7.667444444444444e-06, 'epoch': 1.2416}
{'loss': 0.9423, 'grad_norm': 3.0674138448627732, 'learning_rate': 7.666444444444446e-06, 'epoch': 1.242}
{'loss': 0.9383, 'grad_norm': 3.400715172106187, 'learning_rate': 7.665333333333333e-06, 'epoch': 1.2424}
{'loss': 0.9373, 'grad_norm': 3.0615946735569493, 'learning_rate': 7.664222222222223e-06, 'epoch': 1.2428}
{'loss': 0.9233, 'grad_norm': 2.9542004781508933, 'learning_rate': 7.663111111111112e-06, 'epoch': 1.2432}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9339, 'grad_norm': 3.226664368358364, 'learning_rate': 7.662e-06, 'epoch': 1.2436}
{'loss': 0.939, 'grad_norm': 3.0901729731462395, 'learning_rate': 7.66088888888889e-06, 'epoch': 1.244}
{'eval_valid_loss': 0.90283203125, 'eval_valid_runtime': 0.0884, 'eval_valid_samples_per_second': 1130.696, 'eval_valid_steps_per_second': 282.674, 'epoch': 1.244}
{'loss': 0.9349, 'grad_norm': 3.249992297236778, 'learning_rate': 7.659777777777778e-06, 'epoch': 1.2444}
{'loss': 0.9374, 'grad_norm': 3.287886411562877, 'learning_rate': 7.658666666666668e-06, 'epoch': 1.2448}
{'loss': 0.9431, 'grad_norm': 3.3175893050579988, 'learning_rate': 7.657555555555555e-06, 'epoch': 1.2452}
{'loss': 0.9426, 'grad_norm': 3.2145620804383066, 'learning_rate': 7.656444444444446e-06, 'epoch': 1.2456}
{'loss': 0.9237, 'grad_norm': 3.430249891464758, 'learning_rate': 7.655333333333333e-06, 'epoch': 1.246}
{'loss': 0.9451, 'grad_norm': 3.1849211283459335, 'learning_rate': 7.654222222222223e-06, 'epoch': 1.2464}
{'loss': 0.9381, 'grad_norm': 2.9166374897632816, 'learning_rate': 7.653111111111112e-06, 'epoch': 1.2468}
{'loss': 0.9469, 'grad_norm': 3.302761084540505, 'learning_rate': 7.652e-06, 'epoch': 1.2472}
{'loss': 0.9448, 'grad_norm': 3.1542495015369125, 'learning_rate': 7.65088888888889e-06, 'epoch': 1.2476}
{'loss': 0.9367, 'grad_norm': 2.9886627552121183, 'learning_rate': 7.649777777777778e-06, 'epoch': 1.248}
{'eval_valid_loss': 0.89990234375, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1109.155, 'eval_valid_steps_per_second': 277.289, 'epoch': 1.248}
{'loss': 0.932, 'grad_norm': 3.2087930354110528, 'learning_rate': 7.648666666666667e-06, 'epoch': 1.2484}
{'loss': 0.9423, 'grad_norm': 3.124621368358479, 'learning_rate': 7.647555555555555e-06, 'epoch': 1.2488}
{'loss': 0.9362, 'grad_norm': 3.1790273482534404, 'learning_rate': 7.646444444444446e-06, 'epoch': 1.2492}
{'loss': 0.9319, 'grad_norm': 3.269396607100244, 'learning_rate': 7.645333333333333e-06, 'epoch': 1.2496}
{'loss': 0.9333, 'grad_norm': 2.953559369228822, 'learning_rate': 7.644222222222223e-06, 'epoch': 1.25}
{'loss': 0.947, 'grad_norm': 3.0747373499775628, 'learning_rate': 7.643111111111112e-06, 'epoch': 1.2504}
{'loss': 0.9431, 'grad_norm': 3.373470392459946, 'learning_rate': 7.642e-06, 'epoch': 1.2508}
{'loss': 0.9362, 'grad_norm': 2.985489162958857, 'learning_rate': 7.64088888888889e-06, 'epoch': 1.2511999999999999}
{'loss': 0.9454, 'grad_norm': 2.978054293566701, 'learning_rate': 7.639777777777778e-06, 'epoch': 1.2516}
{'loss': 0.9334, 'grad_norm': 3.220398795402836, 'learning_rate': 7.638666666666667e-06, 'epoch': 1.252}
{'eval_valid_loss': 0.9013671875, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1106.729, 'eval_valid_steps_per_second': 276.682, 'epoch': 1.252}
{'loss': 0.9358, 'grad_norm': 3.123605894021536, 'learning_rate': 7.637555555555556e-06, 'epoch': 1.2524}
{'loss': 0.9352, 'grad_norm': 3.429566452914206, 'learning_rate': 7.636444444444446e-06, 'epoch': 1.2528000000000001}
{'loss': 0.9315, 'grad_norm': 3.3279381730917192, 'learning_rate': 7.635333333333333e-06, 'epoch': 1.2532}
{'loss': 0.9363, 'grad_norm': 3.2493273515797547, 'learning_rate': 7.634222222222223e-06, 'epoch': 1.2536}
{'loss': 0.9346, 'grad_norm': 3.6446118770074887, 'learning_rate': 7.633111111111112e-06, 'epoch': 1.254}
{'loss': 0.94, 'grad_norm': 2.9440914020874964, 'learning_rate': 7.632e-06, 'epoch': 1.2544}
{'loss': 0.9429, 'grad_norm': 3.1133162665222676, 'learning_rate': 7.63088888888889e-06, 'epoch': 1.2548}
{'loss': 0.9324, 'grad_norm': 3.1876124848358356, 'learning_rate': 7.629777777777778e-06, 'epoch': 1.2551999999999999}
{'loss': 0.9328, 'grad_norm': 3.118413927704196, 'learning_rate': 7.628666666666668e-06, 'epoch': 1.2556}
{'loss': 0.9243, 'grad_norm': 3.281459365704741, 'learning_rate': 7.627555555555556e-06, 'epoch': 1.256}
{'eval_valid_loss': 0.8994140625, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1110.142, 'eval_valid_steps_per_second': 277.535, 'epoch': 1.256}
{'loss': 0.9374, 'grad_norm': 3.0309817303585485, 'learning_rate': 7.626444444444445e-06, 'epoch': 1.2564}
{'loss': 0.9271, 'grad_norm': 3.388185634411765, 'learning_rate': 7.625333333333333e-06, 'epoch': 1.2568}
{'loss': 0.946, 'grad_norm': 3.156190012371204, 'learning_rate': 7.624222222222223e-06, 'epoch': 1.2572}
{'loss': 0.9475, 'grad_norm': 3.357744291313044, 'learning_rate': 7.623111111111112e-06, 'epoch': 1.2576}
{'loss': 0.943, 'grad_norm': 3.0529772758862688, 'learning_rate': 7.622000000000001e-06, 'epoch': 1.258}
{'loss': 0.9408, 'grad_norm': 3.1677112310049864, 'learning_rate': 7.62088888888889e-06, 'epoch': 1.2584}
{'loss': 0.9406, 'grad_norm': 2.9373516390244205, 'learning_rate': 7.619777777777778e-06, 'epoch': 1.2588}
{'loss': 0.9434, 'grad_norm': 2.9489055745157344, 'learning_rate': 7.618666666666668e-06, 'epoch': 1.2591999999999999}
{'loss': 0.9412, 'grad_norm': 3.3696244044509447, 'learning_rate': 7.617555555555556e-06, 'epoch': 1.2596}
{'loss': 0.931, 'grad_norm': 3.225662900380127, 'learning_rate': 7.616444444444445e-06, 'epoch': 1.26}
{'eval_valid_loss': 0.90185546875, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1122.15, 'eval_valid_steps_per_second': 280.537, 'epoch': 1.26}
{'loss': 0.9355, 'grad_norm': 2.929356853900146, 'learning_rate': 7.615333333333333e-06, 'epoch': 1.2604}
{'loss': 0.9387, 'grad_norm': 3.2296513142471035, 'learning_rate': 7.614222222222223e-06, 'epoch': 1.2608}
{'loss': 0.9326, 'grad_norm': 2.88319958824992, 'learning_rate': 7.613111111111112e-06, 'epoch': 1.2612}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9375, 'grad_norm': 3.3048096311830797, 'learning_rate': 7.612e-06, 'epoch': 1.2616}
{'loss': 0.9425, 'grad_norm': 3.2997289752205137, 'learning_rate': 7.61088888888889e-06, 'epoch': 1.262}
{'loss': 0.9299, 'grad_norm': 3.239195516296062, 'learning_rate': 7.609777777777778e-06, 'epoch': 1.2624}
{'loss': 0.9322, 'grad_norm': 3.1140325350547116, 'learning_rate': 7.608666666666667e-06, 'epoch': 1.2628}
{'loss': 0.931, 'grad_norm': 3.2197157235117873, 'learning_rate': 7.607555555555556e-06, 'epoch': 1.2631999999999999}
{'loss': 0.9376, 'grad_norm': 2.934947568792287, 'learning_rate': 7.606444444444445e-06, 'epoch': 1.2636}
{'loss': 0.9402, 'grad_norm': 3.168406013011006, 'learning_rate': 7.605333333333333e-06, 'epoch': 1.264}
{'eval_valid_loss': 0.9033203125, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1121.691, 'eval_valid_steps_per_second': 280.423, 'epoch': 1.264}
{'loss': 0.9319, 'grad_norm': 3.2353928848607634, 'learning_rate': 7.604222222222223e-06, 'epoch': 1.2644}
{'loss': 0.9534, 'grad_norm': 3.014545623044297, 'learning_rate': 7.603111111111112e-06, 'epoch': 1.2648}
{'loss': 0.935, 'grad_norm': 2.98984128833806, 'learning_rate': 7.602e-06, 'epoch': 1.2652}
{'loss': 0.9244, 'grad_norm': 3.0025948289702313, 'learning_rate': 7.60088888888889e-06, 'epoch': 1.2656}
{'loss': 0.9418, 'grad_norm': 2.8679271856053465, 'learning_rate': 7.5997777777777785e-06, 'epoch': 1.266}
{'loss': 0.9285, 'grad_norm': 3.159243278511965, 'learning_rate': 7.598666666666667e-06, 'epoch': 1.2664}
{'loss': 0.9387, 'grad_norm': 3.2576148798806486, 'learning_rate': 7.597555555555556e-06, 'epoch': 1.2668}
{'loss': 0.9396, 'grad_norm': 3.0361254800270547, 'learning_rate': 7.5964444444444455e-06, 'epoch': 1.2671999999999999}
{'loss': 0.9478, 'grad_norm': 3.222874369320092, 'learning_rate': 7.595333333333333e-06, 'epoch': 1.2676}
{'loss': 0.9347, 'grad_norm': 3.0947217763693415, 'learning_rate': 7.594222222222223e-06, 'epoch': 1.268}
{'eval_valid_loss': 0.8994140625, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1124.798, 'eval_valid_steps_per_second': 281.199, 'epoch': 1.268}
{'loss': 0.9405, 'grad_norm': 3.0240437000948184, 'learning_rate': 7.5931111111111125e-06, 'epoch': 1.2684}
{'loss': 0.9341, 'grad_norm': 3.1989932417427025, 'learning_rate': 7.592e-06, 'epoch': 1.2688}
{'loss': 0.9354, 'grad_norm': 2.8000500414838467, 'learning_rate': 7.59088888888889e-06, 'epoch': 1.2692}
{'loss': 0.9271, 'grad_norm': 2.9669688704197643, 'learning_rate': 7.589777777777778e-06, 'epoch': 1.2696}
{'loss': 0.9339, 'grad_norm': 3.081255139391097, 'learning_rate': 7.588666666666667e-06, 'epoch': 1.27}
{'loss': 0.9287, 'grad_norm': 2.738379782782702, 'learning_rate': 7.587555555555556e-06, 'epoch': 1.2704}
{'loss': 0.9509, 'grad_norm': 3.171072571827762, 'learning_rate': 7.5864444444444456e-06, 'epoch': 1.2708}
{'loss': 0.9369, 'grad_norm': 2.992724517360699, 'learning_rate': 7.5853333333333334e-06, 'epoch': 1.2711999999999999}
{'loss': 0.9409, 'grad_norm': 3.1293758559030587, 'learning_rate': 7.584222222222223e-06, 'epoch': 1.2716}
{'loss': 0.9333, 'grad_norm': 3.341509184055702, 'learning_rate': 7.5831111111111126e-06, 'epoch': 1.272}
{'eval_valid_loss': 0.9013671875, 'eval_valid_runtime': 0.089, 'eval_valid_samples_per_second': 1124.207, 'eval_valid_steps_per_second': 281.052, 'epoch': 1.272}
{'loss': 0.9181, 'grad_norm': 3.12413752098018, 'learning_rate': 7.582e-06, 'epoch': 1.2724}
{'loss': 0.9234, 'grad_norm': 3.2185252490787404, 'learning_rate': 7.58088888888889e-06, 'epoch': 1.2728}
{'loss': 0.9305, 'grad_norm': 3.328422201793177, 'learning_rate': 7.579777777777778e-06, 'epoch': 1.2732}
{'loss': 0.933, 'grad_norm': 3.1469311577522276, 'learning_rate': 7.578666666666667e-06, 'epoch': 1.2736}
{'loss': 0.9404, 'grad_norm': 2.8037220347791743, 'learning_rate': 7.577555555555556e-06, 'epoch': 1.274}
{'loss': 0.9429, 'grad_norm': 3.1359938357490904, 'learning_rate': 7.576444444444445e-06, 'epoch': 1.2744}
{'loss': 0.9269, 'grad_norm': 3.004166888176587, 'learning_rate': 7.5753333333333335e-06, 'epoch': 1.2748}
{'loss': 0.932, 'grad_norm': 3.2015863677401613, 'learning_rate': 7.574222222222223e-06, 'epoch': 1.2752}
{'loss': 0.9298, 'grad_norm': 3.253020890811737, 'learning_rate': 7.573111111111112e-06, 'epoch': 1.2756}
{'loss': 0.9303, 'grad_norm': 3.0198360353447202, 'learning_rate': 7.5720000000000005e-06, 'epoch': 1.276}
{'eval_valid_loss': 0.90234375, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1129.183, 'eval_valid_steps_per_second': 282.296, 'epoch': 1.276}
{'loss': 0.9489, 'grad_norm': 3.5906081522122193, 'learning_rate': 7.57088888888889e-06, 'epoch': 1.2764}
{'loss': 0.9409, 'grad_norm': 3.1161068450938734, 'learning_rate': 7.569777777777778e-06, 'epoch': 1.2768}
{'loss': 0.9252, 'grad_norm': 2.9263840424913385, 'learning_rate': 7.5686666666666675e-06, 'epoch': 1.2772000000000001}
{'loss': 0.9263, 'grad_norm': 3.2249735690667602, 'learning_rate': 7.567555555555555e-06, 'epoch': 1.2776}
{'loss': 0.9327, 'grad_norm': 3.030343519919274, 'learning_rate': 7.566444444444445e-06, 'epoch': 1.278}
{'loss': 0.941, 'grad_norm': 3.244841241024126, 'learning_rate': 7.565333333333334e-06, 'epoch': 1.2784}
{'loss': 0.9444, 'grad_norm': 3.3247265007714004, 'learning_rate': 7.564222222222223e-06, 'epoch': 1.2788}
{'loss': 0.9344, 'grad_norm': 3.2203408263625084, 'learning_rate': 7.563111111111112e-06, 'epoch': 1.2792}
{'loss': 0.94, 'grad_norm': 3.0488777132174327, 'learning_rate': 7.562000000000001e-06, 'epoch': 1.2796}
{'loss': 0.9279, 'grad_norm': 3.1435062514926013, 'learning_rate': 7.56088888888889e-06, 'epoch': 1.28}
{'eval_valid_loss': 0.8994140625, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.747, 'eval_valid_steps_per_second': 281.937, 'epoch': 1.28}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.9371, 'grad_norm': 3.2932648078807367, 'learning_rate': 7.559777777777778e-06, 'epoch': 1.2804}
{'loss': 0.9346, 'grad_norm': 3.2533230482164255, 'learning_rate': 7.558666666666668e-06, 'epoch': 1.2808}
{'loss': 0.9293, 'grad_norm': 3.003517532496864, 'learning_rate': 7.5575555555555555e-06, 'epoch': 1.2812000000000001}
{'loss': 0.9317, 'grad_norm': 3.137081462558758, 'learning_rate': 7.556444444444445e-06, 'epoch': 1.2816}
{'loss': 0.9269, 'grad_norm': 3.504958234067429, 'learning_rate': 7.555444444444445e-06, 'epoch': 1.282}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9245, 'grad_norm': 3.1259519366909436, 'learning_rate': 7.554333333333333e-06, 'epoch': 1.2824}
{'loss': 0.9276, 'grad_norm': 4.358000781401887, 'learning_rate': 7.553222222222223e-06, 'epoch': 1.2828}
{'loss': 0.9393, 'grad_norm': 3.062930534228305, 'learning_rate': 7.552111111111112e-06, 'epoch': 1.2832}
{'loss': 0.9393, 'grad_norm': 3.312253924892443, 'learning_rate': 7.551e-06, 'epoch': 1.2836}
{'loss': 0.9334, 'grad_norm': 3.5557962050971836, 'learning_rate': 7.54988888888889e-06, 'epoch': 1.284}
{'eval_valid_loss': 0.8994140625, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.962, 'eval_valid_steps_per_second': 279.991, 'epoch': 1.284}
{'loss': 0.9294, 'grad_norm': 2.875701476640744, 'learning_rate': 7.5487777777777785e-06, 'epoch': 1.2844}
{'loss': 0.9296, 'grad_norm': 3.017413937938209, 'learning_rate': 7.547666666666667e-06, 'epoch': 1.2848}
{'loss': 0.938, 'grad_norm': 2.82637782424044, 'learning_rate': 7.546555555555556e-06, 'epoch': 1.2852000000000001}
{'loss': 0.952, 'grad_norm': 3.0346795374188686, 'learning_rate': 7.5454444444444455e-06, 'epoch': 1.2856}
{'loss': 0.9219, 'grad_norm': 3.4073638057429, 'learning_rate': 7.544333333333333e-06, 'epoch': 1.286}
{'loss': 0.9378, 'grad_norm': 3.242369478932577, 'learning_rate': 7.543222222222223e-06, 'epoch': 1.2864}
{'loss': 0.9417, 'grad_norm': 3.560876844625196, 'learning_rate': 7.5421111111111124e-06, 'epoch': 1.2868}
{'loss': 0.9393, 'grad_norm': 3.030105315697835, 'learning_rate': 7.541e-06, 'epoch': 1.2872}
{'loss': 0.9386, 'grad_norm': 3.0906086781440103, 'learning_rate': 7.53988888888889e-06, 'epoch': 1.2876}
{'loss': 0.9267, 'grad_norm': 3.1594708693209252, 'learning_rate': 7.5387777777777786e-06, 'epoch': 1.288}
{'eval_valid_loss': 0.9013671875, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1124.97, 'eval_valid_steps_per_second': 281.242, 'epoch': 1.288}
{'loss': 0.9247, 'grad_norm': 3.0219575872644975, 'learning_rate': 7.537666666666667e-06, 'epoch': 1.2884}
{'loss': 0.9198, 'grad_norm': 3.05188523171495, 'learning_rate': 7.536555555555556e-06, 'epoch': 1.2888}
{'loss': 0.9456, 'grad_norm': 3.01602173206673, 'learning_rate': 7.5354444444444456e-06, 'epoch': 1.2892000000000001}
{'loss': 0.9481, 'grad_norm': 3.0297243762317336, 'learning_rate': 7.534333333333333e-06, 'epoch': 1.2896}
{'loss': 0.9307, 'grad_norm': 3.016334272393842, 'learning_rate': 7.533222222222223e-06, 'epoch': 1.29}
{'loss': 0.9322, 'grad_norm': 3.0768385101237476, 'learning_rate': 7.5321111111111125e-06, 'epoch': 1.2904}
{'loss': 0.9246, 'grad_norm': 3.571242079974703, 'learning_rate': 7.531e-06, 'epoch': 1.2908}
{'loss': 0.9258, 'grad_norm': 3.4601973782039672, 'learning_rate': 7.52988888888889e-06, 'epoch': 1.2912}
{'loss': 0.933, 'grad_norm': 2.965297970763833, 'learning_rate': 7.528777777777778e-06, 'epoch': 1.2916}
{'loss': 0.93, 'grad_norm': 3.3429531591827, 'learning_rate': 7.527666666666667e-06, 'epoch': 1.292}
{'eval_valid_loss': 0.900390625, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1128.693, 'eval_valid_steps_per_second': 282.173, 'epoch': 1.292}
{'loss': 0.9433, 'grad_norm': 2.95934932459939, 'learning_rate': 7.526555555555556e-06, 'epoch': 1.2924}
{'loss': 0.9258, 'grad_norm': 3.2359322655164378, 'learning_rate': 7.525444444444445e-06, 'epoch': 1.2928}
{'loss': 0.9352, 'grad_norm': 2.9312898450148293, 'learning_rate': 7.5243333333333335e-06, 'epoch': 1.2932000000000001}
{'loss': 0.9396, 'grad_norm': 2.866655087216596, 'learning_rate': 7.523222222222223e-06, 'epoch': 1.2936}
{'loss': 0.9417, 'grad_norm': 3.0231966519101023, 'learning_rate': 7.522111111111113e-06, 'epoch': 1.294}
{'loss': 0.9307, 'grad_norm': 3.113459343883843, 'learning_rate': 7.5210000000000005e-06, 'epoch': 1.2944}
{'loss': 0.9386, 'grad_norm': 2.992844870444939, 'learning_rate': 7.51988888888889e-06, 'epoch': 1.2948}
{'loss': 0.9455, 'grad_norm': 3.310303472612666, 'learning_rate': 7.518777777777778e-06, 'epoch': 1.2952}
{'loss': 0.9314, 'grad_norm': 3.3964207420069776, 'learning_rate': 7.5176666666666675e-06, 'epoch': 1.2955999999999999}
{'loss': 0.9331, 'grad_norm': 3.4569234367843613, 'learning_rate': 7.516555555555556e-06, 'epoch': 1.296}
{'eval_valid_loss': 0.89990234375, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1119.185, 'eval_valid_steps_per_second': 279.796, 'epoch': 1.296}
{'loss': 0.9568, 'grad_norm': 3.110000547427408, 'learning_rate': 7.515444444444445e-06, 'epoch': 1.2964}
{'loss': 0.9284, 'grad_norm': 3.2720868367258507, 'learning_rate': 7.514333333333334e-06, 'epoch': 1.2968}
{'loss': 0.9295, 'grad_norm': 3.1807381208756103, 'learning_rate': 7.513222222222223e-06, 'epoch': 1.2972000000000001}
{'loss': 0.9391, 'grad_norm': 3.1205863301444294, 'learning_rate': 7.512111111111112e-06, 'epoch': 1.2976}
{'loss': 0.9434, 'grad_norm': 3.3077847190847174, 'learning_rate': 7.511000000000001e-06, 'epoch': 1.298}
{'loss': 0.9331, 'grad_norm': 3.2424102890498476, 'learning_rate': 7.50988888888889e-06, 'epoch': 1.2984}
{'loss': 0.9269, 'grad_norm': 3.1512262474202912, 'learning_rate': 7.508777777777778e-06, 'epoch': 1.2988}
{'loss': 0.9265, 'grad_norm': 3.021569278562278, 'learning_rate': 7.5076666666666676e-06, 'epoch': 1.2992}
{'loss': 0.937, 'grad_norm': 3.161025919033518, 'learning_rate': 7.5065555555555554e-06, 'epoch': 1.2995999999999999}
{'loss': 0.9263, 'grad_norm': 3.124462434308745, 'learning_rate': 7.505444444444445e-06, 'epoch': 1.3}
{'eval_valid_loss': 0.8994140625, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1125.861, 'eval_valid_steps_per_second': 281.465, 'epoch': 1.3}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9174, 'grad_norm': 3.1347234895435347, 'learning_rate': 7.504333333333334e-06, 'epoch': 1.3004}
{'loss': 0.9355, 'grad_norm': 2.87240894322705, 'learning_rate': 7.503222222222223e-06, 'epoch': 1.3008}
{'loss': 0.9346, 'grad_norm': 3.2054949018140046, 'learning_rate': 7.502111111111112e-06, 'epoch': 1.3012000000000001}
{'loss': 0.9336, 'grad_norm': 3.4309347834786115, 'learning_rate': 7.501000000000001e-06, 'epoch': 1.3016}
{'loss': 0.9227, 'grad_norm': 3.2411830035434286, 'learning_rate': 7.49988888888889e-06, 'epoch': 1.302}
{'loss': 0.9244, 'grad_norm': 3.2789223362217004, 'learning_rate': 7.498777777777778e-06, 'epoch': 1.3024}
{'loss': 0.9245, 'grad_norm': 3.3831448556911794, 'learning_rate': 7.497666666666668e-06, 'epoch': 1.3028}
{'loss': 0.9357, 'grad_norm': 3.1061307573361456, 'learning_rate': 7.4965555555555555e-06, 'epoch': 1.3032}
{'loss': 0.9384, 'grad_norm': 3.389752587795782, 'learning_rate': 7.495444444444445e-06, 'epoch': 1.3035999999999999}
{'loss': 0.9359, 'grad_norm': 2.593488657090913, 'learning_rate': 7.494333333333334e-06, 'epoch': 1.304}
{'eval_valid_loss': 0.89794921875, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.895, 'eval_valid_steps_per_second': 278.974, 'epoch': 1.304}
{'loss': 0.9271, 'grad_norm': 3.1061667083840803, 'learning_rate': 7.4932222222222225e-06, 'epoch': 1.3044}
{'loss': 0.9288, 'grad_norm': 3.4173053032166676, 'learning_rate': 7.492111111111112e-06, 'epoch': 1.3048}
{'loss': 0.9314, 'grad_norm': 3.172865783340178, 'learning_rate': 7.491000000000001e-06, 'epoch': 1.3052000000000001}
{'loss': 0.9356, 'grad_norm': 2.8963338821835882, 'learning_rate': 7.4898888888888895e-06, 'epoch': 1.3056}
{'loss': 0.9364, 'grad_norm': 2.977689915004488, 'learning_rate': 7.488777777777778e-06, 'epoch': 1.306}
{'loss': 0.9419, 'grad_norm': 3.1970597039465667, 'learning_rate': 7.487666666666668e-06, 'epoch': 1.3064}
{'loss': 0.9262, 'grad_norm': 3.3180881185057283, 'learning_rate': 7.486555555555556e-06, 'epoch': 1.3068}
{'loss': 0.9329, 'grad_norm': 3.192210988963342, 'learning_rate': 7.485444444444445e-06, 'epoch': 1.3072}
{'loss': 0.9369, 'grad_norm': 2.9114444723237205, 'learning_rate': 7.484333333333333e-06, 'epoch': 1.3075999999999999}
{'loss': 0.9292, 'grad_norm': 3.3417249322902634, 'learning_rate': 7.483222222222223e-06, 'epoch': 1.308}
{'eval_valid_loss': 0.896484375, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.679, 'eval_valid_steps_per_second': 278.42, 'epoch': 1.308}
{'loss': 0.9374, 'grad_norm': 3.4373784650645525, 'learning_rate': 7.482111111111112e-06, 'epoch': 1.3084}
{'loss': 0.9372, 'grad_norm': 3.268937753204925, 'learning_rate': 7.481000000000001e-06, 'epoch': 1.3088}
{'loss': 0.9182, 'grad_norm': 3.32360448664368, 'learning_rate': 7.47988888888889e-06, 'epoch': 1.3092}
{'loss': 0.9224, 'grad_norm': 3.213815740256424, 'learning_rate': 7.478777777777778e-06, 'epoch': 1.3096}
{'loss': 0.9466, 'grad_norm': 3.082505560712911, 'learning_rate': 7.477666666666668e-06, 'epoch': 1.31}
{'loss': 0.9227, 'grad_norm': 3.205774281626159, 'learning_rate': 7.476555555555556e-06, 'epoch': 1.3104}
{'loss': 0.9388, 'grad_norm': 3.065215531980725, 'learning_rate': 7.475444444444445e-06, 'epoch': 1.3108}
{'loss': 0.9412, 'grad_norm': 3.18398343375104, 'learning_rate': 7.474333333333333e-06, 'epoch': 1.3112}
{'loss': 0.9302, 'grad_norm': 2.9593722551891, 'learning_rate': 7.473222222222223e-06, 'epoch': 1.3115999999999999}
{'loss': 0.9356, 'grad_norm': 3.483396675060223, 'learning_rate': 7.472111111111112e-06, 'epoch': 1.312}
{'eval_valid_loss': 0.9013671875, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1116.998, 'eval_valid_steps_per_second': 279.249, 'epoch': 1.312}
{'loss': 0.9313, 'grad_norm': 3.117872024370153, 'learning_rate': 7.471e-06, 'epoch': 1.3124}
{'loss': 0.9319, 'grad_norm': 2.9492647079019543, 'learning_rate': 7.46988888888889e-06, 'epoch': 1.3128}
{'loss': 0.9343, 'grad_norm': 3.2887672690500347, 'learning_rate': 7.468777777777778e-06, 'epoch': 1.3132}
{'loss': 0.9272, 'grad_norm': 4.102483809402569, 'learning_rate': 7.467666666666667e-06, 'epoch': 1.3136}
{'loss': 0.9352, 'grad_norm': 2.9038940950785337, 'learning_rate': 7.466555555555556e-06, 'epoch': 1.314}
{'loss': 0.923, 'grad_norm': 3.3979738642690522, 'learning_rate': 7.465444444444445e-06, 'epoch': 1.3144}
{'loss': 0.9244, 'grad_norm': 3.1518673637277965, 'learning_rate': 7.464333333333333e-06, 'epoch': 1.3148}
{'loss': 0.9387, 'grad_norm': 3.137924437671346, 'learning_rate': 7.463222222222223e-06, 'epoch': 1.3152}
{'loss': 0.9282, 'grad_norm': 3.1469102946540017, 'learning_rate': 7.462111111111112e-06, 'epoch': 1.3155999999999999}
{'loss': 0.9273, 'grad_norm': 3.255876546813314, 'learning_rate': 7.461e-06, 'epoch': 1.316}
{'eval_valid_loss': 0.8984375, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1120.244, 'eval_valid_steps_per_second': 280.061, 'epoch': 1.316}
{'loss': 0.9355, 'grad_norm': 3.0463131215345784, 'learning_rate': 7.45988888888889e-06, 'epoch': 1.3164}
{'loss': 0.9323, 'grad_norm': 3.4327997758687707, 'learning_rate': 7.4587777777777785e-06, 'epoch': 1.3168}
{'loss': 0.9457, 'grad_norm': 3.0594031966783293, 'learning_rate': 7.457666666666667e-06, 'epoch': 1.3172}
{'loss': 0.93, 'grad_norm': 3.1543615470374116, 'learning_rate': 7.456555555555556e-06, 'epoch': 1.3176}
{'loss': 0.9386, 'grad_norm': 3.1760836336477043, 'learning_rate': 7.4554444444444455e-06, 'epoch': 1.318}
{'loss': 0.9381, 'grad_norm': 3.2805098652562683, 'learning_rate': 7.454333333333333e-06, 'epoch': 1.3184}
{'loss': 0.9326, 'grad_norm': 2.8999087668734904, 'learning_rate': 7.453222222222223e-06, 'epoch': 1.3188}
{'loss': 0.9412, 'grad_norm': 3.11681671614355, 'learning_rate': 7.4521111111111125e-06, 'epoch': 1.3192}
{'loss': 0.9348, 'grad_norm': 3.0117985557738876, 'learning_rate': 7.451e-06, 'epoch': 1.3195999999999999}
{'loss': 0.9303, 'grad_norm': 2.846530550522912, 'learning_rate': 7.44988888888889e-06, 'epoch': 1.32}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'eval_valid_loss': 0.89794921875, 'eval_valid_runtime': 0.0903, 'eval_valid_samples_per_second': 1107.746, 'eval_valid_steps_per_second': 276.937, 'epoch': 1.32}
{'loss': 0.9427, 'grad_norm': 3.1356990957373245, 'learning_rate': 7.448777777777778e-06, 'epoch': 1.3204}
{'loss': 0.927, 'grad_norm': 2.879967244263315, 'learning_rate': 7.447666666666667e-06, 'epoch': 1.3208}
{'loss': 0.945, 'grad_norm': 3.350433470665648, 'learning_rate': 7.446555555555556e-06, 'epoch': 1.3212}
{'loss': 0.9421, 'grad_norm': 3.3729303159824378, 'learning_rate': 7.445444444444445e-06, 'epoch': 1.3216}
{'loss': 0.9284, 'grad_norm': 2.7866831442202815, 'learning_rate': 7.444444444444445e-06, 'epoch': 1.322}
{'loss': 0.9229, 'grad_norm': 3.101141720918415, 'learning_rate': 7.443333333333334e-06, 'epoch': 1.3224}
{'loss': 0.9343, 'grad_norm': 3.1847726148764686, 'learning_rate': 7.4422222222222225e-06, 'epoch': 1.3228}
{'loss': 0.9292, 'grad_norm': 3.1435289858869186, 'learning_rate': 7.441111111111111e-06, 'epoch': 1.3232}
{'loss': 0.9381, 'grad_norm': 3.4065170970802936, 'learning_rate': 7.440000000000001e-06, 'epoch': 1.3235999999999999}
{'loss': 0.9203, 'grad_norm': 3.1529782967139584, 'learning_rate': 7.43888888888889e-06, 'epoch': 1.324}
{'eval_valid_loss': 0.89599609375, 'eval_valid_runtime': 0.089, 'eval_valid_samples_per_second': 1123.286, 'eval_valid_steps_per_second': 280.821, 'epoch': 1.324}
{'loss': 0.9386, 'grad_norm': 3.0761728147475003, 'learning_rate': 7.437777777777778e-06, 'epoch': 1.3244}
{'loss': 0.938, 'grad_norm': 2.999125979181894, 'learning_rate': 7.436666666666668e-06, 'epoch': 1.3248}
{'loss': 0.9203, 'grad_norm': 3.4025696461575654, 'learning_rate': 7.435555555555556e-06, 'epoch': 1.3252}
{'loss': 0.946, 'grad_norm': 3.1206128510250704, 'learning_rate': 7.434444444444445e-06, 'epoch': 1.3256000000000001}
{'loss': 0.9398, 'grad_norm': 3.3695485451367433, 'learning_rate': 7.433333333333334e-06, 'epoch': 1.326}
{'loss': 0.9349, 'grad_norm': 3.1417692102364168, 'learning_rate': 7.432222222222223e-06, 'epoch': 1.3264}
{'loss': 0.9273, 'grad_norm': 3.123858548553218, 'learning_rate': 7.431111111111111e-06, 'epoch': 1.3268}
{'loss': 0.9465, 'grad_norm': 2.9230666764896354, 'learning_rate': 7.430000000000001e-06, 'epoch': 1.3272}
{'loss': 0.941, 'grad_norm': 3.1195428788340718, 'learning_rate': 7.4288888888888896e-06, 'epoch': 1.3276}
{'loss': 0.9284, 'grad_norm': 3.2475526196223097, 'learning_rate': 7.427777777777778e-06, 'epoch': 1.328}
{'eval_valid_loss': 0.89794921875, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.093, 'eval_valid_steps_per_second': 279.523, 'epoch': 1.328}
{'loss': 0.9223, 'grad_norm': 3.0646307889495703, 'learning_rate': 7.426666666666668e-06, 'epoch': 1.3284}
{'loss': 0.934, 'grad_norm': 3.1173053996417517, 'learning_rate': 7.425555555555556e-06, 'epoch': 1.3288}
{'loss': 0.9255, 'grad_norm': 3.3664732792914394, 'learning_rate': 7.424444444444445e-06, 'epoch': 1.3292}
{'loss': 0.9281, 'grad_norm': 3.318064361624628, 'learning_rate': 7.423333333333333e-06, 'epoch': 1.3296000000000001}
{'loss': 0.9469, 'grad_norm': 2.981943158274457, 'learning_rate': 7.422222222222223e-06, 'epoch': 1.33}
{'loss': 0.9332, 'grad_norm': 3.384711659700692, 'learning_rate': 7.421111111111112e-06, 'epoch': 1.3304}
{'loss': 0.9333, 'grad_norm': 3.052965034659486, 'learning_rate': 7.420000000000001e-06, 'epoch': 1.3308}
{'loss': 0.9385, 'grad_norm': 3.00964724575877, 'learning_rate': 7.41888888888889e-06, 'epoch': 1.3312}
{'loss': 0.9266, 'grad_norm': 3.2761827669955297, 'learning_rate': 7.417777777777778e-06, 'epoch': 1.3316}
{'loss': 0.9217, 'grad_norm': 3.1875103688538893, 'learning_rate': 7.416666666666668e-06, 'epoch': 1.332}
{'eval_valid_loss': 0.89892578125, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.551, 'eval_valid_steps_per_second': 279.388, 'epoch': 1.332}
{'loss': 0.9146, 'grad_norm': 3.366221110010671, 'learning_rate': 7.415555555555556e-06, 'epoch': 1.3324}
{'loss': 0.9163, 'grad_norm': 3.066779202663107, 'learning_rate': 7.414444444444445e-06, 'epoch': 1.3328}
{'loss': 0.9286, 'grad_norm': 3.1851811389076996, 'learning_rate': 7.413333333333333e-06, 'epoch': 1.3332}
{'loss': 0.9298, 'grad_norm': 3.2527839034757875, 'learning_rate': 7.412222222222223e-06, 'epoch': 1.3336000000000001}
{'loss': 0.9304, 'grad_norm': 3.072044776889439, 'learning_rate': 7.411111111111112e-06, 'epoch': 1.334}
{'loss': 0.9382, 'grad_norm': 3.0865438710587405, 'learning_rate': 7.41e-06, 'epoch': 1.3344}
{'loss': 0.9255, 'grad_norm': 2.775722797872414, 'learning_rate': 7.40888888888889e-06, 'epoch': 1.3348}
{'loss': 0.9382, 'grad_norm': 3.2464643741928367, 'learning_rate': 7.4077777777777785e-06, 'epoch': 1.3352}
{'loss': 0.9264, 'grad_norm': 3.2327196704337675, 'learning_rate': 7.406666666666667e-06, 'epoch': 1.3356}
{'loss': 0.9229, 'grad_norm': 2.6962589813510816, 'learning_rate': 7.405555555555556e-06, 'epoch': 1.336}
{'eval_valid_loss': 0.8974609375, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1121.796, 'eval_valid_steps_per_second': 280.449, 'epoch': 1.336}
{'loss': 0.9248, 'grad_norm': 3.3556680409198223, 'learning_rate': 7.4044444444444455e-06, 'epoch': 1.3364}
{'loss': 0.941, 'grad_norm': 3.2945244984221755, 'learning_rate': 7.403333333333333e-06, 'epoch': 1.3368}
{'loss': 0.9273, 'grad_norm': 2.9968193399075638, 'learning_rate': 7.402222222222223e-06, 'epoch': 1.3372}
{'loss': 0.9236, 'grad_norm': 3.2879034433005097, 'learning_rate': 7.4011111111111124e-06, 'epoch': 1.3376000000000001}
{'loss': 0.9381, 'grad_norm': 3.8365062284246285, 'learning_rate': 7.4e-06, 'epoch': 1.338}
{'loss': 0.9294, 'grad_norm': 3.211347289283995, 'learning_rate': 7.39888888888889e-06, 'epoch': 1.3384}
{'loss': 0.9189, 'grad_norm': 3.1815095409650125, 'learning_rate': 7.3977777777777786e-06, 'epoch': 1.3388}
{'loss': 0.9317, 'grad_norm': 3.305800545831714, 'learning_rate': 7.396666666666667e-06, 'epoch': 1.3392}
{'loss': 0.9374, 'grad_norm': 3.0129296704708346, 'learning_rate': 7.395555555555556e-06, 'epoch': 1.3396}
{'loss': 0.9434, 'grad_norm': 3.130422560501254, 'learning_rate': 7.3944444444444456e-06, 'epoch': 1.34}
{'eval_valid_loss': 0.89599609375, 'eval_valid_runtime': 0.0907, 'eval_valid_samples_per_second': 1102.517, 'eval_valid_steps_per_second': 275.629, 'epoch': 1.34}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9372, 'grad_norm': 3.1365529891855504, 'learning_rate': 7.393333333333333e-06, 'epoch': 1.3404}
{'loss': 0.9318, 'grad_norm': 3.218957357301334, 'learning_rate': 7.392222222222223e-06, 'epoch': 1.3408}
{'loss': 0.9154, 'grad_norm': 2.870809849826195, 'learning_rate': 7.3911111111111125e-06, 'epoch': 1.3412}
{'loss': 0.925, 'grad_norm': 3.375154006058024, 'learning_rate': 7.39e-06, 'epoch': 1.3416000000000001}
{'loss': 0.9304, 'grad_norm': 2.8804107109742905, 'learning_rate': 7.38888888888889e-06, 'epoch': 1.342}
{'loss': 0.9438, 'grad_norm': 3.079868249025977, 'learning_rate': 7.387777777777778e-06, 'epoch': 1.3424}
{'loss': 0.9237, 'grad_norm': 3.2081704480244064, 'learning_rate': 7.386666666666667e-06, 'epoch': 1.3428}
{'loss': 0.9429, 'grad_norm': 3.188948760184115, 'learning_rate': 7.385555555555556e-06, 'epoch': 1.3432}
{'loss': 0.936, 'grad_norm': 3.1995416927712133, 'learning_rate': 7.384444444444445e-06, 'epoch': 1.3436}
{'loss': 0.9263, 'grad_norm': 3.178414026506935, 'learning_rate': 7.3833333333333335e-06, 'epoch': 1.3439999999999999}
{'eval_valid_loss': 0.896484375, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.765, 'eval_valid_steps_per_second': 278.441, 'epoch': 1.3439999999999999}
{'loss': 0.9236, 'grad_norm': 3.299640200969846, 'learning_rate': 7.382222222222223e-06, 'epoch': 1.3444}
{'loss': 0.9399, 'grad_norm': 3.17819209632827, 'learning_rate': 7.381111111111113e-06, 'epoch': 1.3448}
{'loss': 0.9277, 'grad_norm': 3.0390780627482257, 'learning_rate': 7.3800000000000005e-06, 'epoch': 1.3452}
{'loss': 0.9159, 'grad_norm': 3.282936543767888, 'learning_rate': 7.37888888888889e-06, 'epoch': 1.3456000000000001}
{'loss': 0.9228, 'grad_norm': 3.535527078136686, 'learning_rate': 7.377777777777778e-06, 'epoch': 1.346}
{'loss': 0.9353, 'grad_norm': 2.7809700557107564, 'learning_rate': 7.3766666666666675e-06, 'epoch': 1.3464}
{'loss': 0.9269, 'grad_norm': 3.246860051051836, 'learning_rate': 7.375555555555556e-06, 'epoch': 1.3468}
{'loss': 0.9166, 'grad_norm': 3.1361446211479063, 'learning_rate': 7.374444444444445e-06, 'epoch': 1.3472}
{'loss': 0.9305, 'grad_norm': 3.108092949489325, 'learning_rate': 7.373333333333334e-06, 'epoch': 1.3476}
{'loss': 0.9347, 'grad_norm': 3.504446202822432, 'learning_rate': 7.372222222222223e-06, 'epoch': 1.3479999999999999}
{'eval_valid_loss': 0.896484375, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1114.075, 'eval_valid_steps_per_second': 278.519, 'epoch': 1.3479999999999999}
{'loss': 0.936, 'grad_norm': 3.1877844533966364, 'learning_rate': 7.371111111111112e-06, 'epoch': 1.3484}
{'loss': 0.925, 'grad_norm': 3.5330958816675344, 'learning_rate': 7.370000000000001e-06, 'epoch': 1.3488}
{'loss': 0.9268, 'grad_norm': 2.889389083171159, 'learning_rate': 7.36888888888889e-06, 'epoch': 1.3492}
{'loss': 0.9324, 'grad_norm': 3.1103377936281538, 'learning_rate': 7.367777777777778e-06, 'epoch': 1.3496000000000001}
{'loss': 0.9253, 'grad_norm': 3.342010339622209, 'learning_rate': 7.3666666666666676e-06, 'epoch': 1.35}
{'loss': 0.925, 'grad_norm': 3.138659512223095, 'learning_rate': 7.3655555555555554e-06, 'epoch': 1.3504}
{'loss': 0.921, 'grad_norm': 2.944778186487021, 'learning_rate': 7.364444444444445e-06, 'epoch': 1.3508}
{'loss': 0.9229, 'grad_norm': 3.181118298785351, 'learning_rate': 7.363333333333334e-06, 'epoch': 1.3512}
{'loss': 0.93, 'grad_norm': 3.145016489440556, 'learning_rate': 7.362222222222222e-06, 'epoch': 1.3516}
{'loss': 0.9266, 'grad_norm': 3.044247340276746, 'learning_rate': 7.361111111111112e-06, 'epoch': 1.3519999999999999}
{'eval_valid_loss': 0.90234375, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.589, 'eval_valid_steps_per_second': 278.147, 'epoch': 1.3519999999999999}
{'loss': 0.9259, 'grad_norm': 3.075197182900377, 'learning_rate': 7.360000000000001e-06, 'epoch': 1.3524}
{'loss': 0.9333, 'grad_norm': 2.9698930848590277, 'learning_rate': 7.35888888888889e-06, 'epoch': 1.3528}
{'loss': 0.9395, 'grad_norm': 3.0679877509608686, 'learning_rate': 7.357777777777778e-06, 'epoch': 1.3532}
{'loss': 0.928, 'grad_norm': 3.1206174923947634, 'learning_rate': 7.356666666666668e-06, 'epoch': 1.3536000000000001}
{'loss': 0.9312, 'grad_norm': 3.150427035873967, 'learning_rate': 7.3555555555555555e-06, 'epoch': 1.354}
{'loss': 0.9406, 'grad_norm': 2.8868714629005225, 'learning_rate': 7.354444444444445e-06, 'epoch': 1.3544}
{'loss': 0.925, 'grad_norm': 3.0618372122119997, 'learning_rate': 7.353333333333334e-06, 'epoch': 1.3548}
{'loss': 0.935, 'grad_norm': 3.319830071799977, 'learning_rate': 7.3522222222222225e-06, 'epoch': 1.3552}
{'loss': 0.9283, 'grad_norm': 2.9257559170409286, 'learning_rate': 7.351111111111112e-06, 'epoch': 1.3556}
{'loss': 0.9245, 'grad_norm': 3.0239420230822898, 'learning_rate': 7.350000000000001e-06, 'epoch': 1.3559999999999999}
{'eval_valid_loss': 0.8974609375, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.429, 'eval_valid_steps_per_second': 278.107, 'epoch': 1.3559999999999999}
{'loss': 0.9327, 'grad_norm': 2.930382618137919, 'learning_rate': 7.3488888888888895e-06, 'epoch': 1.3564}
{'loss': 0.941, 'grad_norm': 3.221439404646113, 'learning_rate': 7.347777777777778e-06, 'epoch': 1.3568}
{'loss': 0.9274, 'grad_norm': 3.1607158245624802, 'learning_rate': 7.346666666666668e-06, 'epoch': 1.3572}
{'loss': 0.9173, 'grad_norm': 2.8600432547886285, 'learning_rate': 7.345555555555556e-06, 'epoch': 1.3576}
{'loss': 0.9395, 'grad_norm': 3.119624874812752, 'learning_rate': 7.344444444444445e-06, 'epoch': 1.358}
{'loss': 0.9247, 'grad_norm': 3.6473075865102516, 'learning_rate': 7.343333333333333e-06, 'epoch': 1.3584}
{'loss': 0.9288, 'grad_norm': 2.79170767317494, 'learning_rate': 7.342222222222223e-06, 'epoch': 1.3588}
{'loss': 0.9247, 'grad_norm': 3.2218473124308784, 'learning_rate': 7.341111111111112e-06, 'epoch': 1.3592}
{'loss': 0.9342, 'grad_norm': 3.513814264861264, 'learning_rate': 7.340000000000001e-06, 'epoch': 1.3596}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.9347, 'grad_norm': 3.2202584333846143, 'learning_rate': 7.33888888888889e-06, 'epoch': 1.3599999999999999}
{'eval_valid_loss': 0.89794921875, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.343, 'eval_valid_steps_per_second': 279.336, 'epoch': 1.3599999999999999}
{'loss': 0.9287, 'grad_norm': 2.850528686408534, 'learning_rate': 7.337777777777778e-06, 'epoch': 1.3604}
{'loss': 0.9138, 'grad_norm': 3.001116961923035, 'learning_rate': 7.336666666666668e-06, 'epoch': 1.3608}
{'loss': 0.9441, 'grad_norm': 3.1618098587772825, 'learning_rate': 7.335555555555556e-06, 'epoch': 1.3612}
{'loss': 0.9244, 'grad_norm': 2.9591404536916164, 'learning_rate': 7.334444444444445e-06, 'epoch': 1.3616}
{'loss': 0.9331, 'grad_norm': 3.001844514608214, 'learning_rate': 7.333333333333333e-06, 'epoch': 1.362}
{'loss': 0.9199, 'grad_norm': 3.016412582726736, 'learning_rate': 7.3323333333333335e-06, 'epoch': 1.3624}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9348, 'grad_norm': 3.3472797266627152, 'learning_rate': 7.331222222222223e-06, 'epoch': 1.3628}
{'loss': 0.9329, 'grad_norm': 3.3322457068993256, 'learning_rate': 7.330111111111111e-06, 'epoch': 1.3632}
{'loss': 0.9207, 'grad_norm': 3.3664308923760893, 'learning_rate': 7.3290000000000005e-06, 'epoch': 1.3636}
{'loss': 0.9277, 'grad_norm': 3.435067929036267, 'learning_rate': 7.32788888888889e-06, 'epoch': 1.3639999999999999}
{'eval_valid_loss': 0.8955078125, 'eval_valid_runtime': 0.0907, 'eval_valid_samples_per_second': 1102.525, 'eval_valid_steps_per_second': 275.631, 'epoch': 1.3639999999999999}
{'loss': 0.9275, 'grad_norm': 3.6257383153732783, 'learning_rate': 7.326777777777778e-06, 'epoch': 1.3644}
{'loss': 0.9279, 'grad_norm': 3.453232741940568, 'learning_rate': 7.3256666666666675e-06, 'epoch': 1.3648}
{'loss': 0.9333, 'grad_norm': 3.0687104333319626, 'learning_rate': 7.324555555555556e-06, 'epoch': 1.3652}
{'loss': 0.9185, 'grad_norm': 3.134799365280966, 'learning_rate': 7.323444444444445e-06, 'epoch': 1.3656}
{'loss': 0.9283, 'grad_norm': 2.916689938497798, 'learning_rate': 7.322333333333334e-06, 'epoch': 1.366}
{'loss': 0.927, 'grad_norm': 3.013638034514363, 'learning_rate': 7.321222222222223e-06, 'epoch': 1.3664}
{'loss': 0.9373, 'grad_norm': 3.1369899570753046, 'learning_rate': 7.320111111111111e-06, 'epoch': 1.3668}
{'loss': 0.932, 'grad_norm': 2.9822412922067154, 'learning_rate': 7.3190000000000006e-06, 'epoch': 1.3672}
{'loss': 0.9242, 'grad_norm': 3.1088714383544986, 'learning_rate': 7.31788888888889e-06, 'epoch': 1.3676}
{'loss': 0.9273, 'grad_norm': 3.206615713800879, 'learning_rate': 7.316777777777778e-06, 'epoch': 1.3679999999999999}
{'eval_valid_loss': 0.89453125, 'eval_valid_runtime': 0.0906, 'eval_valid_samples_per_second': 1103.848, 'eval_valid_steps_per_second': 275.962, 'epoch': 1.3679999999999999}
{'loss': 0.926, 'grad_norm': 3.2286690769547577, 'learning_rate': 7.3156666666666675e-06, 'epoch': 1.3684}
{'loss': 0.9279, 'grad_norm': 3.055913839984915, 'learning_rate': 7.314555555555556e-06, 'epoch': 1.3688}
{'loss': 0.9405, 'grad_norm': 3.1353031731444654, 'learning_rate': 7.313444444444445e-06, 'epoch': 1.3692}
{'loss': 0.9309, 'grad_norm': 3.152605653303775, 'learning_rate': 7.312333333333334e-06, 'epoch': 1.3696}
{'loss': 0.9376, 'grad_norm': 2.8652722564876645, 'learning_rate': 7.311222222222223e-06, 'epoch': 1.37}
{'loss': 0.921, 'grad_norm': 3.2001679424830156, 'learning_rate': 7.310111111111111e-06, 'epoch': 1.3704}
{'loss': 0.9223, 'grad_norm': 2.695156043114202, 'learning_rate': 7.309000000000001e-06, 'epoch': 1.3708}
{'loss': 0.9328, 'grad_norm': 3.1221623316172216, 'learning_rate': 7.30788888888889e-06, 'epoch': 1.3712}
{'loss': 0.9179, 'grad_norm': 2.7557028625296085, 'learning_rate': 7.306777777777778e-06, 'epoch': 1.3716}
{'loss': 0.9296, 'grad_norm': 2.9529746789366595, 'learning_rate': 7.305666666666668e-06, 'epoch': 1.3719999999999999}
{'eval_valid_loss': 0.89599609375, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.393, 'eval_valid_steps_per_second': 277.098, 'epoch': 1.3719999999999999}
{'loss': 0.9287, 'grad_norm': 2.9542447848319573, 'learning_rate': 7.3045555555555555e-06, 'epoch': 1.3724}
{'loss': 0.9267, 'grad_norm': 3.5235561765320407, 'learning_rate': 7.303444444444445e-06, 'epoch': 1.3728}
{'loss': 0.9211, 'grad_norm': 3.210095065349148, 'learning_rate': 7.302333333333334e-06, 'epoch': 1.3732}
{'loss': 0.9306, 'grad_norm': 3.5344437191975207, 'learning_rate': 7.3012222222222225e-06, 'epoch': 1.3736}
{'loss': 0.9351, 'grad_norm': 3.143576075329152, 'learning_rate': 7.300111111111112e-06, 'epoch': 1.374}
{'loss': 0.9393, 'grad_norm': 3.0364216016551033, 'learning_rate': 7.299000000000001e-06, 'epoch': 1.3744}
{'loss': 0.9353, 'grad_norm': 3.0689704718705904, 'learning_rate': 7.29788888888889e-06, 'epoch': 1.3748}
{'loss': 0.9237, 'grad_norm': 3.0204116406378962, 'learning_rate': 7.296777777777778e-06, 'epoch': 1.3752}
{'loss': 0.9339, 'grad_norm': 3.0401264868008346, 'learning_rate': 7.295666666666668e-06, 'epoch': 1.3756}
{'loss': 0.9147, 'grad_norm': 3.167263992131122, 'learning_rate': 7.294555555555556e-06, 'epoch': 1.376}
{'eval_valid_loss': 0.8955078125, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.46, 'eval_valid_steps_per_second': 278.365, 'epoch': 1.376}
{'loss': 0.9129, 'grad_norm': 3.3398372717008167, 'learning_rate': 7.293444444444445e-06, 'epoch': 1.3764}
{'loss': 0.9302, 'grad_norm': 3.092960642288269, 'learning_rate': 7.292333333333334e-06, 'epoch': 1.3768}
{'loss': 0.9342, 'grad_norm': 3.3307832659318763, 'learning_rate': 7.291222222222223e-06, 'epoch': 1.3772}
{'loss': 0.9415, 'grad_norm': 3.0260915221387648, 'learning_rate': 7.290111111111112e-06, 'epoch': 1.3776}
{'loss': 0.9372, 'grad_norm': 3.0959281189802286, 'learning_rate': 7.289000000000001e-06, 'epoch': 1.3780000000000001}
{'loss': 0.9207, 'grad_norm': 3.086191374560682, 'learning_rate': 7.2878888888888896e-06, 'epoch': 1.3784}
{'loss': 0.9387, 'grad_norm': 2.9901917477589857, 'learning_rate': 7.286777777777778e-06, 'epoch': 1.3788}
{'loss': 0.9212, 'grad_norm': 3.0961123030609836, 'learning_rate': 7.285666666666668e-06, 'epoch': 1.3792}
{'loss': 0.9244, 'grad_norm': 3.047630685847205, 'learning_rate': 7.284555555555556e-06, 'epoch': 1.3796}
{'loss': 0.9238, 'grad_norm': 2.932899443730332, 'learning_rate': 7.283444444444445e-06, 'epoch': 1.38}
{'eval_valid_loss': 0.89501953125, 'eval_valid_runtime': 0.0911, 'eval_valid_samples_per_second': 1097.549, 'eval_valid_steps_per_second': 274.387, 'epoch': 1.38}
{'loss': 0.9338, 'grad_norm': 2.971170823739822, 'learning_rate': 7.282333333333333e-06, 'epoch': 1.3804}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9161, 'grad_norm': 3.0889932496369745, 'learning_rate': 7.281222222222223e-06, 'epoch': 1.3808}
{'loss': 0.933, 'grad_norm': 3.1053771191400004, 'learning_rate': 7.280111111111112e-06, 'epoch': 1.3812}
{'loss': 0.9314, 'grad_norm': 2.6843284978158266, 'learning_rate': 7.279e-06, 'epoch': 1.3816}
{'loss': 0.9388, 'grad_norm': 3.597785069316254, 'learning_rate': 7.27788888888889e-06, 'epoch': 1.3820000000000001}
{'loss': 0.9326, 'grad_norm': 3.4055102446220955, 'learning_rate': 7.276777777777778e-06, 'epoch': 1.3824}
{'loss': 0.912, 'grad_norm': 2.9143179507953647, 'learning_rate': 7.275666666666668e-06, 'epoch': 1.3828}
{'loss': 0.9253, 'grad_norm': 2.9876142268509516, 'learning_rate': 7.274555555555556e-06, 'epoch': 1.3832}
{'loss': 0.9207, 'grad_norm': 3.2564583852648883, 'learning_rate': 7.273444444444445e-06, 'epoch': 1.3836}
{'loss': 0.9279, 'grad_norm': 2.9095598262503923, 'learning_rate': 7.272333333333333e-06, 'epoch': 1.384}
{'eval_valid_loss': 0.8955078125, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.135, 'eval_valid_steps_per_second': 279.284, 'epoch': 1.384}
{'loss': 0.9235, 'grad_norm': 3.0535442037173435, 'learning_rate': 7.271222222222223e-06, 'epoch': 1.3844}
{'loss': 0.9339, 'grad_norm': 2.984274238363361, 'learning_rate': 7.270111111111112e-06, 'epoch': 1.3848}
{'loss': 0.9441, 'grad_norm': 3.2253715035018535, 'learning_rate': 7.269e-06, 'epoch': 1.3852}
{'loss': 0.9295, 'grad_norm': 2.946039125375632, 'learning_rate': 7.26788888888889e-06, 'epoch': 1.3856}
{'loss': 0.9286, 'grad_norm': 3.284235894535796, 'learning_rate': 7.2667777777777785e-06, 'epoch': 1.3860000000000001}
{'loss': 0.9305, 'grad_norm': 3.140550603032844, 'learning_rate': 7.265666666666667e-06, 'epoch': 1.3864}
{'loss': 0.9352, 'grad_norm': 3.2564149049675284, 'learning_rate': 7.264555555555556e-06, 'epoch': 1.3868}
{'loss': 0.9253, 'grad_norm': 3.1289715420772044, 'learning_rate': 7.2634444444444454e-06, 'epoch': 1.3872}
{'loss': 0.9272, 'grad_norm': 2.845101307923958, 'learning_rate': 7.262333333333333e-06, 'epoch': 1.3876}
{'loss': 0.9215, 'grad_norm': 3.1175314347824448, 'learning_rate': 7.261222222222223e-06, 'epoch': 1.388}
{'eval_valid_loss': 0.8955078125, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.51, 'eval_valid_steps_per_second': 279.128, 'epoch': 1.388}
{'loss': 0.9194, 'grad_norm': 2.833616878308906, 'learning_rate': 7.2601111111111124e-06, 'epoch': 1.3884}
{'loss': 0.9367, 'grad_norm': 2.9424537424779973, 'learning_rate': 7.259e-06, 'epoch': 1.3888}
{'loss': 0.9294, 'grad_norm': 3.5875324284353907, 'learning_rate': 7.25788888888889e-06, 'epoch': 1.3892}
{'loss': 0.9284, 'grad_norm': 2.918645042476417, 'learning_rate': 7.2567777777777786e-06, 'epoch': 1.3896}
{'loss': 0.9321, 'grad_norm': 3.3489110279845637, 'learning_rate': 7.255666666666667e-06, 'epoch': 1.3900000000000001}
{'loss': 0.9295, 'grad_norm': 2.7772313670667286, 'learning_rate': 7.254555555555556e-06, 'epoch': 1.3904}
{'loss': 0.9234, 'grad_norm': 3.43283962435244, 'learning_rate': 7.2534444444444455e-06, 'epoch': 1.3908}
{'loss': 0.9388, 'grad_norm': 3.374564902198788, 'learning_rate': 7.252333333333333e-06, 'epoch': 1.3912}
{'loss': 0.9262, 'grad_norm': 2.8933983051477026, 'learning_rate': 7.251222222222223e-06, 'epoch': 1.3916}
{'loss': 0.9269, 'grad_norm': 3.139180662628425, 'learning_rate': 7.2501111111111125e-06, 'epoch': 1.392}
{'eval_valid_loss': 0.89453125, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.898, 'eval_valid_steps_per_second': 281.975, 'epoch': 1.392}
{'loss': 0.9309, 'grad_norm': 2.838628506279959, 'learning_rate': 7.249e-06, 'epoch': 1.3924}
{'loss': 0.9323, 'grad_norm': 2.7607413077040985, 'learning_rate': 7.24788888888889e-06, 'epoch': 1.3928}
{'loss': 0.9142, 'grad_norm': 2.9677525902756807, 'learning_rate': 7.246777777777778e-06, 'epoch': 1.3932}
{'loss': 0.9313, 'grad_norm': 3.0826685721532856, 'learning_rate': 7.245666666666667e-06, 'epoch': 1.3936}
{'loss': 0.9343, 'grad_norm': 2.9622216874584626, 'learning_rate': 7.244555555555556e-06, 'epoch': 1.3940000000000001}
{'loss': 0.9278, 'grad_norm': 3.006139631882375, 'learning_rate': 7.243444444444445e-06, 'epoch': 1.3944}
{'loss': 0.9291, 'grad_norm': 3.2853984766141924, 'learning_rate': 7.2423333333333335e-06, 'epoch': 1.3948}
{'loss': 0.9254, 'grad_norm': 3.1328550797230097, 'learning_rate': 7.241222222222223e-06, 'epoch': 1.3952}
{'loss': 0.9324, 'grad_norm': 3.1192564061985637, 'learning_rate': 7.240111111111113e-06, 'epoch': 1.3956}
{'loss': 0.9234, 'grad_norm': 2.671781967729637, 'learning_rate': 7.2390000000000005e-06, 'epoch': 1.396}
{'eval_valid_loss': 0.8935546875, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1126.841, 'eval_valid_steps_per_second': 281.71, 'epoch': 1.396}
{'loss': 0.9347, 'grad_norm': 2.9609906939154484, 'learning_rate': 7.23788888888889e-06, 'epoch': 1.3963999999999999}
{'loss': 0.9404, 'grad_norm': 3.542074711050996, 'learning_rate': 7.236777777777778e-06, 'epoch': 1.3968}
{'loss': 0.9219, 'grad_norm': 3.0908708083139427, 'learning_rate': 7.2356666666666675e-06, 'epoch': 1.3972}
{'loss': 0.9288, 'grad_norm': 3.2427307041539044, 'learning_rate': 7.234555555555556e-06, 'epoch': 1.3976}
{'loss': 0.9268, 'grad_norm': 2.937350117126444, 'learning_rate': 7.233444444444445e-06, 'epoch': 1.3980000000000001}
{'loss': 0.9257, 'grad_norm': 3.2735151586675424, 'learning_rate': 7.232333333333334e-06, 'epoch': 1.3984}
{'loss': 0.9396, 'grad_norm': 3.436262601605202, 'learning_rate': 7.231222222222223e-06, 'epoch': 1.3988}
{'loss': 0.9291, 'grad_norm': 3.1781659715350203, 'learning_rate': 7.230111111111112e-06, 'epoch': 1.3992}
{'loss': 0.9396, 'grad_norm': 3.147943538413907, 'learning_rate': 7.229000000000001e-06, 'epoch': 1.3996}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.933, 'grad_norm': 2.8388732553160665, 'learning_rate': 7.22788888888889e-06, 'epoch': 1.4}
{'eval_valid_loss': 0.89111328125, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.922, 'eval_valid_steps_per_second': 278.98, 'epoch': 1.4}
{'loss': 0.917, 'grad_norm': 3.2256106896277963, 'learning_rate': 7.226777777777778e-06, 'epoch': 1.4003999999999999}
{'loss': 0.9226, 'grad_norm': 3.479625664012972, 'learning_rate': 7.2256666666666676e-06, 'epoch': 1.4008}
{'loss': 0.931, 'grad_norm': 3.219622206797386, 'learning_rate': 7.2245555555555554e-06, 'epoch': 1.4012}
{'loss': 0.9293, 'grad_norm': 3.007987029721699, 'learning_rate': 7.223444444444445e-06, 'epoch': 1.4016}
{'loss': 0.9279, 'grad_norm': 3.216623724311496, 'learning_rate': 7.222333333333334e-06, 'epoch': 1.4020000000000001}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9218, 'grad_norm': 2.9012194544192105, 'learning_rate': 7.221333333333333e-06, 'epoch': 1.4024}
{'loss': 0.9233, 'grad_norm': 2.8353813088366246, 'learning_rate': 7.220222222222223e-06, 'epoch': 1.4028}
{'loss': 0.9342, 'grad_norm': 2.7140106349946125, 'learning_rate': 7.2191111111111115e-06, 'epoch': 1.4032}
{'loss': 0.9321, 'grad_norm': 3.0220572403845893, 'learning_rate': 7.218e-06, 'epoch': 1.4036}
{'loss': 0.9351, 'grad_norm': 3.2635860283437426, 'learning_rate': 7.21688888888889e-06, 'epoch': 1.404}
{'eval_valid_loss': 0.8916015625, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.985, 'eval_valid_steps_per_second': 279.746, 'epoch': 1.404}
{'loss': 0.9321, 'grad_norm': 2.8723465865413313, 'learning_rate': 7.2157777777777784e-06, 'epoch': 1.4043999999999999}
{'loss': 0.9358, 'grad_norm': 2.9839348992796473, 'learning_rate': 7.214666666666668e-06, 'epoch': 1.4048}
{'loss': 0.9296, 'grad_norm': 3.3519766047106256, 'learning_rate': 7.213555555555556e-06, 'epoch': 1.4052}
{'loss': 0.944, 'grad_norm': 3.2563802283049847, 'learning_rate': 7.2124444444444454e-06, 'epoch': 1.4056}
{'loss': 0.9193, 'grad_norm': 2.942786532462973, 'learning_rate': 7.211333333333333e-06, 'epoch': 1.4060000000000001}
{'loss': 0.9292, 'grad_norm': 2.8974549820918796, 'learning_rate': 7.210222222222223e-06, 'epoch': 1.4064}
{'loss': 0.9272, 'grad_norm': 3.262074068554679, 'learning_rate': 7.2091111111111116e-06, 'epoch': 1.4068}
{'loss': 0.928, 'grad_norm': 3.1866006049803066, 'learning_rate': 7.208e-06, 'epoch': 1.4072}
{'loss': 0.9329, 'grad_norm': 3.1442936019658636, 'learning_rate': 7.20688888888889e-06, 'epoch': 1.4076}
{'loss': 0.9313, 'grad_norm': 3.139382377492813, 'learning_rate': 7.2057777777777785e-06, 'epoch': 1.408}
{'eval_valid_loss': 0.8916015625, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1125.525, 'eval_valid_steps_per_second': 281.381, 'epoch': 1.408}
{'loss': 0.9335, 'grad_norm': 3.129350456871423, 'learning_rate': 7.204666666666667e-06, 'epoch': 1.4083999999999999}
{'loss': 0.9306, 'grad_norm': 2.9078510294170057, 'learning_rate': 7.203555555555556e-06, 'epoch': 1.4088}
{'loss': 0.9309, 'grad_norm': 2.99270840485614, 'learning_rate': 7.2024444444444455e-06, 'epoch': 1.4092}
{'loss': 0.9169, 'grad_norm': 3.3271299495468876, 'learning_rate': 7.201333333333333e-06, 'epoch': 1.4096}
{'loss': 0.9302, 'grad_norm': 3.0684418257447685, 'learning_rate': 7.200222222222223e-06, 'epoch': 1.41}
{'loss': 0.9145, 'grad_norm': 2.7911411841775497, 'learning_rate': 7.199111111111111e-06, 'epoch': 1.4104}
{'loss': 0.9433, 'grad_norm': 3.196951373577955, 'learning_rate': 7.198e-06, 'epoch': 1.4108}
{'loss': 0.9235, 'grad_norm': 3.164153071861475, 'learning_rate': 7.19688888888889e-06, 'epoch': 1.4112}
{'loss': 0.9201, 'grad_norm': 3.3302063232088073, 'learning_rate': 7.195777777777778e-06, 'epoch': 1.4116}
{'loss': 0.9223, 'grad_norm': 2.9452068502161524, 'learning_rate': 7.194666666666667e-06, 'epoch': 1.412}
{'eval_valid_loss': 0.89306640625, 'eval_valid_runtime': 0.0907, 'eval_valid_samples_per_second': 1102.488, 'eval_valid_steps_per_second': 275.622, 'epoch': 1.412}
{'loss': 0.924, 'grad_norm': 2.8950598127316773, 'learning_rate': 7.193555555555556e-06, 'epoch': 1.4123999999999999}
{'loss': 0.9139, 'grad_norm': 2.936983245686225, 'learning_rate': 7.192444444444446e-06, 'epoch': 1.4128}
{'loss': 0.9213, 'grad_norm': 3.0712577587770267, 'learning_rate': 7.1913333333333335e-06, 'epoch': 1.4132}
{'loss': 0.935, 'grad_norm': 3.1656832182220125, 'learning_rate': 7.190222222222223e-06, 'epoch': 1.4136}
{'loss': 0.927, 'grad_norm': 2.8305417004333417, 'learning_rate': 7.189111111111111e-06, 'epoch': 1.414}
{'loss': 0.928, 'grad_norm': 2.805105746650773, 'learning_rate': 7.1880000000000005e-06, 'epoch': 1.4144}
{'loss': 0.9309, 'grad_norm': 3.0068037546138457, 'learning_rate': 7.18688888888889e-06, 'epoch': 1.4148}
{'loss': 0.9182, 'grad_norm': 3.3345781455682237, 'learning_rate': 7.185777777777778e-06, 'epoch': 1.4152}
{'loss': 0.9225, 'grad_norm': 3.115718696279659, 'learning_rate': 7.1846666666666674e-06, 'epoch': 1.4156}
{'loss': 0.9314, 'grad_norm': 3.1228812760088207, 'learning_rate': 7.183555555555556e-06, 'epoch': 1.416}
{'eval_valid_loss': 0.89404296875, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.58, 'eval_valid_steps_per_second': 279.895, 'epoch': 1.416}
{'loss': 0.9271, 'grad_norm': 3.068899834309927, 'learning_rate': 7.182444444444445e-06, 'epoch': 1.4163999999999999}
{'loss': 0.935, 'grad_norm': 2.8989968595550253, 'learning_rate': 7.181333333333334e-06, 'epoch': 1.4168}
{'loss': 0.9281, 'grad_norm': 3.044564177795378, 'learning_rate': 7.180222222222223e-06, 'epoch': 1.4172}
{'loss': 0.926, 'grad_norm': 2.92896766465931, 'learning_rate': 7.179111111111112e-06, 'epoch': 1.4176}
{'loss': 0.9408, 'grad_norm': 2.889928001708133, 'learning_rate': 7.1780000000000006e-06, 'epoch': 1.418}
{'loss': 0.9229, 'grad_norm': 2.9373574374484868, 'learning_rate': 7.17688888888889e-06, 'epoch': 1.4184}
{'loss': 0.9326, 'grad_norm': 3.05545541618479, 'learning_rate': 7.175777777777778e-06, 'epoch': 1.4188}
{'loss': 0.925, 'grad_norm': 3.17248574376191, 'learning_rate': 7.1746666666666675e-06, 'epoch': 1.4192}
{'loss': 0.9207, 'grad_norm': 2.759752018402962, 'learning_rate': 7.173555555555555e-06, 'epoch': 1.4196}
{'loss': 0.9224, 'grad_norm': 3.5258756520544594, 'learning_rate': 7.172444444444445e-06, 'epoch': 1.42}
{'eval_valid_loss': 0.89306640625, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.333, 'eval_valid_steps_per_second': 278.333, 'epoch': 1.42}
{'loss': 0.9196, 'grad_norm': 2.9612732951003586, 'learning_rate': 7.171333333333334e-06, 'epoch': 1.4203999999999999}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9337, 'grad_norm': 3.228714140081107, 'learning_rate': 7.170222222222223e-06, 'epoch': 1.4208}
{'loss': 0.932, 'grad_norm': 3.0552190235279104, 'learning_rate': 7.169111111111112e-06, 'epoch': 1.4212}
{'loss': 0.9199, 'grad_norm': 3.6554343952244888, 'learning_rate': 7.168000000000001e-06, 'epoch': 1.4216}
{'loss': 0.9252, 'grad_norm': 3.35896346654328, 'learning_rate': 7.16688888888889e-06, 'epoch': 1.422}
{'loss': 0.9341, 'grad_norm': 3.404281379808889, 'learning_rate': 7.165777777777778e-06, 'epoch': 1.4224}
{'loss': 0.9247, 'grad_norm': 3.211364893998002, 'learning_rate': 7.164666666666668e-06, 'epoch': 1.4228}
{'loss': 0.9185, 'grad_norm': 3.672816930849507, 'learning_rate': 7.1635555555555555e-06, 'epoch': 1.4232}
{'loss': 0.9108, 'grad_norm': 2.984147447628744, 'learning_rate': 7.162444444444445e-06, 'epoch': 1.4236}
{'loss': 0.918, 'grad_norm': 2.9697452583464603, 'learning_rate': 7.161333333333334e-06, 'epoch': 1.424}
{'eval_valid_loss': 0.8935546875, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1125.504, 'eval_valid_steps_per_second': 281.376, 'epoch': 1.424}
{'loss': 0.926, 'grad_norm': 3.0521655879909204, 'learning_rate': 7.1602222222222225e-06, 'epoch': 1.4243999999999999}
{'loss': 0.9157, 'grad_norm': 3.000217131863209, 'learning_rate': 7.159111111111112e-06, 'epoch': 1.4248}
{'loss': 0.9151, 'grad_norm': 3.1024975075969166, 'learning_rate': 7.158000000000001e-06, 'epoch': 1.4252}
{'loss': 0.9219, 'grad_norm': 3.0733526771837227, 'learning_rate': 7.1568888888888895e-06, 'epoch': 1.4256}
{'loss': 0.9176, 'grad_norm': 3.2117173113456663, 'learning_rate': 7.155777777777778e-06, 'epoch': 1.426}
{'loss': 0.9242, 'grad_norm': 2.9243941131932485, 'learning_rate': 7.154666666666668e-06, 'epoch': 1.4264000000000001}
{'loss': 0.9118, 'grad_norm': 3.198443289687708, 'learning_rate': 7.153555555555556e-06, 'epoch': 1.4268}
{'loss': 0.912, 'grad_norm': 3.13758017372951, 'learning_rate': 7.152444444444445e-06, 'epoch': 1.4272}
{'loss': 0.9283, 'grad_norm': 3.216252144769457, 'learning_rate': 7.151333333333334e-06, 'epoch': 1.4276}
{'loss': 0.9251, 'grad_norm': 2.8949913657921096, 'learning_rate': 7.150222222222223e-06, 'epoch': 1.428}
{'eval_valid_loss': 0.890625, 'eval_valid_runtime': 0.135, 'eval_valid_samples_per_second': 740.713, 'eval_valid_steps_per_second': 185.178, 'epoch': 1.428}
{'loss': 0.9091, 'grad_norm': 2.8926356977648466, 'learning_rate': 7.149111111111112e-06, 'epoch': 1.4284}
{'loss': 0.9231, 'grad_norm': 3.0482071531893764, 'learning_rate': 7.148000000000001e-06, 'epoch': 1.4288}
{'loss': 0.9324, 'grad_norm': 2.972802377886278, 'learning_rate': 7.1468888888888896e-06, 'epoch': 1.4292}
{'loss': 0.929, 'grad_norm': 3.342505785039034, 'learning_rate': 7.145777777777778e-06, 'epoch': 1.4296}
{'loss': 0.9252, 'grad_norm': 2.9179105331557618, 'learning_rate': 7.144666666666668e-06, 'epoch': 1.43}
{'loss': 0.936, 'grad_norm': 2.872537823254576, 'learning_rate': 7.143555555555556e-06, 'epoch': 1.4304000000000001}
{'loss': 0.9275, 'grad_norm': 3.071422424817192, 'learning_rate': 7.142444444444445e-06, 'epoch': 1.4308}
{'loss': 0.9266, 'grad_norm': 2.9172082852244756, 'learning_rate': 7.141333333333333e-06, 'epoch': 1.4312}
{'loss': 0.9311, 'grad_norm': 3.1198917408953344, 'learning_rate': 7.140222222222223e-06, 'epoch': 1.4316}
{'loss': 0.919, 'grad_norm': 3.0382036108453927, 'learning_rate': 7.139111111111112e-06, 'epoch': 1.432}
{'eval_valid_loss': 0.89306640625, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.985, 'eval_valid_steps_per_second': 279.746, 'epoch': 1.432}
{'loss': 0.9267, 'grad_norm': 2.9391041697458897, 'learning_rate': 7.138e-06, 'epoch': 1.4324}
{'loss': 0.9259, 'grad_norm': 3.3875249147378708, 'learning_rate': 7.13688888888889e-06, 'epoch': 1.4328}
{'loss': 0.9407, 'grad_norm': 3.3090493024828915, 'learning_rate': 7.135777777777778e-06, 'epoch': 1.4332}
{'loss': 0.9258, 'grad_norm': 3.0314255889689408, 'learning_rate': 7.134666666666668e-06, 'epoch': 1.4336}
{'loss': 0.9424, 'grad_norm': 3.041828730372176, 'learning_rate': 7.133555555555556e-06, 'epoch': 1.434}
{'loss': 0.9302, 'grad_norm': 3.141795808421277, 'learning_rate': 7.132444444444445e-06, 'epoch': 1.4344000000000001}
{'loss': 0.9271, 'grad_norm': 2.7883414717584176, 'learning_rate': 7.131333333333333e-06, 'epoch': 1.4348}
{'loss': 0.9325, 'grad_norm': 2.9317803600538483, 'learning_rate': 7.130222222222223e-06, 'epoch': 1.4352}
{'loss': 0.9212, 'grad_norm': 2.896657505121511, 'learning_rate': 7.129111111111112e-06, 'epoch': 1.4356}
{'loss': 0.9169, 'grad_norm': 2.8497876588036695, 'learning_rate': 7.128e-06, 'epoch': 1.436}
{'eval_valid_loss': 0.89208984375, 'eval_valid_runtime': 0.089, 'eval_valid_samples_per_second': 1123.605, 'eval_valid_steps_per_second': 280.901, 'epoch': 1.436}
{'loss': 0.9288, 'grad_norm': 3.38241814479761, 'learning_rate': 7.12688888888889e-06, 'epoch': 1.4364}
{'loss': 0.9356, 'grad_norm': 2.967995517627077, 'learning_rate': 7.1257777777777785e-06, 'epoch': 1.4368}
{'loss': 0.9242, 'grad_norm': 3.043498168256023, 'learning_rate': 7.124666666666667e-06, 'epoch': 1.4372}
{'loss': 0.9218, 'grad_norm': 2.8919864747488284, 'learning_rate': 7.123555555555556e-06, 'epoch': 1.4376}
{'loss': 0.9321, 'grad_norm': 3.408961170791154, 'learning_rate': 7.1224444444444454e-06, 'epoch': 1.438}
{'loss': 0.9183, 'grad_norm': 3.4489306267989104, 'learning_rate': 7.121333333333333e-06, 'epoch': 1.4384000000000001}
{'loss': 0.9328, 'grad_norm': 3.2361529066300116, 'learning_rate': 7.120222222222223e-06, 'epoch': 1.4388}
{'loss': 0.9221, 'grad_norm': 2.8322250457753295, 'learning_rate': 7.1191111111111124e-06, 'epoch': 1.4392}
{'loss': 0.9245, 'grad_norm': 3.0405838009904183, 'learning_rate': 7.118e-06, 'epoch': 1.4396}
{'loss': 0.9328, 'grad_norm': 3.3924190855110314, 'learning_rate': 7.11688888888889e-06, 'epoch': 1.44}
{'eval_valid_loss': 0.8916015625, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1121.802, 'eval_valid_steps_per_second': 280.45, 'epoch': 1.44}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.9323, 'grad_norm': 3.3805491427971366, 'learning_rate': 7.115777777777778e-06, 'epoch': 1.4404}
{'loss': 0.9295, 'grad_norm': 3.333362666636783, 'learning_rate': 7.114666666666667e-06, 'epoch': 1.4408}
{'loss': 0.9122, 'grad_norm': 2.927478537346456, 'learning_rate': 7.113555555555556e-06, 'epoch': 1.4412}
{'loss': 0.915, 'grad_norm': 3.1884984902158036, 'learning_rate': 7.1124444444444455e-06, 'epoch': 1.4416}
{'loss': 0.9287, 'grad_norm': 3.0227512103141345, 'learning_rate': 7.111333333333333e-06, 'epoch': 1.442}
{'loss': 0.9183, 'grad_norm': 3.10408003364019, 'learning_rate': 7.110333333333334e-06, 'epoch': 1.4424000000000001}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9205, 'grad_norm': 2.965429476853731, 'learning_rate': 7.109222222222223e-06, 'epoch': 1.4428}
{'loss': 0.9187, 'grad_norm': 2.824681002827492, 'learning_rate': 7.108111111111111e-06, 'epoch': 1.4432}
{'loss': 0.93, 'grad_norm': 3.406401184646594, 'learning_rate': 7.107000000000001e-06, 'epoch': 1.4436}
{'loss': 0.9248, 'grad_norm': 3.098576576162619, 'learning_rate': 7.10588888888889e-06, 'epoch': 1.444}
{'eval_valid_loss': 0.8916015625, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.141, 'eval_valid_steps_per_second': 277.035, 'epoch': 1.444}
{'loss': 0.9184, 'grad_norm': 3.143145285400784, 'learning_rate': 7.104777777777778e-06, 'epoch': 1.4444}
{'loss': 0.9252, 'grad_norm': 3.2985391529778565, 'learning_rate': 7.103666666666668e-06, 'epoch': 1.4447999999999999}
{'loss': 0.9179, 'grad_norm': 2.968975861641089, 'learning_rate': 7.102555555555556e-06, 'epoch': 1.4452}
{'loss': 0.9251, 'grad_norm': 2.9497157303382373, 'learning_rate': 7.101444444444445e-06, 'epoch': 1.4456}
{'loss': 0.9285, 'grad_norm': 3.019555914655971, 'learning_rate': 7.100333333333334e-06, 'epoch': 1.446}
{'loss': 0.9083, 'grad_norm': 2.929955818604802, 'learning_rate': 7.0992222222222226e-06, 'epoch': 1.4464000000000001}
{'loss': 0.9214, 'grad_norm': 3.3617991727499463, 'learning_rate': 7.098111111111111e-06, 'epoch': 1.4468}
{'loss': 0.9234, 'grad_norm': 3.073373351133086, 'learning_rate': 7.097000000000001e-06, 'epoch': 1.4472}
{'loss': 0.9287, 'grad_norm': 3.217698221673429, 'learning_rate': 7.0958888888888895e-06, 'epoch': 1.4476}
{'loss': 0.9093, 'grad_norm': 3.283848578226213, 'learning_rate': 7.094777777777778e-06, 'epoch': 1.448}
{'eval_valid_loss': 0.89111328125, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.949, 'eval_valid_steps_per_second': 278.987, 'epoch': 1.448}
{'loss': 0.9458, 'grad_norm': 2.9619504056638397, 'learning_rate': 7.093666666666668e-06, 'epoch': 1.4484}
{'loss': 0.9304, 'grad_norm': 3.2741060325391516, 'learning_rate': 7.092555555555556e-06, 'epoch': 1.4487999999999999}
{'loss': 0.9257, 'grad_norm': 3.081653151146392, 'learning_rate': 7.091444444444445e-06, 'epoch': 1.4492}
{'loss': 0.9245, 'grad_norm': 3.1939754813339434, 'learning_rate': 7.090333333333333e-06, 'epoch': 1.4496}
{'loss': 0.9145, 'grad_norm': 2.964292422211521, 'learning_rate': 7.089222222222223e-06, 'epoch': 1.45}
{'loss': 0.9186, 'grad_norm': 2.9462197219432213, 'learning_rate': 7.088111111111111e-06, 'epoch': 1.4504000000000001}
{'loss': 0.93, 'grad_norm': 2.6883177732887256, 'learning_rate': 7.087000000000001e-06, 'epoch': 1.4508}
{'loss': 0.9219, 'grad_norm': 3.0009900783901067, 'learning_rate': 7.08588888888889e-06, 'epoch': 1.4512}
{'loss': 0.9244, 'grad_norm': 3.2698699520510623, 'learning_rate': 7.084777777777778e-06, 'epoch': 1.4516}
{'loss': 0.9369, 'grad_norm': 3.0382514302798014, 'learning_rate': 7.083666666666668e-06, 'epoch': 1.452}
{'eval_valid_loss': 0.890625, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.338, 'eval_valid_steps_per_second': 277.084, 'epoch': 1.452}
{'loss': 0.9231, 'grad_norm': 3.028825753996809, 'learning_rate': 7.082555555555556e-06, 'epoch': 1.4524}
{'loss': 0.9215, 'grad_norm': 2.9109296705434513, 'learning_rate': 7.081444444444445e-06, 'epoch': 1.4527999999999999}
{'loss': 0.9289, 'grad_norm': 3.011403682076052, 'learning_rate': 7.080333333333333e-06, 'epoch': 1.4532}
{'loss': 0.9301, 'grad_norm': 3.165603949805387, 'learning_rate': 7.079222222222223e-06, 'epoch': 1.4536}
{'loss': 0.9172, 'grad_norm': 3.0485682453095357, 'learning_rate': 7.0781111111111115e-06, 'epoch': 1.454}
{'loss': 0.9128, 'grad_norm': 3.077387358295231, 'learning_rate': 7.077e-06, 'epoch': 1.4544000000000001}
{'loss': 0.9426, 'grad_norm': 3.252909879873225, 'learning_rate': 7.07588888888889e-06, 'epoch': 1.4548}
{'loss': 0.9296, 'grad_norm': 3.174339558402965, 'learning_rate': 7.0747777777777784e-06, 'epoch': 1.4552}
{'loss': 0.9344, 'grad_norm': 3.0184628371768762, 'learning_rate': 7.073666666666667e-06, 'epoch': 1.4556}
{'loss': 0.9336, 'grad_norm': 3.257402828813656, 'learning_rate': 7.072555555555556e-06, 'epoch': 1.456}
{'eval_valid_loss': 0.888671875, 'eval_valid_runtime': 0.0903, 'eval_valid_samples_per_second': 1107.319, 'eval_valid_steps_per_second': 276.83, 'epoch': 1.456}
{'loss': 0.9177, 'grad_norm': 2.936862409669817, 'learning_rate': 7.0714444444444454e-06, 'epoch': 1.4564}
{'loss': 0.9127, 'grad_norm': 2.9910724843232788, 'learning_rate': 7.070333333333333e-06, 'epoch': 1.4567999999999999}
{'loss': 0.9114, 'grad_norm': 3.113224589024881, 'learning_rate': 7.069222222222223e-06, 'epoch': 1.4572}
{'loss': 0.9334, 'grad_norm': 3.411288358613171, 'learning_rate': 7.0681111111111116e-06, 'epoch': 1.4576}
{'loss': 0.9163, 'grad_norm': 2.9253433368379103, 'learning_rate': 7.067e-06, 'epoch': 1.458}
{'loss': 0.9265, 'grad_norm': 3.323861673364897, 'learning_rate': 7.06588888888889e-06, 'epoch': 1.4584}
{'loss': 0.9295, 'grad_norm': 3.2805636278437706, 'learning_rate': 7.0647777777777785e-06, 'epoch': 1.4588}
{'loss': 0.9062, 'grad_norm': 3.113213254791538, 'learning_rate': 7.063666666666667e-06, 'epoch': 1.4592}
{'loss': 0.9322, 'grad_norm': 3.178521732260228, 'learning_rate': 7.062555555555556e-06, 'epoch': 1.4596}
{'loss': 0.9234, 'grad_norm': 2.684671693554867, 'learning_rate': 7.0614444444444455e-06, 'epoch': 1.46}
{'eval_valid_loss': 0.89111328125, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.699, 'eval_valid_steps_per_second': 278.925, 'epoch': 1.46}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9246, 'grad_norm': 3.1635140591244633, 'learning_rate': 7.060333333333333e-06, 'epoch': 1.4604}
{'loss': 0.9254, 'grad_norm': 3.1001237275443336, 'learning_rate': 7.059222222222223e-06, 'epoch': 1.4607999999999999}
{'loss': 0.9287, 'grad_norm': 3.330759277486017, 'learning_rate': 7.058111111111111e-06, 'epoch': 1.4612}
{'loss': 0.9181, 'grad_norm': 2.900581687516745, 'learning_rate': 7.057e-06, 'epoch': 1.4616}
{'loss': 0.9263, 'grad_norm': 2.885715449640169, 'learning_rate': 7.05588888888889e-06, 'epoch': 1.462}
{'loss': 0.9278, 'grad_norm': 3.010987555459441, 'learning_rate': 7.054777777777778e-06, 'epoch': 1.4624}
{'loss': 0.9122, 'grad_norm': 3.111083543722503, 'learning_rate': 7.053666666666667e-06, 'epoch': 1.4628}
{'loss': 0.9061, 'grad_norm': 3.43332833402387, 'learning_rate': 7.052555555555556e-06, 'epoch': 1.4632}
{'loss': 0.933, 'grad_norm': 3.289753023634517, 'learning_rate': 7.051444444444446e-06, 'epoch': 1.4636}
{'loss': 0.9368, 'grad_norm': 3.312308008999939, 'learning_rate': 7.0503333333333335e-06, 'epoch': 1.464}
{'eval_valid_loss': 0.89208984375, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.091, 'eval_valid_steps_per_second': 279.023, 'epoch': 1.464}
{'loss': 0.913, 'grad_norm': 3.080802034154229, 'learning_rate': 7.049222222222223e-06, 'epoch': 1.4644}
{'loss': 0.9288, 'grad_norm': 3.0344020146936255, 'learning_rate': 7.048111111111113e-06, 'epoch': 1.4647999999999999}
{'loss': 0.9321, 'grad_norm': 3.1122594231006224, 'learning_rate': 7.0470000000000005e-06, 'epoch': 1.4652}
{'loss': 0.9172, 'grad_norm': 2.8686241843282128, 'learning_rate': 7.04588888888889e-06, 'epoch': 1.4656}
{'loss': 0.9083, 'grad_norm': 2.851324484310893, 'learning_rate': 7.044777777777778e-06, 'epoch': 1.466}
{'loss': 0.92, 'grad_norm': 2.8882211329647802, 'learning_rate': 7.0436666666666674e-06, 'epoch': 1.4664}
{'loss': 0.9229, 'grad_norm': 3.0789831098184317, 'learning_rate': 7.042555555555556e-06, 'epoch': 1.4668}
{'loss': 0.9289, 'grad_norm': 2.734448023229733, 'learning_rate': 7.041444444444445e-06, 'epoch': 1.4672}
{'loss': 0.9319, 'grad_norm': 3.2931084201646597, 'learning_rate': 7.0403333333333336e-06, 'epoch': 1.4676}
{'loss': 0.91, 'grad_norm': 2.9785970307779186, 'learning_rate': 7.039222222222223e-06, 'epoch': 1.468}
{'eval_valid_loss': 0.89208984375, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1109.117, 'eval_valid_steps_per_second': 277.279, 'epoch': 1.468}
{'loss': 0.9228, 'grad_norm': 3.462867508675003, 'learning_rate': 7.038111111111112e-06, 'epoch': 1.4684}
{'loss': 0.9274, 'grad_norm': 2.84684828820034, 'learning_rate': 7.0370000000000006e-06, 'epoch': 1.4687999999999999}
{'loss': 0.9297, 'grad_norm': 2.8172624638337838, 'learning_rate': 7.03588888888889e-06, 'epoch': 1.4692}
{'loss': 0.9274, 'grad_norm': 3.3247662013897084, 'learning_rate': 7.034777777777778e-06, 'epoch': 1.4696}
{'loss': 0.9212, 'grad_norm': 3.1804155645606373, 'learning_rate': 7.0336666666666675e-06, 'epoch': 1.47}
{'loss': 0.9259, 'grad_norm': 2.9240786050799614, 'learning_rate': 7.032555555555555e-06, 'epoch': 1.4704}
{'loss': 0.9121, 'grad_norm': 2.877839355293737, 'learning_rate': 7.031444444444445e-06, 'epoch': 1.4708}
{'loss': 0.9386, 'grad_norm': 2.9931045501418225, 'learning_rate': 7.030333333333334e-06, 'epoch': 1.4712}
{'loss': 0.9312, 'grad_norm': 3.136637903979222, 'learning_rate': 7.029222222222223e-06, 'epoch': 1.4716}
{'loss': 0.9246, 'grad_norm': 3.4843112272576278, 'learning_rate': 7.028111111111112e-06, 'epoch': 1.472}
{'eval_valid_loss': 0.89111328125, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1114.075, 'eval_valid_steps_per_second': 278.519, 'epoch': 1.472}
{'loss': 0.9161, 'grad_norm': 2.997683634171861, 'learning_rate': 7.027000000000001e-06, 'epoch': 1.4724}
{'loss': 0.9224, 'grad_norm': 2.8567360232842463, 'learning_rate': 7.02588888888889e-06, 'epoch': 1.4727999999999999}
{'loss': 0.9197, 'grad_norm': 3.226844627478299, 'learning_rate': 7.024777777777778e-06, 'epoch': 1.4732}
{'loss': 0.9245, 'grad_norm': 3.112190744823699, 'learning_rate': 7.023666666666668e-06, 'epoch': 1.4736}
{'loss': 0.9339, 'grad_norm': 2.94684783421411, 'learning_rate': 7.0225555555555555e-06, 'epoch': 1.474}
{'loss': 0.9165, 'grad_norm': 2.9446228494867475, 'learning_rate': 7.021444444444445e-06, 'epoch': 1.4744}
{'loss': 0.927, 'grad_norm': 3.2197477868178646, 'learning_rate': 7.020333333333334e-06, 'epoch': 1.4748}
{'loss': 0.9152, 'grad_norm': 3.3260614687056065, 'learning_rate': 7.0192222222222225e-06, 'epoch': 1.4752}
{'loss': 0.9257, 'grad_norm': 3.1134689925386354, 'learning_rate': 7.018111111111112e-06, 'epoch': 1.4756}
{'loss': 0.9226, 'grad_norm': 3.306826474171317, 'learning_rate': 7.017000000000001e-06, 'epoch': 1.476}
{'eval_valid_loss': 0.89013671875, 'eval_valid_runtime': 0.0911, 'eval_valid_samples_per_second': 1097.557, 'eval_valid_steps_per_second': 274.389, 'epoch': 1.476}
{'loss': 0.9277, 'grad_norm': 3.017264429471803, 'learning_rate': 7.0158888888888895e-06, 'epoch': 1.4764}
{'loss': 0.9177, 'grad_norm': 3.0900977955005637, 'learning_rate': 7.014777777777778e-06, 'epoch': 1.4768}
{'loss': 0.9447, 'grad_norm': 3.085382136336927, 'learning_rate': 7.013666666666668e-06, 'epoch': 1.4772}
{'loss': 0.9177, 'grad_norm': 2.9833222774393104, 'learning_rate': 7.012555555555556e-06, 'epoch': 1.4776}
{'loss': 0.92, 'grad_norm': 2.810806554020596, 'learning_rate': 7.011444444444445e-06, 'epoch': 1.478}
{'loss': 0.9246, 'grad_norm': 2.732321802124102, 'learning_rate': 7.010333333333333e-06, 'epoch': 1.4784}
{'loss': 0.9185, 'grad_norm': 3.099965162235287, 'learning_rate': 7.0092222222222226e-06, 'epoch': 1.4788000000000001}
{'loss': 0.9102, 'grad_norm': 3.004112335794882, 'learning_rate': 7.008111111111112e-06, 'epoch': 1.4792}
{'loss': 0.9295, 'grad_norm': 3.3406073427938034, 'learning_rate': 7.007000000000001e-06, 'epoch': 1.4796}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.9287, 'grad_norm': 2.9104692533999166, 'learning_rate': 7.0058888888888896e-06, 'epoch': 1.48}
{'eval_valid_loss': 0.8896484375, 'eval_valid_runtime': 0.0909, 'eval_valid_samples_per_second': 1099.508, 'eval_valid_steps_per_second': 274.877, 'epoch': 1.48}
{'loss': 0.9216, 'grad_norm': 3.061184026226081, 'learning_rate': 7.004777777777778e-06, 'epoch': 1.4804}
{'loss': 0.9396, 'grad_norm': 3.033040530531701, 'learning_rate': 7.003666666666668e-06, 'epoch': 1.4808}
{'loss': 0.9135, 'grad_norm': 2.9417557932956395, 'learning_rate': 7.002555555555556e-06, 'epoch': 1.4812}
{'loss': 0.9344, 'grad_norm': 3.1952060317976585, 'learning_rate': 7.001444444444445e-06, 'epoch': 1.4816}
{'loss': 0.9133, 'grad_norm': 2.814817109883174, 'learning_rate': 7.000333333333333e-06, 'epoch': 1.482}
{'loss': 0.9241, 'grad_norm': 3.306014901367616, 'learning_rate': 6.9993333333333335e-06, 'epoch': 1.4824}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.936, 'grad_norm': 3.2067731226052274, 'learning_rate': 6.998222222222223e-06, 'epoch': 1.4828000000000001}
{'loss': 0.9246, 'grad_norm': 3.0844167414723023, 'learning_rate': 6.997111111111111e-06, 'epoch': 1.4832}
{'loss': 0.9339, 'grad_norm': 3.0959690208443797, 'learning_rate': 6.9960000000000004e-06, 'epoch': 1.4836}
{'loss': 0.9091, 'grad_norm': 3.001958118600856, 'learning_rate': 6.99488888888889e-06, 'epoch': 1.484}
{'eval_valid_loss': 0.89111328125, 'eval_valid_runtime': 0.0973, 'eval_valid_samples_per_second': 1028.089, 'eval_valid_steps_per_second': 257.022, 'epoch': 1.484}
{'loss': 0.9209, 'grad_norm': 3.3339637080816313, 'learning_rate': 6.993777777777778e-06, 'epoch': 1.4844}
{'loss': 0.919, 'grad_norm': 2.994067286360259, 'learning_rate': 6.992666666666667e-06, 'epoch': 1.4848}
{'loss': 0.9178, 'grad_norm': 3.03284359365224, 'learning_rate': 6.991555555555556e-06, 'epoch': 1.4852}
{'loss': 0.9238, 'grad_norm': 2.926951664489357, 'learning_rate': 6.990444444444445e-06, 'epoch': 1.4856}
{'loss': 0.9148, 'grad_norm': 2.9437801715900163, 'learning_rate': 6.9893333333333336e-06, 'epoch': 1.486}
{'loss': 0.9267, 'grad_norm': 2.8814022160535933, 'learning_rate': 6.988222222222223e-06, 'epoch': 1.4864}
{'loss': 0.9172, 'grad_norm': 3.1160781530806587, 'learning_rate': 6.987111111111111e-06, 'epoch': 1.4868000000000001}
{'loss': 0.9345, 'grad_norm': 2.976882196514202, 'learning_rate': 6.9860000000000005e-06, 'epoch': 1.4872}
{'loss': 0.9333, 'grad_norm': 3.133845075877612, 'learning_rate': 6.98488888888889e-06, 'epoch': 1.4876}
{'loss': 0.9244, 'grad_norm': 3.0469678033242538, 'learning_rate': 6.983777777777778e-06, 'epoch': 1.488}
{'eval_valid_loss': 0.89013671875, 'eval_valid_runtime': 0.0903, 'eval_valid_samples_per_second': 1107.433, 'eval_valid_steps_per_second': 276.858, 'epoch': 1.488}
{'loss': 0.895, 'grad_norm': 2.9112419100297267, 'learning_rate': 6.9826666666666675e-06, 'epoch': 1.4884}
{'loss': 0.9267, 'grad_norm': 2.842253479865443, 'learning_rate': 6.981555555555556e-06, 'epoch': 1.4888}
{'loss': 0.9293, 'grad_norm': 3.0407015344881154, 'learning_rate': 6.980444444444445e-06, 'epoch': 1.4892}
{'loss': 0.9183, 'grad_norm': 3.122320437800314, 'learning_rate': 6.979333333333334e-06, 'epoch': 1.4896}
{'loss': 0.9287, 'grad_norm': 2.743659121804895, 'learning_rate': 6.978222222222223e-06, 'epoch': 1.49}
{'loss': 0.9199, 'grad_norm': 3.065590476600925, 'learning_rate': 6.977111111111111e-06, 'epoch': 1.4904}
{'loss': 0.9274, 'grad_norm': 2.83990372459331, 'learning_rate': 6.976000000000001e-06, 'epoch': 1.4908000000000001}
{'loss': 0.937, 'grad_norm': 3.454717294882833, 'learning_rate': 6.97488888888889e-06, 'epoch': 1.4912}
{'loss': 0.9287, 'grad_norm': 3.1836942785574287, 'learning_rate': 6.973777777777778e-06, 'epoch': 1.4916}
{'loss': 0.9177, 'grad_norm': 2.940804651754133, 'learning_rate': 6.972666666666668e-06, 'epoch': 1.492}
{'eval_valid_loss': 0.88916015625, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.768, 'eval_valid_steps_per_second': 278.692, 'epoch': 1.492}
{'loss': 0.9222, 'grad_norm': 3.3533505469667855, 'learning_rate': 6.9715555555555555e-06, 'epoch': 1.4924}
{'loss': 0.9185, 'grad_norm': 2.8347723277573746, 'learning_rate': 6.970444444444445e-06, 'epoch': 1.4928}
{'loss': 0.9326, 'grad_norm': 3.2110067485386757, 'learning_rate': 6.969333333333334e-06, 'epoch': 1.4932}
{'loss': 0.9244, 'grad_norm': 2.7024622198352453, 'learning_rate': 6.968222222222223e-06, 'epoch': 1.4936}
{'loss': 0.9213, 'grad_norm': 3.3415159088399924, 'learning_rate': 6.967111111111111e-06, 'epoch': 1.494}
{'loss': 0.9184, 'grad_norm': 2.915747505631746, 'learning_rate': 6.966000000000001e-06, 'epoch': 1.4944}
{'loss': 0.9276, 'grad_norm': 2.7835885835054146, 'learning_rate': 6.96488888888889e-06, 'epoch': 1.4948000000000001}
{'loss': 0.9224, 'grad_norm': 3.128751372315323, 'learning_rate': 6.963777777777778e-06, 'epoch': 1.4952}
{'loss': 0.924, 'grad_norm': 3.045247587213828, 'learning_rate': 6.962666666666668e-06, 'epoch': 1.4956}
{'loss': 0.9224, 'grad_norm': 3.12102778702833, 'learning_rate': 6.9615555555555556e-06, 'epoch': 1.496}
{'eval_valid_loss': 0.8876953125, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.285, 'eval_valid_steps_per_second': 278.571, 'epoch': 1.496}
{'loss': 0.9257, 'grad_norm': 3.0121511601649233, 'learning_rate': 6.960444444444445e-06, 'epoch': 1.4964}
{'loss': 0.9142, 'grad_norm': 3.424896291220732, 'learning_rate': 6.959333333333334e-06, 'epoch': 1.4968}
{'loss': 0.9044, 'grad_norm': 3.2318680856454067, 'learning_rate': 6.9582222222222226e-06, 'epoch': 1.4971999999999999}
{'loss': 0.9203, 'grad_norm': 2.839479803510238, 'learning_rate': 6.957111111111111e-06, 'epoch': 1.4976}
{'loss': 0.917, 'grad_norm': 3.156965712801849, 'learning_rate': 6.956000000000001e-06, 'epoch': 1.498}
{'loss': 0.9095, 'grad_norm': 3.607667676051823, 'learning_rate': 6.9548888888888895e-06, 'epoch': 1.4984}
{'loss': 0.9135, 'grad_norm': 2.8582763228741257, 'learning_rate': 6.953777777777778e-06, 'epoch': 1.4988000000000001}
{'loss': 0.9333, 'grad_norm': 3.2396654913850074, 'learning_rate': 6.952666666666668e-06, 'epoch': 1.4992}
{'loss': 0.9265, 'grad_norm': 3.121827871140638, 'learning_rate': 6.951555555555556e-06, 'epoch': 1.4996}
{'loss': 0.9186, 'grad_norm': 3.080319237327735, 'learning_rate': 6.950444444444445e-06, 'epoch': 1.5}
{'eval_valid_loss': 0.890625, 'eval_valid_runtime': 0.1842, 'eval_valid_samples_per_second': 542.906, 'eval_valid_steps_per_second': 135.727, 'epoch': 1.5}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9132, 'grad_norm': 3.0100631616308555, 'learning_rate': 6.949333333333333e-06, 'epoch': 1.5004}
{'loss': 0.9082, 'grad_norm': 3.0457208550467842, 'learning_rate': 6.948222222222223e-06, 'epoch': 1.5008}
{'loss': 0.9238, 'grad_norm': 3.074993438248495, 'learning_rate': 6.947111111111111e-06, 'epoch': 1.5011999999999999}
{'loss': 0.9222, 'grad_norm': 3.4254596520106317, 'learning_rate': 6.946000000000001e-06, 'epoch': 1.5016}
{'loss': 0.9228, 'grad_norm': 3.0446989554090167, 'learning_rate': 6.94488888888889e-06, 'epoch': 1.502}
{'loss': 0.9187, 'grad_norm': 3.0925034699385843, 'learning_rate': 6.943777777777778e-06, 'epoch': 1.5024}
{'loss': 0.9342, 'grad_norm': 2.702908942788488, 'learning_rate': 6.942666666666668e-06, 'epoch': 1.5028000000000001}
{'loss': 0.9182, 'grad_norm': 3.176452856615593, 'learning_rate': 6.941555555555556e-06, 'epoch': 1.5032}
{'loss': 0.9261, 'grad_norm': 3.18100939742028, 'learning_rate': 6.940444444444445e-06, 'epoch': 1.5036}
{'loss': 0.925, 'grad_norm': 3.2978977316487312, 'learning_rate': 6.939333333333333e-06, 'epoch': 1.504}
{'eval_valid_loss': 0.890625, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.131, 'eval_valid_steps_per_second': 281.783, 'epoch': 1.504}
{'loss': 0.9243, 'grad_norm': 3.0297338587424365, 'learning_rate': 6.938222222222223e-06, 'epoch': 1.5044}
{'loss': 0.9244, 'grad_norm': 3.0755487010618117, 'learning_rate': 6.9371111111111115e-06, 'epoch': 1.5048}
{'loss': 0.9237, 'grad_norm': 3.168126103530243, 'learning_rate': 6.936e-06, 'epoch': 1.5051999999999999}
{'loss': 0.9258, 'grad_norm': 3.088841069437007, 'learning_rate': 6.93488888888889e-06, 'epoch': 1.5056}
{'loss': 0.9172, 'grad_norm': 3.0404157788079904, 'learning_rate': 6.9337777777777784e-06, 'epoch': 1.506}
{'loss': 0.9198, 'grad_norm': 2.8795054212063103, 'learning_rate': 6.932666666666667e-06, 'epoch': 1.5064}
{'loss': 0.9221, 'grad_norm': 2.8611870115720497, 'learning_rate': 6.931555555555556e-06, 'epoch': 1.5068000000000001}
{'loss': 0.9153, 'grad_norm': 2.9592110524993145, 'learning_rate': 6.930444444444445e-06, 'epoch': 1.5072}
{'loss': 0.9345, 'grad_norm': 2.9267569978831864, 'learning_rate': 6.929333333333333e-06, 'epoch': 1.5076}
{'loss': 0.9273, 'grad_norm': 2.9119132248989024, 'learning_rate': 6.928222222222223e-06, 'epoch': 1.508}
{'eval_valid_loss': 0.88916015625, 'eval_valid_runtime': 0.0908, 'eval_valid_samples_per_second': 1101.9, 'eval_valid_steps_per_second': 275.475, 'epoch': 1.508}
{'loss': 0.9185, 'grad_norm': 2.7277315743126187, 'learning_rate': 6.927111111111112e-06, 'epoch': 1.5084}
{'loss': 0.9237, 'grad_norm': 3.274641502939015, 'learning_rate': 6.926e-06, 'epoch': 1.5088}
{'loss': 0.9266, 'grad_norm': 3.138481158080402, 'learning_rate': 6.92488888888889e-06, 'epoch': 1.5091999999999999}
{'loss': 0.9292, 'grad_norm': 2.801424311789707, 'learning_rate': 6.9237777777777785e-06, 'epoch': 1.5096}
{'loss': 0.9183, 'grad_norm': 2.9159031697464983, 'learning_rate': 6.922666666666667e-06, 'epoch': 1.51}
{'loss': 0.9227, 'grad_norm': 3.118410496780698, 'learning_rate': 6.921555555555556e-06, 'epoch': 1.5104}
{'loss': 0.9145, 'grad_norm': 2.93862747306903, 'learning_rate': 6.9204444444444455e-06, 'epoch': 1.5108000000000001}
{'loss': 0.9074, 'grad_norm': 2.874312982157654, 'learning_rate': 6.919333333333333e-06, 'epoch': 1.5112}
{'loss': 0.9356, 'grad_norm': 3.0694563256739085, 'learning_rate': 6.918222222222223e-06, 'epoch': 1.5116}
{'loss': 0.9199, 'grad_norm': 3.3077422738673015, 'learning_rate': 6.9171111111111125e-06, 'epoch': 1.512}
{'eval_valid_loss': 0.88720703125, 'eval_valid_runtime': 0.0905, 'eval_valid_samples_per_second': 1105.28, 'eval_valid_steps_per_second': 276.32, 'epoch': 1.512}
{'loss': 0.9137, 'grad_norm': 3.038071556633585, 'learning_rate': 6.916e-06, 'epoch': 1.5124}
{'loss': 0.9121, 'grad_norm': 3.1590774071481604, 'learning_rate': 6.91488888888889e-06, 'epoch': 1.5128}
{'loss': 0.9131, 'grad_norm': 3.260325452891634, 'learning_rate': 6.913777777777778e-06, 'epoch': 1.5131999999999999}
{'loss': 0.9232, 'grad_norm': 3.068011996919062, 'learning_rate': 6.912666666666667e-06, 'epoch': 1.5135999999999998}
{'loss': 0.9201, 'grad_norm': 3.1245824439510455, 'learning_rate': 6.911555555555556e-06, 'epoch': 1.514}
{'loss': 0.9067, 'grad_norm': 3.1367380467031514, 'learning_rate': 6.910444444444445e-06, 'epoch': 1.5144}
{'loss': 0.902, 'grad_norm': 3.0369833671703454, 'learning_rate': 6.9093333333333335e-06, 'epoch': 1.5148000000000001}
{'loss': 0.9243, 'grad_norm': 3.393958321038383, 'learning_rate': 6.908222222222223e-06, 'epoch': 1.5152}
{'loss': 0.9083, 'grad_norm': 3.2841256755705763, 'learning_rate': 6.907111111111113e-06, 'epoch': 1.5156}
{'loss': 0.9238, 'grad_norm': 2.8066401365488116, 'learning_rate': 6.9060000000000005e-06, 'epoch': 1.516}
{'eval_valid_loss': 0.8896484375, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1119.188, 'eval_valid_steps_per_second': 279.797, 'epoch': 1.516}
{'loss': 0.9312, 'grad_norm': 2.9863256301021197, 'learning_rate': 6.90488888888889e-06, 'epoch': 1.5164}
{'loss': 0.9117, 'grad_norm': 2.8803504313251462, 'learning_rate': 6.903777777777778e-06, 'epoch': 1.5168}
{'loss': 0.9183, 'grad_norm': 2.6069695469702188, 'learning_rate': 6.9026666666666674e-06, 'epoch': 1.5171999999999999}
{'loss': 0.9103, 'grad_norm': 3.076051488064697, 'learning_rate': 6.901555555555556e-06, 'epoch': 1.5175999999999998}
{'loss': 0.9196, 'grad_norm': 3.047537521322212, 'learning_rate': 6.900444444444445e-06, 'epoch': 1.518}
{'loss': 0.9244, 'grad_norm': 2.9697687709911005, 'learning_rate': 6.8993333333333336e-06, 'epoch': 1.5184}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.9181, 'grad_norm': 2.8358488568200473, 'learning_rate': 6.898222222222223e-06, 'epoch': 1.5188000000000001}
{'loss': 0.9202, 'grad_norm': 3.169353901953294, 'learning_rate': 6.897111111111112e-06, 'epoch': 1.5192}
{'loss': 0.9277, 'grad_norm': 2.910443572127309, 'learning_rate': 6.8960000000000006e-06, 'epoch': 1.5196}
{'loss': 0.9115, 'grad_norm': 3.00210824043236, 'learning_rate': 6.89488888888889e-06, 'epoch': 1.52}
{'eval_valid_loss': 0.89013671875, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1126.547, 'eval_valid_steps_per_second': 281.637, 'epoch': 1.52}
{'loss': 0.918, 'grad_norm': 2.6590621479575303, 'learning_rate': 6.893777777777778e-06, 'epoch': 1.5204}
{'loss': 0.9192, 'grad_norm': 3.2369961644793093, 'learning_rate': 6.8926666666666675e-06, 'epoch': 1.5208}
{'loss': 0.9125, 'grad_norm': 2.9468418471412208, 'learning_rate': 6.891555555555555e-06, 'epoch': 1.5211999999999999}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9282, 'grad_norm': 2.84458644847961, 'learning_rate': 6.890444444444445e-06, 'epoch': 1.5215999999999998}
{'loss': 0.9337, 'grad_norm': 3.363608367642907, 'learning_rate': 6.889333333333334e-06, 'epoch': 1.522}
{'loss': 0.9129, 'grad_norm': 2.938726402506, 'learning_rate': 6.888333333333333e-06, 'epoch': 1.5224}
{'loss': 0.9128, 'grad_norm': 2.892198618862845, 'learning_rate': 6.887222222222223e-06, 'epoch': 1.5228000000000002}
{'loss': 0.916, 'grad_norm': 2.8689815978350577, 'learning_rate': 6.8861111111111114e-06, 'epoch': 1.5232}
{'loss': 0.9175, 'grad_norm': 3.1232287632482723, 'learning_rate': 6.885e-06, 'epoch': 1.5236}
{'loss': 0.9153, 'grad_norm': 3.038883870815347, 'learning_rate': 6.88388888888889e-06, 'epoch': 1.524}
{'eval_valid_loss': 0.888671875, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.792, 'eval_valid_steps_per_second': 279.948, 'epoch': 1.524}
{'loss': 0.9264, 'grad_norm': 3.1067947627657118, 'learning_rate': 6.882777777777778e-06, 'epoch': 1.5244}
{'loss': 0.9346, 'grad_norm': 3.3291545403792417, 'learning_rate': 6.881666666666668e-06, 'epoch': 1.5248}
{'loss': 0.9247, 'grad_norm': 2.6648409704475142, 'learning_rate': 6.880555555555556e-06, 'epoch': 1.5252}
{'loss': 0.9194, 'grad_norm': 3.1200365990393353, 'learning_rate': 6.879444444444445e-06, 'epoch': 1.5255999999999998}
{'loss': 0.9178, 'grad_norm': 3.437994349219276, 'learning_rate': 6.878333333333333e-06, 'epoch': 1.526}
{'loss': 0.9167, 'grad_norm': 2.739964777109237, 'learning_rate': 6.877222222222223e-06, 'epoch': 1.5264}
{'loss': 0.922, 'grad_norm': 2.9936839213873427, 'learning_rate': 6.8761111111111115e-06, 'epoch': 1.5268000000000002}
{'loss': 0.9187, 'grad_norm': 2.938249938684413, 'learning_rate': 6.875e-06, 'epoch': 1.5272000000000001}
{'loss': 0.925, 'grad_norm': 3.386225751393614, 'learning_rate': 6.87388888888889e-06, 'epoch': 1.5276}
{'loss': 0.9144, 'grad_norm': 3.2260533316703843, 'learning_rate': 6.8727777777777785e-06, 'epoch': 1.528}
{'eval_valid_loss': 0.88623046875, 'eval_valid_runtime': 0.0903, 'eval_valid_samples_per_second': 1107.957, 'eval_valid_steps_per_second': 276.989, 'epoch': 1.528}
{'loss': 0.9173, 'grad_norm': 3.0207009856511107, 'learning_rate': 6.871666666666667e-06, 'epoch': 1.5284}
{'loss': 0.9326, 'grad_norm': 3.3836394393890705, 'learning_rate': 6.870555555555556e-06, 'epoch': 1.5288}
{'loss': 0.9196, 'grad_norm': 2.886590487633671, 'learning_rate': 6.8694444444444455e-06, 'epoch': 1.5292}
{'loss': 0.916, 'grad_norm': 2.908048196512577, 'learning_rate': 6.868333333333333e-06, 'epoch': 1.5295999999999998}
{'loss': 0.9071, 'grad_norm': 3.2652144105503766, 'learning_rate': 6.867222222222223e-06, 'epoch': 1.53}
{'loss': 0.9271, 'grad_norm': 3.3046111871371737, 'learning_rate': 6.866111111111111e-06, 'epoch': 1.5304}
{'loss': 0.9162, 'grad_norm': 3.066442287743549, 'learning_rate': 6.865e-06, 'epoch': 1.5308000000000002}
{'loss': 0.9265, 'grad_norm': 2.997601672986304, 'learning_rate': 6.86388888888889e-06, 'epoch': 1.5312000000000001}
{'loss': 0.9277, 'grad_norm': 2.9906883966468856, 'learning_rate': 6.862777777777779e-06, 'epoch': 1.5316}
{'loss': 0.9126, 'grad_norm': 3.4362354119921967, 'learning_rate': 6.861666666666667e-06, 'epoch': 1.532}
{'eval_valid_loss': 0.88671875, 'eval_valid_runtime': 0.0909, 'eval_valid_samples_per_second': 1100.039, 'eval_valid_steps_per_second': 275.01, 'epoch': 1.532}
{'loss': 0.9285, 'grad_norm': 3.043107388316752, 'learning_rate': 6.860555555555556e-06, 'epoch': 1.5324}
{'loss': 0.9227, 'grad_norm': 3.005774057427986, 'learning_rate': 6.859444444444446e-06, 'epoch': 1.5328}
{'loss': 0.9263, 'grad_norm': 3.0589869941872654, 'learning_rate': 6.8583333333333335e-06, 'epoch': 1.5332}
{'loss': 0.9097, 'grad_norm': 3.2817440932519952, 'learning_rate': 6.857222222222223e-06, 'epoch': 1.5335999999999999}
{'loss': 0.9003, 'grad_norm': 2.9638261317719943, 'learning_rate': 6.856111111111111e-06, 'epoch': 1.534}
{'loss': 0.9247, 'grad_norm': 3.0007393243865623, 'learning_rate': 6.8550000000000004e-06, 'epoch': 1.5344}
{'loss': 0.9174, 'grad_norm': 2.8220272293514173, 'learning_rate': 6.85388888888889e-06, 'epoch': 1.5348000000000002}
{'loss': 0.9137, 'grad_norm': 2.963665553466201, 'learning_rate': 6.852777777777778e-06, 'epoch': 1.5352000000000001}
{'loss': 0.9294, 'grad_norm': 3.3671233753019587, 'learning_rate': 6.851666666666667e-06, 'epoch': 1.5356}
{'loss': 0.9292, 'grad_norm': 3.348983376954662, 'learning_rate': 6.850555555555556e-06, 'epoch': 1.536}
{'eval_valid_loss': 0.88671875, 'eval_valid_runtime': 0.0916, 'eval_valid_samples_per_second': 1092.085, 'eval_valid_steps_per_second': 273.021, 'epoch': 1.536}
{'loss': 0.9191, 'grad_norm': 3.2490950021294935, 'learning_rate': 6.849444444444445e-06, 'epoch': 1.5364}
{'loss': 0.9185, 'grad_norm': 3.0574327411412217, 'learning_rate': 6.8483333333333336e-06, 'epoch': 1.5368}
{'loss': 0.9253, 'grad_norm': 3.128663490583849, 'learning_rate': 6.847222222222223e-06, 'epoch': 1.5372}
{'loss': 0.9263, 'grad_norm': 3.3608102260948884, 'learning_rate': 6.846111111111111e-06, 'epoch': 1.5375999999999999}
{'loss': 0.9187, 'grad_norm': 2.942049591916532, 'learning_rate': 6.8450000000000005e-06, 'epoch': 1.538}
{'loss': 0.9209, 'grad_norm': 3.197784111473264, 'learning_rate': 6.84388888888889e-06, 'epoch': 1.5384}
{'loss': 0.9183, 'grad_norm': 3.108915304583073, 'learning_rate': 6.842777777777778e-06, 'epoch': 1.5388}
{'loss': 0.9203, 'grad_norm': 3.0617220824803395, 'learning_rate': 6.8416666666666675e-06, 'epoch': 1.5392000000000001}
{'loss': 0.924, 'grad_norm': 3.33041263813138, 'learning_rate': 6.840555555555556e-06, 'epoch': 1.5396}
{'loss': 0.9256, 'grad_norm': 3.1474121828531496, 'learning_rate': 6.839444444444445e-06, 'epoch': 1.54}
{'eval_valid_loss': 0.88818359375, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.362, 'eval_valid_steps_per_second': 279.84, 'epoch': 1.54}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.907, 'grad_norm': 3.116186894414015, 'learning_rate': 6.838333333333334e-06, 'epoch': 1.5404}
{'loss': 0.9259, 'grad_norm': 3.7164120016320155, 'learning_rate': 6.837222222222223e-06, 'epoch': 1.5408}
{'loss': 0.9104, 'grad_norm': 2.8758393078420195, 'learning_rate': 6.836111111111111e-06, 'epoch': 1.5412}
{'loss': 0.945, 'grad_norm': 3.0904478309695205, 'learning_rate': 6.835000000000001e-06, 'epoch': 1.5415999999999999}
{'loss': 0.922, 'grad_norm': 2.8081429216503686, 'learning_rate': 6.83388888888889e-06, 'epoch': 1.542}
{'loss': 0.9131, 'grad_norm': 3.105226077788664, 'learning_rate': 6.832777777777778e-06, 'epoch': 1.5424}
{'loss': 0.9178, 'grad_norm': 3.1999128657637264, 'learning_rate': 6.831666666666668e-06, 'epoch': 1.5428}
{'loss': 0.9231, 'grad_norm': 3.019480380361394, 'learning_rate': 6.8305555555555555e-06, 'epoch': 1.5432000000000001}
{'loss': 0.9245, 'grad_norm': 2.796810618250951, 'learning_rate': 6.829444444444445e-06, 'epoch': 1.5436}
{'loss': 0.9257, 'grad_norm': 3.110178723765522, 'learning_rate': 6.828333333333334e-06, 'epoch': 1.544}
{'eval_valid_loss': 0.88623046875, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.189, 'eval_valid_steps_per_second': 279.547, 'epoch': 1.544}
{'loss': 0.9078, 'grad_norm': 2.86047986056395, 'learning_rate': 6.8272222222222225e-06, 'epoch': 1.5444}
{'loss': 0.9158, 'grad_norm': 2.9150760002926943, 'learning_rate': 6.826111111111111e-06, 'epoch': 1.5448}
{'loss': 0.9066, 'grad_norm': 2.9660448297725472, 'learning_rate': 6.825000000000001e-06, 'epoch': 1.5452}
{'loss': 0.9023, 'grad_norm': 2.8677187435258187, 'learning_rate': 6.82388888888889e-06, 'epoch': 1.5455999999999999}
{'loss': 0.9177, 'grad_norm': 3.0054474387864776, 'learning_rate': 6.822777777777778e-06, 'epoch': 1.546}
{'loss': 0.9127, 'grad_norm': 2.7415328371918926, 'learning_rate': 6.821666666666668e-06, 'epoch': 1.5464}
{'loss': 0.9218, 'grad_norm': 3.483783808620448, 'learning_rate': 6.8205555555555556e-06, 'epoch': 1.5468}
{'loss': 0.9123, 'grad_norm': 3.2351578488377055, 'learning_rate': 6.819444444444445e-06, 'epoch': 1.5472000000000001}
{'loss': 0.9171, 'grad_norm': 2.7103177167397354, 'learning_rate': 6.818333333333334e-06, 'epoch': 1.5476}
{'loss': 0.92, 'grad_norm': 2.8167004054680396, 'learning_rate': 6.8172222222222225e-06, 'epoch': 1.548}
{'eval_valid_loss': 0.888671875, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1109.548, 'eval_valid_steps_per_second': 277.387, 'epoch': 1.548}
{'loss': 0.9165, 'grad_norm': 3.2049507985229053, 'learning_rate': 6.816111111111111e-06, 'epoch': 1.5484}
{'loss': 0.9162, 'grad_norm': 2.910640722309725, 'learning_rate': 6.815000000000001e-06, 'epoch': 1.5488}
{'loss': 0.9332, 'grad_norm': 3.371554576366991, 'learning_rate': 6.8138888888888895e-06, 'epoch': 1.5492}
{'loss': 0.9119, 'grad_norm': 3.1965079274872115, 'learning_rate': 6.812777777777778e-06, 'epoch': 1.5495999999999999}
{'loss': 0.9216, 'grad_norm': 3.0462021304796516, 'learning_rate': 6.811666666666668e-06, 'epoch': 1.55}
{'loss': 0.9122, 'grad_norm': 2.776962141386434, 'learning_rate': 6.810555555555556e-06, 'epoch': 1.5504}
{'loss': 0.9201, 'grad_norm': 2.999294754420656, 'learning_rate': 6.809444444444445e-06, 'epoch': 1.5508}
{'loss': 0.9229, 'grad_norm': 3.0067683599794357, 'learning_rate': 6.808333333333333e-06, 'epoch': 1.5512000000000001}
{'loss': 0.9196, 'grad_norm': 2.890492018656458, 'learning_rate': 6.807222222222223e-06, 'epoch': 1.5516}
{'loss': 0.9226, 'grad_norm': 3.129935544145546, 'learning_rate': 6.806111111111111e-06, 'epoch': 1.552}
{'eval_valid_loss': 0.88671875, 'eval_valid_runtime': 0.0908, 'eval_valid_samples_per_second': 1101.24, 'eval_valid_steps_per_second': 275.31, 'epoch': 1.552}
{'loss': 0.9148, 'grad_norm': 2.9481740654943773, 'learning_rate': 6.805000000000001e-06, 'epoch': 1.5524}
{'loss': 0.9161, 'grad_norm': 3.2214946711044843, 'learning_rate': 6.80388888888889e-06, 'epoch': 1.5528}
{'loss': 0.9218, 'grad_norm': 2.6357067737553526, 'learning_rate': 6.802777777777778e-06, 'epoch': 1.5532}
{'loss': 0.9272, 'grad_norm': 3.2837982092296443, 'learning_rate': 6.801666666666668e-06, 'epoch': 1.5535999999999999}
{'loss': 0.9065, 'grad_norm': 3.048639784084064, 'learning_rate': 6.800555555555556e-06, 'epoch': 1.554}
{'loss': 0.9214, 'grad_norm': 3.0178774589538353, 'learning_rate': 6.799444444444445e-06, 'epoch': 1.5544}
{'loss': 0.9048, 'grad_norm': 3.2017370054497043, 'learning_rate': 6.798333333333333e-06, 'epoch': 1.5548}
{'loss': 0.9207, 'grad_norm': 2.844167343368323, 'learning_rate': 6.797222222222223e-06, 'epoch': 1.5552000000000001}
{'loss': 0.9208, 'grad_norm': 3.0137845288406564, 'learning_rate': 6.796111111111112e-06, 'epoch': 1.5556}
{'loss': 0.9236, 'grad_norm': 2.965704851914487, 'learning_rate': 6.795e-06, 'epoch': 1.556}
{'eval_valid_loss': 0.88671875, 'eval_valid_runtime': 0.1579, 'eval_valid_samples_per_second': 633.351, 'eval_valid_steps_per_second': 158.338, 'epoch': 1.556}
{'loss': 0.934, 'grad_norm': 3.1373416191947627, 'learning_rate': 6.79388888888889e-06, 'epoch': 1.5564}
{'loss': 0.928, 'grad_norm': 3.273564265671304, 'learning_rate': 6.7927777777777784e-06, 'epoch': 1.5568}
{'loss': 0.9169, 'grad_norm': 2.8202736130672634, 'learning_rate': 6.791666666666667e-06, 'epoch': 1.5572}
{'loss': 0.9184, 'grad_norm': 3.027710529381631, 'learning_rate': 6.790555555555556e-06, 'epoch': 1.5575999999999999}
{'loss': 0.9173, 'grad_norm': 3.314987148587962, 'learning_rate': 6.789444444444445e-06, 'epoch': 1.558}
{'loss': 0.9214, 'grad_norm': 2.9537854944551096, 'learning_rate': 6.788333333333333e-06, 'epoch': 1.5584}
{'loss': 0.9306, 'grad_norm': 2.993120979135919, 'learning_rate': 6.787222222222223e-06, 'epoch': 1.5588}
{'loss': 0.922, 'grad_norm': 2.8509527416714553, 'learning_rate': 6.786111111111112e-06, 'epoch': 1.5592000000000001}
{'loss': 0.9236, 'grad_norm': 3.214340782450689, 'learning_rate': 6.785e-06, 'epoch': 1.5596}
{'loss': 0.9222, 'grad_norm': 2.960646934645453, 'learning_rate': 6.78388888888889e-06, 'epoch': 1.56}
{'eval_valid_loss': 0.88720703125, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1115.067, 'eval_valid_steps_per_second': 278.767, 'epoch': 1.56}
{'loss': 0.9133, 'grad_norm': 2.869961771045837, 'learning_rate': 6.7827777777777785e-06, 'epoch': 1.5604}
{'loss': 0.9218, 'grad_norm': 3.4140524962254664, 'learning_rate': 6.781666666666667e-06, 'epoch': 1.5608}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9147, 'grad_norm': 3.110023105112337, 'learning_rate': 6.780555555555556e-06, 'epoch': 1.5612}
{'loss': 0.9292, 'grad_norm': 2.8071068447518126, 'learning_rate': 6.7794444444444455e-06, 'epoch': 1.5615999999999999}
{'loss': 0.918, 'grad_norm': 2.8845421634147543, 'learning_rate': 6.778333333333333e-06, 'epoch': 1.562}
{'loss': 0.9099, 'grad_norm': 3.0984413046916894, 'learning_rate': 6.777333333333334e-06, 'epoch': 1.5624}
{'loss': 0.909, 'grad_norm': 3.059449116449079, 'learning_rate': 6.776222222222223e-06, 'epoch': 1.5628}
{'loss': 0.9134, 'grad_norm': 3.038906348367368, 'learning_rate': 6.775111111111111e-06, 'epoch': 1.5632000000000001}
{'loss': 0.9233, 'grad_norm': 2.931350551336536, 'learning_rate': 6.774000000000001e-06, 'epoch': 1.5636}
{'loss': 0.9185, 'grad_norm': 3.1422671774119184, 'learning_rate': 6.77288888888889e-06, 'epoch': 1.564}
{'eval_valid_loss': 0.88525390625, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1109.848, 'eval_valid_steps_per_second': 277.462, 'epoch': 1.564}
{'loss': 0.909, 'grad_norm': 3.1529153071715186, 'learning_rate': 6.771777777777778e-06, 'epoch': 1.5644}
{'loss': 0.9102, 'grad_norm': 2.9219176447403177, 'learning_rate': 6.770666666666668e-06, 'epoch': 1.5648}
{'loss': 0.9219, 'grad_norm': 3.091327147186378, 'learning_rate': 6.7695555555555555e-06, 'epoch': 1.5652}
{'loss': 0.9249, 'grad_norm': 2.8737858406078596, 'learning_rate': 6.768444444444445e-06, 'epoch': 1.5655999999999999}
{'loss': 0.9186, 'grad_norm': 3.0713075379511867, 'learning_rate': 6.767333333333334e-06, 'epoch': 1.5659999999999998}
{'loss': 0.9158, 'grad_norm': 2.998733749981667, 'learning_rate': 6.7662222222222225e-06, 'epoch': 1.5664}
{'loss': 0.9155, 'grad_norm': 3.4199567669929976, 'learning_rate': 6.765111111111111e-06, 'epoch': 1.5668}
{'loss': 0.9157, 'grad_norm': 3.083154519583764, 'learning_rate': 6.764000000000001e-06, 'epoch': 1.5672000000000001}
{'loss': 0.9134, 'grad_norm': 2.937645330283492, 'learning_rate': 6.76288888888889e-06, 'epoch': 1.5676}
{'loss': 0.9147, 'grad_norm': 2.872105364438063, 'learning_rate': 6.761777777777778e-06, 'epoch': 1.568}
{'eval_valid_loss': 0.88671875, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1121.598, 'eval_valid_steps_per_second': 280.399, 'epoch': 1.568}
{'loss': 0.9233, 'grad_norm': 2.7933693338439887, 'learning_rate': 6.760666666666668e-06, 'epoch': 1.5684}
{'loss': 0.9121, 'grad_norm': 2.8967070953696017, 'learning_rate': 6.759555555555556e-06, 'epoch': 1.5688}
{'loss': 0.9138, 'grad_norm': 3.215534224123268, 'learning_rate': 6.758444444444445e-06, 'epoch': 1.5692}
{'loss': 0.9174, 'grad_norm': 3.1399261209428313, 'learning_rate': 6.757333333333334e-06, 'epoch': 1.5695999999999999}
{'loss': 0.9209, 'grad_norm': 3.029501243402176, 'learning_rate': 6.756222222222223e-06, 'epoch': 1.5699999999999998}
{'loss': 0.9243, 'grad_norm': 2.9316927443311918, 'learning_rate': 6.755111111111111e-06, 'epoch': 1.5704}
{'loss': 0.9045, 'grad_norm': 2.72419917255759, 'learning_rate': 6.754000000000001e-06, 'epoch': 1.5708}
{'loss': 0.9207, 'grad_norm': 3.1375935096028247, 'learning_rate': 6.75288888888889e-06, 'epoch': 1.5712000000000002}
{'loss': 0.9154, 'grad_norm': 2.990117514910016, 'learning_rate': 6.751777777777778e-06, 'epoch': 1.5716}
{'loss': 0.9182, 'grad_norm': 2.9825797847970303, 'learning_rate': 6.750666666666668e-06, 'epoch': 1.572}
{'eval_valid_loss': 0.88623046875, 'eval_valid_runtime': 0.0905, 'eval_valid_samples_per_second': 1104.584, 'eval_valid_steps_per_second': 276.146, 'epoch': 1.572}
{'loss': 0.933, 'grad_norm': 3.1114231150195972, 'learning_rate': 6.749555555555556e-06, 'epoch': 1.5724}
{'loss': 0.9151, 'grad_norm': 2.8497252515904057, 'learning_rate': 6.748444444444445e-06, 'epoch': 1.5728}
{'loss': 0.9203, 'grad_norm': 2.9317342297212896, 'learning_rate': 6.747333333333333e-06, 'epoch': 1.5732}
{'loss': 0.9326, 'grad_norm': 2.797052846613904, 'learning_rate': 6.746222222222223e-06, 'epoch': 1.5735999999999999}
{'loss': 0.9218, 'grad_norm': 2.9924941241972305, 'learning_rate': 6.7451111111111114e-06, 'epoch': 1.5739999999999998}
{'loss': 0.9127, 'grad_norm': 3.078592313351722, 'learning_rate': 6.744e-06, 'epoch': 1.5744}
{'loss': 0.9022, 'grad_norm': 2.8434675830994167, 'learning_rate': 6.74288888888889e-06, 'epoch': 1.5748}
{'loss': 0.9198, 'grad_norm': 3.6051291722238177, 'learning_rate': 6.741777777777778e-06, 'epoch': 1.5752000000000002}
{'loss': 0.9313, 'grad_norm': 3.3362044287921173, 'learning_rate': 6.740666666666668e-06, 'epoch': 1.5756000000000001}
{'loss': 0.9256, 'grad_norm': 3.068665030909388, 'learning_rate': 6.739555555555556e-06, 'epoch': 1.576}
{'eval_valid_loss': 0.884765625, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.744, 'eval_valid_steps_per_second': 279.686, 'epoch': 1.576}
{'loss': 0.9095, 'grad_norm': 3.317915870527164, 'learning_rate': 6.738444444444445e-06, 'epoch': 1.5764}
{'loss': 0.919, 'grad_norm': 3.234843280082624, 'learning_rate': 6.737333333333333e-06, 'epoch': 1.5768}
{'loss': 0.9184, 'grad_norm': 3.2997083466423445, 'learning_rate': 6.736222222222223e-06, 'epoch': 1.5772}
{'loss': 0.9307, 'grad_norm': 2.91146058416308, 'learning_rate': 6.7351111111111115e-06, 'epoch': 1.5776}
{'loss': 0.9313, 'grad_norm': 3.053495101205817, 'learning_rate': 6.734e-06, 'epoch': 1.5779999999999998}
{'loss': 0.9235, 'grad_norm': 2.909702496068505, 'learning_rate': 6.73288888888889e-06, 'epoch': 1.5784}
{'loss': 0.9236, 'grad_norm': 3.233301629610724, 'learning_rate': 6.7317777777777785e-06, 'epoch': 1.5788}
{'loss': 0.9043, 'grad_norm': 3.0347072313366676, 'learning_rate': 6.730666666666667e-06, 'epoch': 1.5792000000000002}
{'loss': 0.9211, 'grad_norm': 3.012966159905894, 'learning_rate': 6.729555555555556e-06, 'epoch': 1.5796000000000001}
{'loss': 0.9278, 'grad_norm': 3.141094371964667, 'learning_rate': 6.7284444444444455e-06, 'epoch': 1.58}
{'eval_valid_loss': 0.88623046875, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.183, 'eval_valid_steps_per_second': 279.546, 'epoch': 1.58}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9289, 'grad_norm': 2.8119942846219788, 'learning_rate': 6.727333333333333e-06, 'epoch': 1.5804}
{'loss': 0.9107, 'grad_norm': 3.2946890142133927, 'learning_rate': 6.726222222222223e-06, 'epoch': 1.5808}
{'loss': 0.923, 'grad_norm': 3.7548496200225463, 'learning_rate': 6.725111111111111e-06, 'epoch': 1.5812}
{'loss': 0.9147, 'grad_norm': 3.2587669675887923, 'learning_rate': 6.724e-06, 'epoch': 1.5816}
{'loss': 0.9223, 'grad_norm': 3.004021651056645, 'learning_rate': 6.72288888888889e-06, 'epoch': 1.5819999999999999}
{'loss': 0.9122, 'grad_norm': 2.6730269575568544, 'learning_rate': 6.721777777777778e-06, 'epoch': 1.5824}
{'loss': 0.9208, 'grad_norm': 3.3080266130909433, 'learning_rate': 6.720666666666667e-06, 'epoch': 1.5828}
{'loss': 0.9205, 'grad_norm': 2.9846347076696538, 'learning_rate': 6.719555555555556e-06, 'epoch': 1.5832000000000002}
{'loss': 0.9175, 'grad_norm': 3.2928793409705777, 'learning_rate': 6.718444444444446e-06, 'epoch': 1.5836000000000001}
{'loss': 0.9249, 'grad_norm': 2.8553769244290375, 'learning_rate': 6.7173333333333335e-06, 'epoch': 1.584}
{'eval_valid_loss': 0.88720703125, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1120.489, 'eval_valid_steps_per_second': 280.122, 'epoch': 1.584}
{'loss': 0.9133, 'grad_norm': 2.9189689382577773, 'learning_rate': 6.716222222222223e-06, 'epoch': 1.5844}
{'loss': 0.9109, 'grad_norm': 2.6881459701809294, 'learning_rate': 6.715111111111111e-06, 'epoch': 1.5848}
{'loss': 0.9069, 'grad_norm': 2.876081294434158, 'learning_rate': 6.7140000000000004e-06, 'epoch': 1.5852}
{'loss': 0.919, 'grad_norm': 3.08799598702296, 'learning_rate': 6.71288888888889e-06, 'epoch': 1.5856}
{'loss': 0.9199, 'grad_norm': 2.7508860861972226, 'learning_rate': 6.711777777777778e-06, 'epoch': 1.5859999999999999}
{'loss': 0.9143, 'grad_norm': 2.7965707453607127, 'learning_rate': 6.710666666666667e-06, 'epoch': 1.5864}
{'loss': 0.9212, 'grad_norm': 3.0188698580939244, 'learning_rate': 6.709555555555556e-06, 'epoch': 1.5868}
{'loss': 0.9193, 'grad_norm': 3.2463611533958194, 'learning_rate': 6.708444444444445e-06, 'epoch': 1.5872000000000002}
{'loss': 0.9224, 'grad_norm': 3.0810166539475996, 'learning_rate': 6.7073333333333335e-06, 'epoch': 1.5876000000000001}
{'loss': 0.9115, 'grad_norm': 3.0220249237456644, 'learning_rate': 6.706222222222223e-06, 'epoch': 1.588}
{'eval_valid_loss': 0.88623046875, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1109.933, 'eval_valid_steps_per_second': 277.483, 'epoch': 1.588}
{'loss': 0.9242, 'grad_norm': 3.2105789077150306, 'learning_rate': 6.705111111111111e-06, 'epoch': 1.5884}
{'loss': 0.9248, 'grad_norm': 2.9018645218219015, 'learning_rate': 6.7040000000000005e-06, 'epoch': 1.5888}
{'loss': 0.9076, 'grad_norm': 3.169900119633978, 'learning_rate': 6.70288888888889e-06, 'epoch': 1.5892}
{'loss': 0.9178, 'grad_norm': 2.8016866500076767, 'learning_rate': 6.701777777777778e-06, 'epoch': 1.5896}
{'loss': 0.9305, 'grad_norm': 3.155995849725704, 'learning_rate': 6.7006666666666675e-06, 'epoch': 1.5899999999999999}
{'loss': 0.9108, 'grad_norm': 3.047561900642242, 'learning_rate': 6.699555555555556e-06, 'epoch': 1.5904}
{'loss': 0.9179, 'grad_norm': 2.844966785307416, 'learning_rate': 6.698444444444445e-06, 'epoch': 1.5908}
{'loss': 0.9174, 'grad_norm': 3.1113731443281534, 'learning_rate': 6.697333333333334e-06, 'epoch': 1.5912}
{'loss': 0.917, 'grad_norm': 2.823781012541682, 'learning_rate': 6.696222222222223e-06, 'epoch': 1.5916000000000001}
{'loss': 0.9257, 'grad_norm': 3.0052107717855643, 'learning_rate': 6.695111111111111e-06, 'epoch': 1.592}
{'eval_valid_loss': 0.88623046875, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1120.432, 'eval_valid_steps_per_second': 280.108, 'epoch': 1.592}
{'loss': 0.9324, 'grad_norm': 3.0545420014445233, 'learning_rate': 6.694000000000001e-06, 'epoch': 1.5924}
{'loss': 0.9136, 'grad_norm': 2.936639487254066, 'learning_rate': 6.69288888888889e-06, 'epoch': 1.5928}
{'loss': 0.9093, 'grad_norm': 3.1561770666779387, 'learning_rate': 6.691777777777778e-06, 'epoch': 1.5932}
{'loss': 0.9189, 'grad_norm': 2.8941159381552413, 'learning_rate': 6.690666666666668e-06, 'epoch': 1.5936}
{'loss': 0.9051, 'grad_norm': 2.987068997500945, 'learning_rate': 6.6895555555555555e-06, 'epoch': 1.5939999999999999}
{'loss': 0.9213, 'grad_norm': 3.1725928710979567, 'learning_rate': 6.688444444444445e-06, 'epoch': 1.5944}
{'loss': 0.9094, 'grad_norm': 3.3251021900105204, 'learning_rate': 6.687333333333334e-06, 'epoch': 1.5948}
{'loss': 0.9184, 'grad_norm': 2.7800162665059323, 'learning_rate': 6.6862222222222224e-06, 'epoch': 1.5952}
{'loss': 0.9204, 'grad_norm': 2.8663507742619734, 'learning_rate': 6.685111111111111e-06, 'epoch': 1.5956000000000001}
{'loss': 0.9207, 'grad_norm': 3.2503541698412572, 'learning_rate': 6.684000000000001e-06, 'epoch': 1.596}
{'eval_valid_loss': 0.88427734375, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.306, 'eval_valid_steps_per_second': 278.577, 'epoch': 1.596}
{'loss': 0.9126, 'grad_norm': 2.992080845936486, 'learning_rate': 6.68288888888889e-06, 'epoch': 1.5964}
{'loss': 0.9149, 'grad_norm': 2.912527308929508, 'learning_rate': 6.681777777777778e-06, 'epoch': 1.5968}
{'loss': 0.9126, 'grad_norm': 2.9435171326782923, 'learning_rate': 6.680666666666668e-06, 'epoch': 1.5972}
{'loss': 0.9149, 'grad_norm': 3.0506740849193275, 'learning_rate': 6.6795555555555556e-06, 'epoch': 1.5976}
{'loss': 0.9167, 'grad_norm': 3.036207903071147, 'learning_rate': 6.678444444444445e-06, 'epoch': 1.5979999999999999}
{'loss': 0.9228, 'grad_norm': 3.0283870843429015, 'learning_rate': 6.677333333333334e-06, 'epoch': 1.5984}
{'loss': 0.9175, 'grad_norm': 2.938350838342008, 'learning_rate': 6.6762222222222225e-06, 'epoch': 1.5988}
{'loss': 0.91, 'grad_norm': 3.217226593832383, 'learning_rate': 6.675111111111112e-06, 'epoch': 1.5992}
{'loss': 0.9162, 'grad_norm': 3.205603919641385, 'learning_rate': 6.674000000000001e-06, 'epoch': 1.5996000000000001}
{'loss': 0.9129, 'grad_norm': 2.994855214426051, 'learning_rate': 6.6728888888888895e-06, 'epoch': 1.6}
{'eval_valid_loss': 0.88916015625, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.978, 'eval_valid_steps_per_second': 278.995, 'epoch': 1.6}
{'loss': 0.9212, 'grad_norm': 3.2012555319169644, 'learning_rate': 6.671777777777778e-06, 'epoch': 1.6004}
{'loss': 0.9183, 'grad_norm': 3.225941697590662, 'learning_rate': 6.670666666666668e-06, 'epoch': 1.6008}
{'loss': 0.9174, 'grad_norm': 2.9282629593274434, 'learning_rate': 6.669555555555556e-06, 'epoch': 1.6012}
{'loss': 0.9104, 'grad_norm': 2.75788453524043, 'learning_rate': 6.668444444444445e-06, 'epoch': 1.6016}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9115, 'grad_norm': 2.861155159054621, 'learning_rate': 6.667333333333333e-06, 'epoch': 1.6019999999999999}
{'loss': 0.9146, 'grad_norm': 3.141976497476045, 'learning_rate': 6.6663333333333334e-06, 'epoch': 1.6024}
{'loss': 0.913, 'grad_norm': 3.192076427078959, 'learning_rate': 6.665222222222223e-06, 'epoch': 1.6028}
{'loss': 0.9287, 'grad_norm': 3.249125959167938, 'learning_rate': 6.664111111111111e-06, 'epoch': 1.6032}
{'loss': 0.9167, 'grad_norm': 2.9365097162289584, 'learning_rate': 6.663e-06, 'epoch': 1.6036000000000001}
{'loss': 0.9005, 'grad_norm': 2.9661112552510684, 'learning_rate': 6.66188888888889e-06, 'epoch': 1.604}
{'eval_valid_loss': 0.8837890625, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1105.667, 'eval_valid_steps_per_second': 276.417, 'epoch': 1.604}
{'loss': 0.9201, 'grad_norm': 2.7451569056614633, 'learning_rate': 6.660777777777778e-06, 'epoch': 1.6044}
{'loss': 0.9175, 'grad_norm': 3.0775731264904214, 'learning_rate': 6.659666666666667e-06, 'epoch': 1.6048}
{'loss': 0.9275, 'grad_norm': 2.896484261819499, 'learning_rate': 6.658555555555556e-06, 'epoch': 1.6052}
{'loss': 0.908, 'grad_norm': 2.8883207570748897, 'learning_rate': 6.657444444444446e-06, 'epoch': 1.6056}
{'loss': 0.9158, 'grad_norm': 2.992577629452809, 'learning_rate': 6.6563333333333335e-06, 'epoch': 1.6059999999999999}
{'loss': 0.9244, 'grad_norm': 3.0792515915054106, 'learning_rate': 6.655222222222223e-06, 'epoch': 1.6064}
{'loss': 0.9125, 'grad_norm': 2.599394080735673, 'learning_rate': 6.654111111111111e-06, 'epoch': 1.6068}
{'loss': 0.908, 'grad_norm': 3.017365887039048, 'learning_rate': 6.6530000000000005e-06, 'epoch': 1.6072}
{'loss': 0.9219, 'grad_norm': 2.7568893994976005, 'learning_rate': 6.65188888888889e-06, 'epoch': 1.6076000000000001}
{'loss': 0.915, 'grad_norm': 3.1425693812939173, 'learning_rate': 6.650777777777778e-06, 'epoch': 1.608}
{'eval_valid_loss': 0.88427734375, 'eval_valid_runtime': 0.0903, 'eval_valid_samples_per_second': 1107.036, 'eval_valid_steps_per_second': 276.759, 'epoch': 1.608}
{'loss': 0.9122, 'grad_norm': 3.0444843205414553, 'learning_rate': 6.6496666666666675e-06, 'epoch': 1.6084}
{'loss': 0.907, 'grad_norm': 2.8713742904100905, 'learning_rate': 6.648555555555556e-06, 'epoch': 1.6088}
{'loss': 0.913, 'grad_norm': 3.1378041974814455, 'learning_rate': 6.647444444444445e-06, 'epoch': 1.6092}
{'loss': 0.9161, 'grad_norm': 3.043086058249369, 'learning_rate': 6.646333333333334e-06, 'epoch': 1.6096}
{'loss': 0.9089, 'grad_norm': 3.0207872529735535, 'learning_rate': 6.645222222222223e-06, 'epoch': 1.6099999999999999}
{'loss': 0.9189, 'grad_norm': 3.0337474012894226, 'learning_rate': 6.644111111111111e-06, 'epoch': 1.6104}
{'loss': 0.9212, 'grad_norm': 3.0377107276346322, 'learning_rate': 6.643000000000001e-06, 'epoch': 1.6108}
{'loss': 0.9214, 'grad_norm': 2.9198568585662374, 'learning_rate': 6.64188888888889e-06, 'epoch': 1.6112}
{'loss': 0.9055, 'grad_norm': 2.96056036447164, 'learning_rate': 6.640777777777778e-06, 'epoch': 1.6116000000000001}
{'loss': 0.9082, 'grad_norm': 3.006282446957793, 'learning_rate': 6.639666666666668e-06, 'epoch': 1.612}
{'eval_valid_loss': 0.8837890625, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.373, 'eval_valid_steps_per_second': 279.343, 'epoch': 1.612}
{'loss': 0.9222, 'grad_norm': 3.474683841209705, 'learning_rate': 6.6385555555555554e-06, 'epoch': 1.6124}
{'loss': 0.9201, 'grad_norm': 3.0423645673338098, 'learning_rate': 6.637444444444445e-06, 'epoch': 1.6128}
{'loss': 0.9094, 'grad_norm': 3.3504596131587157, 'learning_rate': 6.636333333333334e-06, 'epoch': 1.6132}
{'loss': 0.9098, 'grad_norm': 3.2569735793921044, 'learning_rate': 6.635222222222223e-06, 'epoch': 1.6136}
{'loss': 0.9169, 'grad_norm': 3.2331277312866926, 'learning_rate': 6.634111111111111e-06, 'epoch': 1.6139999999999999}
{'loss': 0.9189, 'grad_norm': 3.194544982134308, 'learning_rate': 6.633000000000001e-06, 'epoch': 1.6143999999999998}
{'loss': 0.9186, 'grad_norm': 3.0591298652921672, 'learning_rate': 6.63188888888889e-06, 'epoch': 1.6148}
{'loss': 0.9176, 'grad_norm': 3.228236025353193, 'learning_rate': 6.630777777777778e-06, 'epoch': 1.6152}
{'loss': 0.9127, 'grad_norm': 3.0594962921101407, 'learning_rate': 6.629666666666668e-06, 'epoch': 1.6156000000000001}
{'loss': 0.9081, 'grad_norm': 2.9330157994496067, 'learning_rate': 6.6285555555555555e-06, 'epoch': 1.616}
{'eval_valid_loss': 0.884765625, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1128.709, 'eval_valid_steps_per_second': 282.177, 'epoch': 1.616}
{'loss': 0.9077, 'grad_norm': 3.0457682433437974, 'learning_rate': 6.627444444444445e-06, 'epoch': 1.6164}
{'loss': 0.9111, 'grad_norm': 2.9144009145639447, 'learning_rate': 6.626333333333334e-06, 'epoch': 1.6168}
{'loss': 0.91, 'grad_norm': 3.186780745568876, 'learning_rate': 6.6252222222222225e-06, 'epoch': 1.6172}
{'loss': 0.91, 'grad_norm': 2.9843460700744955, 'learning_rate': 6.624111111111111e-06, 'epoch': 1.6176}
{'loss': 0.9265, 'grad_norm': 3.2360453600109174, 'learning_rate': 6.623000000000001e-06, 'epoch': 1.6179999999999999}
{'loss': 0.9067, 'grad_norm': 3.1443343770256504, 'learning_rate': 6.6218888888888895e-06, 'epoch': 1.6183999999999998}
{'loss': 0.9074, 'grad_norm': 3.049988647346005, 'learning_rate': 6.620777777777778e-06, 'epoch': 1.6188}
{'loss': 0.9149, 'grad_norm': 2.9478341294361234, 'learning_rate': 6.619666666666668e-06, 'epoch': 1.6192}
{'loss': 0.9092, 'grad_norm': 2.993568689275364, 'learning_rate': 6.618555555555556e-06, 'epoch': 1.6196000000000002}
{'loss': 0.9195, 'grad_norm': 2.9635439449808834, 'learning_rate': 6.617444444444445e-06, 'epoch': 1.62}
{'eval_valid_loss': 0.88232421875, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1122.159, 'eval_valid_steps_per_second': 280.54, 'epoch': 1.62}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.9097, 'grad_norm': 3.9011772176351314, 'learning_rate': 6.616333333333334e-06, 'epoch': 1.6204}
{'loss': 0.921, 'grad_norm': 3.0228397557214794, 'learning_rate': 6.615222222222223e-06, 'epoch': 1.6208}
{'loss': 0.9248, 'grad_norm': 3.008499869538872, 'learning_rate': 6.614111111111111e-06, 'epoch': 1.6212}
{'loss': 0.9112, 'grad_norm': 2.892478884502018, 'learning_rate': 6.613000000000001e-06, 'epoch': 1.6216}
{'loss': 0.9157, 'grad_norm': 3.3733831170678092, 'learning_rate': 6.61188888888889e-06, 'epoch': 1.6219999999999999}
{'loss': 0.9196, 'grad_norm': 2.954691647950826, 'learning_rate': 6.610777777777778e-06, 'epoch': 1.6223999999999998}
{'loss': 0.9225, 'grad_norm': 2.820084205961383, 'learning_rate': 6.609666666666668e-06, 'epoch': 1.6228}
{'loss': 0.9213, 'grad_norm': 3.058350018647195, 'learning_rate': 6.608555555555556e-06, 'epoch': 1.6232}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9176, 'grad_norm': 2.7192379360161083, 'learning_rate': 6.607444444444445e-06, 'epoch': 1.6236000000000002}
{'loss': 0.9193, 'grad_norm': 3.136982185826355, 'learning_rate': 6.606333333333333e-06, 'epoch': 1.624}
{'eval_valid_loss': 0.8837890625, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1124.913, 'eval_valid_steps_per_second': 281.228, 'epoch': 1.624}
{'loss': 0.9127, 'grad_norm': 2.898579637687332, 'learning_rate': 6.605222222222223e-06, 'epoch': 1.6244}
{'loss': 0.9179, 'grad_norm': 3.2691047873431702, 'learning_rate': 6.6041111111111114e-06, 'epoch': 1.6248}
{'loss': 0.9258, 'grad_norm': 3.3163848444918465, 'learning_rate': 6.603e-06, 'epoch': 1.6252}
{'loss': 0.9197, 'grad_norm': 2.7522219762790034, 'learning_rate': 6.60188888888889e-06, 'epoch': 1.6256}
{'loss': 0.9085, 'grad_norm': 3.2977036524015695, 'learning_rate': 6.600777777777778e-06, 'epoch': 1.626}
{'loss': 0.9129, 'grad_norm': 2.8562398409506264, 'learning_rate': 6.599666666666668e-06, 'epoch': 1.6263999999999998}
{'loss': 0.9224, 'grad_norm': 2.77267686326186, 'learning_rate': 6.598555555555556e-06, 'epoch': 1.6268}
{'loss': 0.911, 'grad_norm': 2.8825532853527265, 'learning_rate': 6.597444444444445e-06, 'epoch': 1.6272}
{'loss': 0.9137, 'grad_norm': 3.0713015994254254, 'learning_rate': 6.596333333333333e-06, 'epoch': 1.6276000000000002}
{'loss': 0.9075, 'grad_norm': 2.819707800373444, 'learning_rate': 6.595222222222223e-06, 'epoch': 1.6280000000000001}
{'eval_valid_loss': 0.8837890625, 'eval_valid_runtime': 0.0885, 'eval_valid_samples_per_second': 1130.114, 'eval_valid_steps_per_second': 282.528, 'epoch': 1.6280000000000001}
{'loss': 0.9038, 'grad_norm': 3.0063507590588427, 'learning_rate': 6.5941111111111115e-06, 'epoch': 1.6284}
{'loss': 0.9292, 'grad_norm': 2.963240510402538, 'learning_rate': 6.593e-06, 'epoch': 1.6288}
{'loss': 0.8983, 'grad_norm': 3.0088316144599356, 'learning_rate': 6.59188888888889e-06, 'epoch': 1.6292}
{'loss': 0.9106, 'grad_norm': 2.9362250361349647, 'learning_rate': 6.5907777777777785e-06, 'epoch': 1.6296}
{'loss': 0.9146, 'grad_norm': 2.827883557293078, 'learning_rate': 6.589666666666667e-06, 'epoch': 1.63}
{'loss': 0.9159, 'grad_norm': 2.7725254170927025, 'learning_rate': 6.588555555555556e-06, 'epoch': 1.6303999999999998}
{'loss': 0.911, 'grad_norm': 3.1881874035140543, 'learning_rate': 6.5874444444444455e-06, 'epoch': 1.6308}
{'loss': 0.9097, 'grad_norm': 2.7374145398669993, 'learning_rate': 6.586333333333333e-06, 'epoch': 1.6312}
{'loss': 0.9139, 'grad_norm': 3.1319572156423474, 'learning_rate': 6.585222222222223e-06, 'epoch': 1.6316000000000002}
{'loss': 0.9121, 'grad_norm': 3.0414497295949254, 'learning_rate': 6.584111111111111e-06, 'epoch': 1.6320000000000001}
{'eval_valid_loss': 0.88330078125, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.9, 'eval_valid_steps_per_second': 279.975, 'epoch': 1.6320000000000001}
{'loss': 0.9114, 'grad_norm': 2.8485604774934448, 'learning_rate': 6.583e-06, 'epoch': 1.6324}
{'loss': 0.9108, 'grad_norm': 3.074095581767049, 'learning_rate': 6.58188888888889e-06, 'epoch': 1.6328}
{'loss': 0.9062, 'grad_norm': 3.0847479157110365, 'learning_rate': 6.580777777777778e-06, 'epoch': 1.6332}
{'loss': 0.9329, 'grad_norm': 2.8214918613664604, 'learning_rate': 6.579666666666667e-06, 'epoch': 1.6336}
{'loss': 0.9084, 'grad_norm': 3.0528077588379876, 'learning_rate': 6.578555555555556e-06, 'epoch': 1.634}
{'loss': 0.9198, 'grad_norm': 2.790395936183092, 'learning_rate': 6.577444444444446e-06, 'epoch': 1.6343999999999999}
{'loss': 0.908, 'grad_norm': 3.2058000326910205, 'learning_rate': 6.5763333333333334e-06, 'epoch': 1.6348}
{'loss': 0.9149, 'grad_norm': 3.038748099727813, 'learning_rate': 6.575222222222223e-06, 'epoch': 1.6352}
{'loss': 0.9067, 'grad_norm': 2.8681733517988297, 'learning_rate': 6.574111111111111e-06, 'epoch': 1.6356000000000002}
{'loss': 0.8989, 'grad_norm': 2.8470304348299997, 'learning_rate': 6.5730000000000004e-06, 'epoch': 1.6360000000000001}
{'eval_valid_loss': 0.884765625, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1115.228, 'eval_valid_steps_per_second': 278.807, 'epoch': 1.6360000000000001}
{'loss': 0.9169, 'grad_norm': 3.302004317813386, 'learning_rate': 6.57188888888889e-06, 'epoch': 1.6364}
{'loss': 0.9287, 'grad_norm': 2.777836494355045, 'learning_rate': 6.570777777777778e-06, 'epoch': 1.6368}
{'loss': 0.9165, 'grad_norm': 3.1144710358923184, 'learning_rate': 6.569666666666667e-06, 'epoch': 1.6372}
{'loss': 0.9145, 'grad_norm': 3.115755971453366, 'learning_rate': 6.568555555555556e-06, 'epoch': 1.6376}
{'loss': 0.9211, 'grad_norm': 2.864655907607461, 'learning_rate': 6.567444444444445e-06, 'epoch': 1.638}
{'loss': 0.9137, 'grad_norm': 2.872982426380168, 'learning_rate': 6.5663333333333335e-06, 'epoch': 1.6383999999999999}
{'loss': 0.9095, 'grad_norm': 2.8671058830265537, 'learning_rate': 6.565222222222223e-06, 'epoch': 1.6388}
{'loss': 0.9179, 'grad_norm': 2.760698121794977, 'learning_rate': 6.564111111111111e-06, 'epoch': 1.6392}
{'loss': 0.9066, 'grad_norm': 3.1875621004226096, 'learning_rate': 6.5630000000000005e-06, 'epoch': 1.6396}
{'loss': 0.9138, 'grad_norm': 3.3915753329224145, 'learning_rate': 6.56188888888889e-06, 'epoch': 1.6400000000000001}
{'eval_valid_loss': 0.88330078125, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1125.565, 'eval_valid_steps_per_second': 281.391, 'epoch': 1.6400000000000001}
{'loss': 0.9166, 'grad_norm': 3.1087920730345213, 'learning_rate': 6.560777777777778e-06, 'epoch': 1.6404}
{'loss': 0.9198, 'grad_norm': 2.9948631156470005, 'learning_rate': 6.5596666666666675e-06, 'epoch': 1.6408}
{'loss': 0.9188, 'grad_norm': 2.6508695806373344, 'learning_rate': 6.558555555555555e-06, 'epoch': 1.6412}
{'loss': 0.9261, 'grad_norm': 2.9504089581164794, 'learning_rate': 6.557444444444445e-06, 'epoch': 1.6416}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9089, 'grad_norm': 2.8445614086837407, 'learning_rate': 6.556333333333334e-06, 'epoch': 1.642}
{'loss': 0.9108, 'grad_norm': 2.899820660932339, 'learning_rate': 6.555333333333333e-06, 'epoch': 1.6423999999999999}
{'loss': 0.916, 'grad_norm': 3.120668069506738, 'learning_rate': 6.554222222222223e-06, 'epoch': 1.6428}
{'loss': 0.9122, 'grad_norm': 2.9541882311505145, 'learning_rate': 6.553111111111111e-06, 'epoch': 1.6432}
{'loss': 0.9149, 'grad_norm': 2.999449232327093, 'learning_rate': 6.552000000000001e-06, 'epoch': 1.6436}
{'loss': 0.9161, 'grad_norm': 2.8240260669198163, 'learning_rate': 6.55088888888889e-06, 'epoch': 1.6440000000000001}
{'eval_valid_loss': 0.8828125, 'eval_valid_runtime': 0.136, 'eval_valid_samples_per_second': 735.212, 'eval_valid_steps_per_second': 183.803, 'epoch': 1.6440000000000001}
{'loss': 0.9123, 'grad_norm': 2.8242585956300523, 'learning_rate': 6.549777777777778e-06, 'epoch': 1.6444}
{'loss': 0.9186, 'grad_norm': 3.2406138266331244, 'learning_rate': 6.548666666666668e-06, 'epoch': 1.6448}
{'loss': 0.9202, 'grad_norm': 2.548708051286733, 'learning_rate': 6.547555555555556e-06, 'epoch': 1.6452}
{'loss': 0.9102, 'grad_norm': 2.887250342867573, 'learning_rate': 6.546444444444445e-06, 'epoch': 1.6456}
{'loss': 0.9318, 'grad_norm': 3.117476896844345, 'learning_rate': 6.545333333333333e-06, 'epoch': 1.646}
{'loss': 0.9141, 'grad_norm': 2.9991645345038145, 'learning_rate': 6.544222222222223e-06, 'epoch': 1.6463999999999999}
{'loss': 0.9281, 'grad_norm': 3.1896140063252183, 'learning_rate': 6.5431111111111115e-06, 'epoch': 1.6468}
{'loss': 0.9104, 'grad_norm': 3.5986282907610243, 'learning_rate': 6.542e-06, 'epoch': 1.6472}
{'loss': 0.916, 'grad_norm': 2.921412946509036, 'learning_rate': 6.54088888888889e-06, 'epoch': 1.6476}
{'loss': 0.9107, 'grad_norm': 2.92482659486135, 'learning_rate': 6.5397777777777785e-06, 'epoch': 1.6480000000000001}
{'eval_valid_loss': 0.8818359375, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.338, 'eval_valid_steps_per_second': 279.584, 'epoch': 1.6480000000000001}
{'loss': 0.9065, 'grad_norm': 2.7916756791116417, 'learning_rate': 6.538666666666667e-06, 'epoch': 1.6484}
{'loss': 0.9048, 'grad_norm': 3.248713871592431, 'learning_rate': 6.537555555555556e-06, 'epoch': 1.6488}
{'loss': 0.913, 'grad_norm': 3.1768672442299333, 'learning_rate': 6.5364444444444455e-06, 'epoch': 1.6492}
{'loss': 0.9133, 'grad_norm': 2.81221011575235, 'learning_rate': 6.535333333333333e-06, 'epoch': 1.6496}
{'loss': 0.906, 'grad_norm': 2.918810987141692, 'learning_rate': 6.534222222222223e-06, 'epoch': 1.65}
{'loss': 0.9128, 'grad_norm': 3.1712371015605916, 'learning_rate': 6.533111111111112e-06, 'epoch': 1.6503999999999999}
{'loss': 0.9187, 'grad_norm': 3.0077228978670134, 'learning_rate': 6.532e-06, 'epoch': 1.6508}
{'loss': 0.9109, 'grad_norm': 2.8378711445107636, 'learning_rate': 6.53088888888889e-06, 'epoch': 1.6512}
{'loss': 0.9165, 'grad_norm': 3.1113922342423335, 'learning_rate': 6.529777777777779e-06, 'epoch': 1.6516}
{'loss': 0.9185, 'grad_norm': 3.2195901422406807, 'learning_rate': 6.528666666666667e-06, 'epoch': 1.6520000000000001}
{'eval_valid_loss': 0.8857421875, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1106.542, 'eval_valid_steps_per_second': 276.636, 'epoch': 1.6520000000000001}
{'loss': 0.9034, 'grad_norm': 2.955819157422207, 'learning_rate': 6.527555555555556e-06, 'epoch': 1.6524}
{'loss': 0.9119, 'grad_norm': 3.0152055418986095, 'learning_rate': 6.5264444444444456e-06, 'epoch': 1.6528}
{'loss': 0.9204, 'grad_norm': 3.171993638624545, 'learning_rate': 6.5253333333333334e-06, 'epoch': 1.6532}
{'loss': 0.9156, 'grad_norm': 2.914463363361564, 'learning_rate': 6.524222222222223e-06, 'epoch': 1.6536}
{'loss': 0.922, 'grad_norm': 2.878558610193217, 'learning_rate': 6.523111111111111e-06, 'epoch': 1.654}
{'loss': 0.9062, 'grad_norm': 2.971084790914681, 'learning_rate': 6.522e-06, 'epoch': 1.6543999999999999}
{'loss': 0.9238, 'grad_norm': 3.229674078955445, 'learning_rate': 6.52088888888889e-06, 'epoch': 1.6548}
{'loss': 0.9188, 'grad_norm': 2.972023303787056, 'learning_rate': 6.519777777777778e-06, 'epoch': 1.6552}
{'loss': 0.9162, 'grad_norm': 2.77812312654962, 'learning_rate': 6.518666666666667e-06, 'epoch': 1.6556}
{'loss': 0.9135, 'grad_norm': 2.910739968150622, 'learning_rate': 6.517555555555556e-06, 'epoch': 1.6560000000000001}
{'eval_valid_loss': 0.88330078125, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1111.837, 'eval_valid_steps_per_second': 277.959, 'epoch': 1.6560000000000001}
{'loss': 0.92, 'grad_norm': 3.0083153143384496, 'learning_rate': 6.516444444444446e-06, 'epoch': 1.6564}
{'loss': 0.9268, 'grad_norm': 2.9587057555306786, 'learning_rate': 6.5153333333333335e-06, 'epoch': 1.6568}
{'loss': 0.9131, 'grad_norm': 2.9056806621935065, 'learning_rate': 6.514222222222223e-06, 'epoch': 1.6572}
{'loss': 0.9197, 'grad_norm': 3.3150745858997546, 'learning_rate': 6.513111111111111e-06, 'epoch': 1.6576}
{'loss': 0.9151, 'grad_norm': 3.1023386889656344, 'learning_rate': 6.5120000000000005e-06, 'epoch': 1.658}
{'loss': 0.9298, 'grad_norm': 2.676075221710468, 'learning_rate': 6.51088888888889e-06, 'epoch': 1.6583999999999999}
{'loss': 0.9089, 'grad_norm': 2.8305305135180934, 'learning_rate': 6.509777777777778e-06, 'epoch': 1.6588}
{'loss': 0.9188, 'grad_norm': 3.0978587061563534, 'learning_rate': 6.5086666666666675e-06, 'epoch': 1.6592}
{'loss': 0.9142, 'grad_norm': 3.0671119509626656, 'learning_rate': 6.507555555555556e-06, 'epoch': 1.6596}
{'loss': 0.9081, 'grad_norm': 2.9725989036693043, 'learning_rate': 6.506444444444445e-06, 'epoch': 1.6600000000000001}
{'eval_valid_loss': 0.8828125, 'eval_valid_runtime': 0.0903, 'eval_valid_samples_per_second': 1107.916, 'eval_valid_steps_per_second': 276.979, 'epoch': 1.6600000000000001}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9035, 'grad_norm': 2.8727839678315665, 'learning_rate': 6.505333333333334e-06, 'epoch': 1.6604}
{'loss': 0.9071, 'grad_norm': 3.058244161492522, 'learning_rate': 6.504222222222223e-06, 'epoch': 1.6608}
{'loss': 0.9049, 'grad_norm': 3.317472082177263, 'learning_rate': 6.503111111111111e-06, 'epoch': 1.6612}
{'loss': 0.9118, 'grad_norm': 3.094968825415908, 'learning_rate': 6.502000000000001e-06, 'epoch': 1.6616}
{'loss': 0.9216, 'grad_norm': 2.841681220463897, 'learning_rate': 6.50088888888889e-06, 'epoch': 1.662}
{'loss': 0.918, 'grad_norm': 2.9884828985655782, 'learning_rate': 6.499777777777778e-06, 'epoch': 1.6623999999999999}
{'loss': 0.9164, 'grad_norm': 3.168050311034653, 'learning_rate': 6.498666666666668e-06, 'epoch': 1.6627999999999998}
{'loss': 0.9106, 'grad_norm': 3.031905526137467, 'learning_rate': 6.4975555555555554e-06, 'epoch': 1.6632}
{'loss': 0.9141, 'grad_norm': 2.689764820617185, 'learning_rate': 6.496444444444445e-06, 'epoch': 1.6636}
{'loss': 0.9061, 'grad_norm': 3.1113566213525288, 'learning_rate': 6.495333333333334e-06, 'epoch': 1.6640000000000001}
{'eval_valid_loss': 0.88330078125, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.962, 'eval_valid_steps_per_second': 279.991, 'epoch': 1.6640000000000001}
{'loss': 0.9061, 'grad_norm': 2.6143923052626103, 'learning_rate': 6.494222222222223e-06, 'epoch': 1.6644}
{'loss': 0.9093, 'grad_norm': 2.768522813401588, 'learning_rate': 6.493111111111111e-06, 'epoch': 1.6648}
{'loss': 0.9171, 'grad_norm': 3.0375790836886756, 'learning_rate': 6.492000000000001e-06, 'epoch': 1.6652}
{'loss': 0.9237, 'grad_norm': 3.4275601334586883, 'learning_rate': 6.49088888888889e-06, 'epoch': 1.6656}
{'loss': 0.9202, 'grad_norm': 2.872674561318472, 'learning_rate': 6.489777777777778e-06, 'epoch': 1.666}
{'loss': 0.9083, 'grad_norm': 2.9625724868115175, 'learning_rate': 6.488666666666668e-06, 'epoch': 1.6663999999999999}
{'loss': 0.9131, 'grad_norm': 2.8608049871401766, 'learning_rate': 6.4875555555555555e-06, 'epoch': 1.6667999999999998}
{'loss': 0.9162, 'grad_norm': 3.0982153581560086, 'learning_rate': 6.486444444444445e-06, 'epoch': 1.6672}
{'loss': 0.9256, 'grad_norm': 3.088168098582852, 'learning_rate': 6.485333333333334e-06, 'epoch': 1.6676}
{'loss': 0.9197, 'grad_norm': 2.9769166850828173, 'learning_rate': 6.4842222222222225e-06, 'epoch': 1.6680000000000001}
{'eval_valid_loss': 0.880859375, 'eval_valid_runtime': 0.0885, 'eval_valid_samples_per_second': 1130.485, 'eval_valid_steps_per_second': 282.621, 'epoch': 1.6680000000000001}
{'loss': 0.9052, 'grad_norm': 3.0308092426097444, 'learning_rate': 6.483111111111111e-06, 'epoch': 1.6684}
{'loss': 0.9192, 'grad_norm': 3.1994061816857653, 'learning_rate': 6.482000000000001e-06, 'epoch': 1.6688}
{'loss': 0.903, 'grad_norm': 3.070087958514105, 'learning_rate': 6.4808888888888895e-06, 'epoch': 1.6692}
{'loss': 0.9069, 'grad_norm': 2.8210039955382666, 'learning_rate': 6.479777777777778e-06, 'epoch': 1.6696}
{'loss': 0.9061, 'grad_norm': 3.1208498481106406, 'learning_rate': 6.478666666666668e-06, 'epoch': 1.67}
{'loss': 0.9126, 'grad_norm': 3.0062954433281894, 'learning_rate': 6.477555555555556e-06, 'epoch': 1.6703999999999999}
{'loss': 0.9097, 'grad_norm': 3.0293892920715533, 'learning_rate': 6.476444444444445e-06, 'epoch': 1.6707999999999998}
{'loss': 0.9154, 'grad_norm': 2.738458755551715, 'learning_rate': 6.475333333333333e-06, 'epoch': 1.6712}
{'loss': 0.9188, 'grad_norm': 3.0731015920222733, 'learning_rate': 6.474222222222223e-06, 'epoch': 1.6716}
{'loss': 0.9159, 'grad_norm': 3.0170115509962687, 'learning_rate': 6.473111111111111e-06, 'epoch': 1.6720000000000002}
{'eval_valid_loss': 0.88232421875, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1122.838, 'eval_valid_steps_per_second': 280.709, 'epoch': 1.6720000000000002}
{'loss': 0.9144, 'grad_norm': 2.825830962334683, 'learning_rate': 6.472000000000001e-06, 'epoch': 1.6724}
{'loss': 0.9045, 'grad_norm': 2.6089388717016475, 'learning_rate': 6.47088888888889e-06, 'epoch': 1.6728}
{'loss': 0.9096, 'grad_norm': 3.1469719270478937, 'learning_rate': 6.469777777777778e-06, 'epoch': 1.6732}
{'loss': 0.9058, 'grad_norm': 2.9425748046467355, 'learning_rate': 6.468666666666668e-06, 'epoch': 1.6736}
{'loss': 0.9122, 'grad_norm': 2.832542773123299, 'learning_rate': 6.467555555555556e-06, 'epoch': 1.674}
{'loss': 0.925, 'grad_norm': 3.033060909358941, 'learning_rate': 6.466444444444445e-06, 'epoch': 1.6743999999999999}
{'loss': 0.9028, 'grad_norm': 2.949891373687217, 'learning_rate': 6.465333333333333e-06, 'epoch': 1.6747999999999998}
{'loss': 0.9118, 'grad_norm': 2.952472352262633, 'learning_rate': 6.464222222222223e-06, 'epoch': 1.6752}
{'loss': 0.9194, 'grad_norm': 2.9345169849596426, 'learning_rate': 6.463111111111111e-06, 'epoch': 1.6756}
{'loss': 0.911, 'grad_norm': 2.778557620782968, 'learning_rate': 6.462e-06, 'epoch': 1.6760000000000002}
{'eval_valid_loss': 0.8818359375, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1126.508, 'eval_valid_steps_per_second': 281.627, 'epoch': 1.6760000000000002}
{'loss': 0.9013, 'grad_norm': 3.009462325194787, 'learning_rate': 6.46088888888889e-06, 'epoch': 1.6764000000000001}
{'loss': 0.9159, 'grad_norm': 2.8878943974870057, 'learning_rate': 6.459777777777778e-06, 'epoch': 1.6768}
{'loss': 0.9056, 'grad_norm': 3.06198837911318, 'learning_rate': 6.458666666666667e-06, 'epoch': 1.6772}
{'loss': 0.9101, 'grad_norm': 2.9490569215547757, 'learning_rate': 6.457555555555556e-06, 'epoch': 1.6776}
{'loss': 0.9123, 'grad_norm': 3.208334673018403, 'learning_rate': 6.456444444444445e-06, 'epoch': 1.678}
{'loss': 0.9076, 'grad_norm': 2.9068280024677473, 'learning_rate': 6.455333333333333e-06, 'epoch': 1.6784}
{'loss': 0.8956, 'grad_norm': 3.266493868798855, 'learning_rate': 6.454222222222223e-06, 'epoch': 1.6787999999999998}
{'loss': 0.9213, 'grad_norm': 3.025189022983946, 'learning_rate': 6.4531111111111115e-06, 'epoch': 1.6792}
{'loss': 0.9174, 'grad_norm': 2.9718346882842837, 'learning_rate': 6.452e-06, 'epoch': 1.6796}
{'loss': 0.9065, 'grad_norm': 3.2866293726130924, 'learning_rate': 6.45088888888889e-06, 'epoch': 1.6800000000000002}
{'eval_valid_loss': 0.880859375, 'eval_valid_runtime': 0.0884, 'eval_valid_samples_per_second': 1130.976, 'eval_valid_steps_per_second': 282.744, 'epoch': 1.6800000000000002}
{'loss': 0.9186, 'grad_norm': 2.9635901734863555, 'learning_rate': 6.4497777777777785e-06, 'epoch': 1.6804000000000001}
{'loss': 0.9209, 'grad_norm': 2.9066563291449703, 'learning_rate': 6.448666666666667e-06, 'epoch': 1.6808}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9187, 'grad_norm': 3.264753508296232, 'learning_rate': 6.447555555555556e-06, 'epoch': 1.6812}
{'loss': 0.9114, 'grad_norm': 2.856376853187711, 'learning_rate': 6.4464444444444455e-06, 'epoch': 1.6816}
{'loss': 0.9218, 'grad_norm': 2.709517698039041, 'learning_rate': 6.445333333333333e-06, 'epoch': 1.682}
{'loss': 0.9137, 'grad_norm': 2.7908543192417197, 'learning_rate': 6.444333333333334e-06, 'epoch': 1.6824}
{'loss': 0.9172, 'grad_norm': 3.2434085155367747, 'learning_rate': 6.443222222222223e-06, 'epoch': 1.6827999999999999}
{'loss': 0.9233, 'grad_norm': 3.1989835901897807, 'learning_rate': 6.442111111111111e-06, 'epoch': 1.6832}
{'loss': 0.919, 'grad_norm': 3.1500400411997633, 'learning_rate': 6.441000000000001e-06, 'epoch': 1.6836}
{'loss': 0.9164, 'grad_norm': 3.106748151842935, 'learning_rate': 6.43988888888889e-06, 'epoch': 1.6840000000000002}
{'eval_valid_loss': 0.8828125, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.2, 'eval_valid_steps_per_second': 279.8, 'epoch': 1.6840000000000002}
{'loss': 0.9126, 'grad_norm': 2.970800725473485, 'learning_rate': 6.438777777777778e-06, 'epoch': 1.6844000000000001}
{'loss': 0.9116, 'grad_norm': 2.8625358350251497, 'learning_rate': 6.437666666666668e-06, 'epoch': 1.6848}
{'loss': 0.9171, 'grad_norm': 3.231798352727087, 'learning_rate': 6.4365555555555555e-06, 'epoch': 1.6852}
{'loss': 0.9241, 'grad_norm': 2.9582934791047424, 'learning_rate': 6.435444444444445e-06, 'epoch': 1.6856}
{'loss': 0.9208, 'grad_norm': 2.898049588220799, 'learning_rate': 6.434333333333334e-06, 'epoch': 1.686}
{'loss': 0.908, 'grad_norm': 3.700561489210297, 'learning_rate': 6.433222222222223e-06, 'epoch': 1.6864}
{'loss': 0.9241, 'grad_norm': 3.241354354990956, 'learning_rate': 6.432111111111111e-06, 'epoch': 1.6867999999999999}
{'loss': 0.9083, 'grad_norm': 3.2106717224761168, 'learning_rate': 6.431000000000001e-06, 'epoch': 1.6872}
{'loss': 0.9069, 'grad_norm': 2.975437081710367, 'learning_rate': 6.42988888888889e-06, 'epoch': 1.6876}
{'loss': 0.9132, 'grad_norm': 2.728165955689447, 'learning_rate': 6.428777777777778e-06, 'epoch': 1.688}
{'eval_valid_loss': 0.8828125, 'eval_valid_runtime': 0.0956, 'eval_valid_samples_per_second': 1045.758, 'eval_valid_steps_per_second': 261.439, 'epoch': 1.688}
{'loss': 0.9271, 'grad_norm': 3.1261442664415795, 'learning_rate': 6.427666666666668e-06, 'epoch': 1.6884000000000001}
{'loss': 0.9074, 'grad_norm': 2.9686077485386195, 'learning_rate': 6.426555555555556e-06, 'epoch': 1.6888}
{'loss': 0.9186, 'grad_norm': 2.809596279370358, 'learning_rate': 6.425444444444445e-06, 'epoch': 1.6892}
{'loss': 0.9274, 'grad_norm': 3.147051428022812, 'learning_rate': 6.424333333333334e-06, 'epoch': 1.6896}
{'loss': 0.9222, 'grad_norm': 3.2385929634070094, 'learning_rate': 6.423222222222223e-06, 'epoch': 1.69}
{'loss': 0.9135, 'grad_norm': 3.0638227914655345, 'learning_rate': 6.422111111111111e-06, 'epoch': 1.6904}
{'loss': 0.9273, 'grad_norm': 2.9004052775106, 'learning_rate': 6.421000000000001e-06, 'epoch': 1.6907999999999999}
{'loss': 0.9153, 'grad_norm': 2.8964849923481095, 'learning_rate': 6.4198888888888896e-06, 'epoch': 1.6912}
{'loss': 0.9094, 'grad_norm': 3.072669059131188, 'learning_rate': 6.418777777777778e-06, 'epoch': 1.6916}
{'loss': 0.9183, 'grad_norm': 2.9161623734522175, 'learning_rate': 6.417666666666668e-06, 'epoch': 1.692}
{'eval_valid_loss': 0.8828125, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1121.964, 'eval_valid_steps_per_second': 280.491, 'epoch': 1.692}
{'loss': 0.9252, 'grad_norm': 2.8451423694708384, 'learning_rate': 6.416555555555556e-06, 'epoch': 1.6924000000000001}
{'loss': 0.9077, 'grad_norm': 3.031293318134172, 'learning_rate': 6.415444444444445e-06, 'epoch': 1.6928}
{'loss': 0.922, 'grad_norm': 2.785773878914204, 'learning_rate': 6.414333333333333e-06, 'epoch': 1.6932}
{'loss': 0.9173, 'grad_norm': 2.8882962099306475, 'learning_rate': 6.413222222222223e-06, 'epoch': 1.6936}
{'loss': 0.9109, 'grad_norm': 3.057335605771329, 'learning_rate': 6.412111111111111e-06, 'epoch': 1.694}
{'loss': 0.9098, 'grad_norm': 3.0572956492426773, 'learning_rate': 6.411000000000001e-06, 'epoch': 1.6944}
{'loss': 0.9208, 'grad_norm': 2.84265712027001, 'learning_rate': 6.40988888888889e-06, 'epoch': 1.6947999999999999}
{'loss': 0.9162, 'grad_norm': 3.038519558299212, 'learning_rate': 6.408777777777778e-06, 'epoch': 1.6952}
{'loss': 0.9189, 'grad_norm': 3.2593807132630643, 'learning_rate': 6.407666666666668e-06, 'epoch': 1.6956}
{'loss': 0.9189, 'grad_norm': 2.950706712883764, 'learning_rate': 6.406555555555556e-06, 'epoch': 1.696}
{'eval_valid_loss': 0.8818359375, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1126.302, 'eval_valid_steps_per_second': 281.576, 'epoch': 1.696}
{'loss': 0.9163, 'grad_norm': 2.8822638550411375, 'learning_rate': 6.405444444444445e-06, 'epoch': 1.6964000000000001}
{'loss': 0.9066, 'grad_norm': 2.920561673804163, 'learning_rate': 6.404333333333333e-06, 'epoch': 1.6968}
{'loss': 0.9053, 'grad_norm': 2.904652258878257, 'learning_rate': 6.403222222222223e-06, 'epoch': 1.6972}
{'loss': 0.9276, 'grad_norm': 3.1052460884546864, 'learning_rate': 6.4021111111111115e-06, 'epoch': 1.6976}
{'loss': 0.9013, 'grad_norm': 2.8516360913539502, 'learning_rate': 6.401e-06, 'epoch': 1.698}
{'loss': 0.9184, 'grad_norm': 3.2812236239871813, 'learning_rate': 6.39988888888889e-06, 'epoch': 1.6984}
{'loss': 0.9019, 'grad_norm': 2.7826432906491703, 'learning_rate': 6.3987777777777785e-06, 'epoch': 1.6987999999999999}
{'loss': 0.9129, 'grad_norm': 2.6197247803165156, 'learning_rate': 6.397666666666667e-06, 'epoch': 1.6992}
{'loss': 0.9067, 'grad_norm': 2.916493881875116, 'learning_rate': 6.396555555555556e-06, 'epoch': 1.6996}
{'loss': 0.9139, 'grad_norm': 3.1481128364314372, 'learning_rate': 6.3954444444444455e-06, 'epoch': 1.7}
{'eval_valid_loss': 0.88330078125, 'eval_valid_runtime': 0.0951, 'eval_valid_samples_per_second': 1051.678, 'eval_valid_steps_per_second': 262.92, 'epoch': 1.7}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9088, 'grad_norm': 2.841320016185393, 'learning_rate': 6.394333333333333e-06, 'epoch': 1.7004000000000001}
{'loss': 0.8981, 'grad_norm': 2.749899477638662, 'learning_rate': 6.393222222222223e-06, 'epoch': 1.7008}
{'loss': 0.9056, 'grad_norm': 2.8892929410246233, 'learning_rate': 6.392111111111111e-06, 'epoch': 1.7012}
{'loss': 0.9195, 'grad_norm': 2.8660009667920003, 'learning_rate': 6.391e-06, 'epoch': 1.7016}
{'loss': 0.9163, 'grad_norm': 3.3996047196898243, 'learning_rate': 6.38988888888889e-06, 'epoch': 1.702}
{'loss': 0.9124, 'grad_norm': 2.7858174998337133, 'learning_rate': 6.3887777777777786e-06, 'epoch': 1.7024}
{'loss': 0.9118, 'grad_norm': 3.0314458213556503, 'learning_rate': 6.387666666666667e-06, 'epoch': 1.7027999999999999}
{'loss': 0.9127, 'grad_norm': 3.0677032157626423, 'learning_rate': 6.386555555555556e-06, 'epoch': 1.7032}
{'loss': 0.8976, 'grad_norm': 2.7798439929604615, 'learning_rate': 6.3854444444444456e-06, 'epoch': 1.7036}
{'loss': 0.9021, 'grad_norm': 2.7409127579123465, 'learning_rate': 6.384333333333333e-06, 'epoch': 1.704}
{'eval_valid_loss': 0.88134765625, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1120.749, 'eval_valid_steps_per_second': 280.187, 'epoch': 1.704}
{'loss': 0.8991, 'grad_norm': 2.8544366017429343, 'learning_rate': 6.383222222222223e-06, 'epoch': 1.7044000000000001}
{'loss': 0.9182, 'grad_norm': 2.7565682173434936, 'learning_rate': 6.382111111111111e-06, 'epoch': 1.7048}
{'loss': 0.9097, 'grad_norm': 2.863252793153749, 'learning_rate': 6.381e-06, 'epoch': 1.7052}
{'loss': 0.907, 'grad_norm': 2.962093359164284, 'learning_rate': 6.37988888888889e-06, 'epoch': 1.7056}
{'loss': 0.9119, 'grad_norm': 3.289466371001095, 'learning_rate': 6.378777777777778e-06, 'epoch': 1.706}
{'loss': 0.9044, 'grad_norm': 2.7344537451168383, 'learning_rate': 6.377666666666667e-06, 'epoch': 1.7064}
{'loss': 0.9123, 'grad_norm': 2.9100651669169593, 'learning_rate': 6.376555555555556e-06, 'epoch': 1.7067999999999999}
{'loss': 0.916, 'grad_norm': 3.3072018718794696, 'learning_rate': 6.375444444444445e-06, 'epoch': 1.7072}
{'loss': 0.9121, 'grad_norm': 3.2460223670942154, 'learning_rate': 6.3743333333333335e-06, 'epoch': 1.7076}
{'loss': 0.9229, 'grad_norm': 3.224940254707796, 'learning_rate': 6.373222222222223e-06, 'epoch': 1.708}
{'eval_valid_loss': 0.88037109375, 'eval_valid_runtime': 0.0908, 'eval_valid_samples_per_second': 1101.231, 'eval_valid_steps_per_second': 275.308, 'epoch': 1.708}
{'loss': 0.915, 'grad_norm': 3.28761941236025, 'learning_rate': 6.372111111111111e-06, 'epoch': 1.7084000000000001}
{'loss': 0.9045, 'grad_norm': 3.124856421033278, 'learning_rate': 6.3710000000000005e-06, 'epoch': 1.7088}
{'loss': 0.9207, 'grad_norm': 2.959829722094696, 'learning_rate': 6.36988888888889e-06, 'epoch': 1.7092}
{'loss': 0.9122, 'grad_norm': 2.8780352906733557, 'learning_rate': 6.368777777777778e-06, 'epoch': 1.7096}
{'loss': 0.9078, 'grad_norm': 3.003566995618752, 'learning_rate': 6.3676666666666675e-06, 'epoch': 1.71}
{'loss': 0.9197, 'grad_norm': 3.0748192129256666, 'learning_rate': 6.366555555555556e-06, 'epoch': 1.7104}
{'loss': 0.9102, 'grad_norm': 3.18597580426937, 'learning_rate': 6.365444444444445e-06, 'epoch': 1.7107999999999999}
{'loss': 0.9132, 'grad_norm': 3.2268006557845785, 'learning_rate': 6.364333333333334e-06, 'epoch': 1.7112}
{'loss': 0.9181, 'grad_norm': 3.1260737863119505, 'learning_rate': 6.363222222222223e-06, 'epoch': 1.7116}
{'loss': 0.9097, 'grad_norm': 3.0869740640516454, 'learning_rate': 6.362111111111111e-06, 'epoch': 1.712}
{'eval_valid_loss': 0.880859375, 'eval_valid_runtime': 0.0905, 'eval_valid_samples_per_second': 1104.965, 'eval_valid_steps_per_second': 276.241, 'epoch': 1.712}
{'loss': 0.909, 'grad_norm': 2.763590355199887, 'learning_rate': 6.361000000000001e-06, 'epoch': 1.7124000000000001}
{'loss': 0.9135, 'grad_norm': 3.393231256015434, 'learning_rate': 6.35988888888889e-06, 'epoch': 1.7128}
{'loss': 0.9061, 'grad_norm': 3.0084079697949595, 'learning_rate': 6.358777777777778e-06, 'epoch': 1.7132}
{'loss': 0.9074, 'grad_norm': 3.0355728334841907, 'learning_rate': 6.3576666666666676e-06, 'epoch': 1.7136}
{'loss': 0.9083, 'grad_norm': 2.8745023566239856, 'learning_rate': 6.3565555555555554e-06, 'epoch': 1.714}
{'loss': 0.9085, 'grad_norm': 2.803059320520281, 'learning_rate': 6.355444444444445e-06, 'epoch': 1.7144}
{'loss': 0.9108, 'grad_norm': 2.93062374159708, 'learning_rate': 6.354333333333334e-06, 'epoch': 1.7147999999999999}
{'loss': 0.8972, 'grad_norm': 2.7693077180128047, 'learning_rate': 6.353222222222223e-06, 'epoch': 1.7151999999999998}
{'loss': 0.9124, 'grad_norm': 2.659757094048119, 'learning_rate': 6.352111111111111e-06, 'epoch': 1.7156}
{'loss': 0.8963, 'grad_norm': 2.9548395518168755, 'learning_rate': 6.351000000000001e-06, 'epoch': 1.716}
{'eval_valid_loss': 0.88037109375, 'eval_valid_runtime': 0.0903, 'eval_valid_samples_per_second': 1107.641, 'eval_valid_steps_per_second': 276.91, 'epoch': 1.716}
{'loss': 0.9158, 'grad_norm': 2.8626560918236286, 'learning_rate': 6.34988888888889e-06, 'epoch': 1.7164000000000001}
{'loss': 0.9014, 'grad_norm': 2.6957763272782604, 'learning_rate': 6.348777777777778e-06, 'epoch': 1.7168}
{'loss': 0.9054, 'grad_norm': 3.2648696938547577, 'learning_rate': 6.347666666666668e-06, 'epoch': 1.7172}
{'loss': 0.9108, 'grad_norm': 3.0855536210039136, 'learning_rate': 6.3465555555555555e-06, 'epoch': 1.7176}
{'loss': 0.9172, 'grad_norm': 2.9182803333614156, 'learning_rate': 6.345444444444445e-06, 'epoch': 1.718}
{'loss': 0.9074, 'grad_norm': 2.870156219414306, 'learning_rate': 6.344333333333334e-06, 'epoch': 1.7184}
{'loss': 0.9113, 'grad_norm': 2.780491425437033, 'learning_rate': 6.3432222222222225e-06, 'epoch': 1.7187999999999999}
{'loss': 0.9164, 'grad_norm': 3.285160794975049, 'learning_rate': 6.342111111111111e-06, 'epoch': 1.7191999999999998}
{'loss': 0.9099, 'grad_norm': 3.029908788035214, 'learning_rate': 6.341000000000001e-06, 'epoch': 1.7196}
{'loss': 0.9225, 'grad_norm': 2.9543417285993017, 'learning_rate': 6.3398888888888895e-06, 'epoch': 1.72}
{'eval_valid_loss': 0.87939453125, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.643, 'eval_valid_steps_per_second': 278.911, 'epoch': 1.72}
{'loss': 0.9106, 'grad_norm': 3.205497505045997, 'learning_rate': 6.338777777777778e-06, 'epoch': 1.7204000000000002}
{'loss': 0.9153, 'grad_norm': 3.0900240916637, 'learning_rate': 6.337666666666668e-06, 'epoch': 1.7208}
{'loss': 0.8914, 'grad_norm': 2.8852497745729004, 'learning_rate': 6.336555555555556e-06, 'epoch': 1.7212}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8944, 'grad_norm': 3.337061181312629, 'learning_rate': 6.335444444444445e-06, 'epoch': 1.7216}
{'loss': 0.9169, 'grad_norm': 2.9515921030332066, 'learning_rate': 6.334333333333333e-06, 'epoch': 1.722}
{'loss': 0.923, 'grad_norm': 2.857187455757355, 'learning_rate': 6.333333333333333e-06, 'epoch': 1.7224}
{'loss': 0.9224, 'grad_norm': 3.0270429400229775, 'learning_rate': 6.332222222222223e-06, 'epoch': 1.7227999999999999}
{'loss': 0.9213, 'grad_norm': 3.391602537867315, 'learning_rate': 6.331111111111111e-06, 'epoch': 1.7231999999999998}
{'loss': 0.922, 'grad_norm': 3.083070490347692, 'learning_rate': 6.33e-06, 'epoch': 1.7236}
{'loss': 0.9167, 'grad_norm': 3.3313790562950567, 'learning_rate': 6.328888888888889e-06, 'epoch': 1.724}
{'eval_valid_loss': 0.88232421875, 'eval_valid_runtime': 0.0949, 'eval_valid_samples_per_second': 1053.242, 'eval_valid_steps_per_second': 263.31, 'epoch': 1.724}
{'loss': 0.9168, 'grad_norm': 3.204219556668763, 'learning_rate': 6.327777777777779e-06, 'epoch': 1.7244000000000002}
{'loss': 0.9195, 'grad_norm': 2.7069339046617777, 'learning_rate': 6.326666666666667e-06, 'epoch': 1.7248}
{'loss': 0.903, 'grad_norm': 3.103828976118384, 'learning_rate': 6.325555555555556e-06, 'epoch': 1.7252}
{'loss': 0.9029, 'grad_norm': 2.869030217382883, 'learning_rate': 6.324444444444446e-06, 'epoch': 1.7256}
{'loss': 0.9142, 'grad_norm': 2.9592962925242006, 'learning_rate': 6.3233333333333335e-06, 'epoch': 1.726}
{'loss': 0.9038, 'grad_norm': 2.8245577997799027, 'learning_rate': 6.322222222222223e-06, 'epoch': 1.7264}
{'loss': 0.9057, 'grad_norm': 2.865158693735175, 'learning_rate': 6.321111111111111e-06, 'epoch': 1.7268}
{'loss': 0.9159, 'grad_norm': 3.058656382497988, 'learning_rate': 6.3200000000000005e-06, 'epoch': 1.7271999999999998}
{'loss': 0.9162, 'grad_norm': 2.87331842919428, 'learning_rate': 6.31888888888889e-06, 'epoch': 1.7276}
{'loss': 0.9086, 'grad_norm': 2.876191700788413, 'learning_rate': 6.317777777777778e-06, 'epoch': 1.728}
{'eval_valid_loss': 0.8798828125, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.445, 'eval_valid_steps_per_second': 278.361, 'epoch': 1.728}
{'loss': 0.9186, 'grad_norm': 3.2978150442246084, 'learning_rate': 6.3166666666666675e-06, 'epoch': 1.7284000000000002}
{'loss': 0.8992, 'grad_norm': 2.75461571394869, 'learning_rate': 6.315555555555556e-06, 'epoch': 1.7288000000000001}
{'loss': 0.9043, 'grad_norm': 3.2053889949656558, 'learning_rate': 6.314444444444445e-06, 'epoch': 1.7292}
{'loss': 0.9045, 'grad_norm': 2.7541858017170644, 'learning_rate': 6.313333333333334e-06, 'epoch': 1.7296}
{'loss': 0.9146, 'grad_norm': 3.549048677770058, 'learning_rate': 6.312222222222223e-06, 'epoch': 1.73}
{'loss': 0.9142, 'grad_norm': 2.939028879771381, 'learning_rate': 6.311111111111111e-06, 'epoch': 1.7304}
{'loss': 0.9214, 'grad_norm': 2.8964137803758865, 'learning_rate': 6.3100000000000006e-06, 'epoch': 1.7308}
{'loss': 0.9063, 'grad_norm': 3.330061532703578, 'learning_rate': 6.30888888888889e-06, 'epoch': 1.7311999999999999}
{'loss': 0.9123, 'grad_norm': 2.9929910183493624, 'learning_rate': 6.307777777777778e-06, 'epoch': 1.7316}
{'loss': 0.9133, 'grad_norm': 3.0837539577110817, 'learning_rate': 6.3066666666666676e-06, 'epoch': 1.732}
{'eval_valid_loss': 0.88134765625, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.484, 'eval_valid_steps_per_second': 278.621, 'epoch': 1.732}
{'loss': 0.9082, 'grad_norm': 2.7948675037056416, 'learning_rate': 6.305555555555556e-06, 'epoch': 1.7324000000000002}
{'loss': 0.9062, 'grad_norm': 2.896316132446943, 'learning_rate': 6.304444444444445e-06, 'epoch': 1.7328000000000001}
{'loss': 0.9097, 'grad_norm': 2.8632459963596752, 'learning_rate': 6.303333333333334e-06, 'epoch': 1.7332}
{'loss': 0.9195, 'grad_norm': 2.95378277027938, 'learning_rate': 6.302222222222223e-06, 'epoch': 1.7336}
{'loss': 0.9153, 'grad_norm': 2.838058356456323, 'learning_rate': 6.301111111111111e-06, 'epoch': 1.734}
{'loss': 0.9116, 'grad_norm': 3.128432677359289, 'learning_rate': 6.300000000000001e-06, 'epoch': 1.7344}
{'loss': 0.906, 'grad_norm': 2.950938025549943, 'learning_rate': 6.29888888888889e-06, 'epoch': 1.7348}
{'loss': 0.9171, 'grad_norm': 3.1395461473665414, 'learning_rate': 6.297777777777778e-06, 'epoch': 1.7351999999999999}
{'loss': 0.9093, 'grad_norm': 2.9627446017307477, 'learning_rate': 6.296666666666668e-06, 'epoch': 1.7356}
{'loss': 0.9061, 'grad_norm': 2.7807038928308367, 'learning_rate': 6.2955555555555555e-06, 'epoch': 1.736}
{'eval_valid_loss': 0.88134765625, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.766, 'eval_valid_steps_per_second': 279.441, 'epoch': 1.736}
{'loss': 0.9173, 'grad_norm': 3.264970522188132, 'learning_rate': 6.294444444444445e-06, 'epoch': 1.7364000000000002}
{'loss': 0.9123, 'grad_norm': 3.0989143347087746, 'learning_rate': 6.293333333333334e-06, 'epoch': 1.7368000000000001}
{'loss': 0.9177, 'grad_norm': 3.2081394487892045, 'learning_rate': 6.2922222222222225e-06, 'epoch': 1.7372}
{'loss': 0.9141, 'grad_norm': 3.4518235028051882, 'learning_rate': 6.291111111111111e-06, 'epoch': 1.7376}
{'loss': 0.9019, 'grad_norm': 2.7676486569652936, 'learning_rate': 6.290000000000001e-06, 'epoch': 1.738}
{'loss': 0.9051, 'grad_norm': 3.1164263988808427, 'learning_rate': 6.28888888888889e-06, 'epoch': 1.7384}
{'loss': 0.9049, 'grad_norm': 2.8087841547700374, 'learning_rate': 6.287777777777778e-06, 'epoch': 1.7388}
{'loss': 0.9107, 'grad_norm': 3.0301950132921522, 'learning_rate': 6.286666666666668e-06, 'epoch': 1.7391999999999999}
{'loss': 0.9142, 'grad_norm': 3.1551044386975127, 'learning_rate': 6.285555555555556e-06, 'epoch': 1.7396}
{'loss': 0.9189, 'grad_norm': 3.1318682914849307, 'learning_rate': 6.284444444444445e-06, 'epoch': 1.74}
{'eval_valid_loss': 0.87939453125, 'eval_valid_runtime': 0.0882, 'eval_valid_samples_per_second': 1134.013, 'eval_valid_steps_per_second': 283.503, 'epoch': 1.74}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9124, 'grad_norm': 3.509190991571844, 'learning_rate': 6.283333333333334e-06, 'epoch': 1.7404}
{'loss': 0.8997, 'grad_norm': 3.0681511646242243, 'learning_rate': 6.282222222222223e-06, 'epoch': 1.7408000000000001}
{'loss': 0.8908, 'grad_norm': 2.687315380053815, 'learning_rate': 6.281111111111111e-06, 'epoch': 1.7412}
{'loss': 0.9208, 'grad_norm': 2.774170351586531, 'learning_rate': 6.280000000000001e-06, 'epoch': 1.7416}
{'loss': 0.9126, 'grad_norm': 2.8895127811541967, 'learning_rate': 6.2788888888888896e-06, 'epoch': 1.742}
{'loss': 0.9041, 'grad_norm': 2.9170950484171123, 'learning_rate': 6.277777777777778e-06, 'epoch': 1.7424}
{'loss': 0.9145, 'grad_norm': 3.279104266891805, 'learning_rate': 6.276666666666668e-06, 'epoch': 1.7428}
{'loss': 0.9184, 'grad_norm': 3.053898546038864, 'learning_rate': 6.275555555555556e-06, 'epoch': 1.7431999999999999}
{'loss': 0.9124, 'grad_norm': 2.749807719531021, 'learning_rate': 6.274444444444445e-06, 'epoch': 1.7436}
{'loss': 0.9142, 'grad_norm': 3.3815337973359103, 'learning_rate': 6.273333333333333e-06, 'epoch': 1.744}
{'eval_valid_loss': 0.8798828125, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1105.688, 'eval_valid_steps_per_second': 276.422, 'epoch': 1.744}
{'loss': 0.9213, 'grad_norm': 2.862693351562228, 'learning_rate': 6.272222222222223e-06, 'epoch': 1.7444}
{'loss': 0.9175, 'grad_norm': 3.347912719588138, 'learning_rate': 6.271111111111111e-06, 'epoch': 1.7448000000000001}
{'loss': 0.908, 'grad_norm': 2.696223074752499, 'learning_rate': 6.27e-06, 'epoch': 1.7452}
{'loss': 0.9084, 'grad_norm': 2.6959924586548585, 'learning_rate': 6.26888888888889e-06, 'epoch': 1.7456}
{'loss': 0.9028, 'grad_norm': 3.092112020888852, 'learning_rate': 6.267777777777778e-06, 'epoch': 1.746}
{'loss': 0.9199, 'grad_norm': 2.885823060530656, 'learning_rate': 6.266666666666668e-06, 'epoch': 1.7464}
{'loss': 0.9178, 'grad_norm': 3.202040912187507, 'learning_rate': 6.265555555555556e-06, 'epoch': 1.7468}
{'loss': 0.9201, 'grad_norm': 2.9507539504982314, 'learning_rate': 6.264444444444445e-06, 'epoch': 1.7471999999999999}
{'loss': 0.9046, 'grad_norm': 3.1442790244219423, 'learning_rate': 6.263333333333333e-06, 'epoch': 1.7476}
{'loss': 0.9027, 'grad_norm': 3.484035885991738, 'learning_rate': 6.262222222222223e-06, 'epoch': 1.748}
{'eval_valid_loss': 0.87939453125, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.86, 'eval_valid_steps_per_second': 278.215, 'epoch': 1.748}
{'loss': 0.9203, 'grad_norm': 2.8665417871794623, 'learning_rate': 6.2611111111111115e-06, 'epoch': 1.7484}
{'loss': 0.9039, 'grad_norm': 2.816652813619673, 'learning_rate': 6.26e-06, 'epoch': 1.7488000000000001}
{'loss': 0.9082, 'grad_norm': 3.005524922206181, 'learning_rate': 6.25888888888889e-06, 'epoch': 1.7492}
{'loss': 0.9091, 'grad_norm': 2.888056531547335, 'learning_rate': 6.2577777777777785e-06, 'epoch': 1.7496}
{'loss': 0.9033, 'grad_norm': 2.8836841764407564, 'learning_rate': 6.256666666666667e-06, 'epoch': 1.75}
{'loss': 0.9082, 'grad_norm': 3.2556159300887426, 'learning_rate': 6.255555555555556e-06, 'epoch': 1.7504}
{'loss': 0.9189, 'grad_norm': 2.9018684295770525, 'learning_rate': 6.2544444444444455e-06, 'epoch': 1.7508}
{'loss': 0.9168, 'grad_norm': 3.180328304569736, 'learning_rate': 6.253333333333333e-06, 'epoch': 1.7511999999999999}
{'loss': 0.9041, 'grad_norm': 3.1636993482371594, 'learning_rate': 6.252222222222223e-06, 'epoch': 1.7516}
{'loss': 0.9151, 'grad_norm': 3.1234477002433865, 'learning_rate': 6.251111111111111e-06, 'epoch': 1.752}
{'eval_valid_loss': 0.8779296875, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1116.709, 'eval_valid_steps_per_second': 279.177, 'epoch': 1.752}
{'loss': 0.9123, 'grad_norm': 2.859082337290935, 'learning_rate': 6.25e-06, 'epoch': 1.7524}
{'loss': 0.9024, 'grad_norm': 2.849799889097788, 'learning_rate': 6.24888888888889e-06, 'epoch': 1.7528000000000001}
{'loss': 0.9009, 'grad_norm': 2.8064975582603546, 'learning_rate': 6.2477777777777786e-06, 'epoch': 1.7532}
{'loss': 0.9045, 'grad_norm': 3.079493416951497, 'learning_rate': 6.246666666666667e-06, 'epoch': 1.7536}
{'loss': 0.9082, 'grad_norm': 3.0182876789695774, 'learning_rate': 6.245555555555556e-06, 'epoch': 1.754}
{'loss': 0.9067, 'grad_norm': 2.982612628744685, 'learning_rate': 6.2444444444444456e-06, 'epoch': 1.7544}
{'loss': 0.9295, 'grad_norm': 3.077879329501583, 'learning_rate': 6.243333333333333e-06, 'epoch': 1.7548}
{'loss': 0.9052, 'grad_norm': 2.738373818785101, 'learning_rate': 6.242222222222223e-06, 'epoch': 1.7551999999999999}
{'loss': 0.8945, 'grad_norm': 2.924316844471994, 'learning_rate': 6.241111111111111e-06, 'epoch': 1.7556}
{'loss': 0.9134, 'grad_norm': 3.2444025585023377, 'learning_rate': 6.24e-06, 'epoch': 1.756}
{'eval_valid_loss': 0.8828125, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1128.879, 'eval_valid_steps_per_second': 282.22, 'epoch': 1.756}
{'loss': 0.9066, 'grad_norm': 2.4219094058638513, 'learning_rate': 6.23888888888889e-06, 'epoch': 1.7564}
{'loss': 0.9126, 'grad_norm': 3.405352964312237, 'learning_rate': 6.237777777777778e-06, 'epoch': 1.7568000000000001}
{'loss': 0.9178, 'grad_norm': 3.2413116834797657, 'learning_rate': 6.236666666666667e-06, 'epoch': 1.7572}
{'loss': 0.9068, 'grad_norm': 2.905330974105293, 'learning_rate': 6.235555555555556e-06, 'epoch': 1.7576}
{'loss': 0.9043, 'grad_norm': 3.2110381006227517, 'learning_rate': 6.234444444444445e-06, 'epoch': 1.758}
{'loss': 0.9194, 'grad_norm': 3.0832953719216487, 'learning_rate': 6.2333333333333335e-06, 'epoch': 1.7584}
{'loss': 0.9068, 'grad_norm': 3.1039769842238374, 'learning_rate': 6.232222222222223e-06, 'epoch': 1.7588}
{'loss': 0.8992, 'grad_norm': 2.9973458927522523, 'learning_rate': 6.231111111111111e-06, 'epoch': 1.7591999999999999}
{'loss': 0.9027, 'grad_norm': 3.0870440080729216, 'learning_rate': 6.2300000000000005e-06, 'epoch': 1.7596}
{'loss': 0.9183, 'grad_norm': 2.954247891924002, 'learning_rate': 6.22888888888889e-06, 'epoch': 1.76}
{'eval_valid_loss': 0.87939453125, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.639, 'eval_valid_steps_per_second': 278.16, 'epoch': 1.76}
{'loss': 0.9233, 'grad_norm': 3.005291216460462, 'learning_rate': 6.227777777777778e-06, 'epoch': 1.7604}
{'loss': 0.9035, 'grad_norm': 2.963724198890722, 'learning_rate': 6.2266666666666675e-06, 'epoch': 1.7608000000000001}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9037, 'grad_norm': 2.8896947652668303, 'learning_rate': 6.225555555555556e-06, 'epoch': 1.7612}
{'loss': 0.905, 'grad_norm': 2.991053951675989, 'learning_rate': 6.224444444444445e-06, 'epoch': 1.7616}
{'loss': 0.9067, 'grad_norm': 3.003277240272395, 'learning_rate': 6.223333333333334e-06, 'epoch': 1.762}
{'loss': 0.9058, 'grad_norm': 2.591977277742606, 'learning_rate': 6.222222222222223e-06, 'epoch': 1.7624}
{'loss': 0.9132, 'grad_norm': 2.9982074607561118, 'learning_rate': 6.221222222222223e-06, 'epoch': 1.7628}
{'loss': 0.9187, 'grad_norm': 2.7362383624213185, 'learning_rate': 6.220111111111111e-06, 'epoch': 1.7631999999999999}
{'loss': 0.9002, 'grad_norm': 2.930653445902442, 'learning_rate': 6.219000000000001e-06, 'epoch': 1.7635999999999998}
{'loss': 0.9036, 'grad_norm': 2.9959562188006683, 'learning_rate': 6.217888888888889e-06, 'epoch': 1.764}
{'eval_valid_loss': 0.87841796875, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.795, 'eval_valid_steps_per_second': 278.199, 'epoch': 1.764}
{'loss': 0.8956, 'grad_norm': 3.430020596397887, 'learning_rate': 6.216777777777778e-06, 'epoch': 1.7644}
{'loss': 0.8969, 'grad_norm': 2.8073728770232687, 'learning_rate': 6.215666666666668e-06, 'epoch': 1.7648000000000001}
{'loss': 0.912, 'grad_norm': 3.03279616063083, 'learning_rate': 6.214555555555556e-06, 'epoch': 1.7652}
{'loss': 0.9083, 'grad_norm': 2.6223717031470435, 'learning_rate': 6.213444444444445e-06, 'epoch': 1.7656}
{'loss': 0.8979, 'grad_norm': 2.997862153466708, 'learning_rate': 6.212333333333333e-06, 'epoch': 1.766}
{'loss': 0.9086, 'grad_norm': 2.744814264010232, 'learning_rate': 6.211222222222223e-06, 'epoch': 1.7664}
{'loss': 0.9068, 'grad_norm': 2.5595069711496725, 'learning_rate': 6.2101111111111115e-06, 'epoch': 1.7668}
{'loss': 0.9019, 'grad_norm': 2.904550116759534, 'learning_rate': 6.209e-06, 'epoch': 1.7671999999999999}
{'loss': 0.9201, 'grad_norm': 2.809109530354928, 'learning_rate': 6.207888888888889e-06, 'epoch': 1.7675999999999998}
{'loss': 0.9014, 'grad_norm': 2.933888691020072, 'learning_rate': 6.2067777777777785e-06, 'epoch': 1.768}
{'eval_valid_loss': 0.87841796875, 'eval_valid_runtime': 0.0906, 'eval_valid_samples_per_second': 1103.445, 'eval_valid_steps_per_second': 275.861, 'epoch': 1.768}
{'loss': 0.9061, 'grad_norm': 2.828103718756594, 'learning_rate': 6.205666666666668e-06, 'epoch': 1.7684}
{'loss': 0.8924, 'grad_norm': 2.902804311198071, 'learning_rate': 6.204555555555556e-06, 'epoch': 1.7688000000000001}
{'loss': 0.8874, 'grad_norm': 3.1409928879746905, 'learning_rate': 6.2034444444444454e-06, 'epoch': 1.7692}
{'loss': 0.8947, 'grad_norm': 3.05665037500941, 'learning_rate': 6.202333333333333e-06, 'epoch': 1.7696}
{'loss': 0.9108, 'grad_norm': 3.325938442279912, 'learning_rate': 6.201222222222223e-06, 'epoch': 1.77}
{'loss': 0.9108, 'grad_norm': 2.984704693463249, 'learning_rate': 6.2001111111111116e-06, 'epoch': 1.7704}
{'loss': 0.9073, 'grad_norm': 3.130379328867877, 'learning_rate': 6.199e-06, 'epoch': 1.7708}
{'loss': 0.8997, 'grad_norm': 2.930713962027157, 'learning_rate': 6.197888888888889e-06, 'epoch': 1.7711999999999999}
{'loss': 0.9073, 'grad_norm': 2.8108404826650015, 'learning_rate': 6.1967777777777785e-06, 'epoch': 1.7715999999999998}
{'loss': 0.9066, 'grad_norm': 2.872635625869462, 'learning_rate': 6.195666666666667e-06, 'epoch': 1.772}
{'eval_valid_loss': 0.87939453125, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.153, 'eval_valid_steps_per_second': 278.288, 'epoch': 1.772}
{'loss': 0.907, 'grad_norm': 2.905403126618914, 'learning_rate': 6.194555555555556e-06, 'epoch': 1.7724}
{'loss': 0.9127, 'grad_norm': 3.1596443789654822, 'learning_rate': 6.1934444444444455e-06, 'epoch': 1.7728000000000002}
{'loss': 0.9052, 'grad_norm': 3.2335427084131902, 'learning_rate': 6.192333333333333e-06, 'epoch': 1.7732}
{'loss': 0.9036, 'grad_norm': 3.320774755276359, 'learning_rate': 6.191222222222223e-06, 'epoch': 1.7736}
{'loss': 0.9199, 'grad_norm': 2.8119130157831225, 'learning_rate': 6.190111111111111e-06, 'epoch': 1.774}
{'loss': 0.9256, 'grad_norm': 2.8136550597769174, 'learning_rate': 6.189e-06, 'epoch': 1.7744}
{'loss': 0.9097, 'grad_norm': 3.3927008607192763, 'learning_rate': 6.18788888888889e-06, 'epoch': 1.7748}
{'loss': 0.8987, 'grad_norm': 2.8340668932457636, 'learning_rate': 6.186777777777778e-06, 'epoch': 1.7752}
{'loss': 0.9073, 'grad_norm': 3.1544739570159384, 'learning_rate': 6.185666666666667e-06, 'epoch': 1.7755999999999998}
{'loss': 0.9083, 'grad_norm': 2.931569718044086, 'learning_rate': 6.184555555555556e-06, 'epoch': 1.776}
{'eval_valid_loss': 0.8779296875, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.002, 'eval_valid_steps_per_second': 278.251, 'epoch': 1.776}
{'loss': 0.9112, 'grad_norm': 3.012216489755068, 'learning_rate': 6.183444444444446e-06, 'epoch': 1.7764}
{'loss': 0.9146, 'grad_norm': 2.908750278913864, 'learning_rate': 6.1823333333333335e-06, 'epoch': 1.7768000000000002}
{'loss': 0.9083, 'grad_norm': 2.8428734382068575, 'learning_rate': 6.181222222222223e-06, 'epoch': 1.7772000000000001}
{'loss': 0.9129, 'grad_norm': 2.8848033636539263, 'learning_rate': 6.180111111111111e-06, 'epoch': 1.7776}
{'loss': 0.9085, 'grad_norm': 2.8368024321504786, 'learning_rate': 6.1790000000000005e-06, 'epoch': 1.778}
{'loss': 0.9132, 'grad_norm': 3.0061186442555643, 'learning_rate': 6.17788888888889e-06, 'epoch': 1.7784}
{'loss': 0.9011, 'grad_norm': 3.1278949393808535, 'learning_rate': 6.176777777777778e-06, 'epoch': 1.7788}
{'loss': 0.901, 'grad_norm': 3.2086230008947103, 'learning_rate': 6.1756666666666675e-06, 'epoch': 1.7792}
{'loss': 0.9099, 'grad_norm': 3.015411072886115, 'learning_rate': 6.174555555555556e-06, 'epoch': 1.7795999999999998}
{'loss': 0.902, 'grad_norm': 3.358957504234187, 'learning_rate': 6.173444444444445e-06, 'epoch': 1.78}
{'eval_valid_loss': 0.88134765625, 'eval_valid_runtime': 0.1826, 'eval_valid_samples_per_second': 547.659, 'eval_valid_steps_per_second': 136.915, 'epoch': 1.78}
{'loss': 0.9126, 'grad_norm': 3.2247988057771684, 'learning_rate': 6.172333333333334e-06, 'epoch': 1.7804}
{'loss': 0.9113, 'grad_norm': 3.013191288660214, 'learning_rate': 6.171222222222223e-06, 'epoch': 1.7808000000000002}
{'loss': 0.9106, 'grad_norm': 2.9186882653742767, 'learning_rate': 6.170111111111111e-06, 'epoch': 1.7812000000000001}
{'loss': 0.9226, 'grad_norm': 3.6548947651034234, 'learning_rate': 6.1690000000000006e-06, 'epoch': 1.7816}
{'loss': 0.8976, 'grad_norm': 2.9349434867629993, 'learning_rate': 6.16788888888889e-06, 'epoch': 1.782}
{'loss': 0.9042, 'grad_norm': 3.0637543892488512, 'learning_rate': 6.166777777777778e-06, 'epoch': 1.7824}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9114, 'grad_norm': 2.9454586690963214, 'learning_rate': 6.1656666666666675e-06, 'epoch': 1.7828}
{'loss': 0.9221, 'grad_norm': 3.0402747729785053, 'learning_rate': 6.164555555555556e-06, 'epoch': 1.7832}
{'loss': 0.9224, 'grad_norm': 2.8019883069986338, 'learning_rate': 6.163444444444445e-06, 'epoch': 1.7835999999999999}
{'loss': 0.9064, 'grad_norm': 3.1251749180477035, 'learning_rate': 6.162333333333334e-06, 'epoch': 1.784}
{'eval_valid_loss': 0.87841796875, 'eval_valid_runtime': 0.0965, 'eval_valid_samples_per_second': 1036.567, 'eval_valid_steps_per_second': 259.142, 'epoch': 1.784}
{'loss': 0.9098, 'grad_norm': 2.800883968283924, 'learning_rate': 6.161222222222223e-06, 'epoch': 1.7844}
{'loss': 0.9055, 'grad_norm': 3.5430079650391, 'learning_rate': 6.160111111111111e-06, 'epoch': 1.7848000000000002}
{'loss': 0.9224, 'grad_norm': 3.1796277389721523, 'learning_rate': 6.159000000000001e-06, 'epoch': 1.7852000000000001}
{'loss': 0.9053, 'grad_norm': 2.885619185096677, 'learning_rate': 6.15788888888889e-06, 'epoch': 1.7856}
{'loss': 0.9068, 'grad_norm': 3.1351020608682334, 'learning_rate': 6.156777777777778e-06, 'epoch': 1.786}
{'loss': 0.9171, 'grad_norm': 3.3114985175673013, 'learning_rate': 6.155666666666668e-06, 'epoch': 1.7864}
{'loss': 0.8998, 'grad_norm': 2.954368672550754, 'learning_rate': 6.1545555555555555e-06, 'epoch': 1.7868}
{'loss': 0.9124, 'grad_norm': 2.7002762684059567, 'learning_rate': 6.153444444444445e-06, 'epoch': 1.7872}
{'loss': 0.9065, 'grad_norm': 3.2310422321423786, 'learning_rate': 6.152333333333334e-06, 'epoch': 1.7875999999999999}
{'loss': 0.9162, 'grad_norm': 2.9370718400466687, 'learning_rate': 6.1512222222222225e-06, 'epoch': 1.788}
{'eval_valid_loss': 0.8779296875, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1109.839, 'eval_valid_steps_per_second': 277.46, 'epoch': 1.788}
{'loss': 0.9043, 'grad_norm': 3.058657161985838, 'learning_rate': 6.150111111111111e-06, 'epoch': 1.7884}
{'loss': 0.9122, 'grad_norm': 3.053416717086898, 'learning_rate': 6.149000000000001e-06, 'epoch': 1.7888}
{'loss': 0.9057, 'grad_norm': 2.8155801119597452, 'learning_rate': 6.14788888888889e-06, 'epoch': 1.7892000000000001}
{'loss': 0.9087, 'grad_norm': 3.2014961939545863, 'learning_rate': 6.146777777777778e-06, 'epoch': 1.7896}
{'loss': 0.9157, 'grad_norm': 2.9345427094492673, 'learning_rate': 6.145666666666668e-06, 'epoch': 1.79}
{'loss': 0.903, 'grad_norm': 2.861314261861814, 'learning_rate': 6.144555555555556e-06, 'epoch': 1.7904}
{'loss': 0.9084, 'grad_norm': 3.0246622921673314, 'learning_rate': 6.143444444444445e-06, 'epoch': 1.7908}
{'loss': 0.8947, 'grad_norm': 3.082601081183941, 'learning_rate': 6.142333333333334e-06, 'epoch': 1.7912}
{'loss': 0.9031, 'grad_norm': 3.1381822072345527, 'learning_rate': 6.141222222222223e-06, 'epoch': 1.7915999999999999}
{'loss': 0.9136, 'grad_norm': 2.973329204726097, 'learning_rate': 6.140111111111111e-06, 'epoch': 1.792}
{'eval_valid_loss': 0.880859375, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1121.856, 'eval_valid_steps_per_second': 280.464, 'epoch': 1.792}
{'loss': 0.9085, 'grad_norm': 2.924729671375995, 'learning_rate': 6.139000000000001e-06, 'epoch': 1.7924}
{'loss': 0.8976, 'grad_norm': 3.1400488994064903, 'learning_rate': 6.1378888888888896e-06, 'epoch': 1.7928}
{'loss': 0.918, 'grad_norm': 2.8139462884929616, 'learning_rate': 6.136777777777778e-06, 'epoch': 1.7932000000000001}
{'loss': 0.912, 'grad_norm': 2.857312349630983, 'learning_rate': 6.135666666666668e-06, 'epoch': 1.7936}
{'loss': 0.9067, 'grad_norm': 2.643205363850774, 'learning_rate': 6.134555555555556e-06, 'epoch': 1.794}
{'loss': 0.9174, 'grad_norm': 3.2354291681480873, 'learning_rate': 6.133444444444445e-06, 'epoch': 1.7944}
{'loss': 0.9081, 'grad_norm': 3.080757757953341, 'learning_rate': 6.132333333333333e-06, 'epoch': 1.7948}
{'loss': 0.8981, 'grad_norm': 2.9289326216598437, 'learning_rate': 6.131222222222223e-06, 'epoch': 1.7952}
{'loss': 0.9038, 'grad_norm': 2.9582428761653317, 'learning_rate': 6.130111111111111e-06, 'epoch': 1.7955999999999999}
{'loss': 0.9092, 'grad_norm': 2.8685382395026218, 'learning_rate': 6.129e-06, 'epoch': 1.796}
{'eval_valid_loss': 0.87744140625, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.605, 'eval_valid_steps_per_second': 279.151, 'epoch': 1.796}
{'loss': 0.915, 'grad_norm': 3.1997078415747247, 'learning_rate': 6.12788888888889e-06, 'epoch': 1.7964}
{'loss': 0.9002, 'grad_norm': 3.297646084326733, 'learning_rate': 6.126777777777778e-06, 'epoch': 1.7968}
{'loss': 0.9086, 'grad_norm': 2.986046088543287, 'learning_rate': 6.125666666666668e-06, 'epoch': 1.7972000000000001}
{'loss': 0.913, 'grad_norm': 2.763250500802095, 'learning_rate': 6.124555555555556e-06, 'epoch': 1.7976}
{'loss': 0.9132, 'grad_norm': 3.2693310565799245, 'learning_rate': 6.123444444444445e-06, 'epoch': 1.798}
{'loss': 0.914, 'grad_norm': 2.9740509891718143, 'learning_rate': 6.122333333333333e-06, 'epoch': 1.7984}
{'loss': 0.9011, 'grad_norm': 2.6816815278003254, 'learning_rate': 6.121222222222223e-06, 'epoch': 1.7988}
{'loss': 0.9138, 'grad_norm': 2.850544347979143, 'learning_rate': 6.1201111111111115e-06, 'epoch': 1.7992}
{'loss': 0.906, 'grad_norm': 2.718047709029467, 'learning_rate': 6.119e-06, 'epoch': 1.7995999999999999}
{'loss': 0.9197, 'grad_norm': 3.0115146988394885, 'learning_rate': 6.11788888888889e-06, 'epoch': 1.8}
{'eval_valid_loss': 0.876953125, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.911, 'eval_valid_steps_per_second': 279.728, 'epoch': 1.8}
{'loss': 0.9072, 'grad_norm': 3.135389576073564, 'learning_rate': 6.1167777777777785e-06, 'epoch': 1.8004}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9148, 'grad_norm': 2.9864895753897276, 'learning_rate': 6.115666666666667e-06, 'epoch': 1.8008}
{'loss': 0.9121, 'grad_norm': 3.3117868177541947, 'learning_rate': 6.114555555555556e-06, 'epoch': 1.8012000000000001}
{'loss': 0.9078, 'grad_norm': 3.2289445964789225, 'learning_rate': 6.1134444444444454e-06, 'epoch': 1.8016}
{'loss': 0.9091, 'grad_norm': 2.7931762618223996, 'learning_rate': 6.112333333333333e-06, 'epoch': 1.802}
{'loss': 0.8975, 'grad_norm': 3.006717651062493, 'learning_rate': 6.111222222222223e-06, 'epoch': 1.8024}
{'loss': 0.9115, 'grad_norm': 3.287922179016422, 'learning_rate': 6.110222222222223e-06, 'epoch': 1.8028}
{'loss': 0.9002, 'grad_norm': 2.8431636918530145, 'learning_rate': 6.109111111111111e-06, 'epoch': 1.8032}
{'loss': 0.9026, 'grad_norm': 2.8229632825392574, 'learning_rate': 6.108000000000001e-06, 'epoch': 1.8035999999999999}
{'loss': 0.9101, 'grad_norm': 3.0089140522175186, 'learning_rate': 6.1068888888888885e-06, 'epoch': 1.804}
{'eval_valid_loss': 0.8779296875, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.325, 'eval_valid_steps_per_second': 281.831, 'epoch': 1.804}
{'loss': 0.9063, 'grad_norm': 3.099900816668973, 'learning_rate': 6.105777777777778e-06, 'epoch': 1.8044}
{'loss': 0.901, 'grad_norm': 3.1926869842329944, 'learning_rate': 6.104666666666668e-06, 'epoch': 1.8048}
{'loss': 0.9, 'grad_norm': 3.1646604467459687, 'learning_rate': 6.1035555555555555e-06, 'epoch': 1.8052000000000001}
{'loss': 0.9006, 'grad_norm': 2.9792880498170966, 'learning_rate': 6.102444444444445e-06, 'epoch': 1.8056}
{'loss': 0.9052, 'grad_norm': 2.947599762416514, 'learning_rate': 6.101333333333334e-06, 'epoch': 1.806}
{'loss': 0.8958, 'grad_norm': 3.263833225172389, 'learning_rate': 6.100222222222223e-06, 'epoch': 1.8064}
{'loss': 0.9035, 'grad_norm': 3.167566558481828, 'learning_rate': 6.099111111111111e-06, 'epoch': 1.8068}
{'loss': 0.8945, 'grad_norm': 2.802820923429249, 'learning_rate': 6.098000000000001e-06, 'epoch': 1.8072}
{'loss': 0.9171, 'grad_norm': 3.149780980700596, 'learning_rate': 6.096888888888889e-06, 'epoch': 1.8075999999999999}
{'loss': 0.9016, 'grad_norm': 3.059152690926882, 'learning_rate': 6.095777777777778e-06, 'epoch': 1.808}
{'eval_valid_loss': 0.87744140625, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1115.266, 'eval_valid_steps_per_second': 278.817, 'epoch': 1.808}
{'loss': 0.9062, 'grad_norm': 2.9740224197795806, 'learning_rate': 6.094666666666668e-06, 'epoch': 1.8084}
{'loss': 0.9047, 'grad_norm': 3.023901339493301, 'learning_rate': 6.093555555555556e-06, 'epoch': 1.8088}
{'loss': 0.9093, 'grad_norm': 2.781343833540596, 'learning_rate': 6.092444444444445e-06, 'epoch': 1.8092000000000001}
{'loss': 0.908, 'grad_norm': 3.100220445132001, 'learning_rate': 6.091333333333334e-06, 'epoch': 1.8096}
{'loss': 0.9245, 'grad_norm': 2.84904298650769, 'learning_rate': 6.0902222222222226e-06, 'epoch': 1.81}
{'loss': 0.9091, 'grad_norm': 2.8076537446668914, 'learning_rate': 6.089111111111111e-06, 'epoch': 1.8104}
{'loss': 0.9104, 'grad_norm': 2.8266372402973565, 'learning_rate': 6.088000000000001e-06, 'epoch': 1.8108}
{'loss': 0.9117, 'grad_norm': 2.892237522799346, 'learning_rate': 6.086888888888889e-06, 'epoch': 1.8112}
{'loss': 0.9155, 'grad_norm': 2.975361439042282, 'learning_rate': 6.085777777777778e-06, 'epoch': 1.8115999999999999}
{'loss': 0.9082, 'grad_norm': 2.9477845702662426, 'learning_rate': 6.084666666666668e-06, 'epoch': 1.812}
{'eval_valid_loss': 0.87841796875, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1121.637, 'eval_valid_steps_per_second': 280.409, 'epoch': 1.812}
{'loss': 0.9098, 'grad_norm': 3.020926080573437, 'learning_rate': 6.083555555555556e-06, 'epoch': 1.8124}
{'loss': 0.898, 'grad_norm': 2.915369144837646, 'learning_rate': 6.082444444444445e-06, 'epoch': 1.8128}
{'loss': 0.9079, 'grad_norm': 3.46066339927232, 'learning_rate': 6.081333333333334e-06, 'epoch': 1.8132000000000001}
{'loss': 0.906, 'grad_norm': 3.00983314512688, 'learning_rate': 6.080222222222223e-06, 'epoch': 1.8136}
{'loss': 0.907, 'grad_norm': 2.9279207420963145, 'learning_rate': 6.079111111111111e-06, 'epoch': 1.814}
{'loss': 0.9154, 'grad_norm': 3.243498332674019, 'learning_rate': 6.078000000000001e-06, 'epoch': 1.8144}
{'loss': 0.9126, 'grad_norm': 3.033329211265432, 'learning_rate': 6.076888888888889e-06, 'epoch': 1.8148}
{'loss': 0.8902, 'grad_norm': 3.1728819860031225, 'learning_rate': 6.075777777777778e-06, 'epoch': 1.8152}
{'loss': 0.9297, 'grad_norm': 3.085796410920644, 'learning_rate': 6.074666666666668e-06, 'epoch': 1.8155999999999999}
{'loss': 0.916, 'grad_norm': 3.0016387099719166, 'learning_rate': 6.073555555555556e-06, 'epoch': 1.8159999999999998}
{'eval_valid_loss': 0.876953125, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.424, 'eval_valid_steps_per_second': 279.606, 'epoch': 1.8159999999999998}
{'loss': 0.9078, 'grad_norm': 2.6756691568104123, 'learning_rate': 6.072444444444445e-06, 'epoch': 1.8164}
{'loss': 0.9118, 'grad_norm': 3.113691161773984, 'learning_rate': 6.071333333333333e-06, 'epoch': 1.8168}
{'loss': 0.9048, 'grad_norm': 2.6714737016353363, 'learning_rate': 6.070222222222223e-06, 'epoch': 1.8172000000000001}
{'loss': 0.9058, 'grad_norm': 2.9517573363506253, 'learning_rate': 6.0691111111111115e-06, 'epoch': 1.8176}
{'loss': 0.9074, 'grad_norm': 3.0977221911267887, 'learning_rate': 6.068e-06, 'epoch': 1.818}
{'loss': 0.9106, 'grad_norm': 2.8311168694090343, 'learning_rate': 6.06688888888889e-06, 'epoch': 1.8184}
{'loss': 0.897, 'grad_norm': 3.169942718268105, 'learning_rate': 6.0657777777777784e-06, 'epoch': 1.8188}
{'loss': 0.8909, 'grad_norm': 3.383621674067695, 'learning_rate': 6.064666666666668e-06, 'epoch': 1.8192}
{'loss': 0.9081, 'grad_norm': 2.923868763857184, 'learning_rate': 6.063555555555556e-06, 'epoch': 1.8195999999999999}
{'loss': 0.9129, 'grad_norm': 3.0484723330120236, 'learning_rate': 6.0624444444444454e-06, 'epoch': 1.8199999999999998}
{'eval_valid_loss': 0.8779296875, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.258, 'eval_valid_steps_per_second': 278.065, 'epoch': 1.8199999999999998}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.9074, 'grad_norm': 3.232759957050179, 'learning_rate': 6.061333333333333e-06, 'epoch': 1.8204}
{'loss': 0.8995, 'grad_norm': 2.935875544927188, 'learning_rate': 6.060222222222223e-06, 'epoch': 1.8208}
{'loss': 0.9044, 'grad_norm': 2.9665830835254985, 'learning_rate': 6.0591111111111116e-06, 'epoch': 1.8212000000000002}
{'loss': 0.9085, 'grad_norm': 3.15525259178121, 'learning_rate': 6.058e-06, 'epoch': 1.8216}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9202, 'grad_norm': 2.8861834451726276, 'learning_rate': 6.05688888888889e-06, 'epoch': 1.822}
{'loss': 0.9084, 'grad_norm': 2.6687785660780503, 'learning_rate': 6.0557777777777785e-06, 'epoch': 1.8224}
{'loss': 0.9006, 'grad_norm': 3.3877210530864383, 'learning_rate': 6.054666666666667e-06, 'epoch': 1.8228}
{'loss': 0.9013, 'grad_norm': 2.9911879221958286, 'learning_rate': 6.053555555555556e-06, 'epoch': 1.8232}
{'loss': 0.8894, 'grad_norm': 3.0941399607870177, 'learning_rate': 6.0524444444444455e-06, 'epoch': 1.8235999999999999}
{'loss': 0.9119, 'grad_norm': 3.3303715461936694, 'learning_rate': 6.051333333333333e-06, 'epoch': 1.8239999999999998}
{'eval_valid_loss': 0.87646484375, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.72, 'eval_valid_steps_per_second': 278.93, 'epoch': 1.8239999999999998}
{'loss': 0.9039, 'grad_norm': 2.420155425420376, 'learning_rate': 6.050222222222223e-06, 'epoch': 1.8244}
{'loss': 0.8995, 'grad_norm': 2.910090175621019, 'learning_rate': 6.049111111111111e-06, 'epoch': 1.8248}
{'loss': 0.9042, 'grad_norm': 3.0734275759530436, 'learning_rate': 6.048e-06, 'epoch': 1.8252000000000002}
{'loss': 0.9079, 'grad_norm': 2.85544779257122, 'learning_rate': 6.04688888888889e-06, 'epoch': 1.8256000000000001}
{'loss': 0.9153, 'grad_norm': 3.0904970792519904, 'learning_rate': 6.045777777777778e-06, 'epoch': 1.826}
{'loss': 0.9032, 'grad_norm': 2.8851475343384068, 'learning_rate': 6.044666666666667e-06, 'epoch': 1.8264}
{'loss': 0.8996, 'grad_norm': 2.9062133294529984, 'learning_rate': 6.043555555555556e-06, 'epoch': 1.8268}
{'loss': 0.9081, 'grad_norm': 3.1022742965635834, 'learning_rate': 6.042444444444446e-06, 'epoch': 1.8272}
{'loss': 0.9007, 'grad_norm': 3.108127823104819, 'learning_rate': 6.0413333333333335e-06, 'epoch': 1.8276}
{'loss': 0.9139, 'grad_norm': 2.9661675615103724, 'learning_rate': 6.040222222222223e-06, 'epoch': 1.8279999999999998}
{'eval_valid_loss': 0.87841796875, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1120.702, 'eval_valid_steps_per_second': 280.175, 'epoch': 1.8279999999999998}
{'loss': 0.8995, 'grad_norm': 3.0540890314731017, 'learning_rate': 6.039111111111111e-06, 'epoch': 1.8284}
{'loss': 0.8949, 'grad_norm': 2.941869843565371, 'learning_rate': 6.0380000000000005e-06, 'epoch': 1.8288}
{'loss': 0.9057, 'grad_norm': 2.878909086435647, 'learning_rate': 6.03688888888889e-06, 'epoch': 1.8292000000000002}
{'loss': 0.9059, 'grad_norm': 2.884021061169602, 'learning_rate': 6.035777777777778e-06, 'epoch': 1.8296000000000001}
{'loss': 0.8981, 'grad_norm': 2.891364276275202, 'learning_rate': 6.0346666666666674e-06, 'epoch': 1.83}
{'loss': 0.9191, 'grad_norm': 2.579272789202132, 'learning_rate': 6.033555555555556e-06, 'epoch': 1.8304}
{'loss': 0.9087, 'grad_norm': 2.8112588316039586, 'learning_rate': 6.032444444444445e-06, 'epoch': 1.8308}
{'loss': 0.9159, 'grad_norm': 3.060227475422093, 'learning_rate': 6.031333333333334e-06, 'epoch': 1.8312}
{'loss': 0.8991, 'grad_norm': 3.156772570265243, 'learning_rate': 6.030222222222223e-06, 'epoch': 1.8316}
{'loss': 0.9141, 'grad_norm': 2.829640772626971, 'learning_rate': 6.029111111111111e-06, 'epoch': 1.8319999999999999}
{'eval_valid_loss': 0.87890625, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1126.544, 'eval_valid_steps_per_second': 281.636, 'epoch': 1.8319999999999999}
{'loss': 0.8919, 'grad_norm': 3.2358567626105574, 'learning_rate': 6.0280000000000006e-06, 'epoch': 1.8324}
{'loss': 0.8928, 'grad_norm': 3.0449541248035334, 'learning_rate': 6.02688888888889e-06, 'epoch': 1.8328}
{'loss': 0.9184, 'grad_norm': 3.177379295238466, 'learning_rate': 6.025777777777778e-06, 'epoch': 1.8332000000000002}
{'loss': 0.9058, 'grad_norm': 2.907516962167556, 'learning_rate': 6.0246666666666675e-06, 'epoch': 1.8336000000000001}
{'loss': 0.8958, 'grad_norm': 3.074056182401897, 'learning_rate': 6.023555555555555e-06, 'epoch': 1.834}
{'loss': 0.9005, 'grad_norm': 2.687895230130083, 'learning_rate': 6.022444444444445e-06, 'epoch': 1.8344}
{'loss': 0.9246, 'grad_norm': 2.871821315209646, 'learning_rate': 6.021333333333334e-06, 'epoch': 1.8348}
{'loss': 0.9037, 'grad_norm': 2.9852485476743396, 'learning_rate': 6.020222222222223e-06, 'epoch': 1.8352}
{'loss': 0.9088, 'grad_norm': 3.186229901555314, 'learning_rate': 6.019111111111111e-06, 'epoch': 1.8356}
{'loss': 0.9098, 'grad_norm': 2.9140218368843125, 'learning_rate': 6.018000000000001e-06, 'epoch': 1.8359999999999999}
{'eval_valid_loss': 0.87646484375, 'eval_valid_runtime': 0.0885, 'eval_valid_samples_per_second': 1129.49, 'eval_valid_steps_per_second': 282.372, 'epoch': 1.8359999999999999}
{'loss': 0.9155, 'grad_norm': 2.9959080624887404, 'learning_rate': 6.01688888888889e-06, 'epoch': 1.8364}
{'loss': 0.8991, 'grad_norm': 2.9448104551939593, 'learning_rate': 6.015777777777778e-06, 'epoch': 1.8368}
{'loss': 0.9094, 'grad_norm': 3.132679772359815, 'learning_rate': 6.014666666666668e-06, 'epoch': 1.8372000000000002}
{'loss': 0.9101, 'grad_norm': 2.9046897288986444, 'learning_rate': 6.0135555555555555e-06, 'epoch': 1.8376000000000001}
{'loss': 0.8976, 'grad_norm': 2.793095752522817, 'learning_rate': 6.012444444444445e-06, 'epoch': 1.838}
{'loss': 0.909, 'grad_norm': 2.794472413674095, 'learning_rate': 6.011333333333334e-06, 'epoch': 1.8384}
{'loss': 0.9087, 'grad_norm': 2.9992343899199096, 'learning_rate': 6.0102222222222225e-06, 'epoch': 1.8388}
{'loss': 0.9093, 'grad_norm': 2.9420325130537126, 'learning_rate': 6.009111111111111e-06, 'epoch': 1.8392}
{'loss': 0.8974, 'grad_norm': 2.9061423353536044, 'learning_rate': 6.008000000000001e-06, 'epoch': 1.8396}
{'loss': 0.9104, 'grad_norm': 3.0244523250141366, 'learning_rate': 6.0068888888888895e-06, 'epoch': 1.8399999999999999}
{'eval_valid_loss': 0.87646484375, 'eval_valid_runtime': 0.088, 'eval_valid_samples_per_second': 1136.951, 'eval_valid_steps_per_second': 284.238, 'epoch': 1.8399999999999999}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9143, 'grad_norm': 2.9522435472356348, 'learning_rate': 6.005777777777778e-06, 'epoch': 1.8404}
{'loss': 0.9018, 'grad_norm': 3.167724966883409, 'learning_rate': 6.004666666666668e-06, 'epoch': 1.8408}
{'loss': 0.9021, 'grad_norm': 2.959485284186421, 'learning_rate': 6.003555555555556e-06, 'epoch': 1.8412}
{'loss': 0.9033, 'grad_norm': 3.0147957822963014, 'learning_rate': 6.002444444444445e-06, 'epoch': 1.8416000000000001}
{'loss': 0.9, 'grad_norm': 3.088230662048563, 'learning_rate': 6.001333333333334e-06, 'epoch': 1.842}
{'loss': 0.903, 'grad_norm': 2.9376486375416997, 'learning_rate': 6.000222222222223e-06, 'epoch': 1.8424}
{'loss': 0.9223, 'grad_norm': 3.113532904549542, 'learning_rate': 5.999222222222223e-06, 'epoch': 1.8428}
{'loss': 0.9116, 'grad_norm': 3.2487757742909906, 'learning_rate': 5.998111111111112e-06, 'epoch': 1.8432}
{'loss': 0.9061, 'grad_norm': 3.2197526740373985, 'learning_rate': 5.997e-06, 'epoch': 1.8436}
{'loss': 0.92, 'grad_norm': 3.010267994299328, 'learning_rate': 5.995888888888889e-06, 'epoch': 1.8439999999999999}
{'eval_valid_loss': 0.87548828125, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1121.25, 'eval_valid_steps_per_second': 280.312, 'epoch': 1.8439999999999999}
{'loss': 0.8967, 'grad_norm': 3.0500861339444887, 'learning_rate': 5.994777777777779e-06, 'epoch': 1.8444}
{'loss': 0.8975, 'grad_norm': 2.9434087353005647, 'learning_rate': 5.993666666666667e-06, 'epoch': 1.8448}
{'loss': 0.9138, 'grad_norm': 2.946527731246602, 'learning_rate': 5.992555555555556e-06, 'epoch': 1.8452}
{'loss': 0.9021, 'grad_norm': 3.2252511229809184, 'learning_rate': 5.991444444444446e-06, 'epoch': 1.8456000000000001}
{'loss': 0.9147, 'grad_norm': 2.7953060960349796, 'learning_rate': 5.9903333333333335e-06, 'epoch': 1.846}
{'loss': 0.8987, 'grad_norm': 2.97162374643755, 'learning_rate': 5.989222222222223e-06, 'epoch': 1.8464}
{'loss': 0.9048, 'grad_norm': 2.9819232196543077, 'learning_rate': 5.988111111111111e-06, 'epoch': 1.8468}
{'loss': 0.9197, 'grad_norm': 2.7566329443501463, 'learning_rate': 5.9870000000000004e-06, 'epoch': 1.8472}
{'loss': 0.9085, 'grad_norm': 3.087425288559, 'learning_rate': 5.985888888888889e-06, 'epoch': 1.8476}
{'loss': 0.904, 'grad_norm': 3.0007828942621875, 'learning_rate': 5.984777777777778e-06, 'epoch': 1.8479999999999999}
{'eval_valid_loss': 0.8759765625, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.053, 'eval_valid_steps_per_second': 281.763, 'epoch': 1.8479999999999999}
{'loss': 0.9183, 'grad_norm': 3.22681522072408, 'learning_rate': 5.9836666666666674e-06, 'epoch': 1.8484}
{'loss': 0.9046, 'grad_norm': 3.2022861777995995, 'learning_rate': 5.982555555555556e-06, 'epoch': 1.8488}
{'loss': 0.9081, 'grad_norm': 3.0128736149532642, 'learning_rate': 5.981444444444446e-06, 'epoch': 1.8492}
{'loss': 0.9099, 'grad_norm': 3.0287330343737167, 'learning_rate': 5.9803333333333336e-06, 'epoch': 1.8496000000000001}
{'loss': 0.9015, 'grad_norm': 3.0984877424066912, 'learning_rate': 5.979222222222223e-06, 'epoch': 1.85}
{'loss': 0.8992, 'grad_norm': 2.9572614330767637, 'learning_rate': 5.978111111111111e-06, 'epoch': 1.8504}
{'loss': 0.9002, 'grad_norm': 3.005156198685151, 'learning_rate': 5.9770000000000005e-06, 'epoch': 1.8508}
{'loss': 0.9092, 'grad_norm': 2.90102059558131, 'learning_rate': 5.975888888888889e-06, 'epoch': 1.8512}
{'loss': 0.9289, 'grad_norm': 2.8412644716709066, 'learning_rate': 5.974777777777778e-06, 'epoch': 1.8516}
{'loss': 0.9129, 'grad_norm': 3.264868571088268, 'learning_rate': 5.9736666666666675e-06, 'epoch': 1.8519999999999999}
{'eval_valid_loss': 0.87451171875, 'eval_valid_runtime': 0.0906, 'eval_valid_samples_per_second': 1103.738, 'eval_valid_steps_per_second': 275.935, 'epoch': 1.8519999999999999}
{'loss': 0.9006, 'grad_norm': 2.9012659158561838, 'learning_rate': 5.972555555555556e-06, 'epoch': 1.8524}
{'loss': 0.913, 'grad_norm': 2.94976730802896, 'learning_rate': 5.971444444444445e-06, 'epoch': 1.8528}
{'loss': 0.9092, 'grad_norm': 2.780491543339047, 'learning_rate': 5.970333333333334e-06, 'epoch': 1.8532}
{'loss': 0.9052, 'grad_norm': 2.901816195652761, 'learning_rate': 5.969222222222223e-06, 'epoch': 1.8536000000000001}
{'loss': 0.8995, 'grad_norm': 2.8494771465242525, 'learning_rate': 5.968111111111111e-06, 'epoch': 1.854}
{'loss': 0.9078, 'grad_norm': 2.8954692561395308, 'learning_rate': 5.967000000000001e-06, 'epoch': 1.8544}
{'loss': 0.9074, 'grad_norm': 2.95500039794519, 'learning_rate': 5.9658888888888885e-06, 'epoch': 1.8548}
{'loss': 0.8939, 'grad_norm': 2.842594519815978, 'learning_rate': 5.964777777777778e-06, 'epoch': 1.8552}
{'loss': 0.9219, 'grad_norm': 3.050948144545815, 'learning_rate': 5.963666666666668e-06, 'epoch': 1.8556}
{'loss': 0.9117, 'grad_norm': 3.0055947983609173, 'learning_rate': 5.9625555555555555e-06, 'epoch': 1.8559999999999999}
{'eval_valid_loss': 0.87548828125, 'eval_valid_runtime': 0.0905, 'eval_valid_samples_per_second': 1105.201, 'eval_valid_steps_per_second': 276.3, 'epoch': 1.8559999999999999}
{'loss': 0.9104, 'grad_norm': 2.7495337004232527, 'learning_rate': 5.961444444444445e-06, 'epoch': 1.8564}
{'loss': 0.8937, 'grad_norm': 2.729799518786923, 'learning_rate': 5.960333333333334e-06, 'epoch': 1.8568}
{'loss': 0.9018, 'grad_norm': 3.1212204011087334, 'learning_rate': 5.959222222222223e-06, 'epoch': 1.8572}
{'loss': 0.8914, 'grad_norm': 3.3989060177616857, 'learning_rate': 5.958111111111111e-06, 'epoch': 1.8576000000000001}
{'loss': 0.8965, 'grad_norm': 3.0201676402552398, 'learning_rate': 5.957000000000001e-06, 'epoch': 1.858}
{'loss': 0.9006, 'grad_norm': 2.8675614614643, 'learning_rate': 5.955888888888889e-06, 'epoch': 1.8584}
{'loss': 0.902, 'grad_norm': 2.9249272545302687, 'learning_rate': 5.954777777777778e-06, 'epoch': 1.8588}
{'loss': 0.9006, 'grad_norm': 3.3126121987828565, 'learning_rate': 5.953666666666668e-06, 'epoch': 1.8592}
{'loss': 0.9013, 'grad_norm': 3.4250841377532515, 'learning_rate': 5.952555555555556e-06, 'epoch': 1.8596}
{'loss': 0.9094, 'grad_norm': 2.9695638444572428, 'learning_rate': 5.951444444444445e-06, 'epoch': 1.8599999999999999}
{'eval_valid_loss': 0.87744140625, 'eval_valid_runtime': 0.0971, 'eval_valid_samples_per_second': 1029.694, 'eval_valid_steps_per_second': 257.423, 'epoch': 1.8599999999999999}
{'loss': 0.9061, 'grad_norm': 3.0237141563937238, 'learning_rate': 5.950333333333334e-06, 'epoch': 1.8604}
{'loss': 0.9042, 'grad_norm': 2.8610398703926694, 'learning_rate': 5.9492222222222226e-06, 'epoch': 1.8608}
{'loss': 0.8979, 'grad_norm': 3.1157983155861886, 'learning_rate': 5.948111111111111e-06, 'epoch': 1.8612}
{'loss': 0.9076, 'grad_norm': 2.693064777951852, 'learning_rate': 5.947000000000001e-06, 'epoch': 1.8616000000000001}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9027, 'grad_norm': 2.6915655822541384, 'learning_rate': 5.945888888888889e-06, 'epoch': 1.862}
{'loss': 0.8958, 'grad_norm': 2.8732142603885293, 'learning_rate': 5.944777777777778e-06, 'epoch': 1.8624}
{'loss': 0.9086, 'grad_norm': 2.924999524793016, 'learning_rate': 5.943666666666668e-06, 'epoch': 1.8628}
{'loss': 0.9121, 'grad_norm': 2.7782084464294714, 'learning_rate': 5.942555555555556e-06, 'epoch': 1.8632}
{'loss': 0.9059, 'grad_norm': 2.997406563738262, 'learning_rate': 5.941444444444445e-06, 'epoch': 1.8636}
{'loss': 0.8961, 'grad_norm': 2.7746902215589633, 'learning_rate': 5.940333333333333e-06, 'epoch': 1.8639999999999999}
{'eval_valid_loss': 0.8759765625, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1110.439, 'eval_valid_steps_per_second': 277.61, 'epoch': 1.8639999999999999}
{'loss': 0.9056, 'grad_norm': 3.313224740188483, 'learning_rate': 5.939222222222223e-06, 'epoch': 1.8643999999999998}
{'loss': 0.905, 'grad_norm': 2.8616887513050764, 'learning_rate': 5.938111111111111e-06, 'epoch': 1.8648}
{'loss': 0.9018, 'grad_norm': 3.0947752419396157, 'learning_rate': 5.937000000000001e-06, 'epoch': 1.8652}
{'loss': 0.9029, 'grad_norm': 2.962874430723449, 'learning_rate': 5.93588888888889e-06, 'epoch': 1.8656000000000001}
{'loss': 0.9009, 'grad_norm': 3.1172680665123704, 'learning_rate': 5.934777777777778e-06, 'epoch': 1.866}
{'loss': 0.9054, 'grad_norm': 2.877237485880448, 'learning_rate': 5.933666666666668e-06, 'epoch': 1.8664}
{'loss': 0.9031, 'grad_norm': 3.2980914741707563, 'learning_rate': 5.932555555555556e-06, 'epoch': 1.8668}
{'loss': 0.9034, 'grad_norm': 2.739476708898524, 'learning_rate': 5.931444444444445e-06, 'epoch': 1.8672}
{'loss': 0.9061, 'grad_norm': 2.7962557570662936, 'learning_rate': 5.930333333333333e-06, 'epoch': 1.8676}
{'loss': 0.9024, 'grad_norm': 2.9258007054006745, 'learning_rate': 5.929222222222223e-06, 'epoch': 1.8679999999999999}
{'eval_valid_loss': 0.8759765625, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1110.021, 'eval_valid_steps_per_second': 277.505, 'epoch': 1.8679999999999999}
{'loss': 0.9079, 'grad_norm': 2.9577994010300728, 'learning_rate': 5.9281111111111115e-06, 'epoch': 1.8683999999999998}
{'loss': 0.9148, 'grad_norm': 2.809880111952043, 'learning_rate': 5.927e-06, 'epoch': 1.8688}
{'loss': 0.9089, 'grad_norm': 2.9021243786470845, 'learning_rate': 5.92588888888889e-06, 'epoch': 1.8692}
{'loss': 0.8997, 'grad_norm': 3.1063082442013568, 'learning_rate': 5.9247777777777784e-06, 'epoch': 1.8696000000000002}
{'loss': 0.9017, 'grad_norm': 3.100128466882179, 'learning_rate': 5.923666666666667e-06, 'epoch': 1.87}
{'loss': 0.9066, 'grad_norm': 3.2902209219487126, 'learning_rate': 5.922555555555556e-06, 'epoch': 1.8704}
{'loss': 0.9199, 'grad_norm': 3.380412380170806, 'learning_rate': 5.9214444444444454e-06, 'epoch': 1.8708}
{'loss': 0.9019, 'grad_norm': 2.4710293761753004, 'learning_rate': 5.920333333333333e-06, 'epoch': 1.8712}
{'loss': 0.9066, 'grad_norm': 2.8549703223027114, 'learning_rate': 5.919222222222223e-06, 'epoch': 1.8716}
{'loss': 0.8984, 'grad_norm': 2.8297227645606093, 'learning_rate': 5.9181111111111116e-06, 'epoch': 1.8719999999999999}
{'eval_valid_loss': 0.87646484375, 'eval_valid_runtime': 0.0916, 'eval_valid_samples_per_second': 1091.459, 'eval_valid_steps_per_second': 272.865, 'epoch': 1.8719999999999999}
{'loss': 0.9077, 'grad_norm': 2.949484321813823, 'learning_rate': 5.917e-06, 'epoch': 1.8723999999999998}
{'loss': 0.9051, 'grad_norm': 2.7876687133520712, 'learning_rate': 5.91588888888889e-06, 'epoch': 1.8728}
{'loss': 0.9019, 'grad_norm': 3.1687558533352997, 'learning_rate': 5.9147777777777785e-06, 'epoch': 1.8732}
{'loss': 0.9215, 'grad_norm': 3.280387919575501, 'learning_rate': 5.913666666666667e-06, 'epoch': 1.8736000000000002}
{'loss': 0.8962, 'grad_norm': 2.8569778180778402, 'learning_rate': 5.912555555555556e-06, 'epoch': 1.874}
{'loss': 0.912, 'grad_norm': 3.031339054175955, 'learning_rate': 5.9114444444444455e-06, 'epoch': 1.8744}
{'loss': 0.9075, 'grad_norm': 3.3709176770107745, 'learning_rate': 5.910333333333333e-06, 'epoch': 1.8748}
{'loss': 0.9068, 'grad_norm': 2.8288139220738775, 'learning_rate': 5.909222222222223e-06, 'epoch': 1.8752}
{'loss': 0.9152, 'grad_norm': 2.993499289031332, 'learning_rate': 5.908111111111111e-06, 'epoch': 1.8756}
{'loss': 0.9111, 'grad_norm': 3.0799143861972245, 'learning_rate': 5.907e-06, 'epoch': 1.876}
{'eval_valid_loss': 0.876953125, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.155, 'eval_valid_steps_per_second': 278.039, 'epoch': 1.876}
{'loss': 0.899, 'grad_norm': 2.79224845828228, 'learning_rate': 5.90588888888889e-06, 'epoch': 1.8763999999999998}
{'loss': 0.9236, 'grad_norm': 3.368428729490305, 'learning_rate': 5.904777777777778e-06, 'epoch': 1.8768}
{'loss': 0.8991, 'grad_norm': 2.8049432218214436, 'learning_rate': 5.903666666666667e-06, 'epoch': 1.8772}
{'loss': 0.909, 'grad_norm': 3.0571785450445184, 'learning_rate': 5.902555555555556e-06, 'epoch': 1.8776000000000002}
{'loss': 0.9066, 'grad_norm': 3.0142242558305283, 'learning_rate': 5.901444444444446e-06, 'epoch': 1.8780000000000001}
{'loss': 0.9007, 'grad_norm': 2.814015224044845, 'learning_rate': 5.9003333333333335e-06, 'epoch': 1.8784}
{'loss': 0.9043, 'grad_norm': 2.991290413662806, 'learning_rate': 5.899222222222223e-06, 'epoch': 1.8788}
{'loss': 0.9, 'grad_norm': 3.163171647486708, 'learning_rate': 5.898111111111111e-06, 'epoch': 1.8792}
{'loss': 0.9083, 'grad_norm': 3.031977153355446, 'learning_rate': 5.8970000000000005e-06, 'epoch': 1.8796}
{'loss': 0.9089, 'grad_norm': 2.786190958950617, 'learning_rate': 5.89588888888889e-06, 'epoch': 1.88}
{'eval_valid_loss': 0.875, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.983, 'eval_valid_steps_per_second': 279.996, 'epoch': 1.88}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9069, 'grad_norm': 2.8770322665784183, 'learning_rate': 5.894777777777778e-06, 'epoch': 1.8803999999999998}
{'loss': 0.911, 'grad_norm': 2.8158832122578668, 'learning_rate': 5.8936666666666674e-06, 'epoch': 1.8808}
{'loss': 0.9048, 'grad_norm': 3.001431798993507, 'learning_rate': 5.892555555555556e-06, 'epoch': 1.8812}
{'loss': 0.8896, 'grad_norm': 2.9351372855017765, 'learning_rate': 5.891444444444445e-06, 'epoch': 1.8816000000000002}
{'loss': 0.9014, 'grad_norm': 2.755543799550434, 'learning_rate': 5.8903333333333336e-06, 'epoch': 1.8820000000000001}
{'loss': 0.8931, 'grad_norm': 2.962254344637915, 'learning_rate': 5.889222222222223e-06, 'epoch': 1.8824}
{'loss': 0.8938, 'grad_norm': 3.0556162327223007, 'learning_rate': 5.888222222222223e-06, 'epoch': 1.8828}
{'loss': 0.9028, 'grad_norm': 2.8197548122038567, 'learning_rate': 5.887111111111111e-06, 'epoch': 1.8832}
{'loss': 0.9034, 'grad_norm': 3.0743630265113056, 'learning_rate': 5.886000000000001e-06, 'epoch': 1.8836}
{'loss': 0.9097, 'grad_norm': 2.7904183006841956, 'learning_rate': 5.884888888888889e-06, 'epoch': 1.884}
{'eval_valid_loss': 0.87744140625, 'eval_valid_runtime': 0.0905, 'eval_valid_samples_per_second': 1104.671, 'eval_valid_steps_per_second': 276.168, 'epoch': 1.884}
{'loss': 0.9038, 'grad_norm': 2.973645801233948, 'learning_rate': 5.883777777777778e-06, 'epoch': 1.8843999999999999}
{'loss': 0.9073, 'grad_norm': 3.127673274072645, 'learning_rate': 5.882666666666668e-06, 'epoch': 1.8848}
{'loss': 0.9085, 'grad_norm': 3.0023857107811187, 'learning_rate': 5.881555555555556e-06, 'epoch': 1.8852}
{'loss': 0.9114, 'grad_norm': 2.7546288752903574, 'learning_rate': 5.880444444444445e-06, 'epoch': 1.8856000000000002}
{'loss': 0.9011, 'grad_norm': 2.8679883810237596, 'learning_rate': 5.879333333333333e-06, 'epoch': 1.8860000000000001}
{'loss': 0.9027, 'grad_norm': 2.8019804468847704, 'learning_rate': 5.878222222222223e-06, 'epoch': 1.8864}
{'loss': 0.9048, 'grad_norm': 2.8343660924365666, 'learning_rate': 5.8771111111111114e-06, 'epoch': 1.8868}
{'loss': 0.9115, 'grad_norm': 3.114780022184754, 'learning_rate': 5.876000000000001e-06, 'epoch': 1.8872}
{'loss': 0.904, 'grad_norm': 2.911836300486515, 'learning_rate': 5.874888888888889e-06, 'epoch': 1.8876}
{'loss': 0.9048, 'grad_norm': 2.9910062445266417, 'learning_rate': 5.873777777777778e-06, 'epoch': 1.888}
{'eval_valid_loss': 0.875, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1120.199, 'eval_valid_steps_per_second': 280.05, 'epoch': 1.888}
{'loss': 0.9088, 'grad_norm': 2.7025476180829786, 'learning_rate': 5.872666666666668e-06, 'epoch': 1.8883999999999999}
{'loss': 0.8954, 'grad_norm': 2.865344783265346, 'learning_rate': 5.871555555555556e-06, 'epoch': 1.8888}
{'loss': 0.9017, 'grad_norm': 2.906219605317686, 'learning_rate': 5.870444444444445e-06, 'epoch': 1.8892}
{'loss': 0.8939, 'grad_norm': 3.0518468248737376, 'learning_rate': 5.869333333333333e-06, 'epoch': 1.8896}
{'loss': 0.9059, 'grad_norm': 3.244304875597233, 'learning_rate': 5.868222222222223e-06, 'epoch': 1.8900000000000001}
{'loss': 0.9067, 'grad_norm': 3.2368640168204936, 'learning_rate': 5.8671111111111115e-06, 'epoch': 1.8904}
{'loss': 0.9113, 'grad_norm': 3.0026069480086868, 'learning_rate': 5.866e-06, 'epoch': 1.8908}
{'loss': 0.9116, 'grad_norm': 2.8081061798185014, 'learning_rate': 5.864888888888889e-06, 'epoch': 1.8912}
{'loss': 0.8855, 'grad_norm': 2.831356036493177, 'learning_rate': 5.8637777777777785e-06, 'epoch': 1.8916}
{'loss': 0.898, 'grad_norm': 2.9645715319643653, 'learning_rate': 5.862666666666667e-06, 'epoch': 1.892}
{'eval_valid_loss': 0.8759765625, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.278, 'eval_valid_steps_per_second': 279.07, 'epoch': 1.892}
{'loss': 0.896, 'grad_norm': 2.750259538887625, 'learning_rate': 5.861555555555556e-06, 'epoch': 1.8923999999999999}
{'loss': 0.9119, 'grad_norm': 2.8658473965406177, 'learning_rate': 5.8604444444444455e-06, 'epoch': 1.8928}
{'loss': 0.9084, 'grad_norm': 3.1670247563543934, 'learning_rate': 5.859333333333333e-06, 'epoch': 1.8932}
{'loss': 0.9178, 'grad_norm': 2.7354108101946, 'learning_rate': 5.858222222222223e-06, 'epoch': 1.8936}
{'loss': 0.9138, 'grad_norm': 3.113593647024038, 'learning_rate': 5.857111111111111e-06, 'epoch': 1.8940000000000001}
{'loss': 0.9174, 'grad_norm': 2.8752273697715016, 'learning_rate': 5.856e-06, 'epoch': 1.8944}
{'loss': 0.9025, 'grad_norm': 2.6288901579365778, 'learning_rate': 5.854888888888889e-06, 'epoch': 1.8948}
{'loss': 0.9031, 'grad_norm': 3.0291454638893907, 'learning_rate': 5.853777777777779e-06, 'epoch': 1.8952}
{'loss': 0.903, 'grad_norm': 2.804386319324016, 'learning_rate': 5.852666666666667e-06, 'epoch': 1.8956}
{'loss': 0.895, 'grad_norm': 2.6201823678228573, 'learning_rate': 5.851555555555556e-06, 'epoch': 1.896}
{'eval_valid_loss': 0.87548828125, 'eval_valid_runtime': 0.0906, 'eval_valid_samples_per_second': 1104.058, 'eval_valid_steps_per_second': 276.014, 'epoch': 1.896}
{'loss': 0.9094, 'grad_norm': 2.848262539175955, 'learning_rate': 5.850444444444446e-06, 'epoch': 1.8963999999999999}
{'loss': 0.9032, 'grad_norm': 3.1140646050690592, 'learning_rate': 5.8493333333333335e-06, 'epoch': 1.8968}
{'loss': 0.9065, 'grad_norm': 3.0407501673247928, 'learning_rate': 5.848222222222223e-06, 'epoch': 1.8972}
{'loss': 0.8998, 'grad_norm': 3.132991129284576, 'learning_rate': 5.847111111111111e-06, 'epoch': 1.8976}
{'loss': 0.9113, 'grad_norm': 3.0426645873696074, 'learning_rate': 5.8460000000000004e-06, 'epoch': 1.8980000000000001}
{'loss': 0.9064, 'grad_norm': 2.66920259466018, 'learning_rate': 5.844888888888889e-06, 'epoch': 1.8984}
{'loss': 0.8993, 'grad_norm': 2.6930427614139507, 'learning_rate': 5.843777777777778e-06, 'epoch': 1.8988}
{'loss': 0.9053, 'grad_norm': 3.1641708826410877, 'learning_rate': 5.842666666666667e-06, 'epoch': 1.8992}
{'loss': 0.8983, 'grad_norm': 2.7828256183724163, 'learning_rate': 5.841555555555556e-06, 'epoch': 1.8996}
{'loss': 0.9206, 'grad_norm': 3.2596920638244, 'learning_rate': 5.840444444444445e-06, 'epoch': 1.9}
{'eval_valid_loss': 0.87451171875, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.652, 'eval_valid_steps_per_second': 278.413, 'epoch': 1.9}
{'loss': 0.9042, 'grad_norm': 3.3618587006434235, 'learning_rate': 5.8393333333333336e-06, 'epoch': 1.9003999999999999}
{'loss': 0.9027, 'grad_norm': 2.853286880212699, 'learning_rate': 5.838222222222223e-06, 'epoch': 1.9008}
{'loss': 0.9017, 'grad_norm': 2.831874816783393, 'learning_rate': 5.837111111111111e-06, 'epoch': 1.9012}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9131, 'grad_norm': 3.1818407158549094, 'learning_rate': 5.8360000000000005e-06, 'epoch': 1.9016}
{'loss': 0.8923, 'grad_norm': 3.0173707267326098, 'learning_rate': 5.834888888888889e-06, 'epoch': 1.9020000000000001}
{'loss': 0.9077, 'grad_norm': 3.2008104568351876, 'learning_rate': 5.833777777777778e-06, 'epoch': 1.9024}
{'loss': 0.8946, 'grad_norm': 2.680826628989565, 'learning_rate': 5.8326666666666675e-06, 'epoch': 1.9028}
{'loss': 0.894, 'grad_norm': 2.9817658248037504, 'learning_rate': 5.831555555555556e-06, 'epoch': 1.9032}
{'loss': 0.8893, 'grad_norm': 2.92577230658393, 'learning_rate': 5.830444444444445e-06, 'epoch': 1.9036}
{'loss': 0.902, 'grad_norm': 3.0003040378206074, 'learning_rate': 5.829333333333334e-06, 'epoch': 1.904}
{'eval_valid_loss': 0.876953125, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1109.214, 'eval_valid_steps_per_second': 277.303, 'epoch': 1.904}
{'loss': 0.8991, 'grad_norm': 2.8377191711006726, 'learning_rate': 5.828222222222223e-06, 'epoch': 1.9043999999999999}
{'loss': 0.8996, 'grad_norm': 3.131918268218872, 'learning_rate': 5.827111111111111e-06, 'epoch': 1.9048}
{'loss': 0.9008, 'grad_norm': 3.241178617577755, 'learning_rate': 5.826000000000001e-06, 'epoch': 1.9052}
{'loss': 0.8981, 'grad_norm': 2.9887083758108512, 'learning_rate': 5.8248888888888885e-06, 'epoch': 1.9056}
{'loss': 0.9151, 'grad_norm': 2.872304668807044, 'learning_rate': 5.823777777777778e-06, 'epoch': 1.9060000000000001}
{'loss': 0.9059, 'grad_norm': 2.798875050973255, 'learning_rate': 5.822666666666668e-06, 'epoch': 1.9064}
{'loss': 0.9206, 'grad_norm': 3.0235426734157405, 'learning_rate': 5.8215555555555555e-06, 'epoch': 1.9068}
{'loss': 0.9035, 'grad_norm': 2.9313416350789865, 'learning_rate': 5.820444444444445e-06, 'epoch': 1.9072}
{'loss': 0.9169, 'grad_norm': 2.92509141640575, 'learning_rate': 5.819333333333334e-06, 'epoch': 1.9076}
{'loss': 0.9007, 'grad_norm': 2.8499122861280854, 'learning_rate': 5.818222222222223e-06, 'epoch': 1.908}
{'eval_valid_loss': 0.875, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.747, 'eval_valid_steps_per_second': 279.687, 'epoch': 1.908}
{'loss': 0.8959, 'grad_norm': 2.86437843399125, 'learning_rate': 5.817111111111111e-06, 'epoch': 1.9083999999999999}
{'loss': 0.9076, 'grad_norm': 3.198175420864992, 'learning_rate': 5.816000000000001e-06, 'epoch': 1.9088}
{'loss': 0.9164, 'grad_norm': 2.880342226325137, 'learning_rate': 5.81488888888889e-06, 'epoch': 1.9092}
{'loss': 0.9028, 'grad_norm': 3.2158536193957774, 'learning_rate': 5.813777777777778e-06, 'epoch': 1.9096}
{'loss': 0.907, 'grad_norm': 2.6943307940830468, 'learning_rate': 5.812666666666668e-06, 'epoch': 1.9100000000000001}
{'loss': 0.9086, 'grad_norm': 2.7969241111322223, 'learning_rate': 5.8115555555555556e-06, 'epoch': 1.9104}
{'loss': 0.9228, 'grad_norm': 2.6706396392467457, 'learning_rate': 5.810444444444445e-06, 'epoch': 1.9108}
{'loss': 0.9064, 'grad_norm': 2.8968275333783087, 'learning_rate': 5.809333333333334e-06, 'epoch': 1.9112}
{'loss': 0.8913, 'grad_norm': 2.725334464773, 'learning_rate': 5.8082222222222226e-06, 'epoch': 1.9116}
{'loss': 0.9105, 'grad_norm': 3.2678598030564903, 'learning_rate': 5.807111111111111e-06, 'epoch': 1.912}
{'eval_valid_loss': 0.875, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.819, 'eval_valid_steps_per_second': 281.955, 'epoch': 1.912}
{'loss': 0.8946, 'grad_norm': 2.6892219892924167, 'learning_rate': 5.806000000000001e-06, 'epoch': 1.9123999999999999}
{'loss': 0.8992, 'grad_norm': 3.109072416525342, 'learning_rate': 5.8048888888888895e-06, 'epoch': 1.9127999999999998}
{'loss': 0.9061, 'grad_norm': 3.13003174045118, 'learning_rate': 5.803777777777778e-06, 'epoch': 1.9132}
{'loss': 0.8941, 'grad_norm': 2.7704129933968624, 'learning_rate': 5.802666666666668e-06, 'epoch': 1.9136}
{'loss': 0.8994, 'grad_norm': 3.086979084241229, 'learning_rate': 5.801555555555556e-06, 'epoch': 1.9140000000000001}
{'loss': 0.8959, 'grad_norm': 2.636677245767094, 'learning_rate': 5.800444444444445e-06, 'epoch': 1.9144}
{'loss': 0.9, 'grad_norm': 3.046751939293562, 'learning_rate': 5.799333333333333e-06, 'epoch': 1.9148}
{'loss': 0.8981, 'grad_norm': 2.7467985591732993, 'learning_rate': 5.798222222222223e-06, 'epoch': 1.9152}
{'loss': 0.8966, 'grad_norm': 3.1136336276751764, 'learning_rate': 5.797111111111111e-06, 'epoch': 1.9156}
{'loss': 0.9071, 'grad_norm': 3.2309809950928123, 'learning_rate': 5.796000000000001e-06, 'epoch': 1.916}
{'eval_valid_loss': 0.87744140625, 'eval_valid_runtime': 0.0885, 'eval_valid_samples_per_second': 1130.19, 'eval_valid_steps_per_second': 282.547, 'epoch': 1.916}
{'loss': 0.9112, 'grad_norm': 2.8517786754030245, 'learning_rate': 5.79488888888889e-06, 'epoch': 1.9163999999999999}
{'loss': 0.8946, 'grad_norm': 2.860032344781935, 'learning_rate': 5.793777777777778e-06, 'epoch': 1.9167999999999998}
{'loss': 0.9065, 'grad_norm': 2.865246378139533, 'learning_rate': 5.792666666666668e-06, 'epoch': 1.9172}
{'loss': 0.9021, 'grad_norm': 2.8643332522498546, 'learning_rate': 5.791555555555556e-06, 'epoch': 1.9176}
{'loss': 0.908, 'grad_norm': 2.7731671913484717, 'learning_rate': 5.790444444444445e-06, 'epoch': 1.9180000000000001}
{'loss': 0.9137, 'grad_norm': 2.8437120623730374, 'learning_rate': 5.789333333333333e-06, 'epoch': 1.9184}
{'loss': 0.909, 'grad_norm': 2.8369945941466126, 'learning_rate': 5.788222222222223e-06, 'epoch': 1.9188}
{'loss': 0.91, 'grad_norm': 2.874869094853847, 'learning_rate': 5.7871111111111115e-06, 'epoch': 1.9192}
{'loss': 0.907, 'grad_norm': 2.7197397118735087, 'learning_rate': 5.786e-06, 'epoch': 1.9196}
{'loss': 0.9039, 'grad_norm': 2.7967611641334096, 'learning_rate': 5.78488888888889e-06, 'epoch': 1.92}
{'eval_valid_loss': 0.87451171875, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1128.724, 'eval_valid_steps_per_second': 282.181, 'epoch': 1.92}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9078, 'grad_norm': 2.949642580411509, 'learning_rate': 5.7837777777777784e-06, 'epoch': 1.9203999999999999}
{'loss': 0.8889, 'grad_norm': 3.0715494842983166, 'learning_rate': 5.782666666666667e-06, 'epoch': 1.9207999999999998}
{'loss': 0.9002, 'grad_norm': 2.985566405706972, 'learning_rate': 5.781555555555556e-06, 'epoch': 1.9212}
{'loss': 0.9003, 'grad_norm': 2.8113291263512283, 'learning_rate': 5.780444444444445e-06, 'epoch': 1.9216}
{'loss': 0.9004, 'grad_norm': 3.007995907034397, 'learning_rate': 5.779333333333333e-06, 'epoch': 1.9220000000000002}
{'loss': 0.9115, 'grad_norm': 3.0354763632991237, 'learning_rate': 5.778222222222223e-06, 'epoch': 1.9224}
{'loss': 0.9013, 'grad_norm': 3.0288162194450745, 'learning_rate': 5.777222222222223e-06, 'epoch': 1.9228}
{'loss': 0.9016, 'grad_norm': 3.4078668466718196, 'learning_rate': 5.776111111111111e-06, 'epoch': 1.9232}
{'loss': 0.9146, 'grad_norm': 3.0207942280485387, 'learning_rate': 5.775000000000001e-06, 'epoch': 1.9236}
{'loss': 0.9041, 'grad_norm': 3.1164097879408357, 'learning_rate': 5.7738888888888885e-06, 'epoch': 1.924}
{'eval_valid_loss': 0.875, 'eval_valid_runtime': 0.091, 'eval_valid_samples_per_second': 1099.007, 'eval_valid_steps_per_second': 274.752, 'epoch': 1.924}
{'loss': 0.9117, 'grad_norm': 3.0830366769478004, 'learning_rate': 5.772777777777778e-06, 'epoch': 1.9243999999999999}
{'loss': 0.9074, 'grad_norm': 2.968263867379497, 'learning_rate': 5.771666666666668e-06, 'epoch': 1.9247999999999998}
{'loss': 0.8956, 'grad_norm': 2.91466582447171, 'learning_rate': 5.770555555555556e-06, 'epoch': 1.9252}
{'loss': 0.9155, 'grad_norm': 2.7821194757678445, 'learning_rate': 5.769444444444445e-06, 'epoch': 1.9256}
{'loss': 0.9196, 'grad_norm': 3.1109375789379836, 'learning_rate': 5.768333333333334e-06, 'epoch': 1.9260000000000002}
{'loss': 0.9108, 'grad_norm': 2.802653050322747, 'learning_rate': 5.767222222222223e-06, 'epoch': 1.9264000000000001}
{'loss': 0.9016, 'grad_norm': 3.318767017385528, 'learning_rate': 5.766111111111111e-06, 'epoch': 1.9268}
{'loss': 0.8888, 'grad_norm': 2.9199628948277523, 'learning_rate': 5.765000000000001e-06, 'epoch': 1.9272}
{'loss': 0.9069, 'grad_norm': 3.194308498618084, 'learning_rate': 5.7638888888888886e-06, 'epoch': 1.9276}
{'loss': 0.9012, 'grad_norm': 3.00838194571833, 'learning_rate': 5.762777777777778e-06, 'epoch': 1.928}
{'eval_valid_loss': 0.875, 'eval_valid_runtime': 0.0911, 'eval_valid_samples_per_second': 1097.913, 'eval_valid_steps_per_second': 274.478, 'epoch': 1.928}
{'loss': 0.9005, 'grad_norm': 2.666065643176699, 'learning_rate': 5.761666666666668e-06, 'epoch': 1.9284}
{'loss': 0.9024, 'grad_norm': 2.9693116710785095, 'learning_rate': 5.7605555555555556e-06, 'epoch': 1.9287999999999998}
{'loss': 0.9062, 'grad_norm': 2.9773078245639595, 'learning_rate': 5.759444444444445e-06, 'epoch': 1.9292}
{'loss': 0.9064, 'grad_norm': 2.862410544367789, 'learning_rate': 5.758333333333334e-06, 'epoch': 1.9296}
{'loss': 0.9084, 'grad_norm': 3.0681646857120484, 'learning_rate': 5.7572222222222225e-06, 'epoch': 1.9300000000000002}
{'loss': 0.8864, 'grad_norm': 2.813066282483758, 'learning_rate': 5.756111111111111e-06, 'epoch': 1.9304000000000001}
{'loss': 0.9105, 'grad_norm': 3.0283251150044554, 'learning_rate': 5.755000000000001e-06, 'epoch': 1.9308}
{'loss': 0.8878, 'grad_norm': 2.583344666522813, 'learning_rate': 5.753888888888889e-06, 'epoch': 1.9312}
{'loss': 0.9049, 'grad_norm': 2.961436357974595, 'learning_rate': 5.752777777777778e-06, 'epoch': 1.9316}
{'loss': 0.9042, 'grad_norm': 3.3080130814042263, 'learning_rate': 5.751666666666668e-06, 'epoch': 1.932}
{'eval_valid_loss': 0.8740234375, 'eval_valid_runtime': 0.0911, 'eval_valid_samples_per_second': 1097.276, 'eval_valid_steps_per_second': 274.319, 'epoch': 1.932}
{'loss': 0.9122, 'grad_norm': 2.976987042526042, 'learning_rate': 5.750555555555556e-06, 'epoch': 1.9324}
{'loss': 0.9046, 'grad_norm': 2.7700633464638167, 'learning_rate': 5.749444444444445e-06, 'epoch': 1.9327999999999999}
{'loss': 0.8888, 'grad_norm': 2.861844398349921, 'learning_rate': 5.748333333333334e-06, 'epoch': 1.9332}
{'loss': 0.9046, 'grad_norm': 2.7061132110137334, 'learning_rate': 5.747222222222223e-06, 'epoch': 1.9336}
{'loss': 0.9163, 'grad_norm': 2.833736500853032, 'learning_rate': 5.746111111111111e-06, 'epoch': 1.9340000000000002}
{'loss': 0.8998, 'grad_norm': 2.7444521146854584, 'learning_rate': 5.745000000000001e-06, 'epoch': 1.9344000000000001}
{'loss': 0.8969, 'grad_norm': 2.8887336316835515, 'learning_rate': 5.743888888888889e-06, 'epoch': 1.9348}
{'loss': 0.9042, 'grad_norm': 2.613316539987994, 'learning_rate': 5.742777777777778e-06, 'epoch': 1.9352}
{'loss': 0.9038, 'grad_norm': 3.143468357309788, 'learning_rate': 5.741666666666668e-06, 'epoch': 1.9356}
{'loss': 0.9116, 'grad_norm': 2.8873436939159607, 'learning_rate': 5.740555555555556e-06, 'epoch': 1.936}
{'eval_valid_loss': 0.87451171875, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.685, 'eval_valid_steps_per_second': 278.671, 'epoch': 1.936}
{'loss': 0.8982, 'grad_norm': 2.8768478705788434, 'learning_rate': 5.739444444444445e-06, 'epoch': 1.9364}
{'loss': 0.8977, 'grad_norm': 2.903723243856136, 'learning_rate': 5.738333333333333e-06, 'epoch': 1.9367999999999999}
{'loss': 0.9046, 'grad_norm': 2.902706031214321, 'learning_rate': 5.737222222222223e-06, 'epoch': 1.9372}
{'loss': 0.8949, 'grad_norm': 2.8715773090906858, 'learning_rate': 5.7361111111111114e-06, 'epoch': 1.9376}
{'loss': 0.9057, 'grad_norm': 2.892699018240631, 'learning_rate': 5.735e-06, 'epoch': 1.938}
{'loss': 0.8997, 'grad_norm': 2.8855622056321684, 'learning_rate': 5.733888888888889e-06, 'epoch': 1.9384000000000001}
{'loss': 0.9003, 'grad_norm': 3.1801340225455794, 'learning_rate': 5.732777777777778e-06, 'epoch': 1.9388}
{'loss': 0.8974, 'grad_norm': 3.1540184535869744, 'learning_rate': 5.731666666666668e-06, 'epoch': 1.9392}
{'loss': 0.8977, 'grad_norm': 3.04071247253369, 'learning_rate': 5.730555555555556e-06, 'epoch': 1.9396}
{'loss': 0.9039, 'grad_norm': 3.069390311322388, 'learning_rate': 5.729444444444445e-06, 'epoch': 1.94}
{'eval_valid_loss': 0.87353515625, 'eval_valid_runtime': 0.0911, 'eval_valid_samples_per_second': 1098.264, 'eval_valid_steps_per_second': 274.566, 'epoch': 1.94}
{'loss': 0.9078, 'grad_norm': 2.994064578927667, 'learning_rate': 5.728333333333333e-06, 'epoch': 1.9404}
{'loss': 0.9283, 'grad_norm': 2.6384352458622082, 'learning_rate': 5.727222222222223e-06, 'epoch': 1.9407999999999999}
{'loss': 0.9121, 'grad_norm': 3.039645102389459, 'learning_rate': 5.7261111111111115e-06, 'epoch': 1.9412}
{'loss': 0.911, 'grad_norm': 3.0172403584793317, 'learning_rate': 5.725e-06, 'epoch': 1.9416}
{'loss': 0.9064, 'grad_norm': 3.100763559134782, 'learning_rate': 5.723888888888889e-06, 'epoch': 1.942}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9162, 'grad_norm': 3.2763046599787504, 'learning_rate': 5.7227777777777785e-06, 'epoch': 1.9424000000000001}
{'loss': 0.8944, 'grad_norm': 3.1224392985207783, 'learning_rate': 5.721666666666667e-06, 'epoch': 1.9428}
{'loss': 0.9015, 'grad_norm': 3.3326685252435957, 'learning_rate': 5.720555555555556e-06, 'epoch': 1.9432}
{'loss': 0.9082, 'grad_norm': 2.891881813864083, 'learning_rate': 5.7194444444444455e-06, 'epoch': 1.9436}
{'loss': 0.8962, 'grad_norm': 2.91959749289051, 'learning_rate': 5.718333333333333e-06, 'epoch': 1.944}
{'eval_valid_loss': 0.8740234375, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.563, 'eval_valid_steps_per_second': 278.891, 'epoch': 1.944}
{'loss': 0.9097, 'grad_norm': 2.760091094244865, 'learning_rate': 5.717222222222223e-06, 'epoch': 1.9444}
{'loss': 0.8961, 'grad_norm': 2.9541989548447147, 'learning_rate': 5.716111111111111e-06, 'epoch': 1.9447999999999999}
{'loss': 0.893, 'grad_norm': 3.052972560969802, 'learning_rate': 5.715e-06, 'epoch': 1.9452}
{'loss': 0.8974, 'grad_norm': 2.652871811089475, 'learning_rate': 5.713888888888889e-06, 'epoch': 1.9456}
{'loss': 0.8968, 'grad_norm': 3.04447208432214, 'learning_rate': 5.712777777777779e-06, 'epoch': 1.946}
{'loss': 0.8984, 'grad_norm': 2.8646111932035527, 'learning_rate': 5.711666666666667e-06, 'epoch': 1.9464000000000001}
{'loss': 0.9007, 'grad_norm': 3.0276923095867776, 'learning_rate': 5.710555555555556e-06, 'epoch': 1.9468}
{'loss': 0.9004, 'grad_norm': 3.133738573522928, 'learning_rate': 5.709444444444446e-06, 'epoch': 1.9472}
{'loss': 0.8963, 'grad_norm': 3.097465490021172, 'learning_rate': 5.7083333333333335e-06, 'epoch': 1.9476}
{'loss': 0.9101, 'grad_norm': 2.8838416332771466, 'learning_rate': 5.707222222222223e-06, 'epoch': 1.948}
{'eval_valid_loss': 0.87060546875, 'eval_valid_runtime': 0.0881, 'eval_valid_samples_per_second': 1135.194, 'eval_valid_steps_per_second': 283.799, 'epoch': 1.948}
{'loss': 0.9058, 'grad_norm': 3.250492370428694, 'learning_rate': 5.706111111111111e-06, 'epoch': 1.9484}
{'loss': 0.8933, 'grad_norm': 2.9386850056829643, 'learning_rate': 5.7050000000000004e-06, 'epoch': 1.9487999999999999}
{'loss': 0.9159, 'grad_norm': 2.965793492682977, 'learning_rate': 5.703888888888889e-06, 'epoch': 1.9492}
{'loss': 0.908, 'grad_norm': 3.003075404159557, 'learning_rate': 5.702777777777778e-06, 'epoch': 1.9496}
{'loss': 0.8908, 'grad_norm': 2.7463852848368946, 'learning_rate': 5.701666666666667e-06, 'epoch': 1.95}
{'loss': 0.8969, 'grad_norm': 3.4056880732404906, 'learning_rate': 5.700555555555556e-06, 'epoch': 1.9504000000000001}
{'loss': 0.8992, 'grad_norm': 2.7798719421058395, 'learning_rate': 5.699444444444445e-06, 'epoch': 1.9508}
{'loss': 0.8955, 'grad_norm': 2.6284151738077717, 'learning_rate': 5.6983333333333336e-06, 'epoch': 1.9512}
{'loss': 0.9078, 'grad_norm': 3.100757657800156, 'learning_rate': 5.697222222222223e-06, 'epoch': 1.9516}
{'loss': 0.9105, 'grad_norm': 2.8982542653529126, 'learning_rate': 5.696111111111111e-06, 'epoch': 1.952}
{'eval_valid_loss': 0.873046875, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.825, 'eval_valid_steps_per_second': 279.456, 'epoch': 1.952}
{'loss': 0.886, 'grad_norm': 3.3163344574291584, 'learning_rate': 5.6950000000000005e-06, 'epoch': 1.9524}
{'loss': 0.9017, 'grad_norm': 2.880184267758663, 'learning_rate': 5.69388888888889e-06, 'epoch': 1.9527999999999999}
{'loss': 0.9018, 'grad_norm': 2.8636294851832877, 'learning_rate': 5.692777777777778e-06, 'epoch': 1.9532}
{'loss': 0.901, 'grad_norm': 3.1008409290140384, 'learning_rate': 5.6916666666666675e-06, 'epoch': 1.9536}
{'loss': 0.9067, 'grad_norm': 2.696492636130669, 'learning_rate': 5.690555555555556e-06, 'epoch': 1.954}
{'loss': 0.9166, 'grad_norm': 2.877088928125141, 'learning_rate': 5.689444444444445e-06, 'epoch': 1.9544000000000001}
{'loss': 0.9053, 'grad_norm': 3.052136333947128, 'learning_rate': 5.688333333333334e-06, 'epoch': 1.9548}
{'loss': 0.8938, 'grad_norm': 2.7184443904948563, 'learning_rate': 5.687222222222223e-06, 'epoch': 1.9552}
{'loss': 0.9042, 'grad_norm': 2.694621741195784, 'learning_rate': 5.686111111111111e-06, 'epoch': 1.9556}
{'loss': 0.8953, 'grad_norm': 2.847073237596951, 'learning_rate': 5.685000000000001e-06, 'epoch': 1.956}
{'eval_valid_loss': 0.873046875, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1115.305, 'eval_valid_steps_per_second': 278.826, 'epoch': 1.956}
{'loss': 0.8935, 'grad_norm': 3.054119886653287, 'learning_rate': 5.68388888888889e-06, 'epoch': 1.9564}
{'loss': 0.9119, 'grad_norm': 2.924806908812022, 'learning_rate': 5.682777777777778e-06, 'epoch': 1.9567999999999999}
{'loss': 0.9055, 'grad_norm': 2.748412280810071, 'learning_rate': 5.681666666666668e-06, 'epoch': 1.9572}
{'loss': 0.9059, 'grad_norm': 3.1072003892840123, 'learning_rate': 5.6805555555555555e-06, 'epoch': 1.9576}
{'loss': 0.8954, 'grad_norm': 3.106695736502118, 'learning_rate': 5.679444444444445e-06, 'epoch': 1.958}
{'loss': 0.9065, 'grad_norm': 3.2109478488579, 'learning_rate': 5.678333333333334e-06, 'epoch': 1.9584000000000001}
{'loss': 0.8989, 'grad_norm': 3.1136098518362987, 'learning_rate': 5.6772222222222225e-06, 'epoch': 1.9588}
{'loss': 0.9055, 'grad_norm': 3.0800879943341206, 'learning_rate': 5.676111111111111e-06, 'epoch': 1.9592}
{'loss': 0.9015, 'grad_norm': 2.793660090320352, 'learning_rate': 5.675000000000001e-06, 'epoch': 1.9596}
{'loss': 0.9023, 'grad_norm': 2.9148503279170828, 'learning_rate': 5.67388888888889e-06, 'epoch': 1.96}
{'eval_valid_loss': 0.8740234375, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1121.691, 'eval_valid_steps_per_second': 280.423, 'epoch': 1.96}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.8968, 'grad_norm': 2.8495738685492746, 'learning_rate': 5.672777777777778e-06, 'epoch': 1.9604}
{'loss': 0.8937, 'grad_norm': 2.780884429394385, 'learning_rate': 5.671666666666668e-06, 'epoch': 1.9607999999999999}
{'loss': 0.8903, 'grad_norm': 2.7572348532728026, 'learning_rate': 5.6705555555555556e-06, 'epoch': 1.9612}
{'loss': 0.907, 'grad_norm': 2.758357704670545, 'learning_rate': 5.669444444444445e-06, 'epoch': 1.9616}
{'loss': 0.896, 'grad_norm': 3.000026901442394, 'learning_rate': 5.668333333333334e-06, 'epoch': 1.962}
{'loss': 0.9009, 'grad_norm': 2.7465748752076915, 'learning_rate': 5.6672222222222225e-06, 'epoch': 1.9624000000000001}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8985, 'grad_norm': 3.0183551565649056, 'learning_rate': 5.666222222222223e-06, 'epoch': 1.9628}
{'loss': 0.901, 'grad_norm': 3.0465193809927076, 'learning_rate': 5.665111111111112e-06, 'epoch': 1.9632}
{'loss': 0.894, 'grad_norm': 3.213478642138821, 'learning_rate': 5.664e-06, 'epoch': 1.9636}
{'loss': 0.9059, 'grad_norm': 3.0083115993429694, 'learning_rate': 5.662888888888889e-06, 'epoch': 1.964}
{'eval_valid_loss': 0.87646484375, 'eval_valid_runtime': 0.089, 'eval_valid_samples_per_second': 1123.466, 'eval_valid_steps_per_second': 280.867, 'epoch': 1.964}
{'loss': 0.907, 'grad_norm': 3.1536078488839396, 'learning_rate': 5.661777777777779e-06, 'epoch': 1.9644}
{'loss': 0.9088, 'grad_norm': 2.954873510967993, 'learning_rate': 5.660666666666667e-06, 'epoch': 1.9647999999999999}
{'loss': 0.9084, 'grad_norm': 2.8409771920191282, 'learning_rate': 5.659555555555556e-06, 'epoch': 1.9651999999999998}
{'loss': 0.8956, 'grad_norm': 2.8525880093082407, 'learning_rate': 5.6584444444444456e-06, 'epoch': 1.9656}
{'loss': 0.8904, 'grad_norm': 3.3181684505328772, 'learning_rate': 5.6573333333333334e-06, 'epoch': 1.966}
{'loss': 0.901, 'grad_norm': 3.2773682021637214, 'learning_rate': 5.656222222222223e-06, 'epoch': 1.9664000000000001}
{'loss': 0.9022, 'grad_norm': 3.1264098801697346, 'learning_rate': 5.655111111111111e-06, 'epoch': 1.9668}
{'loss': 0.91, 'grad_norm': 3.372591969838672, 'learning_rate': 5.654e-06, 'epoch': 1.9672}
{'loss': 0.8901, 'grad_norm': 3.104192277090089, 'learning_rate': 5.652888888888889e-06, 'epoch': 1.9676}
{'loss': 0.895, 'grad_norm': 2.7811559596932804, 'learning_rate': 5.651777777777778e-06, 'epoch': 1.968}
{'eval_valid_loss': 0.8720703125, 'eval_valid_runtime': 0.0952, 'eval_valid_samples_per_second': 1050.604, 'eval_valid_steps_per_second': 262.651, 'epoch': 1.968}
{'loss': 0.8873, 'grad_norm': 3.158821889906876, 'learning_rate': 5.650666666666667e-06, 'epoch': 1.9684}
{'loss': 0.9069, 'grad_norm': 3.036641517247833, 'learning_rate': 5.649555555555556e-06, 'epoch': 1.9687999999999999}
{'loss': 0.9131, 'grad_norm': 2.737304464306858, 'learning_rate': 5.648444444444446e-06, 'epoch': 1.9691999999999998}
{'loss': 0.8932, 'grad_norm': 3.020822246575016, 'learning_rate': 5.6473333333333335e-06, 'epoch': 1.9696}
{'loss': 0.9031, 'grad_norm': 3.492608490899259, 'learning_rate': 5.646222222222223e-06, 'epoch': 1.97}
{'loss': 0.9012, 'grad_norm': 3.0529617058963456, 'learning_rate': 5.645111111111111e-06, 'epoch': 1.9704000000000002}
{'loss': 0.9056, 'grad_norm': 2.7308843948468757, 'learning_rate': 5.6440000000000005e-06, 'epoch': 1.9708}
{'loss': 0.8896, 'grad_norm': 2.862915983750091, 'learning_rate': 5.642888888888889e-06, 'epoch': 1.9712}
{'loss': 0.9023, 'grad_norm': 3.2841471370702604, 'learning_rate': 5.641777777777778e-06, 'epoch': 1.9716}
{'loss': 0.8832, 'grad_norm': 2.9147960567943443, 'learning_rate': 5.6406666666666675e-06, 'epoch': 1.972}
{'eval_valid_loss': 0.87548828125, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1128.153, 'eval_valid_steps_per_second': 282.038, 'epoch': 1.972}
{'loss': 0.9043, 'grad_norm': 3.0277501774812188, 'learning_rate': 5.639555555555556e-06, 'epoch': 1.9724}
{'loss': 0.9073, 'grad_norm': 3.0206212078901813, 'learning_rate': 5.638444444444445e-06, 'epoch': 1.9727999999999999}
{'loss': 0.8914, 'grad_norm': 2.6106492032332596, 'learning_rate': 5.637333333333334e-06, 'epoch': 1.9731999999999998}
{'loss': 0.911, 'grad_norm': 2.725725947371471, 'learning_rate': 5.636222222222223e-06, 'epoch': 1.9736}
{'loss': 0.8844, 'grad_norm': 2.765100698827563, 'learning_rate': 5.635111111111111e-06, 'epoch': 1.974}
{'loss': 0.9065, 'grad_norm': 3.061940628659688, 'learning_rate': 5.634000000000001e-06, 'epoch': 1.9744000000000002}
{'loss': 0.8926, 'grad_norm': 3.2176913214756926, 'learning_rate': 5.6328888888888885e-06, 'epoch': 1.9748}
{'loss': 0.9062, 'grad_norm': 3.057972831596655, 'learning_rate': 5.631777777777778e-06, 'epoch': 1.9752}
{'loss': 0.9039, 'grad_norm': 2.768506106534055, 'learning_rate': 5.630666666666668e-06, 'epoch': 1.9756}
{'loss': 0.8989, 'grad_norm': 2.665631215534365, 'learning_rate': 5.629555555555556e-06, 'epoch': 1.976}
{'eval_valid_loss': 0.873046875, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.04, 'eval_valid_steps_per_second': 279.51, 'epoch': 1.976}
{'loss': 0.8974, 'grad_norm': 2.776069531232301, 'learning_rate': 5.628444444444445e-06, 'epoch': 1.9764}
{'loss': 0.9083, 'grad_norm': 2.9821238290768695, 'learning_rate': 5.627333333333334e-06, 'epoch': 1.9768}
{'loss': 0.8993, 'grad_norm': 3.03229883080207, 'learning_rate': 5.626222222222223e-06, 'epoch': 1.9771999999999998}
{'loss': 0.906, 'grad_norm': 2.8936250119992653, 'learning_rate': 5.625111111111111e-06, 'epoch': 1.9776}
{'loss': 0.8982, 'grad_norm': 2.9794427948255082, 'learning_rate': 5.624000000000001e-06, 'epoch': 1.978}
{'loss': 0.9016, 'grad_norm': 2.771205164882635, 'learning_rate': 5.6228888888888886e-06, 'epoch': 1.9784000000000002}
{'loss': 0.8969, 'grad_norm': 2.997108645674103, 'learning_rate': 5.621777777777778e-06, 'epoch': 1.9788000000000001}
{'loss': 0.8994, 'grad_norm': 3.0622223027502975, 'learning_rate': 5.620666666666668e-06, 'epoch': 1.9792}
{'loss': 0.9114, 'grad_norm': 2.921611427209992, 'learning_rate': 5.6195555555555555e-06, 'epoch': 1.9796}
{'loss': 0.8942, 'grad_norm': 2.8475284207594447, 'learning_rate': 5.618444444444445e-06, 'epoch': 1.98}
{'eval_valid_loss': 0.87158203125, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1120.291, 'eval_valid_steps_per_second': 280.073, 'epoch': 1.98}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9062, 'grad_norm': 3.189044998696585, 'learning_rate': 5.617333333333334e-06, 'epoch': 1.9804}
{'loss': 0.8983, 'grad_norm': 2.644841399945319, 'learning_rate': 5.6162222222222225e-06, 'epoch': 1.9808}
{'loss': 0.8937, 'grad_norm': 2.8713643835282583, 'learning_rate': 5.615111111111111e-06, 'epoch': 1.9811999999999999}
{'loss': 0.9121, 'grad_norm': 3.0530242977499906, 'learning_rate': 5.614000000000001e-06, 'epoch': 1.9816}
{'loss': 0.8912, 'grad_norm': 2.723939973369354, 'learning_rate': 5.612888888888889e-06, 'epoch': 1.982}
{'loss': 0.906, 'grad_norm': 2.885231631275802, 'learning_rate': 5.611777777777778e-06, 'epoch': 1.9824000000000002}
{'loss': 0.8973, 'grad_norm': 2.8403673594420336, 'learning_rate': 5.610666666666668e-06, 'epoch': 1.9828000000000001}
{'loss': 0.9078, 'grad_norm': 2.884796493664714, 'learning_rate': 5.609555555555556e-06, 'epoch': 1.9832}
{'loss': 0.8981, 'grad_norm': 2.5909491894096766, 'learning_rate': 5.608444444444445e-06, 'epoch': 1.9836}
{'loss': 0.8864, 'grad_norm': 2.989687280919061, 'learning_rate': 5.607333333333334e-06, 'epoch': 1.984}
{'eval_valid_loss': 0.8720703125, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.136, 'eval_valid_steps_per_second': 279.034, 'epoch': 1.984}
{'loss': 0.8911, 'grad_norm': 2.8734758421189555, 'learning_rate': 5.606222222222223e-06, 'epoch': 1.9844}
{'loss': 0.9002, 'grad_norm': 3.152577671603728, 'learning_rate': 5.605111111111111e-06, 'epoch': 1.9848}
{'loss': 0.8972, 'grad_norm': 3.1036249601239763, 'learning_rate': 5.604000000000001e-06, 'epoch': 1.9851999999999999}
{'loss': 0.8941, 'grad_norm': 2.739687478270106, 'learning_rate': 5.602888888888889e-06, 'epoch': 1.9856}
{'loss': 0.8994, 'grad_norm': 2.966783143062593, 'learning_rate': 5.601777777777778e-06, 'epoch': 1.986}
{'loss': 0.9049, 'grad_norm': 3.0255018476084348, 'learning_rate': 5.600666666666668e-06, 'epoch': 1.9864000000000002}
{'loss': 0.8958, 'grad_norm': 2.8669692639857844, 'learning_rate': 5.599555555555556e-06, 'epoch': 1.9868000000000001}
{'loss': 0.8991, 'grad_norm': 2.8574376865244138, 'learning_rate': 5.598444444444445e-06, 'epoch': 1.9872}
{'loss': 0.9009, 'grad_norm': 2.855395408763509, 'learning_rate': 5.597333333333333e-06, 'epoch': 1.9876}
{'loss': 0.9062, 'grad_norm': 3.12318817067151, 'learning_rate': 5.596222222222223e-06, 'epoch': 1.988}
{'eval_valid_loss': 0.873046875, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1120.516, 'eval_valid_steps_per_second': 280.129, 'epoch': 1.988}
{'loss': 0.9076, 'grad_norm': 3.0185960157780722, 'learning_rate': 5.5951111111111114e-06, 'epoch': 1.9884}
{'loss': 0.9093, 'grad_norm': 2.897307193535563, 'learning_rate': 5.594e-06, 'epoch': 1.9888}
{'loss': 0.9084, 'grad_norm': 3.1255917179659924, 'learning_rate': 5.592888888888889e-06, 'epoch': 1.9891999999999999}
{'loss': 0.9026, 'grad_norm': 2.619331045717213, 'learning_rate': 5.591777777777778e-06, 'epoch': 1.9896}
{'loss': 0.8985, 'grad_norm': 2.6182904154287074, 'learning_rate': 5.590666666666668e-06, 'epoch': 1.99}
{'loss': 0.9058, 'grad_norm': 2.653482560640111, 'learning_rate': 5.589555555555556e-06, 'epoch': 1.9904}
{'loss': 0.9043, 'grad_norm': 2.6962905104542414, 'learning_rate': 5.588444444444445e-06, 'epoch': 1.9908000000000001}
{'loss': 0.904, 'grad_norm': 3.0017222884706696, 'learning_rate': 5.587333333333333e-06, 'epoch': 1.9912}
{'loss': 0.9102, 'grad_norm': 3.02460801091164, 'learning_rate': 5.586222222222223e-06, 'epoch': 1.9916}
{'loss': 0.9163, 'grad_norm': 3.1399447809970678, 'learning_rate': 5.5851111111111115e-06, 'epoch': 1.992}
{'eval_valid_loss': 0.87353515625, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.897, 'eval_valid_steps_per_second': 279.474, 'epoch': 1.992}
{'loss': 0.8918, 'grad_norm': 2.557561138947423, 'learning_rate': 5.584e-06, 'epoch': 1.9924}
{'loss': 0.9036, 'grad_norm': 2.947416732686127, 'learning_rate': 5.582888888888889e-06, 'epoch': 1.9928}
{'loss': 0.9019, 'grad_norm': 3.0143306800736815, 'learning_rate': 5.5817777777777785e-06, 'epoch': 1.9931999999999999}
{'loss': 0.8966, 'grad_norm': 3.1810696852419413, 'learning_rate': 5.580666666666667e-06, 'epoch': 1.9936}
{'loss': 0.8969, 'grad_norm': 2.8755785525806927, 'learning_rate': 5.579555555555556e-06, 'epoch': 1.994}
{'loss': 0.8854, 'grad_norm': 3.05453094705655, 'learning_rate': 5.5784444444444455e-06, 'epoch': 1.9944}
{'loss': 0.9059, 'grad_norm': 3.1111222251105097, 'learning_rate': 5.577333333333333e-06, 'epoch': 1.9948000000000001}
{'loss': 0.9014, 'grad_norm': 3.0284326576176084, 'learning_rate': 5.576222222222223e-06, 'epoch': 1.9952}
{'loss': 0.8973, 'grad_norm': 3.03532126431028, 'learning_rate': 5.575111111111111e-06, 'epoch': 1.9956}
{'loss': 0.9017, 'grad_norm': 2.870591048262677, 'learning_rate': 5.574e-06, 'epoch': 1.996}
{'eval_valid_loss': 0.87646484375, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.738, 'eval_valid_steps_per_second': 278.934, 'epoch': 1.996}
{'loss': 0.9023, 'grad_norm': 2.8056811960548953, 'learning_rate': 5.572888888888889e-06, 'epoch': 1.9964}
{'loss': 0.901, 'grad_norm': 3.1261900733838472, 'learning_rate': 5.571777777777778e-06, 'epoch': 1.9968}
{'loss': 0.8812, 'grad_norm': 2.9719348889402575, 'learning_rate': 5.570666666666667e-06, 'epoch': 1.9971999999999999}
{'loss': 0.891, 'grad_norm': 2.8197717755878173, 'learning_rate': 5.569555555555556e-06, 'epoch': 1.9976}
{'loss': 0.8878, 'grad_norm': 2.894103498680257, 'learning_rate': 5.568444444444446e-06, 'epoch': 1.998}
{'loss': 0.8986, 'grad_norm': 3.229944151935855, 'learning_rate': 5.5673333333333334e-06, 'epoch': 1.9984}
{'loss': 0.9068, 'grad_norm': 3.12055168171159, 'learning_rate': 5.566222222222223e-06, 'epoch': 1.9988000000000001}
{'loss': 0.9022, 'grad_norm': 3.0248194550002068, 'learning_rate': 5.565111111111111e-06, 'epoch': 1.9992}
{'loss': 0.9055, 'grad_norm': 3.1474599906311598, 'learning_rate': 5.5640000000000004e-06, 'epoch': 1.9996}
{'loss': 0.8987, 'grad_norm': 3.067473013400852, 'learning_rate': 5.56288888888889e-06, 'epoch': 2.0}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'eval_valid_loss': 0.873046875, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1111.371, 'eval_valid_steps_per_second': 277.843, 'epoch': 2.0}
{'loss': 0.9036, 'grad_norm': 2.807580088494702, 'learning_rate': 5.561777777777778e-06, 'epoch': 2.0004}
{'loss': 0.8986, 'grad_norm': 3.102396432529659, 'learning_rate': 5.560666666666667e-06, 'epoch': 2.0008}
{'loss': 0.895, 'grad_norm': 3.076062930160974, 'learning_rate': 5.559555555555556e-06, 'epoch': 2.0012}
{'loss': 0.9015, 'grad_norm': 2.7927487106610527, 'learning_rate': 5.558444444444445e-06, 'epoch': 2.0016}
{'loss': 0.9063, 'grad_norm': 3.302059535513402, 'learning_rate': 5.5573333333333335e-06, 'epoch': 2.002}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8918, 'grad_norm': 2.7137946647296407, 'learning_rate': 5.556222222222223e-06, 'epoch': 2.0024}
{'loss': 0.8933, 'grad_norm': 2.679884833807816, 'learning_rate': 5.555222222222223e-06, 'epoch': 2.0028}
{'loss': 0.8958, 'grad_norm': 2.907272999693424, 'learning_rate': 5.554111111111111e-06, 'epoch': 2.0032}
{'loss': 0.8944, 'grad_norm': 2.893342395607502, 'learning_rate': 5.553000000000001e-06, 'epoch': 2.0036}
{'loss': 0.9004, 'grad_norm': 2.8973428556207073, 'learning_rate': 5.551888888888889e-06, 'epoch': 2.004}
{'eval_valid_loss': 0.87255859375, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1116.959, 'eval_valid_steps_per_second': 279.24, 'epoch': 2.004}
{'loss': 0.9044, 'grad_norm': 3.3337786208198263, 'learning_rate': 5.550777777777778e-06, 'epoch': 2.0044}
{'loss': 0.8807, 'grad_norm': 2.7082914031279754, 'learning_rate': 5.549666666666668e-06, 'epoch': 2.0048}
{'loss': 0.8974, 'grad_norm': 2.782195337775737, 'learning_rate': 5.548555555555556e-06, 'epoch': 2.0052}
{'loss': 0.887, 'grad_norm': 2.8625058090784026, 'learning_rate': 5.547444444444445e-06, 'epoch': 2.0056}
{'loss': 0.8938, 'grad_norm': 2.6626076739213884, 'learning_rate': 5.546333333333334e-06, 'epoch': 2.006}
{'loss': 0.9104, 'grad_norm': 3.0862034936539655, 'learning_rate': 5.545222222222223e-06, 'epoch': 2.0064}
{'loss': 0.8984, 'grad_norm': 3.278369229349195, 'learning_rate': 5.544111111111111e-06, 'epoch': 2.0068}
{'loss': 0.8982, 'grad_norm': 3.0553121586631953, 'learning_rate': 5.543000000000001e-06, 'epoch': 2.0072}
{'loss': 0.8985, 'grad_norm': 2.734126662830096, 'learning_rate': 5.541888888888889e-06, 'epoch': 2.0076}
{'loss': 0.9158, 'grad_norm': 2.614588788142093, 'learning_rate': 5.540777777777778e-06, 'epoch': 2.008}
{'eval_valid_loss': 0.8720703125, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1110.406, 'eval_valid_steps_per_second': 277.602, 'epoch': 2.008}
{'loss': 0.8887, 'grad_norm': 3.087282336821358, 'learning_rate': 5.539666666666668e-06, 'epoch': 2.0084}
{'loss': 0.9013, 'grad_norm': 3.2516324271767383, 'learning_rate': 5.538555555555556e-06, 'epoch': 2.0088}
{'loss': 0.8967, 'grad_norm': 2.7175371819192327, 'learning_rate': 5.537444444444445e-06, 'epoch': 2.0092}
{'loss': 0.8979, 'grad_norm': 3.0585671103378202, 'learning_rate': 5.536333333333333e-06, 'epoch': 2.0096}
{'loss': 0.8948, 'grad_norm': 3.2414493593781937, 'learning_rate': 5.535222222222223e-06, 'epoch': 2.01}
{'loss': 0.8906, 'grad_norm': 2.932479728147354, 'learning_rate': 5.5341111111111115e-06, 'epoch': 2.0104}
{'loss': 0.8999, 'grad_norm': 2.721977242258684, 'learning_rate': 5.533e-06, 'epoch': 2.0108}
{'loss': 0.9132, 'grad_norm': 2.88360206522191, 'learning_rate': 5.531888888888889e-06, 'epoch': 2.0112}
{'loss': 0.8953, 'grad_norm': 2.856602591072691, 'learning_rate': 5.5307777777777785e-06, 'epoch': 2.0116}
{'loss': 0.8984, 'grad_norm': 2.950417463215142, 'learning_rate': 5.529666666666668e-06, 'epoch': 2.012}
{'eval_valid_loss': 0.87109375, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1110.004, 'eval_valid_steps_per_second': 277.501, 'epoch': 2.012}
{'loss': 0.9055, 'grad_norm': 2.7251196981528696, 'learning_rate': 5.528555555555556e-06, 'epoch': 2.0124}
{'loss': 0.89, 'grad_norm': 3.257252999614916, 'learning_rate': 5.5274444444444455e-06, 'epoch': 2.0128}
{'loss': 0.9015, 'grad_norm': 3.2558793935177506, 'learning_rate': 5.526333333333333e-06, 'epoch': 2.0132}
{'loss': 0.8945, 'grad_norm': 2.734658929524976, 'learning_rate': 5.525222222222223e-06, 'epoch': 2.0136}
{'loss': 0.9032, 'grad_norm': 2.9797224457826452, 'learning_rate': 5.524111111111112e-06, 'epoch': 2.014}
{'loss': 0.8923, 'grad_norm': 3.0849011866140774, 'learning_rate': 5.523e-06, 'epoch': 2.0144}
{'loss': 0.9055, 'grad_norm': 2.826766061701612, 'learning_rate': 5.521888888888889e-06, 'epoch': 2.0148}
{'loss': 0.8999, 'grad_norm': 2.5997717596706704, 'learning_rate': 5.520777777777779e-06, 'epoch': 2.0152}
{'loss': 0.8995, 'grad_norm': 3.151490806327723, 'learning_rate': 5.519666666666667e-06, 'epoch': 2.0156}
{'loss': 0.9054, 'grad_norm': 3.125382152079123, 'learning_rate': 5.518555555555556e-06, 'epoch': 2.016}
{'eval_valid_loss': 0.87109375, 'eval_valid_runtime': 0.0912, 'eval_valid_samples_per_second': 1096.063, 'eval_valid_steps_per_second': 274.016, 'epoch': 2.016}
{'loss': 0.9011, 'grad_norm': 2.8557133915546817, 'learning_rate': 5.5174444444444456e-06, 'epoch': 2.0164}
{'loss': 0.906, 'grad_norm': 3.1228567784839796, 'learning_rate': 5.5163333333333334e-06, 'epoch': 2.0168}
{'loss': 0.8906, 'grad_norm': 2.862525132331296, 'learning_rate': 5.515222222222223e-06, 'epoch': 2.0172}
{'loss': 0.8889, 'grad_norm': 2.764378525397278, 'learning_rate': 5.514111111111111e-06, 'epoch': 2.0176}
{'loss': 0.8882, 'grad_norm': 2.989886512181413, 'learning_rate': 5.513e-06, 'epoch': 2.018}
{'loss': 0.8969, 'grad_norm': 3.0132672970967738, 'learning_rate': 5.511888888888889e-06, 'epoch': 2.0184}
{'loss': 0.8943, 'grad_norm': 2.9569001061650266, 'learning_rate': 5.510777777777778e-06, 'epoch': 2.0188}
{'loss': 0.8982, 'grad_norm': 3.0257192178361416, 'learning_rate': 5.509666666666667e-06, 'epoch': 2.0192}
{'loss': 0.8972, 'grad_norm': 3.1916055045148166, 'learning_rate': 5.508555555555556e-06, 'epoch': 2.0196}
{'loss': 0.8865, 'grad_norm': 2.836658365147461, 'learning_rate': 5.507444444444446e-06, 'epoch': 2.02}
{'eval_valid_loss': 0.86962890625, 'eval_valid_runtime': 0.091, 'eval_valid_samples_per_second': 1099.099, 'eval_valid_steps_per_second': 274.775, 'epoch': 2.02}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8816, 'grad_norm': 2.8825540814450377, 'learning_rate': 5.5063333333333335e-06, 'epoch': 2.0204}
{'loss': 0.8993, 'grad_norm': 2.9656946220305196, 'learning_rate': 5.505222222222223e-06, 'epoch': 2.0208}
{'loss': 0.8875, 'grad_norm': 2.9488694950485894, 'learning_rate': 5.504111111111111e-06, 'epoch': 2.0212}
{'loss': 0.8841, 'grad_norm': 3.009471287281262, 'learning_rate': 5.5030000000000005e-06, 'epoch': 2.0216}
{'loss': 0.9001, 'grad_norm': 2.9808172019761603, 'learning_rate': 5.501888888888889e-06, 'epoch': 2.022}
{'loss': 0.8844, 'grad_norm': 3.0868255979080153, 'learning_rate': 5.500777777777778e-06, 'epoch': 2.0224}
{'loss': 0.8913, 'grad_norm': 3.3160492667833887, 'learning_rate': 5.4996666666666675e-06, 'epoch': 2.0228}
{'loss': 0.8934, 'grad_norm': 3.0299449254529236, 'learning_rate': 5.498555555555556e-06, 'epoch': 2.0232}
{'loss': 0.9058, 'grad_norm': 2.866166704866581, 'learning_rate': 5.497444444444445e-06, 'epoch': 2.0236}
{'loss': 0.8863, 'grad_norm': 3.088827088898062, 'learning_rate': 5.496333333333334e-06, 'epoch': 2.024}
{'eval_valid_loss': 0.8720703125, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.727, 'eval_valid_steps_per_second': 278.682, 'epoch': 2.024}
{'loss': 0.9027, 'grad_norm': 3.053252856448224, 'learning_rate': 5.495222222222223e-06, 'epoch': 2.0244}
{'loss': 0.9067, 'grad_norm': 3.3065471602162386, 'learning_rate': 5.494111111111111e-06, 'epoch': 2.0248}
{'loss': 0.8981, 'grad_norm': 2.7664156015405577, 'learning_rate': 5.493000000000001e-06, 'epoch': 2.0252}
{'loss': 0.897, 'grad_norm': 2.9204540367382403, 'learning_rate': 5.4918888888888885e-06, 'epoch': 2.0256}
{'loss': 0.8979, 'grad_norm': 2.886872582988962, 'learning_rate': 5.490777777777778e-06, 'epoch': 2.026}
{'loss': 0.8885, 'grad_norm': 2.7692197078347385, 'learning_rate': 5.489666666666668e-06, 'epoch': 2.0264}
{'loss': 0.8847, 'grad_norm': 2.8636593328348816, 'learning_rate': 5.4885555555555554e-06, 'epoch': 2.0268}
{'loss': 0.9039, 'grad_norm': 2.999055018207471, 'learning_rate': 5.487444444444445e-06, 'epoch': 2.0272}
{'loss': 0.9024, 'grad_norm': 3.215390878427058, 'learning_rate': 5.486333333333334e-06, 'epoch': 2.0276}
{'loss': 0.9067, 'grad_norm': 2.937342740983078, 'learning_rate': 5.485222222222223e-06, 'epoch': 2.028}
{'eval_valid_loss': 0.87158203125, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1115.222, 'eval_valid_steps_per_second': 278.805, 'epoch': 2.028}
{'loss': 0.9052, 'grad_norm': 3.129302524803927, 'learning_rate': 5.484111111111111e-06, 'epoch': 2.0284}
{'loss': 0.9026, 'grad_norm': 2.9117745118038387, 'learning_rate': 5.483000000000001e-06, 'epoch': 2.0288}
{'loss': 0.9028, 'grad_norm': 2.6123742780596357, 'learning_rate': 5.4818888888888886e-06, 'epoch': 2.0292}
{'loss': 0.899, 'grad_norm': 2.685455120946776, 'learning_rate': 5.480777777777778e-06, 'epoch': 2.0296}
{'loss': 0.8956, 'grad_norm': 3.0473688556783354, 'learning_rate': 5.479666666666668e-06, 'epoch': 2.03}
{'loss': 0.9004, 'grad_norm': 2.9510665056767915, 'learning_rate': 5.4785555555555555e-06, 'epoch': 2.0304}
{'loss': 0.909, 'grad_norm': 2.7101858069797666, 'learning_rate': 5.477444444444445e-06, 'epoch': 2.0308}
{'loss': 0.8959, 'grad_norm': 3.2217581127434913, 'learning_rate': 5.476333333333334e-06, 'epoch': 2.0312}
{'loss': 0.889, 'grad_norm': 2.9455097749651817, 'learning_rate': 5.4752222222222225e-06, 'epoch': 2.0316}
{'loss': 0.9013, 'grad_norm': 2.711404804112182, 'learning_rate': 5.474111111111111e-06, 'epoch': 2.032}
{'eval_valid_loss': 0.87060546875, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.223, 'eval_valid_steps_per_second': 278.556, 'epoch': 2.032}
{'loss': 0.8919, 'grad_norm': 3.1032972312989386, 'learning_rate': 5.473000000000001e-06, 'epoch': 2.0324}
{'loss': 0.8942, 'grad_norm': 2.9397120466289186, 'learning_rate': 5.471888888888889e-06, 'epoch': 2.0328}
{'loss': 0.8885, 'grad_norm': 3.0344225316985223, 'learning_rate': 5.470777777777778e-06, 'epoch': 2.0332}
{'loss': 0.8959, 'grad_norm': 3.0949335916786795, 'learning_rate': 5.469666666666668e-06, 'epoch': 2.0336}
{'loss': 0.9045, 'grad_norm': 2.8477184670850546, 'learning_rate': 5.468555555555556e-06, 'epoch': 2.034}
{'loss': 0.8969, 'grad_norm': 3.259203488088187, 'learning_rate': 5.467444444444445e-06, 'epoch': 2.0344}
{'loss': 0.8948, 'grad_norm': 3.0595604159441363, 'learning_rate': 5.466333333333334e-06, 'epoch': 2.0348}
{'loss': 0.8993, 'grad_norm': 3.2448146700872846, 'learning_rate': 5.465222222222223e-06, 'epoch': 2.0352}
{'loss': 0.8845, 'grad_norm': 2.99580816626202, 'learning_rate': 5.464111111111111e-06, 'epoch': 2.0356}
{'loss': 0.8988, 'grad_norm': 2.6522452739249536, 'learning_rate': 5.463000000000001e-06, 'epoch': 2.036}
{'eval_valid_loss': 0.87158203125, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1115.142, 'eval_valid_steps_per_second': 278.785, 'epoch': 2.036}
{'loss': 0.8889, 'grad_norm': 2.55462469087903, 'learning_rate': 5.461888888888889e-06, 'epoch': 2.0364}
{'loss': 0.8937, 'grad_norm': 2.98634998021633, 'learning_rate': 5.460777777777778e-06, 'epoch': 2.0368}
{'loss': 0.8988, 'grad_norm': 3.0341507609805305, 'learning_rate': 5.459666666666668e-06, 'epoch': 2.0372}
{'loss': 0.8988, 'grad_norm': 2.787102296281579, 'learning_rate': 5.458555555555556e-06, 'epoch': 2.0376}
{'loss': 0.9092, 'grad_norm': 3.036197616263468, 'learning_rate': 5.457444444444445e-06, 'epoch': 2.038}
{'loss': 0.8937, 'grad_norm': 3.020425631974275, 'learning_rate': 5.456333333333333e-06, 'epoch': 2.0384}
{'loss': 0.8954, 'grad_norm': 2.8007255931735022, 'learning_rate': 5.455222222222223e-06, 'epoch': 2.0388}
{'loss': 0.8948, 'grad_norm': 3.0499345725499305, 'learning_rate': 5.4541111111111114e-06, 'epoch': 2.0392}
{'loss': 0.8982, 'grad_norm': 2.915679646559458, 'learning_rate': 5.453e-06, 'epoch': 2.0396}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.893, 'grad_norm': 2.899605944802456, 'learning_rate': 5.451888888888889e-06, 'epoch': 2.04}
{'eval_valid_loss': 0.8701171875, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.904, 'eval_valid_steps_per_second': 278.726, 'epoch': 2.04}
{'loss': 0.9038, 'grad_norm': 2.8539967532757613, 'learning_rate': 5.450777777777778e-06, 'epoch': 2.0404}
{'loss': 0.891, 'grad_norm': 2.888257835897935, 'learning_rate': 5.449666666666668e-06, 'epoch': 2.0408}
{'loss': 0.9029, 'grad_norm': 2.8088659915902694, 'learning_rate': 5.448555555555556e-06, 'epoch': 2.0412}
{'loss': 0.9028, 'grad_norm': 2.88533010364419, 'learning_rate': 5.447444444444445e-06, 'epoch': 2.0416}
{'loss': 0.891, 'grad_norm': 2.6955098453091435, 'learning_rate': 5.446333333333333e-06, 'epoch': 2.042}
{'loss': 0.9066, 'grad_norm': 2.898221380602655, 'learning_rate': 5.445222222222223e-06, 'epoch': 2.0424}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8957, 'grad_norm': 2.9402667620930085, 'learning_rate': 5.444222222222223e-06, 'epoch': 2.0428}
{'loss': 0.8962, 'grad_norm': 3.085985593783165, 'learning_rate': 5.443111111111111e-06, 'epoch': 2.0432}
{'loss': 0.8946, 'grad_norm': 2.8666840196978804, 'learning_rate': 5.442000000000001e-06, 'epoch': 2.0436}
{'loss': 0.8945, 'grad_norm': 2.847535830700165, 'learning_rate': 5.440888888888889e-06, 'epoch': 2.044}
{'eval_valid_loss': 0.87060546875, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.225, 'eval_valid_steps_per_second': 279.056, 'epoch': 2.044}
{'loss': 0.8874, 'grad_norm': 2.85072078039591, 'learning_rate': 5.439777777777778e-06, 'epoch': 2.0444}
{'loss': 0.8901, 'grad_norm': 2.9415781241933785, 'learning_rate': 5.4386666666666676e-06, 'epoch': 2.0448}
{'loss': 0.8962, 'grad_norm': 2.7198539498594227, 'learning_rate': 5.437555555555556e-06, 'epoch': 2.0452}
{'loss': 0.8961, 'grad_norm': 3.22290852801909, 'learning_rate': 5.436444444444445e-06, 'epoch': 2.0456}
{'loss': 0.8979, 'grad_norm': 2.9220048498176103, 'learning_rate': 5.435333333333334e-06, 'epoch': 2.046}
{'loss': 0.8831, 'grad_norm': 2.6873657614427917, 'learning_rate': 5.434222222222223e-06, 'epoch': 2.0464}
{'loss': 0.8965, 'grad_norm': 3.0125453027508313, 'learning_rate': 5.433111111111111e-06, 'epoch': 2.0468}
{'loss': 0.9009, 'grad_norm': 2.9908289302706006, 'learning_rate': 5.432000000000001e-06, 'epoch': 2.0472}
{'loss': 0.8995, 'grad_norm': 3.0522215663258225, 'learning_rate': 5.4308888888888885e-06, 'epoch': 2.0476}
{'loss': 0.8877, 'grad_norm': 2.8718489918657073, 'learning_rate': 5.429777777777778e-06, 'epoch': 2.048}
{'eval_valid_loss': 0.86962890625, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.581, 'eval_valid_steps_per_second': 278.395, 'epoch': 2.048}
{'loss': 0.8923, 'grad_norm': 2.945741566537574, 'learning_rate': 5.428666666666668e-06, 'epoch': 2.0484}
{'loss': 0.8915, 'grad_norm': 2.9456357904985664, 'learning_rate': 5.4275555555555555e-06, 'epoch': 2.0488}
{'loss': 0.8954, 'grad_norm': 2.921782456426719, 'learning_rate': 5.426444444444445e-06, 'epoch': 2.0492}
{'loss': 0.8972, 'grad_norm': 2.9949011685904563, 'learning_rate': 5.425333333333334e-06, 'epoch': 2.0496}
{'loss': 0.8959, 'grad_norm': 3.090473732944527, 'learning_rate': 5.424222222222223e-06, 'epoch': 2.05}
{'loss': 0.8905, 'grad_norm': 2.8636268833858067, 'learning_rate': 5.423111111111111e-06, 'epoch': 2.0504}
{'loss': 0.901, 'grad_norm': 3.3429453050861517, 'learning_rate': 5.422000000000001e-06, 'epoch': 2.0508}
{'loss': 0.8964, 'grad_norm': 2.718456208589725, 'learning_rate': 5.420888888888889e-06, 'epoch': 2.0512}
{'loss': 0.8796, 'grad_norm': 3.1222036152464714, 'learning_rate': 5.419777777777778e-06, 'epoch': 2.0516}
{'loss': 0.9015, 'grad_norm': 3.0703119661364684, 'learning_rate': 5.418666666666668e-06, 'epoch': 2.052}
{'eval_valid_loss': 0.87060546875, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1109.983, 'eval_valid_steps_per_second': 277.496, 'epoch': 2.052}
{'loss': 0.9069, 'grad_norm': 3.059704078455635, 'learning_rate': 5.417555555555556e-06, 'epoch': 2.0524}
{'loss': 0.8999, 'grad_norm': 2.9639976108267265, 'learning_rate': 5.416444444444445e-06, 'epoch': 2.0528}
{'loss': 0.8925, 'grad_norm': 3.318286124819197, 'learning_rate': 5.415333333333334e-06, 'epoch': 2.0532}
{'loss': 0.9049, 'grad_norm': 2.9463515485257745, 'learning_rate': 5.414222222222223e-06, 'epoch': 2.0536}
{'loss': 0.9028, 'grad_norm': 3.0024170675266766, 'learning_rate': 5.413111111111111e-06, 'epoch': 2.054}
{'loss': 0.8964, 'grad_norm': 2.7178427728162426, 'learning_rate': 5.412000000000001e-06, 'epoch': 2.0544}
{'loss': 0.8917, 'grad_norm': 2.960269812436519, 'learning_rate': 5.410888888888889e-06, 'epoch': 2.0548}
{'loss': 0.8967, 'grad_norm': 2.802062088887017, 'learning_rate': 5.409777777777778e-06, 'epoch': 2.0552}
{'loss': 0.8959, 'grad_norm': 2.748116937651096, 'learning_rate': 5.408666666666668e-06, 'epoch': 2.0556}
{'loss': 0.8948, 'grad_norm': 2.8508591398479632, 'learning_rate': 5.407555555555556e-06, 'epoch': 2.056}
{'eval_valid_loss': 0.869140625, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.688, 'eval_valid_steps_per_second': 278.672, 'epoch': 2.056}
{'loss': 0.8968, 'grad_norm': 2.842902927208003, 'learning_rate': 5.406444444444445e-06, 'epoch': 2.0564}
{'loss': 0.8995, 'grad_norm': 2.915947537096758, 'learning_rate': 5.405333333333333e-06, 'epoch': 2.0568}
{'loss': 0.8996, 'grad_norm': 2.7368424318217843, 'learning_rate': 5.404222222222223e-06, 'epoch': 2.0572}
{'loss': 0.8889, 'grad_norm': 2.798265543030986, 'learning_rate': 5.403111111111111e-06, 'epoch': 2.0576}
{'loss': 0.8922, 'grad_norm': 3.3868846214708257, 'learning_rate': 5.402000000000001e-06, 'epoch': 2.058}
{'loss': 0.9134, 'grad_norm': 2.755715029131142, 'learning_rate': 5.400888888888889e-06, 'epoch': 2.0584}
{'loss': 0.8969, 'grad_norm': 2.9998462160748973, 'learning_rate': 5.399777777777778e-06, 'epoch': 2.0588}
{'loss': 0.9062, 'grad_norm': 3.112417323764756, 'learning_rate': 5.398666666666668e-06, 'epoch': 2.0592}
{'loss': 0.8939, 'grad_norm': 3.3396441662142715, 'learning_rate': 5.397555555555556e-06, 'epoch': 2.0596}
{'loss': 0.8937, 'grad_norm': 3.0175998526473022, 'learning_rate': 5.396444444444445e-06, 'epoch': 2.06}
{'eval_valid_loss': 0.869140625, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1129.064, 'eval_valid_steps_per_second': 282.266, 'epoch': 2.06}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8936, 'grad_norm': 3.1056750247033267, 'learning_rate': 5.395333333333333e-06, 'epoch': 2.0604}
{'loss': 0.8887, 'grad_norm': 3.0798282654989264, 'learning_rate': 5.394222222222223e-06, 'epoch': 2.0608}
{'loss': 0.8872, 'grad_norm': 2.8866521288213494, 'learning_rate': 5.3931111111111115e-06, 'epoch': 2.0612}
{'loss': 0.8964, 'grad_norm': 3.0461838745341057, 'learning_rate': 5.392e-06, 'epoch': 2.0616}
{'loss': 0.9037, 'grad_norm': 3.2031933288729593, 'learning_rate': 5.390888888888889e-06, 'epoch': 2.062}
{'loss': 0.8965, 'grad_norm': 2.937546709379197, 'learning_rate': 5.3897777777777785e-06, 'epoch': 2.0624}
{'loss': 0.8975, 'grad_norm': 2.762782915654014, 'learning_rate': 5.388666666666667e-06, 'epoch': 2.0628}
{'loss': 0.9107, 'grad_norm': 3.315566353094975, 'learning_rate': 5.387555555555556e-06, 'epoch': 2.0632}
{'loss': 0.894, 'grad_norm': 2.9074535441438836, 'learning_rate': 5.3864444444444455e-06, 'epoch': 2.0636}
{'loss': 0.9001, 'grad_norm': 3.0984531738838963, 'learning_rate': 5.385333333333333e-06, 'epoch': 2.064}
{'eval_valid_loss': 0.87060546875, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.703, 'eval_valid_steps_per_second': 278.426, 'epoch': 2.064}
{'loss': 0.9067, 'grad_norm': 2.970847894375193, 'learning_rate': 5.384222222222223e-06, 'epoch': 2.0644}
{'loss': 0.9047, 'grad_norm': 3.046780941807408, 'learning_rate': 5.383111111111112e-06, 'epoch': 2.0648}
{'loss': 0.897, 'grad_norm': 2.8107424648746804, 'learning_rate': 5.382e-06, 'epoch': 2.0652}
{'loss': 0.894, 'grad_norm': 2.9760204933080017, 'learning_rate': 5.380888888888889e-06, 'epoch': 2.0656}
{'loss': 0.8845, 'grad_norm': 2.937212727074399, 'learning_rate': 5.379777777777779e-06, 'epoch': 2.066}
{'loss': 0.897, 'grad_norm': 2.8669394715841605, 'learning_rate': 5.378666666666667e-06, 'epoch': 2.0664}
{'loss': 0.8875, 'grad_norm': 3.0222648889807715, 'learning_rate': 5.377555555555556e-06, 'epoch': 2.0668}
{'loss': 0.8931, 'grad_norm': 2.6461096522954612, 'learning_rate': 5.3764444444444456e-06, 'epoch': 2.0672}
{'loss': 0.8836, 'grad_norm': 2.779511025945071, 'learning_rate': 5.3753333333333334e-06, 'epoch': 2.0676}
{'loss': 0.8988, 'grad_norm': 2.965727190723122, 'learning_rate': 5.374222222222223e-06, 'epoch': 2.068}
{'eval_valid_loss': 0.869140625, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1122.937, 'eval_valid_steps_per_second': 280.734, 'epoch': 2.068}
{'loss': 0.9067, 'grad_norm': 3.240692906417732, 'learning_rate': 5.373111111111111e-06, 'epoch': 2.0684}
{'loss': 0.8901, 'grad_norm': 3.0991014955080356, 'learning_rate': 5.372e-06, 'epoch': 2.0688}
{'loss': 0.8987, 'grad_norm': 2.8859484810199434, 'learning_rate': 5.370888888888889e-06, 'epoch': 2.0692}
{'loss': 0.9032, 'grad_norm': 3.023083865753575, 'learning_rate': 5.369777777777778e-06, 'epoch': 2.0696}
{'loss': 0.8955, 'grad_norm': 3.2250132408905268, 'learning_rate': 5.368666666666667e-06, 'epoch': 2.07}
{'loss': 0.893, 'grad_norm': 2.96710052844883, 'learning_rate': 5.367555555555556e-06, 'epoch': 2.0704}
{'loss': 0.9066, 'grad_norm': 2.963928543788055, 'learning_rate': 5.366444444444446e-06, 'epoch': 2.0708}
{'loss': 0.8958, 'grad_norm': 3.0358469605504332, 'learning_rate': 5.3653333333333335e-06, 'epoch': 2.0712}
{'loss': 0.9008, 'grad_norm': 2.9683241287751287, 'learning_rate': 5.364222222222223e-06, 'epoch': 2.0716}
{'loss': 0.8923, 'grad_norm': 3.005831306232276, 'learning_rate': 5.363111111111111e-06, 'epoch': 2.072}
{'eval_valid_loss': 0.8701171875, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.037, 'eval_valid_steps_per_second': 278.009, 'epoch': 2.072}
{'loss': 0.9018, 'grad_norm': 3.0630550270829264, 'learning_rate': 5.3620000000000005e-06, 'epoch': 2.0724}
{'loss': 0.9027, 'grad_norm': 2.804601163252531, 'learning_rate': 5.360888888888889e-06, 'epoch': 2.0728}
{'loss': 0.9024, 'grad_norm': 3.1146512816213963, 'learning_rate': 5.359777777777778e-06, 'epoch': 2.0732}
{'loss': 0.8868, 'grad_norm': 3.0110357477364884, 'learning_rate': 5.3586666666666675e-06, 'epoch': 2.0736}
{'loss': 0.9066, 'grad_norm': 3.210801764447792, 'learning_rate': 5.357555555555556e-06, 'epoch': 2.074}
{'loss': 0.8979, 'grad_norm': 2.823332635676948, 'learning_rate': 5.356444444444445e-06, 'epoch': 2.0744}
{'loss': 0.8834, 'grad_norm': 2.9544361070091916, 'learning_rate': 5.355333333333334e-06, 'epoch': 2.0748}
{'loss': 0.8982, 'grad_norm': 3.270297034856296, 'learning_rate': 5.354222222222223e-06, 'epoch': 2.0752}
{'loss': 0.8927, 'grad_norm': 3.01289412029533, 'learning_rate': 5.353111111111111e-06, 'epoch': 2.0756}
{'loss': 0.9021, 'grad_norm': 2.7823531181283565, 'learning_rate': 5.352000000000001e-06, 'epoch': 2.076}
{'eval_valid_loss': 0.87109375, 'eval_valid_runtime': 0.0907, 'eval_valid_samples_per_second': 1102.317, 'eval_valid_steps_per_second': 275.579, 'epoch': 2.076}
{'loss': 0.8867, 'grad_norm': 2.914257298726372, 'learning_rate': 5.3508888888888885e-06, 'epoch': 2.0764}
{'loss': 0.8968, 'grad_norm': 3.0209332427680184, 'learning_rate': 5.349777777777778e-06, 'epoch': 2.0768}
{'loss': 0.8971, 'grad_norm': 2.9991710729602845, 'learning_rate': 5.348666666666668e-06, 'epoch': 2.0772}
{'loss': 0.8985, 'grad_norm': 3.1793007955258648, 'learning_rate': 5.3475555555555554e-06, 'epoch': 2.0776}
{'loss': 0.8898, 'grad_norm': 2.8861269931511244, 'learning_rate': 5.346444444444445e-06, 'epoch': 2.078}
{'loss': 0.9108, 'grad_norm': 2.963440071514349, 'learning_rate': 5.345333333333334e-06, 'epoch': 2.0784}
{'loss': 0.8968, 'grad_norm': 2.9254868159332004, 'learning_rate': 5.344222222222223e-06, 'epoch': 2.0788}
{'loss': 0.9031, 'grad_norm': 3.056360922986741, 'learning_rate': 5.343111111111111e-06, 'epoch': 2.0792}
{'loss': 0.893, 'grad_norm': 2.7940613003561694, 'learning_rate': 5.342000000000001e-06, 'epoch': 2.0796}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.8973, 'grad_norm': 3.2267950311327676, 'learning_rate': 5.3408888888888886e-06, 'epoch': 2.08}
{'eval_valid_loss': 0.869140625, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.696, 'eval_valid_steps_per_second': 279.924, 'epoch': 2.08}
{'loss': 0.9064, 'grad_norm': 2.838776819597119, 'learning_rate': 5.339777777777778e-06, 'epoch': 2.0804}
{'loss': 0.8987, 'grad_norm': 3.09449816818294, 'learning_rate': 5.338666666666668e-06, 'epoch': 2.0808}
{'loss': 0.8973, 'grad_norm': 3.1795503928877236, 'learning_rate': 5.3375555555555555e-06, 'epoch': 2.0812}
{'loss': 0.9124, 'grad_norm': 3.375931849582205, 'learning_rate': 5.336444444444445e-06, 'epoch': 2.0816}
{'loss': 0.8964, 'grad_norm': 2.9145494928547264, 'learning_rate': 5.335333333333334e-06, 'epoch': 2.082}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9061, 'grad_norm': 3.1344369401069647, 'learning_rate': 5.3342222222222225e-06, 'epoch': 2.0824}
{'loss': 0.8965, 'grad_norm': 3.129508866512856, 'learning_rate': 5.333222222222223e-06, 'epoch': 2.0828}
{'loss': 0.8966, 'grad_norm': 3.2173470522874865, 'learning_rate': 5.332111111111112e-06, 'epoch': 2.0832}
{'loss': 0.8913, 'grad_norm': 2.663629037938044, 'learning_rate': 5.331e-06, 'epoch': 2.0836}
{'loss': 0.8896, 'grad_norm': 3.0482240086913683, 'learning_rate': 5.329888888888889e-06, 'epoch': 2.084}
{'eval_valid_loss': 0.8681640625, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.779, 'eval_valid_steps_per_second': 278.445, 'epoch': 2.084}
{'loss': 0.8893, 'grad_norm': 2.7781968127860917, 'learning_rate': 5.3287777777777786e-06, 'epoch': 2.0844}
{'loss': 0.8992, 'grad_norm': 2.7539528051458846, 'learning_rate': 5.327666666666667e-06, 'epoch': 2.0848}
{'loss': 0.8954, 'grad_norm': 2.809257810819596, 'learning_rate': 5.326555555555556e-06, 'epoch': 2.0852}
{'loss': 0.8963, 'grad_norm': 2.866497725999703, 'learning_rate': 5.3254444444444455e-06, 'epoch': 2.0856}
{'loss': 0.9023, 'grad_norm': 3.05253264577656, 'learning_rate': 5.324333333333333e-06, 'epoch': 2.086}
{'loss': 0.8944, 'grad_norm': 2.8344890005870957, 'learning_rate': 5.323222222222223e-06, 'epoch': 2.0864}
{'loss': 0.8811, 'grad_norm': 2.7838089240132486, 'learning_rate': 5.322111111111111e-06, 'epoch': 2.0868}
{'loss': 0.9005, 'grad_norm': 3.029502699332476, 'learning_rate': 5.321e-06, 'epoch': 2.0872}
{'loss': 0.8959, 'grad_norm': 2.9356661814837985, 'learning_rate': 5.319888888888889e-06, 'epoch': 2.0876}
{'loss': 0.8989, 'grad_norm': 3.0118825054055396, 'learning_rate': 5.318777777777779e-06, 'epoch': 2.088}
{'eval_valid_loss': 0.86767578125, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1106.388, 'eval_valid_steps_per_second': 276.597, 'epoch': 2.088}
{'loss': 0.8968, 'grad_norm': 2.7354967159829555, 'learning_rate': 5.317666666666667e-06, 'epoch': 2.0884}
{'loss': 0.8971, 'grad_norm': 2.6730334966007843, 'learning_rate': 5.316555555555556e-06, 'epoch': 2.0888}
{'loss': 0.8992, 'grad_norm': 2.8247348319147005, 'learning_rate': 5.315444444444446e-06, 'epoch': 2.0892}
{'loss': 0.8977, 'grad_norm': 2.727022126814822, 'learning_rate': 5.3143333333333335e-06, 'epoch': 2.0896}
{'loss': 0.9096, 'grad_norm': 2.7899106870390975, 'learning_rate': 5.313222222222223e-06, 'epoch': 2.09}
{'loss': 0.8817, 'grad_norm': 2.809795361343873, 'learning_rate': 5.312111111111111e-06, 'epoch': 2.0904}
{'loss': 0.9112, 'grad_norm': 3.02072807764422, 'learning_rate': 5.3110000000000005e-06, 'epoch': 2.0908}
{'loss': 0.8895, 'grad_norm': 2.743130578787001, 'learning_rate': 5.309888888888889e-06, 'epoch': 2.0912}
{'loss': 0.8941, 'grad_norm': 2.835702092932768, 'learning_rate': 5.308777777777778e-06, 'epoch': 2.0916}
{'loss': 0.8856, 'grad_norm': 3.139938478732759, 'learning_rate': 5.3076666666666675e-06, 'epoch': 2.092}
{'eval_valid_loss': 0.86962890625, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1124.861, 'eval_valid_steps_per_second': 281.215, 'epoch': 2.092}
{'loss': 0.9028, 'grad_norm': 2.9100100999014544, 'learning_rate': 5.306555555555556e-06, 'epoch': 2.0924}
{'loss': 0.8977, 'grad_norm': 3.1062172137704223, 'learning_rate': 5.305444444444445e-06, 'epoch': 2.0928}
{'loss': 0.8865, 'grad_norm': 2.700450224338036, 'learning_rate': 5.304333333333334e-06, 'epoch': 2.0932}
{'loss': 0.8987, 'grad_norm': 2.649756859700386, 'learning_rate': 5.303222222222223e-06, 'epoch': 2.0936}
{'loss': 0.9031, 'grad_norm': 2.9394903744590097, 'learning_rate': 5.302111111111111e-06, 'epoch': 2.094}
{'loss': 0.8925, 'grad_norm': 2.9399676712198453, 'learning_rate': 5.3010000000000006e-06, 'epoch': 2.0944}
{'loss': 0.8968, 'grad_norm': 3.2023984131494876, 'learning_rate': 5.2998888888888884e-06, 'epoch': 2.0948}
{'loss': 0.9046, 'grad_norm': 2.858293745797351, 'learning_rate': 5.298777777777778e-06, 'epoch': 2.0952}
{'loss': 0.8923, 'grad_norm': 3.0967786394788823, 'learning_rate': 5.2976666666666676e-06, 'epoch': 2.0956}
{'loss': 0.8947, 'grad_norm': 2.8297545864803766, 'learning_rate': 5.296555555555556e-06, 'epoch': 2.096}
{'eval_valid_loss': 0.8681640625, 'eval_valid_runtime': 0.0885, 'eval_valid_samples_per_second': 1129.459, 'eval_valid_steps_per_second': 282.365, 'epoch': 2.096}
{'loss': 0.895, 'grad_norm': 3.217805816533792, 'learning_rate': 5.295444444444445e-06, 'epoch': 2.0964}
{'loss': 0.9013, 'grad_norm': 2.854244402826907, 'learning_rate': 5.294333333333334e-06, 'epoch': 2.0968}
{'loss': 0.8975, 'grad_norm': 2.736147191820463, 'learning_rate': 5.293222222222223e-06, 'epoch': 2.0972}
{'loss': 0.8854, 'grad_norm': 2.930468757729059, 'learning_rate': 5.292111111111111e-06, 'epoch': 2.0976}
{'loss': 0.9045, 'grad_norm': 3.317943409966015, 'learning_rate': 5.291000000000001e-06, 'epoch': 2.098}
{'loss': 0.8947, 'grad_norm': 3.0215536354969363, 'learning_rate': 5.2898888888888885e-06, 'epoch': 2.0984}
{'loss': 0.8908, 'grad_norm': 3.1550923008876, 'learning_rate': 5.288777777777778e-06, 'epoch': 2.0987999999999998}
{'loss': 0.8901, 'grad_norm': 2.828900436399608, 'learning_rate': 5.287666666666668e-06, 'epoch': 2.0992}
{'loss': 0.8955, 'grad_norm': 2.918297326560973, 'learning_rate': 5.2865555555555555e-06, 'epoch': 2.0996}
{'loss': 0.8993, 'grad_norm': 3.086087119422871, 'learning_rate': 5.285444444444445e-06, 'epoch': 2.1}
{'eval_valid_loss': 0.86767578125, 'eval_valid_runtime': 0.0924, 'eval_valid_samples_per_second': 1081.938, 'eval_valid_steps_per_second': 270.484, 'epoch': 2.1}
{'loss': 0.8982, 'grad_norm': 3.1111107874247597, 'learning_rate': 5.284333333333334e-06, 'epoch': 2.1004}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8931, 'grad_norm': 2.7557751421691203, 'learning_rate': 5.2832222222222225e-06, 'epoch': 2.1008}
{'loss': 0.893, 'grad_norm': 3.0401609048956266, 'learning_rate': 5.282111111111111e-06, 'epoch': 2.1012}
{'loss': 0.9001, 'grad_norm': 3.1610494325122103, 'learning_rate': 5.281000000000001e-06, 'epoch': 2.1016}
{'loss': 0.8989, 'grad_norm': 2.9467091777663637, 'learning_rate': 5.279888888888889e-06, 'epoch': 2.102}
{'loss': 0.8819, 'grad_norm': 2.7618641147368805, 'learning_rate': 5.278777777777778e-06, 'epoch': 2.1024}
{'loss': 0.8827, 'grad_norm': 2.9532749904911153, 'learning_rate': 5.277666666666668e-06, 'epoch': 2.1028000000000002}
{'loss': 0.8881, 'grad_norm': 3.234644542280227, 'learning_rate': 5.276555555555556e-06, 'epoch': 2.1032}
{'loss': 0.8958, 'grad_norm': 3.1912307691013333, 'learning_rate': 5.275444444444445e-06, 'epoch': 2.1036}
{'loss': 0.9061, 'grad_norm': 2.8407192113883584, 'learning_rate': 5.274333333333334e-06, 'epoch': 2.104}
{'eval_valid_loss': 0.86767578125, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.862, 'eval_valid_steps_per_second': 281.965, 'epoch': 2.104}
{'loss': 0.8965, 'grad_norm': 3.2185391292199457, 'learning_rate': 5.273222222222223e-06, 'epoch': 2.1044}
{'loss': 0.8801, 'grad_norm': 2.9617303274980418, 'learning_rate': 5.272111111111111e-06, 'epoch': 2.1048}
{'loss': 0.9016, 'grad_norm': 3.005353075681544, 'learning_rate': 5.271000000000001e-06, 'epoch': 2.1052}
{'loss': 0.8989, 'grad_norm': 2.873128167615982, 'learning_rate': 5.269888888888889e-06, 'epoch': 2.1056}
{'loss': 0.9103, 'grad_norm': 2.8572794213184984, 'learning_rate': 5.268777777777778e-06, 'epoch': 2.106}
{'loss': 0.8915, 'grad_norm': 3.014798243748347, 'learning_rate': 5.267666666666668e-06, 'epoch': 2.1064}
{'loss': 0.892, 'grad_norm': 3.390946061012177, 'learning_rate': 5.266555555555556e-06, 'epoch': 2.1068}
{'loss': 0.894, 'grad_norm': 2.7992182510976007, 'learning_rate': 5.265444444444445e-06, 'epoch': 2.1072}
{'loss': 0.8933, 'grad_norm': 2.958430474437069, 'learning_rate': 5.264333333333333e-06, 'epoch': 2.1076}
{'loss': 0.891, 'grad_norm': 3.0507952876628917, 'learning_rate': 5.263222222222223e-06, 'epoch': 2.108}
{'eval_valid_loss': 0.8681640625, 'eval_valid_runtime': 0.088, 'eval_valid_samples_per_second': 1136.304, 'eval_valid_steps_per_second': 284.076, 'epoch': 2.108}
{'loss': 0.8928, 'grad_norm': 3.1255332015054034, 'learning_rate': 5.262111111111111e-06, 'epoch': 2.1084}
{'loss': 0.9041, 'grad_norm': 3.14059988173124, 'learning_rate': 5.261000000000001e-06, 'epoch': 2.1088}
{'loss': 0.8968, 'grad_norm': 2.8787471359775356, 'learning_rate': 5.259888888888889e-06, 'epoch': 2.1092}
{'loss': 0.8943, 'grad_norm': 3.0368034184151735, 'learning_rate': 5.258777777777778e-06, 'epoch': 2.1096}
{'loss': 0.8992, 'grad_norm': 3.0217474814870235, 'learning_rate': 5.257666666666668e-06, 'epoch': 2.11}
{'loss': 0.9046, 'grad_norm': 2.9447567261083796, 'learning_rate': 5.256555555555556e-06, 'epoch': 2.1104}
{'loss': 0.8936, 'grad_norm': 2.719800247481763, 'learning_rate': 5.255444444444445e-06, 'epoch': 2.1108}
{'loss': 0.8949, 'grad_norm': 2.9612748852155573, 'learning_rate': 5.254333333333333e-06, 'epoch': 2.1112}
{'loss': 0.8965, 'grad_norm': 2.8926469484290016, 'learning_rate': 5.253222222222223e-06, 'epoch': 2.1116}
{'loss': 0.8889, 'grad_norm': 2.7671058591046314, 'learning_rate': 5.2521111111111115e-06, 'epoch': 2.112}
{'eval_valid_loss': 0.8681640625, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.996, 'eval_valid_steps_per_second': 278.999, 'epoch': 2.112}
{'loss': 0.8871, 'grad_norm': 2.926738394009994, 'learning_rate': 5.251e-06, 'epoch': 2.1124}
{'loss': 0.9058, 'grad_norm': 3.084018409932574, 'learning_rate': 5.249888888888889e-06, 'epoch': 2.1128}
{'loss': 0.8969, 'grad_norm': 2.9749973216966388, 'learning_rate': 5.2487777777777785e-06, 'epoch': 2.1132}
{'loss': 0.9025, 'grad_norm': 2.744357236953688, 'learning_rate': 5.247666666666667e-06, 'epoch': 2.1136}
{'loss': 0.8917, 'grad_norm': 2.923890841314073, 'learning_rate': 5.246555555555556e-06, 'epoch': 2.114}
{'loss': 0.8975, 'grad_norm': 3.1999917872144694, 'learning_rate': 5.2454444444444455e-06, 'epoch': 2.1144}
{'loss': 0.891, 'grad_norm': 3.0423666832207306, 'learning_rate': 5.244333333333333e-06, 'epoch': 2.1148}
{'loss': 0.8797, 'grad_norm': 3.064292295671484, 'learning_rate': 5.243222222222223e-06, 'epoch': 2.1152}
{'loss': 0.895, 'grad_norm': 2.8930453606334994, 'learning_rate': 5.242111111111111e-06, 'epoch': 2.1156}
{'loss': 0.8874, 'grad_norm': 3.194243077252349, 'learning_rate': 5.241e-06, 'epoch': 2.116}
{'eval_valid_loss': 0.8671875, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1126.747, 'eval_valid_steps_per_second': 281.687, 'epoch': 2.116}
{'loss': 0.8765, 'grad_norm': 3.0724908415158922, 'learning_rate': 5.239888888888889e-06, 'epoch': 2.1164}
{'loss': 0.8944, 'grad_norm': 3.030675479675409, 'learning_rate': 5.2387777777777786e-06, 'epoch': 2.1168}
{'loss': 0.8837, 'grad_norm': 2.8114768021410255, 'learning_rate': 5.237666666666667e-06, 'epoch': 2.1172}
{'loss': 0.8877, 'grad_norm': 3.0082153248734587, 'learning_rate': 5.236555555555556e-06, 'epoch': 2.1176}
{'loss': 0.8899, 'grad_norm': 3.3187864409348014, 'learning_rate': 5.2354444444444456e-06, 'epoch': 2.118}
{'loss': 0.8882, 'grad_norm': 2.896919783664773, 'learning_rate': 5.234333333333333e-06, 'epoch': 2.1184}
{'loss': 0.8988, 'grad_norm': 2.991650685096777, 'learning_rate': 5.233222222222223e-06, 'epoch': 2.1188}
{'loss': 0.9027, 'grad_norm': 2.993359308891191, 'learning_rate': 5.232111111111111e-06, 'epoch': 2.1192}
{'loss': 0.8966, 'grad_norm': 2.9714758455513377, 'learning_rate': 5.231e-06, 'epoch': 2.1196}
{'loss': 0.8976, 'grad_norm': 3.010040112256663, 'learning_rate': 5.229888888888889e-06, 'epoch': 2.12}
{'eval_valid_loss': 0.86865234375, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.565, 'eval_valid_steps_per_second': 281.891, 'epoch': 2.12}
{'loss': 0.898, 'grad_norm': 3.1914802552027335, 'learning_rate': 5.228777777777778e-06, 'epoch': 2.1204}
{'loss': 0.887, 'grad_norm': 2.890698323737474, 'learning_rate': 5.227666666666667e-06, 'epoch': 2.1208}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.9025, 'grad_norm': 3.014309027731133, 'learning_rate': 5.226555555555556e-06, 'epoch': 2.1212}
{'loss': 0.8956, 'grad_norm': 3.030288317610325, 'learning_rate': 5.225444444444445e-06, 'epoch': 2.1216}
{'loss': 0.9038, 'grad_norm': 2.9553755404410507, 'learning_rate': 5.2243333333333335e-06, 'epoch': 2.122}
{'loss': 0.8967, 'grad_norm': 3.029533244195347, 'learning_rate': 5.223222222222223e-06, 'epoch': 2.1224}
{'loss': 0.9017, 'grad_norm': 3.231599409222587, 'learning_rate': 5.2222222222222226e-06, 'epoch': 2.1228}
{'loss': 0.9002, 'grad_norm': 2.904332543611355, 'learning_rate': 5.221111111111111e-06, 'epoch': 2.1232}
{'loss': 0.8898, 'grad_norm': 3.2185542593390357, 'learning_rate': 5.220000000000001e-06, 'epoch': 2.1236}
{'loss': 0.8918, 'grad_norm': 2.882474553838738, 'learning_rate': 5.218888888888889e-06, 'epoch': 2.124}
{'eval_valid_loss': 0.8671875, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1110.521, 'eval_valid_steps_per_second': 277.63, 'epoch': 2.124}
{'loss': 0.8922, 'grad_norm': 2.8798058190051252, 'learning_rate': 5.217777777777778e-06, 'epoch': 2.1244}
{'loss': 0.8963, 'grad_norm': 3.2403583284459883, 'learning_rate': 5.216666666666666e-06, 'epoch': 2.1248}
{'loss': 0.8932, 'grad_norm': 2.7572644691485397, 'learning_rate': 5.215555555555556e-06, 'epoch': 2.1252}
{'loss': 0.8841, 'grad_norm': 2.7786546824396883, 'learning_rate': 5.214444444444445e-06, 'epoch': 2.1256}
{'loss': 0.8931, 'grad_norm': 2.7400861769371287, 'learning_rate': 5.213333333333334e-06, 'epoch': 2.126}
{'loss': 0.9037, 'grad_norm': 2.8326895218428643, 'learning_rate': 5.212222222222223e-06, 'epoch': 2.1264}
{'loss': 0.8796, 'grad_norm': 2.718879411620938, 'learning_rate': 5.211111111111111e-06, 'epoch': 2.1268}
{'loss': 0.8914, 'grad_norm': 2.9180679764020026, 'learning_rate': 5.210000000000001e-06, 'epoch': 2.1272}
{'loss': 0.8913, 'grad_norm': 3.38151968728392, 'learning_rate': 5.208888888888889e-06, 'epoch': 2.1276}
{'loss': 0.8913, 'grad_norm': 2.998017072995454, 'learning_rate': 5.207777777777778e-06, 'epoch': 2.128}
{'eval_valid_loss': 0.8681640625, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1110.765, 'eval_valid_steps_per_second': 277.691, 'epoch': 2.128}
{'loss': 0.8913, 'grad_norm': 2.949979560183173, 'learning_rate': 5.206666666666668e-06, 'epoch': 2.1284}
{'loss': 0.8902, 'grad_norm': 2.9302466914250447, 'learning_rate': 5.205555555555556e-06, 'epoch': 2.1288}
{'loss': 0.9124, 'grad_norm': 3.2423166823791165, 'learning_rate': 5.204444444444445e-06, 'epoch': 2.1292}
{'loss': 0.8941, 'grad_norm': 2.7447507404024454, 'learning_rate': 5.203333333333333e-06, 'epoch': 2.1296}
{'loss': 0.9019, 'grad_norm': 2.8062631967824725, 'learning_rate': 5.202222222222223e-06, 'epoch': 2.13}
{'loss': 0.8986, 'grad_norm': 2.875749055059902, 'learning_rate': 5.2011111111111115e-06, 'epoch': 2.1304}
{'loss': 0.8924, 'grad_norm': 2.8590054507728966, 'learning_rate': 5.2e-06, 'epoch': 2.1308}
{'loss': 0.8941, 'grad_norm': 2.959961974553776, 'learning_rate': 5.198888888888889e-06, 'epoch': 2.1312}
{'loss': 0.9037, 'grad_norm': 2.925859967179972, 'learning_rate': 5.1977777777777785e-06, 'epoch': 2.1316}
{'loss': 0.9036, 'grad_norm': 3.1664167805422934, 'learning_rate': 5.196666666666668e-06, 'epoch': 2.132}
{'eval_valid_loss': 0.865234375, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.245, 'eval_valid_steps_per_second': 279.811, 'epoch': 2.132}
{'loss': 0.9105, 'grad_norm': 2.629506318984388, 'learning_rate': 5.195555555555556e-06, 'epoch': 2.1324}
{'loss': 0.8839, 'grad_norm': 2.8906728792091534, 'learning_rate': 5.1944444444444454e-06, 'epoch': 2.1328}
{'loss': 0.8975, 'grad_norm': 2.9926916748461343, 'learning_rate': 5.193333333333333e-06, 'epoch': 2.1332}
{'loss': 0.885, 'grad_norm': 3.2521596115586555, 'learning_rate': 5.192222222222223e-06, 'epoch': 2.1336}
{'loss': 0.8884, 'grad_norm': 3.2725643914563434, 'learning_rate': 5.1911111111111116e-06, 'epoch': 2.134}
{'loss': 0.8802, 'grad_norm': 2.9140074062508567, 'learning_rate': 5.19e-06, 'epoch': 2.1344}
{'loss': 0.8951, 'grad_norm': 3.135777323860707, 'learning_rate': 5.188888888888889e-06, 'epoch': 2.1348}
{'loss': 0.9028, 'grad_norm': 2.808243571855212, 'learning_rate': 5.1877777777777786e-06, 'epoch': 2.1352}
{'loss': 0.8952, 'grad_norm': 2.6623461723663633, 'learning_rate': 5.186666666666667e-06, 'epoch': 2.1356}
{'loss': 0.9003, 'grad_norm': 3.039271565677023, 'learning_rate': 5.185555555555556e-06, 'epoch': 2.136}
{'eval_valid_loss': 0.8671875, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1106.02, 'eval_valid_steps_per_second': 276.505, 'epoch': 2.136}
{'loss': 0.8872, 'grad_norm': 2.8976145396003683, 'learning_rate': 5.1844444444444455e-06, 'epoch': 2.1364}
{'loss': 0.8991, 'grad_norm': 2.7733109619932086, 'learning_rate': 5.183333333333333e-06, 'epoch': 2.1368}
{'loss': 0.8975, 'grad_norm': 2.9558613426914775, 'learning_rate': 5.182222222222223e-06, 'epoch': 2.1372}
{'loss': 0.8929, 'grad_norm': 2.868467788278714, 'learning_rate': 5.181111111111111e-06, 'epoch': 2.1376}
{'loss': 0.8888, 'grad_norm': 2.961159036043556, 'learning_rate': 5.18e-06, 'epoch': 2.138}
{'loss': 0.9053, 'grad_norm': 2.720854350403662, 'learning_rate': 5.178888888888889e-06, 'epoch': 2.1384}
{'loss': 0.8972, 'grad_norm': 3.046872486211278, 'learning_rate': 5.177777777777779e-06, 'epoch': 2.1388}
{'loss': 0.8939, 'grad_norm': 3.1446411492708317, 'learning_rate': 5.176666666666667e-06, 'epoch': 2.1391999999999998}
{'loss': 0.8937, 'grad_norm': 2.7403046808991496, 'learning_rate': 5.175555555555556e-06, 'epoch': 2.1396}
{'loss': 0.8929, 'grad_norm': 2.8064580551368956, 'learning_rate': 5.174444444444446e-06, 'epoch': 2.14}
{'eval_valid_loss': 0.8681640625, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.15, 'eval_valid_steps_per_second': 279.538, 'epoch': 2.14}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.8849, 'grad_norm': 2.9240716133330347, 'learning_rate': 5.1733333333333335e-06, 'epoch': 2.1404}
{'loss': 0.8877, 'grad_norm': 2.866538491451603, 'learning_rate': 5.172222222222223e-06, 'epoch': 2.1408}
{'loss': 0.893, 'grad_norm': 3.0579298621623825, 'learning_rate': 5.171111111111111e-06, 'epoch': 2.1412}
{'loss': 0.893, 'grad_norm': 3.1089286196248467, 'learning_rate': 5.1700000000000005e-06, 'epoch': 2.1416}
{'loss': 0.9002, 'grad_norm': 2.716931112827523, 'learning_rate': 5.168888888888889e-06, 'epoch': 2.142}
{'loss': 0.9005, 'grad_norm': 2.985961901586486, 'learning_rate': 5.167777777777778e-06, 'epoch': 2.1424}
{'loss': 0.8881, 'grad_norm': 2.8722281051088103, 'learning_rate': 5.1666666666666675e-06, 'epoch': 2.1428}
{'loss': 0.8944, 'grad_norm': 2.9418154225456776, 'learning_rate': 5.165555555555556e-06, 'epoch': 2.1432}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8984, 'grad_norm': 2.8135486872920987, 'learning_rate': 5.164444444444445e-06, 'epoch': 2.1436}
{'loss': 0.8909, 'grad_norm': 2.743525697494272, 'learning_rate': 5.163333333333334e-06, 'epoch': 2.144}
{'eval_valid_loss': 0.8681640625, 'eval_valid_runtime': 0.0907, 'eval_valid_samples_per_second': 1102.821, 'eval_valid_steps_per_second': 275.705, 'epoch': 2.144}
{'loss': 0.9014, 'grad_norm': 3.145283560470501, 'learning_rate': 5.162222222222223e-06, 'epoch': 2.1444}
{'loss': 0.9057, 'grad_norm': 3.150297301885519, 'learning_rate': 5.161111111111111e-06, 'epoch': 2.1448}
{'loss': 0.8918, 'grad_norm': 2.9541076155671453, 'learning_rate': 5.1600000000000006e-06, 'epoch': 2.1452}
{'loss': 0.902, 'grad_norm': 2.750509269115541, 'learning_rate': 5.1588888888888884e-06, 'epoch': 2.1456}
{'loss': 0.8923, 'grad_norm': 2.971328448831502, 'learning_rate': 5.157777777777778e-06, 'epoch': 2.146}
{'loss': 0.8911, 'grad_norm': 3.494754266136071, 'learning_rate': 5.1566666666666676e-06, 'epoch': 2.1464}
{'loss': 0.8999, 'grad_norm': 2.8201448808138956, 'learning_rate': 5.155555555555556e-06, 'epoch': 2.1468}
{'loss': 0.8971, 'grad_norm': 3.1674434165145864, 'learning_rate': 5.154444444444445e-06, 'epoch': 2.1471999999999998}
{'loss': 0.9011, 'grad_norm': 3.0006031284436996, 'learning_rate': 5.153333333333334e-06, 'epoch': 2.1476}
{'loss': 0.9004, 'grad_norm': 2.7962403563024925, 'learning_rate': 5.152222222222223e-06, 'epoch': 2.148}
{'eval_valid_loss': 0.8671875, 'eval_valid_runtime': 0.091, 'eval_valid_samples_per_second': 1099.24, 'eval_valid_steps_per_second': 274.81, 'epoch': 2.148}
{'loss': 0.8992, 'grad_norm': 2.8650990917718175, 'learning_rate': 5.151111111111111e-06, 'epoch': 2.1484}
{'loss': 0.9008, 'grad_norm': 2.861197552614833, 'learning_rate': 5.150000000000001e-06, 'epoch': 2.1488}
{'loss': 0.8875, 'grad_norm': 2.7973785160267095, 'learning_rate': 5.1488888888888885e-06, 'epoch': 2.1492}
{'loss': 0.9014, 'grad_norm': 2.818228757584828, 'learning_rate': 5.147777777777778e-06, 'epoch': 2.1496}
{'loss': 0.8907, 'grad_norm': 2.802281997804271, 'learning_rate': 5.146666666666668e-06, 'epoch': 2.15}
{'loss': 0.8875, 'grad_norm': 2.6751126087408497, 'learning_rate': 5.1455555555555555e-06, 'epoch': 2.1504}
{'loss': 0.8913, 'grad_norm': 3.21135710784488, 'learning_rate': 5.144444444444445e-06, 'epoch': 2.1508}
{'loss': 0.891, 'grad_norm': 2.9520261829437526, 'learning_rate': 5.143333333333334e-06, 'epoch': 2.1512000000000002}
{'loss': 0.8795, 'grad_norm': 3.100668117605556, 'learning_rate': 5.1422222222222225e-06, 'epoch': 2.1516}
{'loss': 0.8956, 'grad_norm': 2.806686921292008, 'learning_rate': 5.141111111111111e-06, 'epoch': 2.152}
{'eval_valid_loss': 0.8662109375, 'eval_valid_runtime': 0.0906, 'eval_valid_samples_per_second': 1103.163, 'eval_valid_steps_per_second': 275.791, 'epoch': 2.152}
{'loss': 0.8977, 'grad_norm': 2.704126789806061, 'learning_rate': 5.140000000000001e-06, 'epoch': 2.1524}
{'loss': 0.8837, 'grad_norm': 2.7852704959673718, 'learning_rate': 5.138888888888889e-06, 'epoch': 2.1528}
{'loss': 0.909, 'grad_norm': 3.189239560211168, 'learning_rate': 5.137777777777778e-06, 'epoch': 2.1532}
{'loss': 0.9026, 'grad_norm': 3.05587156328211, 'learning_rate': 5.136666666666668e-06, 'epoch': 2.1536}
{'loss': 0.8891, 'grad_norm': 3.107308184871489, 'learning_rate': 5.135555555555556e-06, 'epoch': 2.154}
{'loss': 0.8926, 'grad_norm': 3.0907691408956404, 'learning_rate': 5.134444444444445e-06, 'epoch': 2.1544}
{'loss': 0.8925, 'grad_norm': 2.9810695317664884, 'learning_rate': 5.133333333333334e-06, 'epoch': 2.1548}
{'loss': 0.8948, 'grad_norm': 2.97141304035798, 'learning_rate': 5.132222222222223e-06, 'epoch': 2.1552}
{'loss': 0.8877, 'grad_norm': 2.8554506001241378, 'learning_rate': 5.131111111111111e-06, 'epoch': 2.1556}
{'loss': 0.8967, 'grad_norm': 3.008170802585835, 'learning_rate': 5.130000000000001e-06, 'epoch': 2.156}
{'eval_valid_loss': 0.8662109375, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1120.615, 'eval_valid_steps_per_second': 280.154, 'epoch': 2.156}
{'loss': 0.8939, 'grad_norm': 3.1438432118454895, 'learning_rate': 5.128888888888889e-06, 'epoch': 2.1564}
{'loss': 0.8985, 'grad_norm': 2.68459175464869, 'learning_rate': 5.127777777777778e-06, 'epoch': 2.1568}
{'loss': 0.8957, 'grad_norm': 2.8919875052625375, 'learning_rate': 5.126666666666668e-06, 'epoch': 2.1572}
{'loss': 0.8966, 'grad_norm': 2.7579875382590693, 'learning_rate': 5.125555555555556e-06, 'epoch': 2.1576}
{'loss': 0.8892, 'grad_norm': 2.969689190524982, 'learning_rate': 5.124444444444445e-06, 'epoch': 2.158}
{'loss': 0.8953, 'grad_norm': 2.873039696825138, 'learning_rate': 5.123333333333333e-06, 'epoch': 2.1584}
{'loss': 0.8813, 'grad_norm': 2.8214203462989556, 'learning_rate': 5.122222222222223e-06, 'epoch': 2.1588}
{'loss': 0.9013, 'grad_norm': 2.853467122317831, 'learning_rate': 5.121111111111111e-06, 'epoch': 2.1592000000000002}
{'loss': 0.9022, 'grad_norm': 3.1849143910692628, 'learning_rate': 5.12e-06, 'epoch': 2.1596}
{'loss': 0.8932, 'grad_norm': 2.841359511864992, 'learning_rate': 5.118888888888889e-06, 'epoch': 2.16}
{'eval_valid_loss': 0.86767578125, 'eval_valid_runtime': 0.0903, 'eval_valid_samples_per_second': 1107.705, 'eval_valid_steps_per_second': 276.926, 'epoch': 2.16}
{'loss': 0.8915, 'grad_norm': 2.9479210480611386, 'learning_rate': 5.117777777777778e-06, 'epoch': 2.1604}
{'loss': 0.9016, 'grad_norm': 2.819242332498134, 'learning_rate': 5.116666666666668e-06, 'epoch': 2.1608}
{'loss': 0.9018, 'grad_norm': 2.924177537526271, 'learning_rate': 5.115555555555556e-06, 'epoch': 2.1612}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8831, 'grad_norm': 2.912085550314388, 'learning_rate': 5.114444444444445e-06, 'epoch': 2.1616}
{'loss': 0.9055, 'grad_norm': 3.053016820375205, 'learning_rate': 5.113333333333333e-06, 'epoch': 2.162}
{'loss': 0.9046, 'grad_norm': 2.991558287675459, 'learning_rate': 5.112222222222223e-06, 'epoch': 2.1624}
{'loss': 0.894, 'grad_norm': 3.1499343797115764, 'learning_rate': 5.1111111111111115e-06, 'epoch': 2.1628}
{'loss': 0.888, 'grad_norm': 2.7729625133982285, 'learning_rate': 5.110111111111111e-06, 'epoch': 2.1632}
{'loss': 0.8853, 'grad_norm': 2.9293039910117744, 'learning_rate': 5.1090000000000006e-06, 'epoch': 2.1636}
{'loss': 0.8943, 'grad_norm': 2.841878600108826, 'learning_rate': 5.107888888888889e-06, 'epoch': 2.164}
{'eval_valid_loss': 0.8681640625, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.607, 'eval_valid_steps_per_second': 277.152, 'epoch': 2.164}
{'loss': 0.8901, 'grad_norm': 2.925955233729486, 'learning_rate': 5.106777777777778e-06, 'epoch': 2.1644}
{'loss': 0.8952, 'grad_norm': 3.130332831108893, 'learning_rate': 5.105666666666667e-06, 'epoch': 2.1648}
{'loss': 0.8924, 'grad_norm': 2.922914401728939, 'learning_rate': 5.104555555555556e-06, 'epoch': 2.1652}
{'loss': 0.905, 'grad_norm': 3.0634591780349547, 'learning_rate': 5.103444444444445e-06, 'epoch': 2.1656}
{'loss': 0.8929, 'grad_norm': 2.8041141529654827, 'learning_rate': 5.102333333333334e-06, 'epoch': 2.166}
{'loss': 0.9029, 'grad_norm': 3.0212278842866267, 'learning_rate': 5.101222222222223e-06, 'epoch': 2.1664}
{'loss': 0.8857, 'grad_norm': 2.845920426156335, 'learning_rate': 5.100111111111111e-06, 'epoch': 2.1668}
{'loss': 0.8954, 'grad_norm': 2.7374174358162415, 'learning_rate': 5.099000000000001e-06, 'epoch': 2.1672}
{'loss': 0.8828, 'grad_norm': 2.8968947487068455, 'learning_rate': 5.0978888888888885e-06, 'epoch': 2.1676}
{'loss': 0.8929, 'grad_norm': 2.9449582177745777, 'learning_rate': 5.096777777777778e-06, 'epoch': 2.168}
{'eval_valid_loss': 0.86962890625, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1119.075, 'eval_valid_steps_per_second': 279.769, 'epoch': 2.168}
{'loss': 0.8913, 'grad_norm': 3.0840655576470644, 'learning_rate': 5.095666666666667e-06, 'epoch': 2.1684}
{'loss': 0.9071, 'grad_norm': 2.9551822823008322, 'learning_rate': 5.094555555555556e-06, 'epoch': 2.1688}
{'loss': 0.8866, 'grad_norm': 2.764809057056322, 'learning_rate': 5.093444444444445e-06, 'epoch': 2.1692}
{'loss': 0.902, 'grad_norm': 2.847127847089515, 'learning_rate': 5.092333333333334e-06, 'epoch': 2.1696}
{'loss': 0.8992, 'grad_norm': 2.816195772430333, 'learning_rate': 5.091222222222223e-06, 'epoch': 2.17}
{'loss': 0.894, 'grad_norm': 2.8981150733131136, 'learning_rate': 5.090111111111111e-06, 'epoch': 2.1704}
{'loss': 0.8826, 'grad_norm': 2.62401375522112, 'learning_rate': 5.089000000000001e-06, 'epoch': 2.1708}
{'loss': 0.8934, 'grad_norm': 2.985667902315012, 'learning_rate': 5.087888888888889e-06, 'epoch': 2.1712}
{'loss': 0.8802, 'grad_norm': 2.9497724606968, 'learning_rate': 5.086777777777778e-06, 'epoch': 2.1716}
{'loss': 0.8857, 'grad_norm': 2.995159306834821, 'learning_rate': 5.085666666666667e-06, 'epoch': 2.172}
{'eval_valid_loss': 0.86767578125, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.62, 'eval_valid_steps_per_second': 279.405, 'epoch': 2.172}
{'loss': 0.8842, 'grad_norm': 3.1089810644219145, 'learning_rate': 5.084555555555556e-06, 'epoch': 2.1724}
{'loss': 0.8865, 'grad_norm': 2.8783084347035843, 'learning_rate': 5.083444444444445e-06, 'epoch': 2.1728}
{'loss': 0.89, 'grad_norm': 3.104667608219463, 'learning_rate': 5.082333333333334e-06, 'epoch': 2.1732}
{'loss': 0.8961, 'grad_norm': 2.925133841052652, 'learning_rate': 5.0812222222222226e-06, 'epoch': 2.1736}
{'loss': 0.8904, 'grad_norm': 3.0148519503523192, 'learning_rate': 5.080111111111111e-06, 'epoch': 2.174}
{'loss': 0.8873, 'grad_norm': 2.9516498272054847, 'learning_rate': 5.079000000000001e-06, 'epoch': 2.1744}
{'loss': 0.8986, 'grad_norm': 2.8527580315618337, 'learning_rate': 5.077888888888889e-06, 'epoch': 2.1748}
{'loss': 0.8902, 'grad_norm': 2.9618837258797406, 'learning_rate': 5.076777777777778e-06, 'epoch': 2.1752}
{'loss': 0.8952, 'grad_norm': 3.0836231969640764, 'learning_rate': 5.075666666666668e-06, 'epoch': 2.1756}
{'loss': 0.8901, 'grad_norm': 2.8851674393437667, 'learning_rate': 5.074555555555556e-06, 'epoch': 2.176}
{'eval_valid_loss': 0.86767578125, 'eval_valid_runtime': 0.0903, 'eval_valid_samples_per_second': 1107.36, 'eval_valid_steps_per_second': 276.84, 'epoch': 2.176}
{'loss': 0.8997, 'grad_norm': 3.558624811801117, 'learning_rate': 5.073444444444445e-06, 'epoch': 2.1764}
{'loss': 0.8965, 'grad_norm': 2.6951430502437876, 'learning_rate': 5.072333333333334e-06, 'epoch': 2.1768}
{'loss': 0.8831, 'grad_norm': 2.9111221041989506, 'learning_rate': 5.071222222222223e-06, 'epoch': 2.1772}
{'loss': 0.8932, 'grad_norm': 2.8923116042159274, 'learning_rate': 5.070111111111111e-06, 'epoch': 2.1776}
{'loss': 0.9082, 'grad_norm': 2.9189495087916004, 'learning_rate': 5.069000000000001e-06, 'epoch': 2.178}
{'loss': 0.8964, 'grad_norm': 3.0076270504543334, 'learning_rate': 5.067888888888889e-06, 'epoch': 2.1784}
{'loss': 0.9051, 'grad_norm': 2.8298675269201734, 'learning_rate': 5.066777777777778e-06, 'epoch': 2.1788}
{'loss': 0.9023, 'grad_norm': 3.0003511402147858, 'learning_rate': 5.065666666666668e-06, 'epoch': 2.1792}
{'loss': 0.8947, 'grad_norm': 2.91554829863539, 'learning_rate': 5.064555555555556e-06, 'epoch': 2.1796}
{'loss': 0.8958, 'grad_norm': 2.933005983924029, 'learning_rate': 5.063444444444445e-06, 'epoch': 2.18}
{'eval_valid_loss': 0.8671875, 'eval_valid_runtime': 0.0885, 'eval_valid_samples_per_second': 1129.995, 'eval_valid_steps_per_second': 282.499, 'epoch': 2.18}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.9015, 'grad_norm': 2.8675185280112316, 'learning_rate': 5.062333333333333e-06, 'epoch': 2.1804}
{'loss': 0.9008, 'grad_norm': 2.8683349537683225, 'learning_rate': 5.061222222222223e-06, 'epoch': 2.1808}
{'loss': 0.8931, 'grad_norm': 3.130268509974522, 'learning_rate': 5.0601111111111115e-06, 'epoch': 2.1812}
{'loss': 0.8954, 'grad_norm': 2.747093376879213, 'learning_rate': 5.059e-06, 'epoch': 2.1816}
{'loss': 0.8887, 'grad_norm': 2.686265667523065, 'learning_rate': 5.057888888888889e-06, 'epoch': 2.182}
{'loss': 0.8978, 'grad_norm': 3.0655964359115395, 'learning_rate': 5.0567777777777785e-06, 'epoch': 2.1824}
{'loss': 0.903, 'grad_norm': 2.953660936518216, 'learning_rate': 5.055666666666668e-06, 'epoch': 2.1828}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8956, 'grad_norm': 2.61932434416249, 'learning_rate': 5.054555555555556e-06, 'epoch': 2.1832}
{'loss': 0.8815, 'grad_norm': 2.8936553226908908, 'learning_rate': 5.0534444444444454e-06, 'epoch': 2.1836}
{'loss': 0.9112, 'grad_norm': 3.0089041475240244, 'learning_rate': 5.052333333333333e-06, 'epoch': 2.184}
{'eval_valid_loss': 0.86767578125, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.469, 'eval_valid_steps_per_second': 278.617, 'epoch': 2.184}
{'loss': 0.8911, 'grad_norm': 2.9572295975271157, 'learning_rate': 5.051222222222223e-06, 'epoch': 2.1844}
{'loss': 0.8774, 'grad_norm': 2.4466567503642, 'learning_rate': 5.0501111111111116e-06, 'epoch': 2.1848}
{'loss': 0.9064, 'grad_norm': 2.5280510385867396, 'learning_rate': 5.049e-06, 'epoch': 2.1852}
{'loss': 0.8875, 'grad_norm': 2.8890838432452037, 'learning_rate': 5.047888888888889e-06, 'epoch': 2.1856}
{'loss': 0.9004, 'grad_norm': 3.102941749286625, 'learning_rate': 5.0467777777777785e-06, 'epoch': 2.186}
{'loss': 0.8956, 'grad_norm': 3.157094540300264, 'learning_rate': 5.045666666666667e-06, 'epoch': 2.1864}
{'loss': 0.8927, 'grad_norm': 3.0312873798700504, 'learning_rate': 5.044555555555556e-06, 'epoch': 2.1868}
{'loss': 0.8797, 'grad_norm': 3.0303104655308255, 'learning_rate': 5.0434444444444455e-06, 'epoch': 2.1872}
{'loss': 0.9037, 'grad_norm': 2.8340950332374377, 'learning_rate': 5.042333333333333e-06, 'epoch': 2.1875999999999998}
{'loss': 0.8946, 'grad_norm': 2.9862683566532424, 'learning_rate': 5.041222222222223e-06, 'epoch': 2.188}
{'eval_valid_loss': 0.865234375, 'eval_valid_runtime': 0.0884, 'eval_valid_samples_per_second': 1131.425, 'eval_valid_steps_per_second': 282.856, 'epoch': 2.188}
{'loss': 0.8911, 'grad_norm': 2.7715300273294052, 'learning_rate': 5.040111111111111e-06, 'epoch': 2.1884}
{'loss': 0.9069, 'grad_norm': 2.928062781911562, 'learning_rate': 5.039e-06, 'epoch': 2.1888}
{'loss': 0.8957, 'grad_norm': 3.3391619069962215, 'learning_rate': 5.037888888888889e-06, 'epoch': 2.1892}
{'loss': 0.8864, 'grad_norm': 2.7838601390939894, 'learning_rate': 5.036777777777778e-06, 'epoch': 2.1896}
{'loss': 0.8917, 'grad_norm': 2.8940655311772083, 'learning_rate': 5.035666666666667e-06, 'epoch': 2.19}
{'loss': 0.8926, 'grad_norm': 2.8172542390624757, 'learning_rate': 5.034555555555556e-06, 'epoch': 2.1904}
{'loss': 0.9025, 'grad_norm': 2.8798941440301133, 'learning_rate': 5.033444444444446e-06, 'epoch': 2.1908}
{'loss': 0.8836, 'grad_norm': 3.1363224144941944, 'learning_rate': 5.0323333333333335e-06, 'epoch': 2.1912}
{'loss': 0.8921, 'grad_norm': 3.0964190211881855, 'learning_rate': 5.031222222222223e-06, 'epoch': 2.1916}
{'loss': 0.9021, 'grad_norm': 2.9039128042521183, 'learning_rate': 5.030111111111111e-06, 'epoch': 2.192}
{'eval_valid_loss': 0.8671875, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.377, 'eval_valid_steps_per_second': 279.594, 'epoch': 2.192}
{'loss': 0.891, 'grad_norm': 3.016679075559803, 'learning_rate': 5.0290000000000005e-06, 'epoch': 2.1924}
{'loss': 0.8901, 'grad_norm': 2.8700501082694814, 'learning_rate': 5.027888888888889e-06, 'epoch': 2.1928}
{'loss': 0.8879, 'grad_norm': 2.7769951849786834, 'learning_rate': 5.026777777777778e-06, 'epoch': 2.1932}
{'loss': 0.9035, 'grad_norm': 3.2536044123902244, 'learning_rate': 5.0256666666666675e-06, 'epoch': 2.1936}
{'loss': 0.8847, 'grad_norm': 2.9003237013368817, 'learning_rate': 5.024555555555556e-06, 'epoch': 2.194}
{'loss': 0.8881, 'grad_norm': 2.675063389178592, 'learning_rate': 5.023444444444445e-06, 'epoch': 2.1944}
{'loss': 0.8926, 'grad_norm': 2.9690646255818, 'learning_rate': 5.022333333333334e-06, 'epoch': 2.1948}
{'loss': 0.8967, 'grad_norm': 2.926624955683492, 'learning_rate': 5.021222222222223e-06, 'epoch': 2.1952}
{'loss': 0.8979, 'grad_norm': 2.478328514401211, 'learning_rate': 5.020111111111111e-06, 'epoch': 2.1955999999999998}
{'loss': 0.8888, 'grad_norm': 3.0129168807648505, 'learning_rate': 5.0190000000000006e-06, 'epoch': 2.196}
{'eval_valid_loss': 0.8662109375, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.525, 'eval_valid_steps_per_second': 279.131, 'epoch': 2.196}
{'loss': 0.8972, 'grad_norm': 3.13968361557494, 'learning_rate': 5.0178888888888884e-06, 'epoch': 2.1964}
{'loss': 0.8851, 'grad_norm': 3.338155201380871, 'learning_rate': 5.016777777777778e-06, 'epoch': 2.1968}
{'loss': 0.8892, 'grad_norm': 2.7197048823316066, 'learning_rate': 5.0156666666666675e-06, 'epoch': 2.1972}
{'loss': 0.8999, 'grad_norm': 3.0395218764313388, 'learning_rate': 5.014555555555556e-06, 'epoch': 2.1976}
{'loss': 0.8864, 'grad_norm': 2.9348326605446307, 'learning_rate': 5.013444444444445e-06, 'epoch': 2.198}
{'loss': 0.8888, 'grad_norm': 3.1023640689842487, 'learning_rate': 5.012333333333334e-06, 'epoch': 2.1984}
{'loss': 0.8856, 'grad_norm': 3.247776426248758, 'learning_rate': 5.011222222222223e-06, 'epoch': 2.1988}
{'loss': 0.9022, 'grad_norm': 3.0729882517520855, 'learning_rate': 5.010111111111111e-06, 'epoch': 2.1992}
{'loss': 0.8995, 'grad_norm': 2.9954930067520382, 'learning_rate': 5.009000000000001e-06, 'epoch': 2.1996}
{'loss': 0.8906, 'grad_norm': 3.022591031283681, 'learning_rate': 5.0078888888888885e-06, 'epoch': 2.2}
{'eval_valid_loss': 0.86669921875, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.927, 'eval_valid_steps_per_second': 278.482, 'epoch': 2.2}
{'loss': 0.886, 'grad_norm': 3.1795011742288692, 'learning_rate': 5.006777777777778e-06, 'epoch': 2.2004}
{'loss': 0.8837, 'grad_norm': 3.0005346060297007, 'learning_rate': 5.005666666666668e-06, 'epoch': 2.2008}
{'loss': 0.8879, 'grad_norm': 2.716937420059754, 'learning_rate': 5.0045555555555555e-06, 'epoch': 2.2012}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.895, 'grad_norm': 2.801049117256565, 'learning_rate': 5.003444444444445e-06, 'epoch': 2.2016}
{'loss': 0.8753, 'grad_norm': 2.99362049698709, 'learning_rate': 5.002333333333334e-06, 'epoch': 2.202}
{'loss': 0.8889, 'grad_norm': 3.1053939714365506, 'learning_rate': 5.0012222222222225e-06, 'epoch': 2.2024}
{'loss': 0.895, 'grad_norm': 2.990879230993858, 'learning_rate': 5.000111111111111e-06, 'epoch': 2.2028}
{'loss': 0.9021, 'grad_norm': 2.789967318030669, 'learning_rate': 4.9991111111111115e-06, 'epoch': 2.2032}
{'loss': 0.8992, 'grad_norm': 2.953384579749572, 'learning_rate': 4.998e-06, 'epoch': 2.2036}
{'loss': 0.894, 'grad_norm': 2.8895563882526276, 'learning_rate': 4.99688888888889e-06, 'epoch': 2.204}
{'eval_valid_loss': 0.8662109375, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1122.835, 'eval_valid_steps_per_second': 280.709, 'epoch': 2.204}
{'loss': 0.9053, 'grad_norm': 2.999962746865716, 'learning_rate': 4.9957777777777785e-06, 'epoch': 2.2044}
{'loss': 0.8941, 'grad_norm': 3.078342507515128, 'learning_rate': 4.994666666666667e-06, 'epoch': 2.2048}
{'loss': 0.9055, 'grad_norm': 3.1102382285940395, 'learning_rate': 4.993555555555556e-06, 'epoch': 2.2052}
{'loss': 0.8954, 'grad_norm': 2.659265231725893, 'learning_rate': 4.992444444444445e-06, 'epoch': 2.2056}
{'loss': 0.9056, 'grad_norm': 2.9934868045819982, 'learning_rate': 4.991333333333333e-06, 'epoch': 2.206}
{'loss': 0.8887, 'grad_norm': 2.8211163777933703, 'learning_rate': 4.990222222222222e-06, 'epoch': 2.2064}
{'loss': 0.9051, 'grad_norm': 3.1359909372381227, 'learning_rate': 4.989111111111112e-06, 'epoch': 2.2068}
{'loss': 0.8921, 'grad_norm': 3.0266066312157283, 'learning_rate': 4.988e-06, 'epoch': 2.2072}
{'loss': 0.8883, 'grad_norm': 2.8830304670764995, 'learning_rate': 4.986888888888889e-06, 'epoch': 2.2076000000000002}
{'loss': 0.8881, 'grad_norm': 3.0068009694448907, 'learning_rate': 4.985777777777779e-06, 'epoch': 2.208}
{'eval_valid_loss': 0.86669921875, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.914, 'eval_valid_steps_per_second': 278.228, 'epoch': 2.208}
{'loss': 0.8839, 'grad_norm': 2.836890488754788, 'learning_rate': 4.984666666666667e-06, 'epoch': 2.2084}
{'loss': 0.8845, 'grad_norm': 2.9527723212793298, 'learning_rate': 4.983555555555556e-06, 'epoch': 2.2088}
{'loss': 0.8871, 'grad_norm': 2.8977352124375693, 'learning_rate': 4.982444444444445e-06, 'epoch': 2.2092}
{'loss': 0.8898, 'grad_norm': 2.7310667133591684, 'learning_rate': 4.9813333333333335e-06, 'epoch': 2.2096}
{'loss': 0.8963, 'grad_norm': 2.9203038810982425, 'learning_rate': 4.980222222222222e-06, 'epoch': 2.21}
{'loss': 0.9059, 'grad_norm': 2.88754634324573, 'learning_rate': 4.979111111111112e-06, 'epoch': 2.2104}
{'loss': 0.8983, 'grad_norm': 2.9943142695629232, 'learning_rate': 4.9780000000000005e-06, 'epoch': 2.2108}
{'loss': 0.8979, 'grad_norm': 3.142649211755227, 'learning_rate': 4.976888888888889e-06, 'epoch': 2.2112}
{'loss': 0.8899, 'grad_norm': 3.0710946462742, 'learning_rate': 4.975777777777778e-06, 'epoch': 2.2116}
{'loss': 0.8848, 'grad_norm': 2.7245280263918548, 'learning_rate': 4.9746666666666674e-06, 'epoch': 2.212}
{'eval_valid_loss': 0.8662109375, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.623, 'eval_valid_steps_per_second': 278.656, 'epoch': 2.212}
{'loss': 0.8916, 'grad_norm': 2.7266603796586426, 'learning_rate': 4.973555555555556e-06, 'epoch': 2.2124}
{'loss': 0.8889, 'grad_norm': 2.8425195148774454, 'learning_rate': 4.972444444444445e-06, 'epoch': 2.2128}
{'loss': 0.9034, 'grad_norm': 2.832625112421875, 'learning_rate': 4.9713333333333336e-06, 'epoch': 2.2132}
{'loss': 0.8884, 'grad_norm': 2.9918586410798795, 'learning_rate': 4.970222222222222e-06, 'epoch': 2.2136}
{'loss': 0.9002, 'grad_norm': 2.9651885303365026, 'learning_rate': 4.969111111111112e-06, 'epoch': 2.214}
{'loss': 0.8976, 'grad_norm': 2.8643337256603965, 'learning_rate': 4.9680000000000005e-06, 'epoch': 2.2144}
{'loss': 0.8846, 'grad_norm': 3.0673632932355854, 'learning_rate': 4.966888888888889e-06, 'epoch': 2.2148}
{'loss': 0.8856, 'grad_norm': 2.9230818168675428, 'learning_rate': 4.965777777777778e-06, 'epoch': 2.2152}
{'loss': 0.898, 'grad_norm': 2.9057642110263586, 'learning_rate': 4.964666666666667e-06, 'epoch': 2.2156000000000002}
{'loss': 0.879, 'grad_norm': 2.8582380358618167, 'learning_rate': 4.963555555555556e-06, 'epoch': 2.216}
{'eval_valid_loss': 0.86669921875, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.563, 'eval_valid_steps_per_second': 277.141, 'epoch': 2.216}
{'loss': 0.8814, 'grad_norm': 2.9336200921187743, 'learning_rate': 4.962444444444445e-06, 'epoch': 2.2164}
{'loss': 0.8839, 'grad_norm': 2.7386882223790248, 'learning_rate': 4.961333333333334e-06, 'epoch': 2.2168}
{'loss': 0.8871, 'grad_norm': 2.809276259132977, 'learning_rate': 4.960222222222222e-06, 'epoch': 2.2172}
{'loss': 0.8865, 'grad_norm': 2.694511305551884, 'learning_rate': 4.959111111111112e-06, 'epoch': 2.2176}
{'loss': 0.8785, 'grad_norm': 3.2031389468750056, 'learning_rate': 4.958000000000001e-06, 'epoch': 2.218}
{'loss': 0.8981, 'grad_norm': 2.845544002904255, 'learning_rate': 4.956888888888889e-06, 'epoch': 2.2184}
{'loss': 0.8986, 'grad_norm': 3.184818598553382, 'learning_rate': 4.955777777777778e-06, 'epoch': 2.2188}
{'loss': 0.9054, 'grad_norm': 3.0741768995293794, 'learning_rate': 4.954666666666667e-06, 'epoch': 2.2192}
{'loss': 0.8847, 'grad_norm': 2.854201613627607, 'learning_rate': 4.9535555555555555e-06, 'epoch': 2.2196}
{'loss': 0.9022, 'grad_norm': 2.8418488487917988, 'learning_rate': 4.952444444444445e-06, 'epoch': 2.22}
{'eval_valid_loss': 0.86572265625, 'eval_valid_runtime': 0.0903, 'eval_valid_samples_per_second': 1106.834, 'eval_valid_steps_per_second': 276.709, 'epoch': 2.22}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.896, 'grad_norm': 2.739189536047843, 'learning_rate': 4.951333333333334e-06, 'epoch': 2.2204}
{'loss': 0.878, 'grad_norm': 2.9251894995445764, 'learning_rate': 4.9502222222222225e-06, 'epoch': 2.2208}
{'loss': 0.8812, 'grad_norm': 2.669456424618991, 'learning_rate': 4.949111111111112e-06, 'epoch': 2.2212}
{'loss': 0.8798, 'grad_norm': 2.7992125338358402, 'learning_rate': 4.948000000000001e-06, 'epoch': 2.2216}
{'loss': 0.8897, 'grad_norm': 3.0723048048194843, 'learning_rate': 4.9468888888888894e-06, 'epoch': 2.222}
{'loss': 0.8945, 'grad_norm': 2.694092051303369, 'learning_rate': 4.945777777777778e-06, 'epoch': 2.2224}
{'loss': 0.8904, 'grad_norm': 2.7691547692091425, 'learning_rate': 4.944666666666667e-06, 'epoch': 2.2228}
{'loss': 0.892, 'grad_norm': 2.9886819009908128, 'learning_rate': 4.943555555555556e-06, 'epoch': 2.2232}
{'loss': 0.8989, 'grad_norm': 2.741647650895269, 'learning_rate': 4.942444444444444e-06, 'epoch': 2.2236}
{'loss': 0.8994, 'grad_norm': 2.807035085291675, 'learning_rate': 4.941333333333334e-06, 'epoch': 2.224}
{'eval_valid_loss': 0.865234375, 'eval_valid_runtime': 0.0912, 'eval_valid_samples_per_second': 1096.791, 'eval_valid_steps_per_second': 274.198, 'epoch': 2.224}
{'loss': 0.8861, 'grad_norm': 3.3050341954099784, 'learning_rate': 4.9402222222222226e-06, 'epoch': 2.2244}
{'loss': 0.8897, 'grad_norm': 2.7059482648525, 'learning_rate': 4.939111111111112e-06, 'epoch': 2.2248}
{'loss': 0.887, 'grad_norm': 2.944241437771431, 'learning_rate': 4.938000000000001e-06, 'epoch': 2.2252}
{'loss': 0.8886, 'grad_norm': 3.0732320440492855, 'learning_rate': 4.9368888888888895e-06, 'epoch': 2.2256}
{'loss': 0.8854, 'grad_norm': 2.6698914521276293, 'learning_rate': 4.935777777777778e-06, 'epoch': 2.226}
{'loss': 0.8808, 'grad_norm': 2.837448248412292, 'learning_rate': 4.934666666666667e-06, 'epoch': 2.2264}
{'loss': 0.8967, 'grad_norm': 2.7607026773708188, 'learning_rate': 4.933555555555556e-06, 'epoch': 2.2268}
{'loss': 0.8955, 'grad_norm': 2.973208773340706, 'learning_rate': 4.932444444444444e-06, 'epoch': 2.2272}
{'loss': 0.8923, 'grad_norm': 2.739544249329544, 'learning_rate': 4.931333333333334e-06, 'epoch': 2.2276}
{'loss': 0.9006, 'grad_norm': 2.967841441011461, 'learning_rate': 4.930222222222223e-06, 'epoch': 2.228}
{'eval_valid_loss': 0.8642578125, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.744, 'eval_valid_steps_per_second': 278.936, 'epoch': 2.228}
{'loss': 0.8932, 'grad_norm': 3.2986702302464663, 'learning_rate': 4.929111111111111e-06, 'epoch': 2.2284}
{'loss': 0.8905, 'grad_norm': 2.9383422881736627, 'learning_rate': 4.928000000000001e-06, 'epoch': 2.2288}
{'loss': 0.8896, 'grad_norm': 2.9165537823675836, 'learning_rate': 4.92688888888889e-06, 'epoch': 2.2292}
{'loss': 0.889, 'grad_norm': 2.853234571627941, 'learning_rate': 4.925777777777778e-06, 'epoch': 2.2296}
{'loss': 0.8819, 'grad_norm': 2.553317498849179, 'learning_rate': 4.924666666666667e-06, 'epoch': 2.23}
{'loss': 0.8858, 'grad_norm': 2.696392080845951, 'learning_rate': 4.923555555555556e-06, 'epoch': 2.2304}
{'loss': 0.8802, 'grad_norm': 3.1761047085946283, 'learning_rate': 4.9224444444444445e-06, 'epoch': 2.2308}
{'loss': 0.8931, 'grad_norm': 2.6778537548340506, 'learning_rate': 4.921333333333333e-06, 'epoch': 2.2312}
{'loss': 0.8889, 'grad_norm': 2.768299113861544, 'learning_rate': 4.920222222222223e-06, 'epoch': 2.2316}
{'loss': 0.8827, 'grad_norm': 2.8681171271822308, 'learning_rate': 4.9191111111111115e-06, 'epoch': 2.232}
{'eval_valid_loss': 0.86572265625, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.6, 'eval_valid_steps_per_second': 279.65, 'epoch': 2.232}
{'loss': 0.9031, 'grad_norm': 2.6733281662219985, 'learning_rate': 4.918e-06, 'epoch': 2.2324}
{'loss': 0.8984, 'grad_norm': 2.7796867349161487, 'learning_rate': 4.91688888888889e-06, 'epoch': 2.2328}
{'loss': 0.8975, 'grad_norm': 2.7019207868506303, 'learning_rate': 4.9157777777777784e-06, 'epoch': 2.2332}
{'loss': 0.9014, 'grad_norm': 2.8936157374615465, 'learning_rate': 4.914666666666667e-06, 'epoch': 2.2336}
{'loss': 0.9034, 'grad_norm': 2.7961370943882176, 'learning_rate': 4.913555555555556e-06, 'epoch': 2.234}
{'loss': 0.8934, 'grad_norm': 2.8076246284717974, 'learning_rate': 4.912444444444445e-06, 'epoch': 2.2344}
{'loss': 0.89, 'grad_norm': 3.3494521123597942, 'learning_rate': 4.911333333333333e-06, 'epoch': 2.2348}
{'loss': 0.8925, 'grad_norm': 2.8184437147647845, 'learning_rate': 4.910222222222223e-06, 'epoch': 2.2352}
{'loss': 0.8859, 'grad_norm': 2.8685905495339075, 'learning_rate': 4.9091111111111116e-06, 'epoch': 2.2356}
{'loss': 0.8926, 'grad_norm': 3.1797526678582053, 'learning_rate': 4.908e-06, 'epoch': 2.2359999999999998}
{'eval_valid_loss': 0.8671875, 'eval_valid_runtime': 0.0906, 'eval_valid_samples_per_second': 1104.247, 'eval_valid_steps_per_second': 276.062, 'epoch': 2.2359999999999998}
{'loss': 0.8918, 'grad_norm': 2.885268950669244, 'learning_rate': 4.906888888888889e-06, 'epoch': 2.2364}
{'loss': 0.8947, 'grad_norm': 2.912219459300198, 'learning_rate': 4.9057777777777785e-06, 'epoch': 2.2368}
{'loss': 0.8846, 'grad_norm': 2.7418656336891987, 'learning_rate': 4.904666666666667e-06, 'epoch': 2.2372}
{'loss': 0.8928, 'grad_norm': 2.8348406727741593, 'learning_rate': 4.903555555555556e-06, 'epoch': 2.2376}
{'loss': 0.9014, 'grad_norm': 3.084765141557006, 'learning_rate': 4.902444444444445e-06, 'epoch': 2.238}
{'loss': 0.9027, 'grad_norm': 2.8460483171479183, 'learning_rate': 4.901333333333333e-06, 'epoch': 2.2384}
{'loss': 0.8857, 'grad_norm': 2.825064863456179, 'learning_rate': 4.900222222222223e-06, 'epoch': 2.2388}
{'loss': 0.896, 'grad_norm': 3.2585042315167776, 'learning_rate': 4.899111111111112e-06, 'epoch': 2.2392}
{'loss': 0.8905, 'grad_norm': 2.9541317369165285, 'learning_rate': 4.898e-06, 'epoch': 2.2396}
{'loss': 0.8936, 'grad_norm': 2.617350638699742, 'learning_rate': 4.896888888888889e-06, 'epoch': 2.24}
{'eval_valid_loss': 0.8662109375, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.533, 'eval_valid_steps_per_second': 278.883, 'epoch': 2.24}
{'loss': 0.9009, 'grad_norm': 2.595969474221766, 'learning_rate': 4.895777777777778e-06, 'epoch': 2.2404}
{'loss': 0.8889, 'grad_norm': 2.895784258712517, 'learning_rate': 4.894666666666667e-06, 'epoch': 2.2408}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8904, 'grad_norm': 2.970739089778213, 'learning_rate': 4.893555555555556e-06, 'epoch': 2.2412}
{'loss': 0.901, 'grad_norm': 2.71043918547975, 'learning_rate': 4.892444444444445e-06, 'epoch': 2.2416}
{'loss': 0.8942, 'grad_norm': 2.9139416953402075, 'learning_rate': 4.8913333333333335e-06, 'epoch': 2.242}
{'loss': 0.8877, 'grad_norm': 2.9987797043391757, 'learning_rate': 4.890222222222223e-06, 'epoch': 2.2424}
{'loss': 0.8934, 'grad_norm': 2.8721832699371523, 'learning_rate': 4.889111111111112e-06, 'epoch': 2.2428}
{'loss': 0.8883, 'grad_norm': 2.952890427169028, 'learning_rate': 4.888111111111111e-06, 'epoch': 2.2432}
{'loss': 0.9009, 'grad_norm': 2.7571083336958666, 'learning_rate': 4.887000000000001e-06, 'epoch': 2.2436}
{'loss': 0.888, 'grad_norm': 2.9045603413935717, 'learning_rate': 4.8858888888888895e-06, 'epoch': 2.2439999999999998}
{'eval_valid_loss': 0.8681640625, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1124.569, 'eval_valid_steps_per_second': 281.142, 'epoch': 2.2439999999999998}
{'loss': 0.895, 'grad_norm': 2.961105100372334, 'learning_rate': 4.884777777777778e-06, 'epoch': 2.2444}
{'loss': 0.8946, 'grad_norm': 2.7897897194336227, 'learning_rate': 4.883666666666667e-06, 'epoch': 2.2448}
{'loss': 0.8883, 'grad_norm': 2.8886493426637694, 'learning_rate': 4.882555555555556e-06, 'epoch': 2.2452}
{'loss': 0.8983, 'grad_norm': 2.867782125737202, 'learning_rate': 4.881444444444444e-06, 'epoch': 2.2456}
{'loss': 0.8997, 'grad_norm': 2.967129455770051, 'learning_rate': 4.880333333333334e-06, 'epoch': 2.246}
{'loss': 0.8891, 'grad_norm': 2.8577332447589656, 'learning_rate': 4.879222222222223e-06, 'epoch': 2.2464}
{'loss': 0.9014, 'grad_norm': 2.863689939883961, 'learning_rate': 4.878111111111111e-06, 'epoch': 2.2468}
{'loss': 0.8936, 'grad_norm': 2.946520196018499, 'learning_rate': 4.877000000000001e-06, 'epoch': 2.2472}
{'loss': 0.8951, 'grad_norm': 2.784404610413254, 'learning_rate': 4.87588888888889e-06, 'epoch': 2.2476}
{'loss': 0.8839, 'grad_norm': 2.7636497146203247, 'learning_rate': 4.874777777777778e-06, 'epoch': 2.248}
{'eval_valid_loss': 0.8671875, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.706, 'eval_valid_steps_per_second': 278.676, 'epoch': 2.248}
{'loss': 0.8903, 'grad_norm': 3.2363918300475567, 'learning_rate': 4.873666666666667e-06, 'epoch': 2.2484}
{'loss': 0.8903, 'grad_norm': 2.7896418250297406, 'learning_rate': 4.872555555555556e-06, 'epoch': 2.2488}
{'loss': 0.8974, 'grad_norm': 3.0407859602820575, 'learning_rate': 4.8714444444444445e-06, 'epoch': 2.2492}
{'loss': 0.8946, 'grad_norm': 2.925245982203289, 'learning_rate': 4.870333333333333e-06, 'epoch': 2.2496}
{'loss': 0.8915, 'grad_norm': 2.972154091166057, 'learning_rate': 4.869222222222223e-06, 'epoch': 2.25}
{'loss': 0.8871, 'grad_norm': 3.0968355724854315, 'learning_rate': 4.8681111111111114e-06, 'epoch': 2.2504}
{'loss': 0.894, 'grad_norm': 2.829645127685374, 'learning_rate': 4.867000000000001e-06, 'epoch': 2.2508}
{'loss': 0.8938, 'grad_norm': 2.7956691038975343, 'learning_rate': 4.86588888888889e-06, 'epoch': 2.2512}
{'loss': 0.8986, 'grad_norm': 2.8928040616482202, 'learning_rate': 4.8647777777777784e-06, 'epoch': 2.2516}
{'loss': 0.8816, 'grad_norm': 2.8961630428844596, 'learning_rate': 4.863666666666667e-06, 'epoch': 2.252}
{'eval_valid_loss': 0.86572265625, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.475, 'eval_valid_steps_per_second': 278.369, 'epoch': 2.252}
{'loss': 0.8845, 'grad_norm': 2.850135885311969, 'learning_rate': 4.862555555555556e-06, 'epoch': 2.2524}
{'loss': 0.8923, 'grad_norm': 3.1633757799702087, 'learning_rate': 4.8614444444444446e-06, 'epoch': 2.2528}
{'loss': 0.8932, 'grad_norm': 2.67567951538038, 'learning_rate': 4.860333333333333e-06, 'epoch': 2.2532}
{'loss': 0.877, 'grad_norm': 2.9062873673856755, 'learning_rate': 4.859222222222222e-06, 'epoch': 2.2536}
{'loss': 0.8901, 'grad_norm': 2.9836787165519674, 'learning_rate': 4.8581111111111115e-06, 'epoch': 2.254}
{'loss': 0.8824, 'grad_norm': 2.85895755201199, 'learning_rate': 4.857e-06, 'epoch': 2.2544}
{'loss': 0.8909, 'grad_norm': 2.7257348364707643, 'learning_rate': 4.85588888888889e-06, 'epoch': 2.2548}
{'loss': 0.8918, 'grad_norm': 3.29624024377796, 'learning_rate': 4.8547777777777785e-06, 'epoch': 2.2552}
{'loss': 0.8825, 'grad_norm': 2.6947213837746014, 'learning_rate': 4.853666666666667e-06, 'epoch': 2.2556}
{'loss': 0.8917, 'grad_norm': 3.0283703841394165, 'learning_rate': 4.852555555555556e-06, 'epoch': 2.2560000000000002}
{'eval_valid_loss': 0.86572265625, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.022, 'eval_valid_steps_per_second': 281.756, 'epoch': 2.2560000000000002}
{'loss': 0.8951, 'grad_norm': 2.8392006094663054, 'learning_rate': 4.851444444444445e-06, 'epoch': 2.2564}
{'loss': 0.9034, 'grad_norm': 3.00976078297933, 'learning_rate': 4.850333333333333e-06, 'epoch': 2.2568}
{'loss': 0.8815, 'grad_norm': 2.8062050201957276, 'learning_rate': 4.849222222222222e-06, 'epoch': 2.2572}
{'loss': 0.8969, 'grad_norm': 3.067308504552023, 'learning_rate': 4.848111111111112e-06, 'epoch': 2.2576}
{'loss': 0.8961, 'grad_norm': 2.9493585212378126, 'learning_rate': 4.847e-06, 'epoch': 2.258}
{'loss': 0.8839, 'grad_norm': 2.866385511446499, 'learning_rate': 4.845888888888889e-06, 'epoch': 2.2584}
{'loss': 0.8885, 'grad_norm': 3.1145619494789605, 'learning_rate': 4.844777777777779e-06, 'epoch': 2.2588}
{'loss': 0.8924, 'grad_norm': 2.9241141648868627, 'learning_rate': 4.843666666666667e-06, 'epoch': 2.2592}
{'loss': 0.8844, 'grad_norm': 2.5903601325809147, 'learning_rate': 4.842555555555556e-06, 'epoch': 2.2596}
{'loss': 0.9033, 'grad_norm': 3.0087756212646815, 'learning_rate': 4.841444444444445e-06, 'epoch': 2.26}
{'eval_valid_loss': 0.86474609375, 'eval_valid_runtime': 0.0884, 'eval_valid_samples_per_second': 1131.443, 'eval_valid_steps_per_second': 282.861, 'epoch': 2.26}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.8848, 'grad_norm': 3.0835065610440657, 'learning_rate': 4.8403333333333335e-06, 'epoch': 2.2604}
{'loss': 0.8914, 'grad_norm': 2.900358072733993, 'learning_rate': 4.839222222222222e-06, 'epoch': 2.2608}
{'loss': 0.8824, 'grad_norm': 2.689345463611165, 'learning_rate': 4.838111111111112e-06, 'epoch': 2.2612}
{'loss': 0.8884, 'grad_norm': 2.665072101571338, 'learning_rate': 4.8370000000000004e-06, 'epoch': 2.2616}
{'loss': 0.8979, 'grad_norm': 3.2646137022315243, 'learning_rate': 4.835888888888889e-06, 'epoch': 2.262}
{'loss': 0.8904, 'grad_norm': 2.91507174730747, 'learning_rate': 4.834777777777778e-06, 'epoch': 2.2624}
{'loss': 0.8749, 'grad_norm': 3.1575272874415687, 'learning_rate': 4.8336666666666674e-06, 'epoch': 2.2628}
{'loss': 0.8961, 'grad_norm': 3.1297914774358726, 'learning_rate': 4.832555555555556e-06, 'epoch': 2.2632}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8918, 'grad_norm': 2.8374137819105893, 'learning_rate': 4.831444444444445e-06, 'epoch': 2.2636}
{'loss': 0.8819, 'grad_norm': 2.762281088116971, 'learning_rate': 4.8303333333333336e-06, 'epoch': 2.2640000000000002}
{'eval_valid_loss': 0.8662109375, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1110.583, 'eval_valid_steps_per_second': 277.646, 'epoch': 2.2640000000000002}
{'loss': 0.8834, 'grad_norm': 3.1538567770422077, 'learning_rate': 4.829222222222222e-06, 'epoch': 2.2644}
{'loss': 0.8948, 'grad_norm': 2.9721521358654237, 'learning_rate': 4.828111111111112e-06, 'epoch': 2.2648}
{'loss': 0.8799, 'grad_norm': 2.74221736533049, 'learning_rate': 4.8270000000000005e-06, 'epoch': 2.2652}
{'loss': 0.8806, 'grad_norm': 2.831484753726799, 'learning_rate': 4.825888888888889e-06, 'epoch': 2.2656}
{'loss': 0.8994, 'grad_norm': 3.020909072759588, 'learning_rate': 4.824777777777778e-06, 'epoch': 2.266}
{'loss': 0.9003, 'grad_norm': 2.8801415638602763, 'learning_rate': 4.823666666666667e-06, 'epoch': 2.2664}
{'loss': 0.8846, 'grad_norm': 3.136461107212049, 'learning_rate': 4.822555555555556e-06, 'epoch': 2.2668}
{'loss': 0.8956, 'grad_norm': 2.991543404201321, 'learning_rate': 4.821444444444445e-06, 'epoch': 2.2672}
{'loss': 0.9019, 'grad_norm': 3.2642243863322293, 'learning_rate': 4.820333333333334e-06, 'epoch': 2.2676}
{'loss': 0.8875, 'grad_norm': 3.1178699214893792, 'learning_rate': 4.819222222222222e-06, 'epoch': 2.268}
{'eval_valid_loss': 0.8662109375, 'eval_valid_runtime': 0.0906, 'eval_valid_samples_per_second': 1103.811, 'eval_valid_steps_per_second': 275.953, 'epoch': 2.268}
{'loss': 0.8929, 'grad_norm': 2.797749846042482, 'learning_rate': 4.818111111111112e-06, 'epoch': 2.2684}
{'loss': 0.8985, 'grad_norm': 2.9523477490194368, 'learning_rate': 4.817000000000001e-06, 'epoch': 2.2688}
{'loss': 0.892, 'grad_norm': 2.8641228577392677, 'learning_rate': 4.815888888888889e-06, 'epoch': 2.2692}
{'loss': 0.8961, 'grad_norm': 3.40512409325778, 'learning_rate': 4.814777777777778e-06, 'epoch': 2.2696}
{'loss': 0.8741, 'grad_norm': 2.8901058143166014, 'learning_rate': 4.813666666666667e-06, 'epoch': 2.27}
{'loss': 0.8924, 'grad_norm': 2.757766820513073, 'learning_rate': 4.8125555555555555e-06, 'epoch': 2.2704}
{'loss': 0.8782, 'grad_norm': 2.7614651014508156, 'learning_rate': 4.811444444444445e-06, 'epoch': 2.2708}
{'loss': 0.8952, 'grad_norm': 2.9673480990288943, 'learning_rate': 4.810333333333334e-06, 'epoch': 2.2712}
{'loss': 0.8928, 'grad_norm': 2.9458400041449817, 'learning_rate': 4.8092222222222225e-06, 'epoch': 2.2716}
{'loss': 0.893, 'grad_norm': 2.675635663826393, 'learning_rate': 4.808111111111112e-06, 'epoch': 2.2720000000000002}
{'eval_valid_loss': 0.86474609375, 'eval_valid_runtime': 0.094, 'eval_valid_samples_per_second': 1063.875, 'eval_valid_steps_per_second': 265.969, 'epoch': 2.2720000000000002}
{'loss': 0.889, 'grad_norm': 2.813290463734131, 'learning_rate': 4.807000000000001e-06, 'epoch': 2.2724}
{'loss': 0.8894, 'grad_norm': 2.8682531304666115, 'learning_rate': 4.8058888888888894e-06, 'epoch': 2.2728}
{'loss': 0.8983, 'grad_norm': 2.711563203043735, 'learning_rate': 4.804777777777778e-06, 'epoch': 2.2732}
{'loss': 0.884, 'grad_norm': 2.8951867888941605, 'learning_rate': 4.803666666666667e-06, 'epoch': 2.2736}
{'loss': 0.8821, 'grad_norm': 2.830039198497806, 'learning_rate': 4.802555555555556e-06, 'epoch': 2.274}
{'loss': 0.8874, 'grad_norm': 2.774279582098713, 'learning_rate': 4.801444444444444e-06, 'epoch': 2.2744}
{'loss': 0.896, 'grad_norm': 2.7096367004528363, 'learning_rate': 4.800333333333334e-06, 'epoch': 2.2748}
{'loss': 0.8975, 'grad_norm': 2.941257528772863, 'learning_rate': 4.7992222222222226e-06, 'epoch': 2.2752}
{'loss': 0.8966, 'grad_norm': 2.7086816025826885, 'learning_rate': 4.798111111111112e-06, 'epoch': 2.2756}
{'loss': 0.904, 'grad_norm': 2.9388894690807694, 'learning_rate': 4.797000000000001e-06, 'epoch': 2.276}
{'eval_valid_loss': 0.8671875, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.546, 'eval_valid_steps_per_second': 279.136, 'epoch': 2.276}
{'loss': 0.8823, 'grad_norm': 3.0863652958892924, 'learning_rate': 4.7958888888888895e-06, 'epoch': 2.2763999999999998}
{'loss': 0.891, 'grad_norm': 2.8037414974299715, 'learning_rate': 4.794777777777778e-06, 'epoch': 2.2768}
{'loss': 0.8929, 'grad_norm': 2.956407196430252, 'learning_rate': 4.793666666666667e-06, 'epoch': 2.2772}
{'loss': 0.8959, 'grad_norm': 3.1131453921207903, 'learning_rate': 4.792555555555556e-06, 'epoch': 2.2776}
{'loss': 0.8821, 'grad_norm': 2.835106236554631, 'learning_rate': 4.791444444444444e-06, 'epoch': 2.278}
{'loss': 0.887, 'grad_norm': 2.8529771143930995, 'learning_rate': 4.790333333333333e-06, 'epoch': 2.2784}
{'loss': 0.8822, 'grad_norm': 2.951860793041358, 'learning_rate': 4.789222222222223e-06, 'epoch': 2.2788}
{'loss': 0.8832, 'grad_norm': 2.9849801972785075, 'learning_rate': 4.788111111111111e-06, 'epoch': 2.2792}
{'loss': 0.8893, 'grad_norm': 3.0133972339497426, 'learning_rate': 4.787000000000001e-06, 'epoch': 2.2796}
{'loss': 0.891, 'grad_norm': 2.824248286049279, 'learning_rate': 4.78588888888889e-06, 'epoch': 2.2800000000000002}
{'eval_valid_loss': 0.86572265625, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1126.263, 'eval_valid_steps_per_second': 281.566, 'epoch': 2.2800000000000002}
{'loss': 0.8908, 'grad_norm': 2.6225926793507184, 'learning_rate': 4.784777777777778e-06, 'epoch': 2.2804}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8929, 'grad_norm': 3.0493891002581575, 'learning_rate': 4.783666666666667e-06, 'epoch': 2.2808}
{'loss': 0.8917, 'grad_norm': 2.72382476338144, 'learning_rate': 4.782555555555556e-06, 'epoch': 2.2812}
{'loss': 0.8948, 'grad_norm': 2.8753680739926106, 'learning_rate': 4.7814444444444445e-06, 'epoch': 2.2816}
{'loss': 0.8944, 'grad_norm': 3.444165858586996, 'learning_rate': 4.780333333333333e-06, 'epoch': 2.282}
{'loss': 0.8813, 'grad_norm': 2.8654949067183177, 'learning_rate': 4.779222222222223e-06, 'epoch': 2.2824}
{'loss': 0.878, 'grad_norm': 3.1270715809007603, 'learning_rate': 4.7781111111111115e-06, 'epoch': 2.2828}
{'loss': 0.8878, 'grad_norm': 3.154122409778826, 'learning_rate': 4.777111111111111e-06, 'epoch': 2.2832}
{'loss': 0.8952, 'grad_norm': 2.8317886669576606, 'learning_rate': 4.7760000000000005e-06, 'epoch': 2.2836}
{'loss': 0.8941, 'grad_norm': 2.8364175437509496, 'learning_rate': 4.774888888888889e-06, 'epoch': 2.284}
{'eval_valid_loss': 0.86328125, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.601, 'eval_valid_steps_per_second': 279.9, 'epoch': 2.284}
{'loss': 0.8913, 'grad_norm': 2.6692209446904775, 'learning_rate': 4.773777777777778e-06, 'epoch': 2.2843999999999998}
{'loss': 0.895, 'grad_norm': 2.856625949984153, 'learning_rate': 4.7726666666666675e-06, 'epoch': 2.2848}
{'loss': 0.889, 'grad_norm': 3.055334056908127, 'learning_rate': 4.771555555555556e-06, 'epoch': 2.2852}
{'loss': 0.8964, 'grad_norm': 2.8446034314123683, 'learning_rate': 4.770444444444445e-06, 'epoch': 2.2856}
{'loss': 0.8814, 'grad_norm': 2.6920382196503447, 'learning_rate': 4.769333333333334e-06, 'epoch': 2.286}
{'loss': 0.8946, 'grad_norm': 3.175447350247693, 'learning_rate': 4.768222222222222e-06, 'epoch': 2.2864}
{'loss': 0.8873, 'grad_norm': 2.877444358059029, 'learning_rate': 4.767111111111111e-06, 'epoch': 2.2868}
{'loss': 0.8807, 'grad_norm': 2.6501521689882463, 'learning_rate': 4.766000000000001e-06, 'epoch': 2.2872}
{'loss': 0.9004, 'grad_norm': 3.0476588585829454, 'learning_rate': 4.764888888888889e-06, 'epoch': 2.2876}
{'loss': 0.8972, 'grad_norm': 2.770334990701106, 'learning_rate': 4.763777777777778e-06, 'epoch': 2.288}
{'eval_valid_loss': 0.86572265625, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1128.001, 'eval_valid_steps_per_second': 282.0, 'epoch': 2.288}
{'loss': 0.8851, 'grad_norm': 3.068563288196522, 'learning_rate': 4.762666666666667e-06, 'epoch': 2.2884}
{'loss': 0.8803, 'grad_norm': 2.742427674124841, 'learning_rate': 4.761555555555556e-06, 'epoch': 2.2888}
{'loss': 0.8851, 'grad_norm': 2.839983793685906, 'learning_rate': 4.760444444444445e-06, 'epoch': 2.2892}
{'loss': 0.8784, 'grad_norm': 2.932817216644803, 'learning_rate': 4.759333333333334e-06, 'epoch': 2.2896}
{'loss': 0.8944, 'grad_norm': 2.9638350307473544, 'learning_rate': 4.7582222222222224e-06, 'epoch': 2.29}
{'loss': 0.8913, 'grad_norm': 2.854525294129852, 'learning_rate': 4.757111111111111e-06, 'epoch': 2.2904}
{'loss': 0.8954, 'grad_norm': 2.9998864112648724, 'learning_rate': 4.756000000000001e-06, 'epoch': 2.2908}
{'loss': 0.885, 'grad_norm': 3.253772847347815, 'learning_rate': 4.7548888888888894e-06, 'epoch': 2.2912}
{'loss': 0.8849, 'grad_norm': 3.174113287219415, 'learning_rate': 4.753777777777778e-06, 'epoch': 2.2916}
{'loss': 0.8834, 'grad_norm': 2.674883353506543, 'learning_rate': 4.752666666666667e-06, 'epoch': 2.292}
{'eval_valid_loss': 0.8642578125, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1126.465, 'eval_valid_steps_per_second': 281.616, 'epoch': 2.292}
{'loss': 0.8869, 'grad_norm': 2.761577937139238, 'learning_rate': 4.7515555555555556e-06, 'epoch': 2.2923999999999998}
{'loss': 0.8889, 'grad_norm': 2.946696970427444, 'learning_rate': 4.750444444444445e-06, 'epoch': 2.2928}
{'loss': 0.8786, 'grad_norm': 2.8519363889789653, 'learning_rate': 4.749333333333334e-06, 'epoch': 2.2932}
{'loss': 0.8804, 'grad_norm': 2.849271664077109, 'learning_rate': 4.7482222222222225e-06, 'epoch': 2.2936}
{'loss': 0.8899, 'grad_norm': 3.3699021692253677, 'learning_rate': 4.747111111111111e-06, 'epoch': 2.294}
{'loss': 0.8705, 'grad_norm': 3.473175610730785, 'learning_rate': 4.746000000000001e-06, 'epoch': 2.2944}
{'loss': 0.8771, 'grad_norm': 3.0392044740282462, 'learning_rate': 4.7448888888888895e-06, 'epoch': 2.2948}
{'loss': 0.8799, 'grad_norm': 3.0670982989157847, 'learning_rate': 4.743777777777778e-06, 'epoch': 2.2952}
{'loss': 0.8942, 'grad_norm': 2.7257163530159025, 'learning_rate': 4.742666666666667e-06, 'epoch': 2.2956}
{'loss': 0.9033, 'grad_norm': 3.0121370710292603, 'learning_rate': 4.741555555555556e-06, 'epoch': 2.296}
{'eval_valid_loss': 0.86376953125, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.466, 'eval_valid_steps_per_second': 277.117, 'epoch': 2.296}
{'loss': 0.8814, 'grad_norm': 2.9023333892021235, 'learning_rate': 4.740444444444444e-06, 'epoch': 2.2964}
{'loss': 0.8918, 'grad_norm': 3.1231075755715945, 'learning_rate': 4.739333333333334e-06, 'epoch': 2.2968}
{'loss': 0.9019, 'grad_norm': 2.932666860712971, 'learning_rate': 4.738222222222223e-06, 'epoch': 2.2972}
{'loss': 0.9022, 'grad_norm': 3.020842125759264, 'learning_rate': 4.737111111111112e-06, 'epoch': 2.2976}
{'loss': 0.8873, 'grad_norm': 2.710485481152953, 'learning_rate': 4.736000000000001e-06, 'epoch': 2.298}
{'loss': 0.8821, 'grad_norm': 2.7363700618430107, 'learning_rate': 4.73488888888889e-06, 'epoch': 2.2984}
{'loss': 0.8838, 'grad_norm': 2.6002459989685107, 'learning_rate': 4.733777777777778e-06, 'epoch': 2.2988}
{'loss': 0.8889, 'grad_norm': 2.8238940969093886, 'learning_rate': 4.732666666666667e-06, 'epoch': 2.2992}
{'loss': 0.8888, 'grad_norm': 2.9764226044547923, 'learning_rate': 4.731555555555556e-06, 'epoch': 2.2996}
{'loss': 0.887, 'grad_norm': 3.0896976113272103, 'learning_rate': 4.7304444444444445e-06, 'epoch': 2.3}
{'eval_valid_loss': 0.86572265625, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1120.33, 'eval_valid_steps_per_second': 280.083, 'epoch': 2.3}
{'loss': 0.909, 'grad_norm': 2.7195924955297186, 'learning_rate': 4.729333333333333e-06, 'epoch': 2.3004}
{'loss': 0.8911, 'grad_norm': 2.833253246933273, 'learning_rate': 4.728222222222223e-06, 'epoch': 2.3008}
{'loss': 0.8961, 'grad_norm': 3.004964842868357, 'learning_rate': 4.7271111111111114e-06, 'epoch': 2.3012}
{'loss': 0.8929, 'grad_norm': 3.1364630741035837, 'learning_rate': 4.726000000000001e-06, 'epoch': 2.3016}
{'loss': 0.8874, 'grad_norm': 3.1894245601947047, 'learning_rate': 4.72488888888889e-06, 'epoch': 2.302}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8811, 'grad_norm': 2.8442108126775123, 'learning_rate': 4.723777777777778e-06, 'epoch': 2.3024}
{'loss': 0.897, 'grad_norm': 3.2523727192116594, 'learning_rate': 4.722666666666667e-06, 'epoch': 2.3028}
{'loss': 0.8767, 'grad_norm': 2.812466843727495, 'learning_rate': 4.721555555555556e-06, 'epoch': 2.3032}
{'loss': 0.8918, 'grad_norm': 3.0290935258723692, 'learning_rate': 4.7204444444444446e-06, 'epoch': 2.3036}
{'loss': 0.8844, 'grad_norm': 2.8699943928962854, 'learning_rate': 4.719333333333333e-06, 'epoch': 2.304}
{'eval_valid_loss': 0.86328125, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.716, 'eval_valid_steps_per_second': 277.179, 'epoch': 2.304}
{'loss': 0.8918, 'grad_norm': 3.0021113178414556, 'learning_rate': 4.718222222222222e-06, 'epoch': 2.3044000000000002}
{'loss': 0.8881, 'grad_norm': 2.6617479757726015, 'learning_rate': 4.7171111111111115e-06, 'epoch': 2.3048}
{'loss': 0.8903, 'grad_norm': 2.743749733231864, 'learning_rate': 4.716e-06, 'epoch': 2.3052}
{'loss': 0.8927, 'grad_norm': 2.5979091148348283, 'learning_rate': 4.71488888888889e-06, 'epoch': 2.3056}
{'loss': 0.8862, 'grad_norm': 2.8422826817090026, 'learning_rate': 4.7137777777777785e-06, 'epoch': 2.306}
{'loss': 0.8858, 'grad_norm': 2.83461723386055, 'learning_rate': 4.712666666666667e-06, 'epoch': 2.3064}
{'loss': 0.8961, 'grad_norm': 2.892825871379304, 'learning_rate': 4.711555555555556e-06, 'epoch': 2.3068}
{'loss': 0.879, 'grad_norm': 2.9146349551737654, 'learning_rate': 4.710444444444445e-06, 'epoch': 2.3072}
{'loss': 0.8977, 'grad_norm': 2.9638378965133922, 'learning_rate': 4.709333333333333e-06, 'epoch': 2.3076}
{'loss': 0.8948, 'grad_norm': 2.738177298886855, 'learning_rate': 4.708222222222222e-06, 'epoch': 2.308}
{'eval_valid_loss': 0.86376953125, 'eval_valid_runtime': 0.0907, 'eval_valid_samples_per_second': 1102.865, 'eval_valid_steps_per_second': 275.716, 'epoch': 2.308}
{'loss': 0.8909, 'grad_norm': 2.8196033579547137, 'learning_rate': 4.707111111111112e-06, 'epoch': 2.3084}
{'loss': 0.8801, 'grad_norm': 2.9459683525369327, 'learning_rate': 4.706e-06, 'epoch': 2.3088}
{'loss': 0.8806, 'grad_norm': 2.9015538615569088, 'learning_rate': 4.704888888888889e-06, 'epoch': 2.3092}
{'loss': 0.8988, 'grad_norm': 2.574292038610871, 'learning_rate': 4.703777777777779e-06, 'epoch': 2.3096}
{'loss': 0.8883, 'grad_norm': 3.014378720035429, 'learning_rate': 4.702666666666667e-06, 'epoch': 2.31}
{'loss': 0.8889, 'grad_norm': 2.6543799044188985, 'learning_rate': 4.701555555555556e-06, 'epoch': 2.3104}
{'loss': 0.8969, 'grad_norm': 2.830564553189621, 'learning_rate': 4.700444444444445e-06, 'epoch': 2.3108}
{'loss': 0.8927, 'grad_norm': 2.9669169539158675, 'learning_rate': 4.6993333333333335e-06, 'epoch': 2.3112}
{'loss': 0.8896, 'grad_norm': 2.697682949876383, 'learning_rate': 4.698222222222222e-06, 'epoch': 2.3116}
{'loss': 0.8971, 'grad_norm': 3.2391542700479676, 'learning_rate': 4.697111111111112e-06, 'epoch': 2.312}
{'eval_valid_loss': 0.86279296875, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1122.42, 'eval_valid_steps_per_second': 280.605, 'epoch': 2.312}
{'loss': 0.8944, 'grad_norm': 2.7228443677322973, 'learning_rate': 4.6960000000000004e-06, 'epoch': 2.3124000000000002}
{'loss': 0.8961, 'grad_norm': 3.312676272110676, 'learning_rate': 4.694888888888889e-06, 'epoch': 2.3128}
{'loss': 0.88, 'grad_norm': 3.0458078912141175, 'learning_rate': 4.693777777777778e-06, 'epoch': 2.3132}
{'loss': 0.8879, 'grad_norm': 2.982338724932052, 'learning_rate': 4.692666666666667e-06, 'epoch': 2.3136}
{'loss': 0.9009, 'grad_norm': 2.903433111407345, 'learning_rate': 4.691555555555556e-06, 'epoch': 2.314}
{'loss': 0.8833, 'grad_norm': 3.0252840578680726, 'learning_rate': 4.690444444444445e-06, 'epoch': 2.3144}
{'loss': 0.8993, 'grad_norm': 2.8302715596079677, 'learning_rate': 4.6893333333333336e-06, 'epoch': 2.3148}
{'loss': 0.8969, 'grad_norm': 2.7456023860549785, 'learning_rate': 4.688222222222222e-06, 'epoch': 2.3152}
{'loss': 0.8845, 'grad_norm': 2.8180599463342815, 'learning_rate': 4.687111111111112e-06, 'epoch': 2.3156}
{'loss': 0.8876, 'grad_norm': 2.8143944082283467, 'learning_rate': 4.6860000000000005e-06, 'epoch': 2.316}
{'eval_valid_loss': 0.86279296875, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.029, 'eval_valid_steps_per_second': 278.257, 'epoch': 2.316}
{'loss': 0.8935, 'grad_norm': 2.998860142646097, 'learning_rate': 4.684888888888889e-06, 'epoch': 2.3164}
{'loss': 0.8896, 'grad_norm': 3.0665411658280135, 'learning_rate': 4.683777777777778e-06, 'epoch': 2.3168}
{'loss': 0.8782, 'grad_norm': 2.675658051933337, 'learning_rate': 4.682666666666667e-06, 'epoch': 2.3172}
{'loss': 0.8893, 'grad_norm': 3.2176806330864967, 'learning_rate': 4.681555555555556e-06, 'epoch': 2.3176}
{'loss': 0.8826, 'grad_norm': 2.6712165712791656, 'learning_rate': 4.680444444444445e-06, 'epoch': 2.318}
{'loss': 0.8958, 'grad_norm': 3.063789397912028, 'learning_rate': 4.679333333333334e-06, 'epoch': 2.3184}
{'loss': 0.8804, 'grad_norm': 3.015188847759135, 'learning_rate': 4.678222222222222e-06, 'epoch': 2.3188}
{'loss': 0.8819, 'grad_norm': 2.692403179613298, 'learning_rate': 4.677111111111112e-06, 'epoch': 2.3192}
{'loss': 0.883, 'grad_norm': 2.798741112346091, 'learning_rate': 4.676000000000001e-06, 'epoch': 2.3196}
{'loss': 0.8983, 'grad_norm': 2.8929384098630506, 'learning_rate': 4.674888888888889e-06, 'epoch': 2.32}
{'eval_valid_loss': 0.8662109375, 'eval_valid_runtime': 0.092, 'eval_valid_samples_per_second': 1087.365, 'eval_valid_steps_per_second': 271.841, 'epoch': 2.32}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.896, 'grad_norm': 3.229455237671805, 'learning_rate': 4.673777777777778e-06, 'epoch': 2.3204000000000002}
{'loss': 0.889, 'grad_norm': 2.8228881624525832, 'learning_rate': 4.672666666666667e-06, 'epoch': 2.3208}
{'loss': 0.8989, 'grad_norm': 3.2423785325727494, 'learning_rate': 4.6715555555555555e-06, 'epoch': 2.3212}
{'loss': 0.8825, 'grad_norm': 2.7827367719812743, 'learning_rate': 4.670444444444445e-06, 'epoch': 2.3216}
{'loss': 0.889, 'grad_norm': 3.0657978207466163, 'learning_rate': 4.669333333333334e-06, 'epoch': 2.322}
{'loss': 0.8834, 'grad_norm': 2.9115908574951392, 'learning_rate': 4.6682222222222225e-06, 'epoch': 2.3224}
{'loss': 0.8818, 'grad_norm': 2.9749780177257934, 'learning_rate': 4.667111111111112e-06, 'epoch': 2.3228}
{'loss': 0.8931, 'grad_norm': 3.009683557142801, 'learning_rate': 4.6661111111111115e-06, 'epoch': 2.3232}
{'loss': 0.8784, 'grad_norm': 3.029791215495553, 'learning_rate': 4.665e-06, 'epoch': 2.3236}
{'loss': 0.8769, 'grad_norm': 2.760421701942506, 'learning_rate': 4.66388888888889e-06, 'epoch': 2.324}
{'eval_valid_loss': 0.8623046875, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.498, 'eval_valid_steps_per_second': 279.374, 'epoch': 2.324}
{'loss': 0.8992, 'grad_norm': 3.016504732592674, 'learning_rate': 4.6627777777777785e-06, 'epoch': 2.3244}
{'loss': 0.8882, 'grad_norm': 2.8598901670213728, 'learning_rate': 4.661666666666667e-06, 'epoch': 2.3247999999999998}
{'loss': 0.8883, 'grad_norm': 2.9051251952233343, 'learning_rate': 4.660555555555556e-06, 'epoch': 2.3252}
{'loss': 0.8873, 'grad_norm': 2.9453467308908685, 'learning_rate': 4.659444444444445e-06, 'epoch': 2.3256}
{'loss': 0.8996, 'grad_norm': 2.9665736101232403, 'learning_rate': 4.658333333333333e-06, 'epoch': 2.326}
{'loss': 0.8846, 'grad_norm': 2.709834492800727, 'learning_rate': 4.657222222222222e-06, 'epoch': 2.3264}
{'loss': 0.8857, 'grad_norm': 2.7956234726452847, 'learning_rate': 4.656111111111112e-06, 'epoch': 2.3268}
{'loss': 0.8974, 'grad_norm': 2.7754780529407275, 'learning_rate': 4.655e-06, 'epoch': 2.3272}
{'loss': 0.8961, 'grad_norm': 3.0772604139577764, 'learning_rate': 4.653888888888889e-06, 'epoch': 2.3276}
{'loss': 0.8876, 'grad_norm': 2.7465354379728493, 'learning_rate': 4.652777777777779e-06, 'epoch': 2.328}
{'eval_valid_loss': 0.86376953125, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.196, 'eval_valid_steps_per_second': 278.049, 'epoch': 2.328}
{'loss': 0.889, 'grad_norm': 2.5169983722335196, 'learning_rate': 4.651666666666667e-06, 'epoch': 2.3284000000000002}
{'loss': 0.8918, 'grad_norm': 2.8784625515061473, 'learning_rate': 4.650555555555556e-06, 'epoch': 2.3288}
{'loss': 0.8892, 'grad_norm': 2.7896309067703386, 'learning_rate': 4.649444444444445e-06, 'epoch': 2.3292}
{'loss': 0.8966, 'grad_norm': 2.916576119561452, 'learning_rate': 4.6483333333333334e-06, 'epoch': 2.3296}
{'loss': 0.8869, 'grad_norm': 2.783032435486882, 'learning_rate': 4.647222222222222e-06, 'epoch': 2.33}
{'loss': 0.8901, 'grad_norm': 3.1044900467569145, 'learning_rate': 4.646111111111111e-06, 'epoch': 2.3304}
{'loss': 0.8948, 'grad_norm': 3.130656054314756, 'learning_rate': 4.645e-06, 'epoch': 2.3308}
{'loss': 0.8833, 'grad_norm': 2.666305062368827, 'learning_rate': 4.643888888888889e-06, 'epoch': 2.3312}
{'loss': 0.9073, 'grad_norm': 3.506303908873732, 'learning_rate': 4.642777777777779e-06, 'epoch': 2.3316}
{'loss': 0.8852, 'grad_norm': 2.7103022399975565, 'learning_rate': 4.641666666666667e-06, 'epoch': 2.332}
{'eval_valid_loss': 0.8642578125, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.109, 'eval_valid_steps_per_second': 279.027, 'epoch': 2.332}
{'loss': 0.8912, 'grad_norm': 2.7427542717507216, 'learning_rate': 4.640555555555556e-06, 'epoch': 2.3324}
{'loss': 0.8844, 'grad_norm': 3.166578728726395, 'learning_rate': 4.639444444444445e-06, 'epoch': 2.3327999999999998}
{'loss': 0.8935, 'grad_norm': 2.9436894708229673, 'learning_rate': 4.6383333333333335e-06, 'epoch': 2.3332}
{'loss': 0.9002, 'grad_norm': 2.9196693540864818, 'learning_rate': 4.637222222222222e-06, 'epoch': 2.3336}
{'loss': 0.88, 'grad_norm': 2.6823140609448326, 'learning_rate': 4.636111111111111e-06, 'epoch': 2.334}
{'loss': 0.8935, 'grad_norm': 2.6926382149151196, 'learning_rate': 4.6350000000000005e-06, 'epoch': 2.3344}
{'loss': 0.8904, 'grad_norm': 3.2055722076061364, 'learning_rate': 4.633888888888889e-06, 'epoch': 2.3348}
{'loss': 0.8979, 'grad_norm': 2.868421637165723, 'learning_rate': 4.632777777777778e-06, 'epoch': 2.3352}
{'loss': 0.8957, 'grad_norm': 2.853707726792486, 'learning_rate': 4.6316666666666675e-06, 'epoch': 2.3356}
{'loss': 0.8931, 'grad_norm': 2.9607823514928757, 'learning_rate': 4.630555555555556e-06, 'epoch': 2.336}
{'eval_valid_loss': 0.8623046875, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1109.827, 'eval_valid_steps_per_second': 277.457, 'epoch': 2.336}
{'loss': 0.8934, 'grad_norm': 3.025376430057055, 'learning_rate': 4.629444444444445e-06, 'epoch': 2.3364}
{'loss': 0.8993, 'grad_norm': 3.252611963014852, 'learning_rate': 4.628333333333334e-06, 'epoch': 2.3368}
{'loss': 0.876, 'grad_norm': 2.5297872296302546, 'learning_rate': 4.627222222222222e-06, 'epoch': 2.3372}
{'loss': 0.8854, 'grad_norm': 3.2070934833245235, 'learning_rate': 4.626111111111111e-06, 'epoch': 2.3376}
{'loss': 0.8887, 'grad_norm': 3.3737355265354614, 'learning_rate': 4.625000000000001e-06, 'epoch': 2.338}
{'loss': 0.8841, 'grad_norm': 2.861932954886608, 'learning_rate': 4.623888888888889e-06, 'epoch': 2.3384}
{'loss': 0.8984, 'grad_norm': 3.0279589021871907, 'learning_rate': 4.622777777777778e-06, 'epoch': 2.3388}
{'loss': 0.8786, 'grad_norm': 3.034933271035873, 'learning_rate': 4.621666666666667e-06, 'epoch': 2.3392}
{'loss': 0.8764, 'grad_norm': 2.718535447698348, 'learning_rate': 4.620555555555556e-06, 'epoch': 2.3396}
{'loss': 0.8759, 'grad_norm': 2.6329436906225694, 'learning_rate': 4.619444444444445e-06, 'epoch': 2.34}
{'eval_valid_loss': 0.865234375, 'eval_valid_runtime': 0.1896, 'eval_valid_samples_per_second': 527.526, 'eval_valid_steps_per_second': 131.882, 'epoch': 2.34}
{'loss': 0.8794, 'grad_norm': 2.917379868819961, 'learning_rate': 4.618333333333334e-06, 'epoch': 2.3404}
{'loss': 0.891, 'grad_norm': 2.773793565701099, 'learning_rate': 4.6172222222222224e-06, 'epoch': 2.3407999999999998}
{'loss': 0.8999, 'grad_norm': 2.7166700902626686, 'learning_rate': 4.616111111111112e-06, 'epoch': 2.3412}
{'loss': 0.8968, 'grad_norm': 2.8794324644998204, 'learning_rate': 4.615000000000001e-06, 'epoch': 2.3416}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8894, 'grad_norm': 2.7638227441807963, 'learning_rate': 4.613888888888889e-06, 'epoch': 2.342}
{'loss': 0.8971, 'grad_norm': 3.199172833570807, 'learning_rate': 4.612777777777778e-06, 'epoch': 2.3424}
{'loss': 0.894, 'grad_norm': 2.790661991638858, 'learning_rate': 4.611666666666667e-06, 'epoch': 2.3428}
{'loss': 0.9014, 'grad_norm': 2.8719342564975046, 'learning_rate': 4.6105555555555556e-06, 'epoch': 2.3432}
{'loss': 0.8893, 'grad_norm': 2.9401498368091885, 'learning_rate': 4.609444444444445e-06, 'epoch': 2.3436}
{'loss': 0.8933, 'grad_norm': 2.942598939510832, 'learning_rate': 4.608333333333334e-06, 'epoch': 2.344}
{'eval_valid_loss': 0.86376953125, 'eval_valid_runtime': 0.091, 'eval_valid_samples_per_second': 1099.272, 'eval_valid_steps_per_second': 274.818, 'epoch': 2.344}
{'loss': 0.886, 'grad_norm': 2.83245772208995, 'learning_rate': 4.6072222222222225e-06, 'epoch': 2.3444}
{'loss': 0.886, 'grad_norm': 2.810241406921275, 'learning_rate': 4.606111111111112e-06, 'epoch': 2.3448}
{'loss': 0.8878, 'grad_norm': 3.0232420668413127, 'learning_rate': 4.605000000000001e-06, 'epoch': 2.3452}
{'loss': 0.8938, 'grad_norm': 3.0321965067137127, 'learning_rate': 4.6038888888888895e-06, 'epoch': 2.3456}
{'loss': 0.8959, 'grad_norm': 3.0494313495556216, 'learning_rate': 4.602777777777778e-06, 'epoch': 2.346}
{'loss': 0.9049, 'grad_norm': 3.137747532932095, 'learning_rate': 4.601666666666667e-06, 'epoch': 2.3464}
{'loss': 0.8916, 'grad_norm': 3.046381631667495, 'learning_rate': 4.600555555555556e-06, 'epoch': 2.3468}
{'loss': 0.8897, 'grad_norm': 3.051912818329073, 'learning_rate': 4.599444444444444e-06, 'epoch': 2.3472}
{'loss': 0.8849, 'grad_norm': 3.039502452787855, 'learning_rate': 4.598333333333334e-06, 'epoch': 2.3476}
{'loss': 0.8863, 'grad_norm': 2.6430418251934036, 'learning_rate': 4.597222222222223e-06, 'epoch': 2.348}
{'eval_valid_loss': 0.86328125, 'eval_valid_runtime': 0.0905, 'eval_valid_samples_per_second': 1105.315, 'eval_valid_steps_per_second': 276.329, 'epoch': 2.348}
{'loss': 0.8914, 'grad_norm': 2.910514507387629, 'learning_rate': 4.596111111111111e-06, 'epoch': 2.3484}
{'loss': 0.9002, 'grad_norm': 3.014654774314713, 'learning_rate': 4.595000000000001e-06, 'epoch': 2.3487999999999998}
{'loss': 0.8876, 'grad_norm': 2.83434594632602, 'learning_rate': 4.59388888888889e-06, 'epoch': 2.3492}
{'loss': 0.8937, 'grad_norm': 2.5762678103916405, 'learning_rate': 4.592777777777778e-06, 'epoch': 2.3496}
{'loss': 0.8864, 'grad_norm': 2.7982712462521886, 'learning_rate': 4.591666666666667e-06, 'epoch': 2.35}
{'loss': 0.9102, 'grad_norm': 2.8651683360305307, 'learning_rate': 4.590555555555556e-06, 'epoch': 2.3504}
{'loss': 0.8843, 'grad_norm': 2.8917158542914754, 'learning_rate': 4.5894444444444445e-06, 'epoch': 2.3508}
{'loss': 0.8861, 'grad_norm': 2.894882623552828, 'learning_rate': 4.588333333333333e-06, 'epoch': 2.3512}
{'loss': 0.8796, 'grad_norm': 2.913051828817087, 'learning_rate': 4.587222222222223e-06, 'epoch': 2.3516}
{'loss': 0.8966, 'grad_norm': 2.75241192254934, 'learning_rate': 4.5861111111111114e-06, 'epoch': 2.352}
{'eval_valid_loss': 0.8642578125, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.317, 'eval_valid_steps_per_second': 279.079, 'epoch': 2.352}
{'loss': 0.8838, 'grad_norm': 2.951446591106153, 'learning_rate': 4.585e-06, 'epoch': 2.3524}
{'loss': 0.8937, 'grad_norm': 2.7853563512369486, 'learning_rate': 4.58388888888889e-06, 'epoch': 2.3528000000000002}
{'loss': 0.8881, 'grad_norm': 2.9166699829536844, 'learning_rate': 4.582777777777778e-06, 'epoch': 2.3532}
{'loss': 0.8889, 'grad_norm': 2.9113568997442227, 'learning_rate': 4.581666666666667e-06, 'epoch': 2.3536}
{'loss': 0.8823, 'grad_norm': 2.895976283755457, 'learning_rate': 4.580555555555556e-06, 'epoch': 2.354}
{'loss': 0.8965, 'grad_norm': 2.9393154580965795, 'learning_rate': 4.5794444444444446e-06, 'epoch': 2.3544}
{'loss': 0.8952, 'grad_norm': 2.956648294098965, 'learning_rate': 4.578333333333333e-06, 'epoch': 2.3548}
{'loss': 0.8915, 'grad_norm': 3.2470888432712535, 'learning_rate': 4.577222222222222e-06, 'epoch': 2.3552}
{'loss': 0.8951, 'grad_norm': 3.0687730341588746, 'learning_rate': 4.5761111111111115e-06, 'epoch': 2.3556}
{'loss': 0.8818, 'grad_norm': 2.7962827428368118, 'learning_rate': 4.575e-06, 'epoch': 2.356}
{'eval_valid_loss': 0.8623046875, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.946, 'eval_valid_steps_per_second': 278.986, 'epoch': 2.356}
{'loss': 0.89, 'grad_norm': 3.021340798910277, 'learning_rate': 4.57388888888889e-06, 'epoch': 2.3564}
{'loss': 0.8895, 'grad_norm': 3.0170896560710867, 'learning_rate': 4.5727777777777785e-06, 'epoch': 2.3568}
{'loss': 0.8965, 'grad_norm': 2.891371749099013, 'learning_rate': 4.571666666666667e-06, 'epoch': 2.3572}
{'loss': 0.8897, 'grad_norm': 2.790759534679249, 'learning_rate': 4.570555555555556e-06, 'epoch': 2.3576}
{'loss': 0.8806, 'grad_norm': 3.0900764522384487, 'learning_rate': 4.569444444444445e-06, 'epoch': 2.358}
{'loss': 0.8899, 'grad_norm': 2.896857527646208, 'learning_rate': 4.568333333333333e-06, 'epoch': 2.3584}
{'loss': 0.8894, 'grad_norm': 3.1153136431615116, 'learning_rate': 4.567222222222222e-06, 'epoch': 2.3588}
{'loss': 0.8912, 'grad_norm': 3.084388286094303, 'learning_rate': 4.566111111111112e-06, 'epoch': 2.3592}
{'loss': 0.8974, 'grad_norm': 2.9095576342695515, 'learning_rate': 4.565e-06, 'epoch': 2.3596}
{'loss': 0.8963, 'grad_norm': 2.942268964436823, 'learning_rate': 4.563888888888889e-06, 'epoch': 2.36}
{'eval_valid_loss': 0.8623046875, 'eval_valid_runtime': 0.089, 'eval_valid_samples_per_second': 1123.725, 'eval_valid_steps_per_second': 280.931, 'epoch': 2.36}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.893, 'grad_norm': 2.774511688029408, 'learning_rate': 4.562777777777779e-06, 'epoch': 2.3604}
{'loss': 0.8901, 'grad_norm': 2.6009570897968897, 'learning_rate': 4.561666666666667e-06, 'epoch': 2.3608000000000002}
{'loss': 0.8813, 'grad_norm': 2.7132559105381584, 'learning_rate': 4.560555555555556e-06, 'epoch': 2.3612}
{'loss': 0.8822, 'grad_norm': 2.687953971623039, 'learning_rate': 4.559444444444445e-06, 'epoch': 2.3616}
{'loss': 0.8973, 'grad_norm': 3.0544819482197174, 'learning_rate': 4.5583333333333335e-06, 'epoch': 2.362}
{'loss': 0.8871, 'grad_norm': 2.729905497422662, 'learning_rate': 4.557222222222222e-06, 'epoch': 2.3624}
{'loss': 0.897, 'grad_norm': 3.0270130296766355, 'learning_rate': 4.556111111111112e-06, 'epoch': 2.3628}
{'loss': 0.8893, 'grad_norm': 2.9913922839313183, 'learning_rate': 4.555111111111111e-06, 'epoch': 2.3632}
{'loss': 0.8885, 'grad_norm': 2.5472437644269665, 'learning_rate': 4.554000000000001e-06, 'epoch': 2.3636}
{'loss': 0.8905, 'grad_norm': 3.1633507010348585, 'learning_rate': 4.5528888888888895e-06, 'epoch': 2.364}
{'eval_valid_loss': 0.86328125, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.123, 'eval_valid_steps_per_second': 278.281, 'epoch': 2.364}
{'loss': 0.8994, 'grad_norm': 2.9497354623213394, 'learning_rate': 4.551777777777778e-06, 'epoch': 2.3644}
{'loss': 0.8879, 'grad_norm': 2.8099774651370457, 'learning_rate': 4.550666666666667e-06, 'epoch': 2.3648}
{'loss': 0.891, 'grad_norm': 2.5819050214098307, 'learning_rate': 4.549555555555556e-06, 'epoch': 2.3652}
{'loss': 0.8943, 'grad_norm': 2.83672322929783, 'learning_rate': 4.548444444444445e-06, 'epoch': 2.3656}
{'loss': 0.8845, 'grad_norm': 2.8670591070963263, 'learning_rate': 4.547333333333334e-06, 'epoch': 2.366}
{'loss': 0.8866, 'grad_norm': 3.048611708394549, 'learning_rate': 4.546222222222223e-06, 'epoch': 2.3664}
{'loss': 0.8827, 'grad_norm': 2.987273960261021, 'learning_rate': 4.545111111111111e-06, 'epoch': 2.3668}
{'loss': 0.8868, 'grad_norm': 2.9774426638273903, 'learning_rate': 4.544000000000001e-06, 'epoch': 2.3672}
{'loss': 0.8948, 'grad_norm': 3.259038406357629, 'learning_rate': 4.54288888888889e-06, 'epoch': 2.3676}
{'loss': 0.889, 'grad_norm': 3.07534342918197, 'learning_rate': 4.541777777777778e-06, 'epoch': 2.368}
{'eval_valid_loss': 0.86328125, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.618, 'eval_valid_steps_per_second': 279.655, 'epoch': 2.368}
{'loss': 0.881, 'grad_norm': 2.8714010061264004, 'learning_rate': 4.540666666666667e-06, 'epoch': 2.3684}
{'loss': 0.8906, 'grad_norm': 3.0024591440125694, 'learning_rate': 4.539555555555556e-06, 'epoch': 2.3688000000000002}
{'loss': 0.8891, 'grad_norm': 2.8331498350767026, 'learning_rate': 4.5384444444444444e-06, 'epoch': 2.3692}
{'loss': 0.8996, 'grad_norm': 2.842808011996947, 'learning_rate': 4.537333333333334e-06, 'epoch': 2.3696}
{'loss': 0.8832, 'grad_norm': 2.9459641744988816, 'learning_rate': 4.536222222222223e-06, 'epoch': 2.37}
{'loss': 0.8902, 'grad_norm': 3.0958667988634843, 'learning_rate': 4.535111111111111e-06, 'epoch': 2.3704}
{'loss': 0.8841, 'grad_norm': 2.968069540643498, 'learning_rate': 4.534000000000001e-06, 'epoch': 2.3708}
{'loss': 0.8899, 'grad_norm': 2.554920489218079, 'learning_rate': 4.53288888888889e-06, 'epoch': 2.3712}
{'loss': 0.894, 'grad_norm': 3.117564099730411, 'learning_rate': 4.531777777777778e-06, 'epoch': 2.3716}
{'loss': 0.8903, 'grad_norm': 2.778763077087794, 'learning_rate': 4.530666666666667e-06, 'epoch': 2.372}
{'eval_valid_loss': 0.861328125, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1128.678, 'eval_valid_steps_per_second': 282.17, 'epoch': 2.372}
{'loss': 0.8777, 'grad_norm': 2.7250049809751573, 'learning_rate': 4.529555555555556e-06, 'epoch': 2.3724}
{'loss': 0.8904, 'grad_norm': 3.2726046429041706, 'learning_rate': 4.5284444444444445e-06, 'epoch': 2.3728}
{'loss': 0.8699, 'grad_norm': 3.087991422085386, 'learning_rate': 4.527333333333333e-06, 'epoch': 2.3731999999999998}
{'loss': 0.884, 'grad_norm': 2.7805264957066256, 'learning_rate': 4.526222222222223e-06, 'epoch': 2.3736}
{'loss': 0.8942, 'grad_norm': 2.752515168256482, 'learning_rate': 4.5251111111111115e-06, 'epoch': 2.374}
{'loss': 0.8833, 'grad_norm': 2.9111630637495165, 'learning_rate': 4.524e-06, 'epoch': 2.3744}
{'loss': 0.8812, 'grad_norm': 2.5922291042399173, 'learning_rate': 4.52288888888889e-06, 'epoch': 2.3748}
{'loss': 0.8926, 'grad_norm': 2.8046582892551744, 'learning_rate': 4.5217777777777785e-06, 'epoch': 2.3752}
{'loss': 0.8895, 'grad_norm': 2.773084961957003, 'learning_rate': 4.520666666666667e-06, 'epoch': 2.3756}
{'loss': 0.8853, 'grad_norm': 2.916141525177255, 'learning_rate': 4.519555555555556e-06, 'epoch': 2.376}
{'eval_valid_loss': 0.86181640625, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.68, 'eval_valid_steps_per_second': 278.17, 'epoch': 2.376}
{'loss': 0.8953, 'grad_norm': 3.0190712404415785, 'learning_rate': 4.518444444444445e-06, 'epoch': 2.3764}
{'loss': 0.8742, 'grad_norm': 3.041933689197889, 'learning_rate': 4.517333333333333e-06, 'epoch': 2.3768000000000002}
{'loss': 0.8872, 'grad_norm': 3.0323668614853867, 'learning_rate': 4.516222222222222e-06, 'epoch': 2.3772}
{'loss': 0.886, 'grad_norm': 2.866961155826194, 'learning_rate': 4.515111111111112e-06, 'epoch': 2.3776}
{'loss': 0.8805, 'grad_norm': 2.737096767034465, 'learning_rate': 4.514e-06, 'epoch': 2.378}
{'loss': 0.8907, 'grad_norm': 2.9375180081566263, 'learning_rate': 4.512888888888889e-06, 'epoch': 2.3784}
{'loss': 0.8869, 'grad_norm': 2.951891747563862, 'learning_rate': 4.511777777777779e-06, 'epoch': 2.3788}
{'loss': 0.9042, 'grad_norm': 2.9940748711399996, 'learning_rate': 4.510666666666667e-06, 'epoch': 2.3792}
{'loss': 0.9025, 'grad_norm': 3.019113756065925, 'learning_rate': 4.509555555555556e-06, 'epoch': 2.3796}
{'loss': 0.901, 'grad_norm': 3.1256985741865133, 'learning_rate': 4.508444444444445e-06, 'epoch': 2.38}
{'eval_valid_loss': 0.861328125, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1111.781, 'eval_valid_steps_per_second': 277.945, 'epoch': 2.38}
{'loss': 0.9024, 'grad_norm': 2.859704108137364, 'learning_rate': 4.5073333333333334e-06, 'epoch': 2.3804}
{'loss': 0.8812, 'grad_norm': 3.0130086429825216, 'learning_rate': 4.506222222222222e-06, 'epoch': 2.3808}
{'loss': 0.8896, 'grad_norm': 2.98270340485692, 'learning_rate': 4.505111111111111e-06, 'epoch': 2.3811999999999998}
{'loss': 0.8904, 'grad_norm': 2.735115784708314, 'learning_rate': 4.504e-06, 'epoch': 2.3816}
{'loss': 0.8954, 'grad_norm': 2.8307468527224127, 'learning_rate': 4.502888888888889e-06, 'epoch': 2.382}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8887, 'grad_norm': 2.900275375182862, 'learning_rate': 4.501777777777778e-06, 'epoch': 2.3824}
{'loss': 0.8775, 'grad_norm': 2.8552788333383887, 'learning_rate': 4.500666666666667e-06, 'epoch': 2.3828}
{'loss': 0.8808, 'grad_norm': 3.2178075114227536, 'learning_rate': 4.499555555555556e-06, 'epoch': 2.3832}
{'loss': 0.8954, 'grad_norm': 2.8608216654455045, 'learning_rate': 4.498444444444445e-06, 'epoch': 2.3836}
{'loss': 0.9042, 'grad_norm': 2.753083505220367, 'learning_rate': 4.4973333333333335e-06, 'epoch': 2.384}
{'eval_valid_loss': 0.86328125, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1105.679, 'eval_valid_steps_per_second': 276.42, 'epoch': 2.384}
{'loss': 0.8832, 'grad_norm': 3.315667662134849, 'learning_rate': 4.496222222222222e-06, 'epoch': 2.3844}
{'loss': 0.8931, 'grad_norm': 3.173323051759215, 'learning_rate': 4.495111111111111e-06, 'epoch': 2.3848}
{'loss': 0.8859, 'grad_norm': 3.065653792468698, 'learning_rate': 4.4940000000000005e-06, 'epoch': 2.3852}
{'loss': 0.8937, 'grad_norm': 3.0572715035098037, 'learning_rate': 4.492888888888889e-06, 'epoch': 2.3856}
{'loss': 0.9025, 'grad_norm': 2.9067278489666255, 'learning_rate': 4.491777777777778e-06, 'epoch': 2.386}
{'loss': 0.877, 'grad_norm': 2.7704258806809112, 'learning_rate': 4.490666666666667e-06, 'epoch': 2.3864}
{'loss': 0.8939, 'grad_norm': 2.96334271134658, 'learning_rate': 4.489555555555556e-06, 'epoch': 2.3868}
{'loss': 0.8885, 'grad_norm': 2.8001811875745846, 'learning_rate': 4.488444444444445e-06, 'epoch': 2.3872}
{'loss': 0.8828, 'grad_norm': 2.9357433037638057, 'learning_rate': 4.487333333333334e-06, 'epoch': 2.3876}
{'loss': 0.8833, 'grad_norm': 2.9284539944749337, 'learning_rate': 4.486222222222222e-06, 'epoch': 2.388}
{'eval_valid_loss': 0.86279296875, 'eval_valid_runtime': 0.0884, 'eval_valid_samples_per_second': 1130.885, 'eval_valid_steps_per_second': 282.721, 'epoch': 2.388}
{'loss': 0.8919, 'grad_norm': 3.222534861734895, 'learning_rate': 4.485111111111112e-06, 'epoch': 2.3884}
{'loss': 0.8901, 'grad_norm': 2.8272555843203335, 'learning_rate': 4.484000000000001e-06, 'epoch': 2.3888}
{'loss': 0.8907, 'grad_norm': 2.7085237448343205, 'learning_rate': 4.482888888888889e-06, 'epoch': 2.3891999999999998}
{'loss': 0.8937, 'grad_norm': 3.3641801401592475, 'learning_rate': 4.481777777777778e-06, 'epoch': 2.3896}
{'loss': 0.8832, 'grad_norm': 2.8064496978188167, 'learning_rate': 4.480666666666667e-06, 'epoch': 2.39}
{'loss': 0.8876, 'grad_norm': 2.887801693765141, 'learning_rate': 4.479555555555556e-06, 'epoch': 2.3904}
{'loss': 0.8812, 'grad_norm': 3.003977711476132, 'learning_rate': 4.478444444444445e-06, 'epoch': 2.3908}
{'loss': 0.8838, 'grad_norm': 2.880124893957341, 'learning_rate': 4.477333333333334e-06, 'epoch': 2.3912}
{'loss': 0.8938, 'grad_norm': 2.9703701818364743, 'learning_rate': 4.4762222222222224e-06, 'epoch': 2.3916}
{'loss': 0.8858, 'grad_norm': 2.704226556040315, 'learning_rate': 4.475111111111112e-06, 'epoch': 2.392}
{'eval_valid_loss': 0.8623046875, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1111.362, 'eval_valid_steps_per_second': 277.841, 'epoch': 2.392}
{'loss': 0.894, 'grad_norm': 3.0696266223579918, 'learning_rate': 4.474000000000001e-06, 'epoch': 2.3924}
{'loss': 0.8865, 'grad_norm': 2.7904269516608995, 'learning_rate': 4.472888888888889e-06, 'epoch': 2.3928}
{'loss': 0.8812, 'grad_norm': 2.604757226103383, 'learning_rate': 4.471777777777778e-06, 'epoch': 2.3932}
{'loss': 0.8867, 'grad_norm': 3.2207888007118286, 'learning_rate': 4.470666666666667e-06, 'epoch': 2.3936}
{'loss': 0.8905, 'grad_norm': 3.14794335853643, 'learning_rate': 4.4695555555555555e-06, 'epoch': 2.394}
{'loss': 0.8983, 'grad_norm': 2.8731174836341538, 'learning_rate': 4.468444444444445e-06, 'epoch': 2.3944}
{'loss': 0.8837, 'grad_norm': 2.928967191520559, 'learning_rate': 4.467333333333334e-06, 'epoch': 2.3948}
{'loss': 0.8827, 'grad_norm': 3.1152617546002435, 'learning_rate': 4.4662222222222225e-06, 'epoch': 2.3952}
{'loss': 0.8909, 'grad_norm': 2.905889868214111, 'learning_rate': 4.465111111111112e-06, 'epoch': 2.3956}
{'loss': 0.8987, 'grad_norm': 2.8545948366389875, 'learning_rate': 4.464000000000001e-06, 'epoch': 2.396}
{'eval_valid_loss': 0.86083984375, 'eval_valid_runtime': 0.176, 'eval_valid_samples_per_second': 568.108, 'eval_valid_steps_per_second': 142.027, 'epoch': 2.396}
{'loss': 0.8932, 'grad_norm': 2.9341025387470574, 'learning_rate': 4.4628888888888895e-06, 'epoch': 2.3964}
{'loss': 0.8859, 'grad_norm': 2.8916750882124136, 'learning_rate': 4.461777777777778e-06, 'epoch': 2.3968}
{'loss': 0.9021, 'grad_norm': 2.8977316487886156, 'learning_rate': 4.460666666666667e-06, 'epoch': 2.3971999999999998}
{'loss': 0.9019, 'grad_norm': 2.928037153165945, 'learning_rate': 4.459555555555556e-06, 'epoch': 2.3976}
{'loss': 0.8925, 'grad_norm': 3.083472568787685, 'learning_rate': 4.458444444444444e-06, 'epoch': 2.398}
{'loss': 0.8904, 'grad_norm': 3.087865907786091, 'learning_rate': 4.457333333333334e-06, 'epoch': 2.3984}
{'loss': 0.8862, 'grad_norm': 2.749972061535493, 'learning_rate': 4.456222222222223e-06, 'epoch': 2.3988}
{'loss': 0.8834, 'grad_norm': 2.9358135008396986, 'learning_rate': 4.455111111111111e-06, 'epoch': 2.3992}
{'loss': 0.8869, 'grad_norm': 2.979235902839811, 'learning_rate': 4.454000000000001e-06, 'epoch': 2.3996}
{'loss': 0.8886, 'grad_norm': 3.3155695800032587, 'learning_rate': 4.45288888888889e-06, 'epoch': 2.4}
{'eval_valid_loss': 0.86279296875, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1122.003, 'eval_valid_steps_per_second': 280.501, 'epoch': 2.4}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8899, 'grad_norm': 2.704615419111592, 'learning_rate': 4.451777777777778e-06, 'epoch': 2.4004}
{'loss': 0.8791, 'grad_norm': 3.0425960621052237, 'learning_rate': 4.450666666666667e-06, 'epoch': 2.4008}
{'loss': 0.8896, 'grad_norm': 2.7770902837240397, 'learning_rate': 4.449555555555556e-06, 'epoch': 2.4012000000000002}
{'loss': 0.8957, 'grad_norm': 2.9585918506085362, 'learning_rate': 4.4484444444444444e-06, 'epoch': 2.4016}
{'loss': 0.8884, 'grad_norm': 2.9017743338896214, 'learning_rate': 4.447333333333333e-06, 'epoch': 2.402}
{'loss': 0.8878, 'grad_norm': 2.8315015468471936, 'learning_rate': 4.446222222222223e-06, 'epoch': 2.4024}
{'loss': 0.8882, 'grad_norm': 2.7105000992282187, 'learning_rate': 4.4451111111111114e-06, 'epoch': 2.4028}
{'loss': 0.8858, 'grad_norm': 2.5872541573428687, 'learning_rate': 4.444111111111111e-06, 'epoch': 2.4032}
{'loss': 0.8882, 'grad_norm': 2.7183608511049013, 'learning_rate': 4.4430000000000005e-06, 'epoch': 2.4036}
{'loss': 0.8813, 'grad_norm': 2.884546383925012, 'learning_rate': 4.441888888888889e-06, 'epoch': 2.404}
{'eval_valid_loss': 0.8603515625, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.188, 'eval_valid_steps_per_second': 277.047, 'epoch': 2.404}
{'loss': 0.89, 'grad_norm': 3.210516565581049, 'learning_rate': 4.440777777777778e-06, 'epoch': 2.4044}
{'loss': 0.8852, 'grad_norm': 2.852115530445793, 'learning_rate': 4.4396666666666675e-06, 'epoch': 2.4048}
{'loss': 0.8807, 'grad_norm': 2.7205108552732646, 'learning_rate': 4.438555555555556e-06, 'epoch': 2.4052}
{'loss': 0.889, 'grad_norm': 2.7760677920907972, 'learning_rate': 4.437444444444445e-06, 'epoch': 2.4056}
{'loss': 0.8844, 'grad_norm': 2.82651330537092, 'learning_rate': 4.436333333333334e-06, 'epoch': 2.406}
{'loss': 0.9043, 'grad_norm': 2.9421142901257955, 'learning_rate': 4.435222222222222e-06, 'epoch': 2.4064}
{'loss': 0.9092, 'grad_norm': 2.832736549684863, 'learning_rate': 4.434111111111111e-06, 'epoch': 2.4068}
{'loss': 0.8969, 'grad_norm': 2.609865707810276, 'learning_rate': 4.433000000000001e-06, 'epoch': 2.4072}
{'loss': 0.8884, 'grad_norm': 2.945357243934163, 'learning_rate': 4.431888888888889e-06, 'epoch': 2.4076}
{'loss': 0.8896, 'grad_norm': 2.7161668476134246, 'learning_rate': 4.430777777777778e-06, 'epoch': 2.408}
{'eval_valid_loss': 0.861328125, 'eval_valid_runtime': 0.0909, 'eval_valid_samples_per_second': 1100.318, 'eval_valid_steps_per_second': 275.08, 'epoch': 2.408}
{'loss': 0.8801, 'grad_norm': 3.0129362581868393, 'learning_rate': 4.429666666666667e-06, 'epoch': 2.4084}
{'loss': 0.8989, 'grad_norm': 2.715648751325087, 'learning_rate': 4.428555555555556e-06, 'epoch': 2.4088}
{'loss': 0.8725, 'grad_norm': 2.8460582126741207, 'learning_rate': 4.427444444444445e-06, 'epoch': 2.4092000000000002}
{'loss': 0.8786, 'grad_norm': 2.7645681430875566, 'learning_rate': 4.426333333333334e-06, 'epoch': 2.4096}
{'loss': 0.8828, 'grad_norm': 3.009267450155756, 'learning_rate': 4.425222222222222e-06, 'epoch': 2.41}
{'loss': 0.8887, 'grad_norm': 3.173682576290042, 'learning_rate': 4.424111111111111e-06, 'epoch': 2.4104}
{'loss': 0.8779, 'grad_norm': 2.775384455975328, 'learning_rate': 4.423000000000001e-06, 'epoch': 2.4108}
{'loss': 0.8997, 'grad_norm': 3.2707664954692293, 'learning_rate': 4.421888888888889e-06, 'epoch': 2.4112}
{'loss': 0.8935, 'grad_norm': 2.8580012330585807, 'learning_rate': 4.420777777777778e-06, 'epoch': 2.4116}
{'loss': 0.8881, 'grad_norm': 3.1222271156489523, 'learning_rate': 4.419666666666667e-06, 'epoch': 2.412}
{'eval_valid_loss': 0.8623046875, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.52, 'eval_valid_steps_per_second': 279.63, 'epoch': 2.412}
{'loss': 0.8959, 'grad_norm': 3.118989686413194, 'learning_rate': 4.4185555555555555e-06, 'epoch': 2.4124}
{'loss': 0.8806, 'grad_norm': 2.6726752025441853, 'learning_rate': 4.417444444444445e-06, 'epoch': 2.4128}
{'loss': 0.8831, 'grad_norm': 3.1436403137850926, 'learning_rate': 4.416333333333334e-06, 'epoch': 2.4132}
{'loss': 0.9003, 'grad_norm': 2.927251102802741, 'learning_rate': 4.4152222222222225e-06, 'epoch': 2.4136}
{'loss': 0.887, 'grad_norm': 2.826428215248975, 'learning_rate': 4.414111111111111e-06, 'epoch': 2.414}
{'loss': 0.8694, 'grad_norm': 2.9472923403995974, 'learning_rate': 4.413000000000001e-06, 'epoch': 2.4144}
{'loss': 0.8774, 'grad_norm': 2.9385412379583085, 'learning_rate': 4.4118888888888895e-06, 'epoch': 2.4148}
{'loss': 0.8779, 'grad_norm': 3.120595125895398, 'learning_rate': 4.410777777777778e-06, 'epoch': 2.4152}
{'loss': 0.8925, 'grad_norm': 2.863968812166657, 'learning_rate': 4.409666666666667e-06, 'epoch': 2.4156}
{'loss': 0.8932, 'grad_norm': 3.058170079768867, 'learning_rate': 4.408555555555556e-06, 'epoch': 2.416}
{'eval_valid_loss': 0.86083984375, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.305, 'eval_valid_steps_per_second': 277.076, 'epoch': 2.416}
{'loss': 0.8995, 'grad_norm': 2.6887845750782104, 'learning_rate': 4.407444444444444e-06, 'epoch': 2.4164}
{'loss': 0.8814, 'grad_norm': 2.8474175414620926, 'learning_rate': 4.406333333333334e-06, 'epoch': 2.4168}
{'loss': 0.8932, 'grad_norm': 3.039872863128161, 'learning_rate': 4.405222222222223e-06, 'epoch': 2.4172000000000002}
{'loss': 0.8854, 'grad_norm': 3.009505877713904, 'learning_rate': 4.404111111111111e-06, 'epoch': 2.4176}
{'loss': 0.8846, 'grad_norm': 2.8831145120608173, 'learning_rate': 4.403000000000001e-06, 'epoch': 2.418}
{'loss': 0.8932, 'grad_norm': 3.114099718054256, 'learning_rate': 4.40188888888889e-06, 'epoch': 2.4184}
{'loss': 0.8812, 'grad_norm': 2.8342420696138513, 'learning_rate': 4.400777777777778e-06, 'epoch': 2.4188}
{'loss': 0.8806, 'grad_norm': 2.7680628425439275, 'learning_rate': 4.399666666666667e-06, 'epoch': 2.4192}
{'loss': 0.8872, 'grad_norm': 2.776960579881845, 'learning_rate': 4.398555555555556e-06, 'epoch': 2.4196}
{'loss': 0.8929, 'grad_norm': 3.012462024528505, 'learning_rate': 4.3974444444444444e-06, 'epoch': 2.42}
{'eval_valid_loss': 0.85986328125, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.845, 'eval_valid_steps_per_second': 278.961, 'epoch': 2.42}
{'loss': 0.8806, 'grad_norm': 3.09129494251448, 'learning_rate': 4.396333333333333e-06, 'epoch': 2.4204}
{'loss': 0.8792, 'grad_norm': 3.01850604262165, 'learning_rate': 4.395222222222223e-06, 'epoch': 2.4208}
{'loss': 0.894, 'grad_norm': 2.8837436113092485, 'learning_rate': 4.394111111111111e-06, 'epoch': 2.4212}
{'loss': 0.8919, 'grad_norm': 3.0795660857213676, 'learning_rate': 4.393000000000001e-06, 'epoch': 2.4215999999999998}
{'loss': 0.8826, 'grad_norm': 2.6946080821501113, 'learning_rate': 4.39188888888889e-06, 'epoch': 2.422}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8951, 'grad_norm': 2.7444922714006434, 'learning_rate': 4.390777777777778e-06, 'epoch': 2.4224}
{'loss': 0.8941, 'grad_norm': 2.7741433226016783, 'learning_rate': 4.389666666666667e-06, 'epoch': 2.4228}
{'loss': 0.8817, 'grad_norm': 2.8706632434100627, 'learning_rate': 4.388555555555556e-06, 'epoch': 2.4232}
{'loss': 0.8796, 'grad_norm': 3.0883371222161076, 'learning_rate': 4.3874444444444445e-06, 'epoch': 2.4236}
{'loss': 0.8892, 'grad_norm': 3.2029280346114164, 'learning_rate': 4.386333333333333e-06, 'epoch': 2.424}
{'eval_valid_loss': 0.8603515625, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1110.647, 'eval_valid_steps_per_second': 277.662, 'epoch': 2.424}
{'loss': 0.8763, 'grad_norm': 2.9178708326872695, 'learning_rate': 4.385222222222223e-06, 'epoch': 2.4244}
{'loss': 0.8766, 'grad_norm': 2.697267641457613, 'learning_rate': 4.3841111111111115e-06, 'epoch': 2.4248}
{'loss': 0.8906, 'grad_norm': 2.848824846021804, 'learning_rate': 4.383e-06, 'epoch': 2.4252000000000002}
{'loss': 0.8866, 'grad_norm': 2.7778743124453205, 'learning_rate': 4.38188888888889e-06, 'epoch': 2.4256}
{'loss': 0.8825, 'grad_norm': 2.817033658135322, 'learning_rate': 4.3807777777777785e-06, 'epoch': 2.426}
{'loss': 0.8776, 'grad_norm': 3.1383855436286754, 'learning_rate': 4.379666666666667e-06, 'epoch': 2.4264}
{'loss': 0.8873, 'grad_norm': 3.1523878716048297, 'learning_rate': 4.378555555555556e-06, 'epoch': 2.4268}
{'loss': 0.8959, 'grad_norm': 3.3880698688935507, 'learning_rate': 4.377444444444445e-06, 'epoch': 2.4272}
{'loss': 0.8978, 'grad_norm': 2.842132946998702, 'learning_rate': 4.376333333333333e-06, 'epoch': 2.4276}
{'loss': 0.8772, 'grad_norm': 3.1008423995036085, 'learning_rate': 4.375222222222222e-06, 'epoch': 2.428}
{'eval_valid_loss': 0.85986328125, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1115.278, 'eval_valid_steps_per_second': 278.819, 'epoch': 2.428}
{'loss': 0.884, 'grad_norm': 3.023525039627835, 'learning_rate': 4.374111111111112e-06, 'epoch': 2.4284}
{'loss': 0.8847, 'grad_norm': 3.0741528476112534, 'learning_rate': 4.373e-06, 'epoch': 2.4288}
{'loss': 0.8753, 'grad_norm': 2.8539679741208706, 'learning_rate': 4.371888888888889e-06, 'epoch': 2.4292}
{'loss': 0.8849, 'grad_norm': 3.114126370737272, 'learning_rate': 4.370777777777779e-06, 'epoch': 2.4295999999999998}
{'loss': 0.8884, 'grad_norm': 2.8851729707829903, 'learning_rate': 4.369666666666667e-06, 'epoch': 2.43}
{'loss': 0.8878, 'grad_norm': 2.982334657804002, 'learning_rate': 4.368555555555556e-06, 'epoch': 2.4304}
{'loss': 0.8876, 'grad_norm': 2.8699635571315985, 'learning_rate': 4.367444444444445e-06, 'epoch': 2.4308}
{'loss': 0.8822, 'grad_norm': 3.0857616712480356, 'learning_rate': 4.3663333333333334e-06, 'epoch': 2.4312}
{'loss': 0.8921, 'grad_norm': 3.03124398299731, 'learning_rate': 4.365222222222222e-06, 'epoch': 2.4316}
{'loss': 0.8855, 'grad_norm': 3.2205351166311202, 'learning_rate': 4.364111111111112e-06, 'epoch': 2.432}
{'eval_valid_loss': 0.861328125, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.45, 'eval_valid_steps_per_second': 279.363, 'epoch': 2.432}
{'loss': 0.8775, 'grad_norm': 2.968638227266009, 'learning_rate': 4.363e-06, 'epoch': 2.4324}
{'loss': 0.8918, 'grad_norm': 2.9867362223200677, 'learning_rate': 4.361888888888889e-06, 'epoch': 2.4328}
{'loss': 0.8832, 'grad_norm': 2.747010714158715, 'learning_rate': 4.360777777777778e-06, 'epoch': 2.4332}
{'loss': 0.8939, 'grad_norm': 2.6013179813064307, 'learning_rate': 4.359666666666667e-06, 'epoch': 2.4336}
{'loss': 0.8845, 'grad_norm': 2.888676218438381, 'learning_rate': 4.358555555555556e-06, 'epoch': 2.434}
{'loss': 0.883, 'grad_norm': 3.0821165549266256, 'learning_rate': 4.357444444444445e-06, 'epoch': 2.4344}
{'loss': 0.8804, 'grad_norm': 3.028446739827453, 'learning_rate': 4.3563333333333335e-06, 'epoch': 2.4348}
{'loss': 0.8743, 'grad_norm': 2.9131495295868657, 'learning_rate': 4.355222222222222e-06, 'epoch': 2.4352}
{'loss': 0.8847, 'grad_norm': 2.961128359559093, 'learning_rate': 4.354111111111112e-06, 'epoch': 2.4356}
{'loss': 0.8839, 'grad_norm': 3.0145339375640483, 'learning_rate': 4.3530000000000005e-06, 'epoch': 2.436}
{'eval_valid_loss': 0.861328125, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.687, 'eval_valid_steps_per_second': 279.922, 'epoch': 2.436}
{'loss': 0.8954, 'grad_norm': 2.988132388327807, 'learning_rate': 4.351888888888889e-06, 'epoch': 2.4364}
{'loss': 0.8747, 'grad_norm': 2.985526666569099, 'learning_rate': 4.350777777777778e-06, 'epoch': 2.4368}
{'loss': 0.8898, 'grad_norm': 3.002374940819861, 'learning_rate': 4.349666666666667e-06, 'epoch': 2.4372}
{'loss': 0.8916, 'grad_norm': 2.8396548243754585, 'learning_rate': 4.348555555555556e-06, 'epoch': 2.4375999999999998}
{'loss': 0.8898, 'grad_norm': 3.0218441236747013, 'learning_rate': 4.347444444444445e-06, 'epoch': 2.438}
{'loss': 0.8868, 'grad_norm': 3.121863211837675, 'learning_rate': 4.346333333333334e-06, 'epoch': 2.4384}
{'loss': 0.8826, 'grad_norm': 2.725991716730193, 'learning_rate': 4.345222222222222e-06, 'epoch': 2.4388}
{'loss': 0.882, 'grad_norm': 2.5522319354694907, 'learning_rate': 4.344111111111112e-06, 'epoch': 2.4392}
{'loss': 0.8926, 'grad_norm': 2.856576216868932, 'learning_rate': 4.343000000000001e-06, 'epoch': 2.4396}
{'loss': 0.8767, 'grad_norm': 3.2996293806262367, 'learning_rate': 4.341888888888889e-06, 'epoch': 2.44}
{'eval_valid_loss': 0.86083984375, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.346, 'eval_valid_steps_per_second': 279.336, 'epoch': 2.44}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8687, 'grad_norm': 2.8303384496613924, 'learning_rate': 4.340777777777778e-06, 'epoch': 2.4404}
{'loss': 0.8819, 'grad_norm': 3.0333547462154735, 'learning_rate': 4.339666666666667e-06, 'epoch': 2.4408}
{'loss': 0.8795, 'grad_norm': 2.86925542197962, 'learning_rate': 4.3385555555555554e-06, 'epoch': 2.4412}
{'loss': 0.8753, 'grad_norm': 2.632287734395693, 'learning_rate': 4.337444444444445e-06, 'epoch': 2.4416}
{'loss': 0.8895, 'grad_norm': 2.7590194740963114, 'learning_rate': 4.336333333333334e-06, 'epoch': 2.442}
{'loss': 0.886, 'grad_norm': 3.11796482690972, 'learning_rate': 4.3352222222222224e-06, 'epoch': 2.4424}
{'loss': 0.8914, 'grad_norm': 2.979362652518772, 'learning_rate': 4.334111111111112e-06, 'epoch': 2.4428}
{'loss': 0.8973, 'grad_norm': 2.8491097444309887, 'learning_rate': 4.3331111111111115e-06, 'epoch': 2.4432}
{'loss': 0.8903, 'grad_norm': 3.026169383012409, 'learning_rate': 4.332e-06, 'epoch': 2.4436}
{'loss': 0.8763, 'grad_norm': 2.8174419218371503, 'learning_rate': 4.33088888888889e-06, 'epoch': 2.444}
{'eval_valid_loss': 0.85986328125, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.948, 'eval_valid_steps_per_second': 278.487, 'epoch': 2.444}
{'loss': 0.8972, 'grad_norm': 2.8638654635882785, 'learning_rate': 4.3297777777777785e-06, 'epoch': 2.4444}
{'loss': 0.8925, 'grad_norm': 2.9536608961583735, 'learning_rate': 4.328666666666667e-06, 'epoch': 2.4448}
{'loss': 0.8876, 'grad_norm': 3.1675098524264063, 'learning_rate': 4.327555555555556e-06, 'epoch': 2.4452}
{'loss': 0.885, 'grad_norm': 2.9065592662783857, 'learning_rate': 4.326444444444445e-06, 'epoch': 2.4455999999999998}
{'loss': 0.8846, 'grad_norm': 2.923963167770696, 'learning_rate': 4.325333333333333e-06, 'epoch': 2.446}
{'loss': 0.8807, 'grad_norm': 2.9314596894180003, 'learning_rate': 4.324222222222222e-06, 'epoch': 2.4464}
{'loss': 0.9004, 'grad_norm': 2.845832669903597, 'learning_rate': 4.323111111111112e-06, 'epoch': 2.4468}
{'loss': 0.8968, 'grad_norm': 3.4382083423098786, 'learning_rate': 4.322e-06, 'epoch': 2.4472}
{'loss': 0.8921, 'grad_norm': 2.781462291587756, 'learning_rate': 4.32088888888889e-06, 'epoch': 2.4476}
{'loss': 0.8898, 'grad_norm': 2.972836562915121, 'learning_rate': 4.3197777777777786e-06, 'epoch': 2.448}
{'eval_valid_loss': 0.85986328125, 'eval_valid_runtime': 0.0912, 'eval_valid_samples_per_second': 1095.98, 'eval_valid_steps_per_second': 273.995, 'epoch': 2.448}
{'loss': 0.8947, 'grad_norm': 2.947591400848608, 'learning_rate': 4.318666666666667e-06, 'epoch': 2.4484}
{'loss': 0.8801, 'grad_norm': 2.910255297312992, 'learning_rate': 4.317555555555556e-06, 'epoch': 2.4488}
{'loss': 0.8933, 'grad_norm': 2.795639084683063, 'learning_rate': 4.316444444444445e-06, 'epoch': 2.4492}
{'loss': 0.883, 'grad_norm': 2.8106284961419967, 'learning_rate': 4.315333333333333e-06, 'epoch': 2.4496}
{'loss': 0.8903, 'grad_norm': 2.856205700336717, 'learning_rate': 4.314222222222222e-06, 'epoch': 2.45}
{'loss': 0.8769, 'grad_norm': 2.7635512362727988, 'learning_rate': 4.313111111111111e-06, 'epoch': 2.4504}
{'loss': 0.8807, 'grad_norm': 2.8081621732443964, 'learning_rate': 4.312e-06, 'epoch': 2.4508}
{'loss': 0.8812, 'grad_norm': 3.0215452418641777, 'learning_rate': 4.310888888888889e-06, 'epoch': 2.4512}
{'loss': 0.8972, 'grad_norm': 3.145690941061661, 'learning_rate': 4.309777777777779e-06, 'epoch': 2.4516}
{'loss': 0.89, 'grad_norm': 3.2216779483953513, 'learning_rate': 4.308666666666667e-06, 'epoch': 2.452}
{'eval_valid_loss': 0.861328125, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1109.484, 'eval_valid_steps_per_second': 277.371, 'epoch': 2.452}
{'loss': 0.8898, 'grad_norm': 3.213327386266566, 'learning_rate': 4.307555555555556e-06, 'epoch': 2.4524}
{'loss': 0.8859, 'grad_norm': 3.0615471311196893, 'learning_rate': 4.306444444444445e-06, 'epoch': 2.4528}
{'loss': 0.8785, 'grad_norm': 2.9128603871901473, 'learning_rate': 4.3053333333333335e-06, 'epoch': 2.4532}
{'loss': 0.8863, 'grad_norm': 2.751658735915604, 'learning_rate': 4.304222222222222e-06, 'epoch': 2.4536}
{'loss': 0.9016, 'grad_norm': 2.8411301345228304, 'learning_rate': 4.303111111111111e-06, 'epoch': 2.454}
{'loss': 0.8871, 'grad_norm': 2.9142105228577306, 'learning_rate': 4.3020000000000005e-06, 'epoch': 2.4544}
{'loss': 0.8931, 'grad_norm': 3.017493514075337, 'learning_rate': 4.300888888888889e-06, 'epoch': 2.4548}
{'loss': 0.879, 'grad_norm': 3.2812011260979514, 'learning_rate': 4.299777777777778e-06, 'epoch': 2.4552}
{'loss': 0.8784, 'grad_norm': 2.751398987815467, 'learning_rate': 4.2986666666666675e-06, 'epoch': 2.4556}
{'loss': 0.8881, 'grad_norm': 2.9091911405329585, 'learning_rate': 4.297555555555556e-06, 'epoch': 2.456}
{'eval_valid_loss': 0.859375, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1116.965, 'eval_valid_steps_per_second': 279.241, 'epoch': 2.456}
{'loss': 0.8845, 'grad_norm': 2.750911745737152, 'learning_rate': 4.296444444444445e-06, 'epoch': 2.4564}
{'loss': 0.8847, 'grad_norm': 2.85420066344533, 'learning_rate': 4.295333333333334e-06, 'epoch': 2.4568}
{'loss': 0.8878, 'grad_norm': 2.6765144568750703, 'learning_rate': 4.294222222222222e-06, 'epoch': 2.4572}
{'loss': 0.8812, 'grad_norm': 2.9146757784468997, 'learning_rate': 4.293111111111111e-06, 'epoch': 2.4576000000000002}
{'loss': 0.8957, 'grad_norm': 2.939245689398014, 'learning_rate': 4.292000000000001e-06, 'epoch': 2.458}
{'loss': 0.877, 'grad_norm': 3.1571453071047175, 'learning_rate': 4.290888888888889e-06, 'epoch': 2.4584}
{'loss': 0.8841, 'grad_norm': 3.097854348158517, 'learning_rate': 4.289777777777778e-06, 'epoch': 2.4588}
{'loss': 0.8844, 'grad_norm': 3.144622232767619, 'learning_rate': 4.288666666666667e-06, 'epoch': 2.4592}
{'loss': 0.8923, 'grad_norm': 2.911220443005551, 'learning_rate': 4.287555555555556e-06, 'epoch': 2.4596}
{'loss': 0.8836, 'grad_norm': 2.925082318052402, 'learning_rate': 4.286444444444445e-06, 'epoch': 2.46}
{'eval_valid_loss': 0.859375, 'eval_valid_runtime': 0.0903, 'eval_valid_samples_per_second': 1107.369, 'eval_valid_steps_per_second': 276.842, 'epoch': 2.46}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8859, 'grad_norm': 2.848203493969943, 'learning_rate': 4.285333333333334e-06, 'epoch': 2.4604}
{'loss': 0.8921, 'grad_norm': 2.956558804583803, 'learning_rate': 4.284222222222222e-06, 'epoch': 2.4608}
{'loss': 0.8919, 'grad_norm': 3.024043453716735, 'learning_rate': 4.283111111111111e-06, 'epoch': 2.4612}
{'loss': 0.8873, 'grad_norm': 3.3034766032627574, 'learning_rate': 4.282000000000001e-06, 'epoch': 2.4616}
{'loss': 0.8887, 'grad_norm': 3.0530664284123716, 'learning_rate': 4.280888888888889e-06, 'epoch': 2.462}
{'loss': 0.8854, 'grad_norm': 3.033577182988806, 'learning_rate': 4.279777777777778e-06, 'epoch': 2.4624}
{'loss': 0.8841, 'grad_norm': 2.8262292715069184, 'learning_rate': 4.278666666666667e-06, 'epoch': 2.4628}
{'loss': 0.8885, 'grad_norm': 2.980797005871842, 'learning_rate': 4.2775555555555555e-06, 'epoch': 2.4632}
{'loss': 0.8884, 'grad_norm': 3.0383489207399603, 'learning_rate': 4.276444444444445e-06, 'epoch': 2.4636}
{'loss': 0.8822, 'grad_norm': 2.741991976209402, 'learning_rate': 4.275333333333334e-06, 'epoch': 2.464}
{'eval_valid_loss': 0.859375, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.556, 'eval_valid_steps_per_second': 279.639, 'epoch': 2.464}
{'loss': 0.8942, 'grad_norm': 3.079033712649159, 'learning_rate': 4.2742222222222225e-06, 'epoch': 2.4644}
{'loss': 0.8749, 'grad_norm': 3.0043689882310747, 'learning_rate': 4.273111111111111e-06, 'epoch': 2.4648}
{'loss': 0.8848, 'grad_norm': 3.129009668972283, 'learning_rate': 4.272000000000001e-06, 'epoch': 2.4652}
{'loss': 0.8886, 'grad_norm': 2.822248032458955, 'learning_rate': 4.2708888888888895e-06, 'epoch': 2.4656000000000002}
{'loss': 0.8775, 'grad_norm': 2.944604571030037, 'learning_rate': 4.269777777777778e-06, 'epoch': 2.466}
{'loss': 0.8874, 'grad_norm': 2.77138264787264, 'learning_rate': 4.268666666666667e-06, 'epoch': 2.4664}
{'loss': 0.876, 'grad_norm': 2.853683216100528, 'learning_rate': 4.267555555555556e-06, 'epoch': 2.4668}
{'loss': 0.8812, 'grad_norm': 2.872090121359482, 'learning_rate': 4.266444444444444e-06, 'epoch': 2.4672}
{'loss': 0.9022, 'grad_norm': 3.167476601713692, 'learning_rate': 4.265333333333334e-06, 'epoch': 2.4676}
{'loss': 0.878, 'grad_norm': 2.8464819549931364, 'learning_rate': 4.264222222222223e-06, 'epoch': 2.468}
{'eval_valid_loss': 0.8603515625, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.537, 'eval_valid_steps_per_second': 278.634, 'epoch': 2.468}
{'loss': 0.8833, 'grad_norm': 2.7292495319414627, 'learning_rate': 4.263111111111111e-06, 'epoch': 2.4684}
{'loss': 0.8854, 'grad_norm': 2.7407749863560555, 'learning_rate': 4.262000000000001e-06, 'epoch': 2.4688}
{'loss': 0.8788, 'grad_norm': 2.752621945071423, 'learning_rate': 4.26088888888889e-06, 'epoch': 2.4692}
{'loss': 0.8912, 'grad_norm': 2.6510709930826355, 'learning_rate': 4.259777777777778e-06, 'epoch': 2.4696}
{'loss': 0.8777, 'grad_norm': 2.9188975805347126, 'learning_rate': 4.258666666666667e-06, 'epoch': 2.4699999999999998}
{'loss': 0.8819, 'grad_norm': 2.7335514081677634, 'learning_rate': 4.257555555555556e-06, 'epoch': 2.4704}
{'loss': 0.8836, 'grad_norm': 2.8273397061439467, 'learning_rate': 4.2564444444444444e-06, 'epoch': 2.4708}
{'loss': 0.8729, 'grad_norm': 2.6288493916908195, 'learning_rate': 4.255333333333333e-06, 'epoch': 2.4712}
{'loss': 0.8781, 'grad_norm': 2.892551357601346, 'learning_rate': 4.254222222222223e-06, 'epoch': 2.4716}
{'loss': 0.8787, 'grad_norm': 2.6663705275182745, 'learning_rate': 4.253111111111111e-06, 'epoch': 2.472}
{'eval_valid_loss': 0.86083984375, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1115.228, 'eval_valid_steps_per_second': 278.807, 'epoch': 2.472}
{'loss': 0.8957, 'grad_norm': 2.7224402430923456, 'learning_rate': 4.252000000000001e-06, 'epoch': 2.4724}
{'loss': 0.8802, 'grad_norm': 2.9238201031673574, 'learning_rate': 4.25088888888889e-06, 'epoch': 2.4728}
{'loss': 0.8828, 'grad_norm': 3.029707260421518, 'learning_rate': 4.249777777777778e-06, 'epoch': 2.4732}
{'loss': 0.8883, 'grad_norm': 2.9283688845690086, 'learning_rate': 4.248666666666667e-06, 'epoch': 2.4736000000000002}
{'loss': 0.8887, 'grad_norm': 2.9660151483260826, 'learning_rate': 4.247555555555556e-06, 'epoch': 2.474}
{'loss': 0.8784, 'grad_norm': 3.0410124595591315, 'learning_rate': 4.2464444444444445e-06, 'epoch': 2.4744}
{'loss': 0.8984, 'grad_norm': 2.867732118442573, 'learning_rate': 4.245333333333333e-06, 'epoch': 2.4748}
{'loss': 0.8876, 'grad_norm': 2.6599670376474167, 'learning_rate': 4.244222222222222e-06, 'epoch': 2.4752}
{'loss': 0.8824, 'grad_norm': 3.235106076899577, 'learning_rate': 4.2431111111111115e-06, 'epoch': 2.4756}
{'loss': 0.8706, 'grad_norm': 2.959251487434428, 'learning_rate': 4.242e-06, 'epoch': 2.476}
{'eval_valid_loss': 0.85986328125, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.364, 'eval_valid_steps_per_second': 277.091, 'epoch': 2.476}
{'loss': 0.8893, 'grad_norm': 3.0954075043380462, 'learning_rate': 4.24088888888889e-06, 'epoch': 2.4764}
{'loss': 0.8782, 'grad_norm': 2.9214785199770383, 'learning_rate': 4.2397777777777785e-06, 'epoch': 2.4768}
{'loss': 0.8902, 'grad_norm': 3.095540443742797, 'learning_rate': 4.238666666666667e-06, 'epoch': 2.4772}
{'loss': 0.8799, 'grad_norm': 3.0793497776169976, 'learning_rate': 4.237555555555556e-06, 'epoch': 2.4776}
{'loss': 0.8905, 'grad_norm': 2.850592654704811, 'learning_rate': 4.236444444444445e-06, 'epoch': 2.4779999999999998}
{'loss': 0.8832, 'grad_norm': 3.1818843160450556, 'learning_rate': 4.235333333333333e-06, 'epoch': 2.4784}
{'loss': 0.8944, 'grad_norm': 2.96780968882747, 'learning_rate': 4.234222222222222e-06, 'epoch': 2.4788}
{'loss': 0.8865, 'grad_norm': 2.9836351566083494, 'learning_rate': 4.233111111111112e-06, 'epoch': 2.4792}
{'loss': 0.894, 'grad_norm': 2.825527062240743, 'learning_rate': 4.232e-06, 'epoch': 2.4796}
{'loss': 0.8846, 'grad_norm': 2.9976781860694253, 'learning_rate': 4.230888888888889e-06, 'epoch': 2.48}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'eval_valid_loss': 0.8583984375, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1111.586, 'eval_valid_steps_per_second': 277.897, 'epoch': 2.48}
{'loss': 0.8938, 'grad_norm': 2.8553462700205245, 'learning_rate': 4.229777777777779e-06, 'epoch': 2.4804}
{'loss': 0.8919, 'grad_norm': 2.772996765363191, 'learning_rate': 4.228666666666667e-06, 'epoch': 2.4808}
{'loss': 0.8983, 'grad_norm': 3.293973902813775, 'learning_rate': 4.227555555555556e-06, 'epoch': 2.4812}
{'loss': 0.8706, 'grad_norm': 3.1120433571977353, 'learning_rate': 4.226444444444445e-06, 'epoch': 2.4816}
{'loss': 0.8845, 'grad_norm': 2.6758371556142975, 'learning_rate': 4.225333333333333e-06, 'epoch': 2.482}
{'loss': 0.8865, 'grad_norm': 2.9956018374793345, 'learning_rate': 4.224222222222222e-06, 'epoch': 2.4824}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8641, 'grad_norm': 2.83524800160281, 'learning_rate': 4.223111111111112e-06, 'epoch': 2.4828}
{'loss': 0.8939, 'grad_norm': 3.0693671928559696, 'learning_rate': 4.222111111111111e-06, 'epoch': 2.4832}
{'loss': 0.8833, 'grad_norm': 3.1896477457108476, 'learning_rate': 4.221e-06, 'epoch': 2.4836}
{'loss': 0.8771, 'grad_norm': 2.7155068939214893, 'learning_rate': 4.2198888888888895e-06, 'epoch': 2.484}
{'eval_valid_loss': 0.859375, 'eval_valid_runtime': 0.0905, 'eval_valid_samples_per_second': 1104.793, 'eval_valid_steps_per_second': 276.198, 'epoch': 2.484}
{'loss': 0.8813, 'grad_norm': 2.693260705591445, 'learning_rate': 4.218777777777778e-06, 'epoch': 2.4844}
{'loss': 0.8879, 'grad_norm': 2.7595386344284267, 'learning_rate': 4.217666666666667e-06, 'epoch': 2.4848}
{'loss': 0.8932, 'grad_norm': 2.8544847277538277, 'learning_rate': 4.216555555555556e-06, 'epoch': 2.4852}
{'loss': 0.8809, 'grad_norm': 3.0029934214209653, 'learning_rate': 4.215444444444445e-06, 'epoch': 2.4856}
{'loss': 0.8896, 'grad_norm': 2.7315935538595024, 'learning_rate': 4.214333333333334e-06, 'epoch': 2.4859999999999998}
{'loss': 0.8871, 'grad_norm': 2.8616776184575268, 'learning_rate': 4.213222222222223e-06, 'epoch': 2.4864}
{'loss': 0.8861, 'grad_norm': 2.957412952091701, 'learning_rate': 4.212111111111111e-06, 'epoch': 2.4868}
{'loss': 0.8788, 'grad_norm': 2.798975784229374, 'learning_rate': 4.211e-06, 'epoch': 2.4872}
{'loss': 0.8825, 'grad_norm': 2.5857831499762494, 'learning_rate': 4.2098888888888896e-06, 'epoch': 2.4876}
{'loss': 0.8929, 'grad_norm': 2.9877572591299386, 'learning_rate': 4.208777777777778e-06, 'epoch': 2.488}
{'eval_valid_loss': 0.85986328125, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1120.869, 'eval_valid_steps_per_second': 280.217, 'epoch': 2.488}
{'loss': 0.9056, 'grad_norm': 3.0709731674879146, 'learning_rate': 4.207666666666667e-06, 'epoch': 2.4884}
{'loss': 0.8908, 'grad_norm': 3.1101147715742745, 'learning_rate': 4.206555555555556e-06, 'epoch': 2.4888}
{'loss': 0.877, 'grad_norm': 2.718355522914985, 'learning_rate': 4.205444444444444e-06, 'epoch': 2.4892}
{'loss': 0.8902, 'grad_norm': 2.703043793136527, 'learning_rate': 4.204333333333334e-06, 'epoch': 2.4896}
{'loss': 0.9003, 'grad_norm': 2.965558906920086, 'learning_rate': 4.203222222222223e-06, 'epoch': 2.49}
{'loss': 0.8906, 'grad_norm': 2.80482155312147, 'learning_rate': 4.202111111111111e-06, 'epoch': 2.4904}
{'loss': 0.8818, 'grad_norm': 2.8053395674178283, 'learning_rate': 4.201e-06, 'epoch': 2.4908}
{'loss': 0.8961, 'grad_norm': 3.134773582377094, 'learning_rate': 4.19988888888889e-06, 'epoch': 2.4912}
{'loss': 0.8853, 'grad_norm': 2.7890209333811162, 'learning_rate': 4.198777777777778e-06, 'epoch': 2.4916}
{'loss': 0.8981, 'grad_norm': 2.9550770559769113, 'learning_rate': 4.197666666666667e-06, 'epoch': 2.492}
{'eval_valid_loss': 0.85986328125, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1115.251, 'eval_valid_steps_per_second': 278.813, 'epoch': 2.492}
{'loss': 0.8879, 'grad_norm': 2.6167085180813165, 'learning_rate': 4.196555555555556e-06, 'epoch': 2.4924}
{'loss': 0.8872, 'grad_norm': 2.660195746667756, 'learning_rate': 4.1954444444444445e-06, 'epoch': 2.4928}
{'loss': 0.8885, 'grad_norm': 2.7820914581476894, 'learning_rate': 4.194333333333333e-06, 'epoch': 2.4932}
{'loss': 0.8833, 'grad_norm': 3.191603039355468, 'learning_rate': 4.193222222222223e-06, 'epoch': 2.4936}
{'loss': 0.8742, 'grad_norm': 3.3286404098651987, 'learning_rate': 4.1921111111111115e-06, 'epoch': 2.4939999999999998}
{'loss': 0.8832, 'grad_norm': 3.0242626434349473, 'learning_rate': 4.191e-06, 'epoch': 2.4944}
{'loss': 0.8815, 'grad_norm': 2.8031545928787898, 'learning_rate': 4.18988888888889e-06, 'epoch': 2.4948}
{'loss': 0.8681, 'grad_norm': 2.9607460141227993, 'learning_rate': 4.1887777777777785e-06, 'epoch': 2.4952}
{'loss': 0.8969, 'grad_norm': 2.878179248750025, 'learning_rate': 4.187666666666667e-06, 'epoch': 2.4956}
{'loss': 0.8992, 'grad_norm': 2.866498864445976, 'learning_rate': 4.186555555555556e-06, 'epoch': 2.496}
{'eval_valid_loss': 0.85888671875, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1109.481, 'eval_valid_steps_per_second': 277.37, 'epoch': 2.496}
{'loss': 0.8912, 'grad_norm': 2.8132908239098433, 'learning_rate': 4.185444444444445e-06, 'epoch': 2.4964}
{'loss': 0.8703, 'grad_norm': 3.069811928204033, 'learning_rate': 4.184333333333333e-06, 'epoch': 2.4968}
{'loss': 0.8994, 'grad_norm': 3.058902914857291, 'learning_rate': 4.183222222222222e-06, 'epoch': 2.4972}
{'loss': 0.8925, 'grad_norm': 2.8568648175358744, 'learning_rate': 4.182111111111112e-06, 'epoch': 2.4976}
{'loss': 0.8959, 'grad_norm': 2.9567373071709184, 'learning_rate': 4.181e-06, 'epoch': 2.498}
{'loss': 0.8897, 'grad_norm': 2.9317786624542097, 'learning_rate': 4.17988888888889e-06, 'epoch': 2.4984}
{'loss': 0.8812, 'grad_norm': 2.8721223610497586, 'learning_rate': 4.1787777777777786e-06, 'epoch': 2.4988}
{'loss': 0.8802, 'grad_norm': 2.9941200808064754, 'learning_rate': 4.177666666666667e-06, 'epoch': 2.4992}
{'loss': 0.8853, 'grad_norm': 2.7253532077953997, 'learning_rate': 4.176555555555556e-06, 'epoch': 2.4996}
{'loss': 0.8862, 'grad_norm': 2.896738983919862, 'learning_rate': 4.175444444444445e-06, 'epoch': 2.5}
{'eval_valid_loss': 0.859375, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1122.198, 'eval_valid_steps_per_second': 280.549, 'epoch': 2.5}
{'loss': 0.8972, 'grad_norm': 2.7584318810882507, 'learning_rate': 4.174333333333333e-06, 'epoch': 2.5004}
{'loss': 0.8829, 'grad_norm': 2.8959033612261975, 'learning_rate': 4.173222222222222e-06, 'epoch': 2.5008}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8739, 'grad_norm': 3.002464533809984, 'learning_rate': 4.172111111111111e-06, 'epoch': 2.5012}
{'loss': 0.8923, 'grad_norm': 2.879938258959663, 'learning_rate': 4.171e-06, 'epoch': 2.5016}
{'loss': 0.8992, 'grad_norm': 2.8850962475864956, 'learning_rate': 4.169888888888889e-06, 'epoch': 2.502}
{'loss': 0.8861, 'grad_norm': 2.801241508345051, 'learning_rate': 4.168777777777779e-06, 'epoch': 2.5023999999999997}
{'loss': 0.8841, 'grad_norm': 2.770490009625812, 'learning_rate': 4.167666666666667e-06, 'epoch': 2.5028}
{'loss': 0.8697, 'grad_norm': 3.050799966869502, 'learning_rate': 4.166555555555556e-06, 'epoch': 2.5032}
{'loss': 0.8858, 'grad_norm': 2.989458488001961, 'learning_rate': 4.165444444444445e-06, 'epoch': 2.5036}
{'loss': 0.8802, 'grad_norm': 2.8513796394975643, 'learning_rate': 4.1643333333333335e-06, 'epoch': 2.504}
{'eval_valid_loss': 0.86083984375, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1114.058, 'eval_valid_steps_per_second': 278.514, 'epoch': 2.504}
{'loss': 0.8985, 'grad_norm': 2.7102556607930004, 'learning_rate': 4.163222222222222e-06, 'epoch': 2.5044}
{'loss': 0.8787, 'grad_norm': 2.85153494521114, 'learning_rate': 4.162111111111111e-06, 'epoch': 2.5048}
{'loss': 0.8788, 'grad_norm': 3.1081781623876346, 'learning_rate': 4.1610000000000005e-06, 'epoch': 2.5052}
{'loss': 0.8957, 'grad_norm': 2.8240059420236747, 'learning_rate': 4.159888888888889e-06, 'epoch': 2.5056000000000003}
{'loss': 0.8751, 'grad_norm': 2.995282019677489, 'learning_rate': 4.158777777777778e-06, 'epoch': 2.5060000000000002}
{'loss': 0.8802, 'grad_norm': 2.9293232500613633, 'learning_rate': 4.1576666666666675e-06, 'epoch': 2.5064}
{'loss': 0.8815, 'grad_norm': 2.7578138075871377, 'learning_rate': 4.156555555555556e-06, 'epoch': 2.5068}
{'loss': 0.8829, 'grad_norm': 2.6367169132932693, 'learning_rate': 4.155444444444445e-06, 'epoch': 2.5072}
{'loss': 0.8883, 'grad_norm': 3.034175179106448, 'learning_rate': 4.154333333333334e-06, 'epoch': 2.5076}
{'loss': 0.8897, 'grad_norm': 2.948695387728712, 'learning_rate': 4.153222222222222e-06, 'epoch': 2.508}
{'eval_valid_loss': 0.85888671875, 'eval_valid_runtime': 0.089, 'eval_valid_samples_per_second': 1123.996, 'eval_valid_steps_per_second': 280.999, 'epoch': 2.508}
{'loss': 0.8918, 'grad_norm': 2.8890642540587512, 'learning_rate': 4.152111111111111e-06, 'epoch': 2.5084}
{'loss': 0.8892, 'grad_norm': 2.7454641822419594, 'learning_rate': 4.1510000000000006e-06, 'epoch': 2.5088}
{'loss': 0.8843, 'grad_norm': 2.911114825398217, 'learning_rate': 4.149888888888889e-06, 'epoch': 2.5092}
{'loss': 0.8957, 'grad_norm': 2.9324506419549214, 'learning_rate': 4.148777777777778e-06, 'epoch': 2.5096}
{'loss': 0.8793, 'grad_norm': 2.936598406064025, 'learning_rate': 4.147666666666667e-06, 'epoch': 2.51}
{'loss': 0.876, 'grad_norm': 2.9077410923360003, 'learning_rate': 4.146555555555556e-06, 'epoch': 2.5103999999999997}
{'loss': 0.8786, 'grad_norm': 2.7244404894262018, 'learning_rate': 4.145444444444445e-06, 'epoch': 2.5108}
{'loss': 0.8701, 'grad_norm': 2.8112557837976753, 'learning_rate': 4.144333333333334e-06, 'epoch': 2.5112}
{'loss': 0.8885, 'grad_norm': 2.89802737046283, 'learning_rate': 4.143222222222222e-06, 'epoch': 2.5116}
{'loss': 0.8915, 'grad_norm': 2.988325181027292, 'learning_rate': 4.142111111111111e-06, 'epoch': 2.512}
{'eval_valid_loss': 0.8583984375, 'eval_valid_runtime': 0.0906, 'eval_valid_samples_per_second': 1104.061, 'eval_valid_steps_per_second': 276.015, 'epoch': 2.512}
{'loss': 0.8776, 'grad_norm': 2.6850374938613073, 'learning_rate': 4.141000000000001e-06, 'epoch': 2.5124}
{'loss': 0.8775, 'grad_norm': 2.672327114915062, 'learning_rate': 4.139888888888889e-06, 'epoch': 2.5128}
{'loss': 0.8892, 'grad_norm': 2.715698700355803, 'learning_rate': 4.138777777777778e-06, 'epoch': 2.5132}
{'loss': 0.8871, 'grad_norm': 2.9064585087950014, 'learning_rate': 4.137666666666667e-06, 'epoch': 2.5136}
{'loss': 0.8875, 'grad_norm': 3.074206021390397, 'learning_rate': 4.1365555555555555e-06, 'epoch': 2.5140000000000002}
{'loss': 0.8817, 'grad_norm': 2.7322435845756905, 'learning_rate': 4.135444444444445e-06, 'epoch': 2.5144}
{'loss': 0.8722, 'grad_norm': 2.8661001414731175, 'learning_rate': 4.134333333333334e-06, 'epoch': 2.5148}
{'loss': 0.8896, 'grad_norm': 3.03760173768513, 'learning_rate': 4.1332222222222225e-06, 'epoch': 2.5152}
{'loss': 0.8804, 'grad_norm': 2.9641658123473777, 'learning_rate': 4.132111111111111e-06, 'epoch': 2.5156}
{'loss': 0.8733, 'grad_norm': 2.8289557601443556, 'learning_rate': 4.131000000000001e-06, 'epoch': 2.516}
{'eval_valid_loss': 0.859375, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.48, 'eval_valid_steps_per_second': 278.12, 'epoch': 2.516}
{'loss': 0.8819, 'grad_norm': 2.8629931608105723, 'learning_rate': 4.1298888888888895e-06, 'epoch': 2.5164}
{'loss': 0.8799, 'grad_norm': 3.0654757017125536, 'learning_rate': 4.128777777777778e-06, 'epoch': 2.5168}
{'loss': 0.8977, 'grad_norm': 2.874473165879181, 'learning_rate': 4.127666666666667e-06, 'epoch': 2.5172}
{'loss': 0.8864, 'grad_norm': 2.5904406554432775, 'learning_rate': 4.126555555555556e-06, 'epoch': 2.5176}
{'loss': 0.8838, 'grad_norm': 2.7780904503852732, 'learning_rate': 4.125444444444444e-06, 'epoch': 2.518}
{'loss': 0.8782, 'grad_norm': 2.766356926311071, 'learning_rate': 4.124333333333334e-06, 'epoch': 2.5183999999999997}
{'loss': 0.885, 'grad_norm': 3.14200464946784, 'learning_rate': 4.123222222222223e-06, 'epoch': 2.5188}
{'loss': 0.8959, 'grad_norm': 3.381925885168941, 'learning_rate': 4.122111111111111e-06, 'epoch': 2.5192}
{'loss': 0.8884, 'grad_norm': 2.8522753621379007, 'learning_rate': 4.121000000000001e-06, 'epoch': 2.5196}
{'loss': 0.8962, 'grad_norm': 2.962055096034014, 'learning_rate': 4.1198888888888896e-06, 'epoch': 2.52}
{'eval_valid_loss': 0.8583984375, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.389, 'eval_valid_steps_per_second': 279.597, 'epoch': 2.52}
{'loss': 0.8883, 'grad_norm': 2.9312262499564907, 'learning_rate': 4.118777777777778e-06, 'epoch': 2.5204}
{'loss': 0.8849, 'grad_norm': 2.873391209240377, 'learning_rate': 4.117666666666667e-06, 'epoch': 2.5208}
{'loss': 0.8844, 'grad_norm': 2.9312889706556757, 'learning_rate': 4.116555555555556e-06, 'epoch': 2.5212}
{'loss': 0.8833, 'grad_norm': 3.140765096851476, 'learning_rate': 4.115444444444444e-06, 'epoch': 2.5216}
{'loss': 0.8832, 'grad_norm': 2.7961736897341734, 'learning_rate': 4.114333333333333e-06, 'epoch': 2.5220000000000002}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.879, 'grad_norm': 2.9717446732850794, 'learning_rate': 4.113222222222223e-06, 'epoch': 2.5224}
{'loss': 0.8736, 'grad_norm': 2.6364258610566815, 'learning_rate': 4.112111111111111e-06, 'epoch': 2.5228}
{'loss': 0.8988, 'grad_norm': 2.7798410393615547, 'learning_rate': 4.111111111111111e-06, 'epoch': 2.5232}
{'loss': 0.8786, 'grad_norm': 2.7935115473494765, 'learning_rate': 4.1100000000000005e-06, 'epoch': 2.5236}
{'loss': 0.8894, 'grad_norm': 2.910175093007369, 'learning_rate': 4.108888888888889e-06, 'epoch': 2.524}
{'eval_valid_loss': 0.85791015625, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.304, 'eval_valid_steps_per_second': 281.826, 'epoch': 2.524}
{'loss': 0.8953, 'grad_norm': 2.7226791818712077, 'learning_rate': 4.107777777777779e-06, 'epoch': 2.5244}
{'loss': 0.9023, 'grad_norm': 2.773987535549068, 'learning_rate': 4.1066666666666674e-06, 'epoch': 2.5248}
{'loss': 0.8913, 'grad_norm': 3.1295530910264033, 'learning_rate': 4.105555555555556e-06, 'epoch': 2.5252}
{'loss': 0.8787, 'grad_norm': 3.0444284348799764, 'learning_rate': 4.104444444444445e-06, 'epoch': 2.5256}
{'loss': 0.8875, 'grad_norm': 2.9144448241742067, 'learning_rate': 4.1033333333333336e-06, 'epoch': 2.526}
{'loss': 0.8754, 'grad_norm': 2.680205870556926, 'learning_rate': 4.102222222222222e-06, 'epoch': 2.5263999999999998}
{'loss': 0.8889, 'grad_norm': 3.1009755576129034, 'learning_rate': 4.101111111111111e-06, 'epoch': 2.5268}
{'loss': 0.8822, 'grad_norm': 2.9364168371674375, 'learning_rate': 4.1e-06, 'epoch': 2.5272}
{'loss': 0.8832, 'grad_norm': 2.779775984343744, 'learning_rate': 4.098888888888889e-06, 'epoch': 2.5276}
{'loss': 0.8888, 'grad_norm': 3.2053695722871023, 'learning_rate': 4.097777777777778e-06, 'epoch': 2.528}
{'eval_valid_loss': 0.8603515625, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.567, 'eval_valid_steps_per_second': 278.392, 'epoch': 2.528}
{'loss': 0.897, 'grad_norm': 3.058353214866259, 'learning_rate': 4.0966666666666675e-06, 'epoch': 2.5284}
{'loss': 0.8874, 'grad_norm': 3.199552106432108, 'learning_rate': 4.095555555555556e-06, 'epoch': 2.5288}
{'loss': 0.8882, 'grad_norm': 3.203787711833537, 'learning_rate': 4.094444444444445e-06, 'epoch': 2.5292}
{'loss': 0.8906, 'grad_norm': 2.9335507975150104, 'learning_rate': 4.093333333333334e-06, 'epoch': 2.5296}
{'loss': 0.8818, 'grad_norm': 3.0023659674770133, 'learning_rate': 4.092222222222222e-06, 'epoch': 2.5300000000000002}
{'loss': 0.8841, 'grad_norm': 2.9381787753625908, 'learning_rate': 4.091111111111111e-06, 'epoch': 2.5304}
{'loss': 0.8695, 'grad_norm': 3.049176457097371, 'learning_rate': 4.09e-06, 'epoch': 2.5308}
{'loss': 0.8953, 'grad_norm': 2.98397567833648, 'learning_rate': 4.088888888888889e-06, 'epoch': 2.5312}
{'loss': 0.8791, 'grad_norm': 2.9077074539257217, 'learning_rate': 4.087777777777778e-06, 'epoch': 2.5316}
{'loss': 0.8872, 'grad_norm': 2.777151903267945, 'learning_rate': 4.086666666666667e-06, 'epoch': 2.532}
{'eval_valid_loss': 0.857421875, 'eval_valid_runtime': 0.0906, 'eval_valid_samples_per_second': 1103.445, 'eval_valid_steps_per_second': 275.861, 'epoch': 2.532}
{'loss': 0.8725, 'grad_norm': 3.0091739397234845, 'learning_rate': 4.085555555555556e-06, 'epoch': 2.5324}
{'loss': 0.8765, 'grad_norm': 3.136450503079917, 'learning_rate': 4.084444444444445e-06, 'epoch': 2.5328}
{'loss': 0.8934, 'grad_norm': 3.2466217235950636, 'learning_rate': 4.083333333333334e-06, 'epoch': 2.5332}
{'loss': 0.8849, 'grad_norm': 2.950452089420732, 'learning_rate': 4.0822222222222225e-06, 'epoch': 2.5336}
{'loss': 0.8851, 'grad_norm': 3.164325222834659, 'learning_rate': 4.081111111111111e-06, 'epoch': 2.534}
{'loss': 0.8915, 'grad_norm': 2.9641889570304487, 'learning_rate': 4.08e-06, 'epoch': 2.5343999999999998}
{'loss': 0.8888, 'grad_norm': 2.8250727015398645, 'learning_rate': 4.0788888888888895e-06, 'epoch': 2.5348}
{'loss': 0.8766, 'grad_norm': 2.997656145013803, 'learning_rate': 4.077777777777778e-06, 'epoch': 2.5352}
{'loss': 0.8827, 'grad_norm': 3.3388594487544903, 'learning_rate': 4.076666666666667e-06, 'epoch': 2.5356}
{'loss': 0.8797, 'grad_norm': 3.1330670660001974, 'learning_rate': 4.075555555555556e-06, 'epoch': 2.536}
{'eval_valid_loss': 0.85888671875, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.399, 'eval_valid_steps_per_second': 277.1, 'epoch': 2.536}
{'loss': 0.8896, 'grad_norm': 2.9767412397329904, 'learning_rate': 4.074444444444445e-06, 'epoch': 2.5364}
{'loss': 0.8877, 'grad_norm': 2.8516118502592316, 'learning_rate': 4.073333333333334e-06, 'epoch': 2.5368}
{'loss': 0.8731, 'grad_norm': 2.7762678452630585, 'learning_rate': 4.0722222222222226e-06, 'epoch': 2.5372}
{'loss': 0.8911, 'grad_norm': 2.964498678237664, 'learning_rate': 4.071111111111111e-06, 'epoch': 2.5376}
{'loss': 0.875, 'grad_norm': 3.050727834001901, 'learning_rate': 4.07e-06, 'epoch': 2.5380000000000003}
{'loss': 0.8903, 'grad_norm': 3.1206427715031566, 'learning_rate': 4.0688888888888896e-06, 'epoch': 2.5384}
{'loss': 0.8821, 'grad_norm': 2.8136961830378184, 'learning_rate': 4.067777777777778e-06, 'epoch': 2.5388}
{'loss': 0.8962, 'grad_norm': 3.088440239941861, 'learning_rate': 4.066666666666667e-06, 'epoch': 2.5392}
{'loss': 0.8823, 'grad_norm': 2.9754966569707695, 'learning_rate': 4.065555555555556e-06, 'epoch': 2.5396}
{'loss': 0.8997, 'grad_norm': 3.0299389452023453, 'learning_rate': 4.064444444444444e-06, 'epoch': 2.54}
{'eval_valid_loss': 0.857421875, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1106.256, 'eval_valid_steps_per_second': 276.564, 'epoch': 2.54}
{'loss': 0.889, 'grad_norm': 2.886924106587396, 'learning_rate': 4.063333333333334e-06, 'epoch': 2.5404}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8885, 'grad_norm': 3.2236640972571227, 'learning_rate': 4.062222222222223e-06, 'epoch': 2.5408}
{'loss': 0.8774, 'grad_norm': 2.945605179994995, 'learning_rate': 4.061111111111111e-06, 'epoch': 2.5412}
{'loss': 0.8835, 'grad_norm': 2.8364548749805727, 'learning_rate': 4.060000000000001e-06, 'epoch': 2.5416}
{'loss': 0.8852, 'grad_norm': 2.6766584754389378, 'learning_rate': 4.05888888888889e-06, 'epoch': 2.542}
{'loss': 0.8823, 'grad_norm': 3.032239663898956, 'learning_rate': 4.057777777777778e-06, 'epoch': 2.5423999999999998}
{'loss': 0.8757, 'grad_norm': 2.9750657310757593, 'learning_rate': 4.056666666666667e-06, 'epoch': 2.5427999999999997}
{'loss': 0.8852, 'grad_norm': 2.890187338611153, 'learning_rate': 4.055555555555556e-06, 'epoch': 2.5432}
{'loss': 0.8844, 'grad_norm': 3.0136350282108784, 'learning_rate': 4.0544444444444445e-06, 'epoch': 2.5436}
{'loss': 0.8818, 'grad_norm': 2.7792059534337707, 'learning_rate': 4.053333333333333e-06, 'epoch': 2.544}
{'eval_valid_loss': 0.85986328125, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1110.712, 'eval_valid_steps_per_second': 277.678, 'epoch': 2.544}
{'loss': 0.8782, 'grad_norm': 2.8447444141271747, 'learning_rate': 4.052222222222223e-06, 'epoch': 2.5444}
{'loss': 0.8834, 'grad_norm': 2.9870304357135775, 'learning_rate': 4.0511111111111115e-06, 'epoch': 2.5448}
{'loss': 0.8938, 'grad_norm': 2.9832986018931384, 'learning_rate': 4.05e-06, 'epoch': 2.5452}
{'loss': 0.8896, 'grad_norm': 2.821053478932825, 'learning_rate': 4.04888888888889e-06, 'epoch': 2.5456}
{'loss': 0.8861, 'grad_norm': 2.8755021278746256, 'learning_rate': 4.0477777777777785e-06, 'epoch': 2.5460000000000003}
{'loss': 0.8928, 'grad_norm': 2.9935484497777867, 'learning_rate': 4.046666666666667e-06, 'epoch': 2.5464}
{'loss': 0.8908, 'grad_norm': 3.247274640760384, 'learning_rate': 4.045555555555556e-06, 'epoch': 2.5468}
{'loss': 0.8932, 'grad_norm': 2.9077177033302215, 'learning_rate': 4.044444444444445e-06, 'epoch': 2.5472}
{'loss': 0.8876, 'grad_norm': 2.9766155648695314, 'learning_rate': 4.043333333333333e-06, 'epoch': 2.5476}
{'loss': 0.8826, 'grad_norm': 3.348662350964586, 'learning_rate': 4.042222222222222e-06, 'epoch': 2.548}
{'eval_valid_loss': 0.85888671875, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.611, 'eval_valid_steps_per_second': 278.403, 'epoch': 2.548}
{'loss': 0.879, 'grad_norm': 2.579983786764877, 'learning_rate': 4.0411111111111116e-06, 'epoch': 2.5484}
{'loss': 0.8873, 'grad_norm': 2.9182235575879694, 'learning_rate': 4.04e-06, 'epoch': 2.5488}
{'loss': 0.8873, 'grad_norm': 3.0274162726790848, 'learning_rate': 4.038888888888889e-06, 'epoch': 2.5492}
{'loss': 0.8813, 'grad_norm': 2.9735100681672653, 'learning_rate': 4.0377777777777786e-06, 'epoch': 2.5496}
{'loss': 0.8943, 'grad_norm': 3.21057695838135, 'learning_rate': 4.036666666666667e-06, 'epoch': 2.55}
{'loss': 0.874, 'grad_norm': 2.911361669984126, 'learning_rate': 4.035555555555556e-06, 'epoch': 2.5504}
{'loss': 0.8891, 'grad_norm': 2.7809066667700377, 'learning_rate': 4.034444444444445e-06, 'epoch': 2.5507999999999997}
{'loss': 0.8882, 'grad_norm': 2.993323745326324, 'learning_rate': 4.033333333333333e-06, 'epoch': 2.5512}
{'loss': 0.8923, 'grad_norm': 2.977544006964112, 'learning_rate': 4.032222222222222e-06, 'epoch': 2.5516}
{'loss': 0.8768, 'grad_norm': 2.784040080262933, 'learning_rate': 4.031111111111111e-06, 'epoch': 2.552}
{'eval_valid_loss': 0.857421875, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1111.961, 'eval_valid_steps_per_second': 277.99, 'epoch': 2.552}
{'loss': 0.8809, 'grad_norm': 2.8343290701680846, 'learning_rate': 4.03e-06, 'epoch': 2.5524}
{'loss': 0.8764, 'grad_norm': 2.662038711203327, 'learning_rate': 4.028888888888889e-06, 'epoch': 2.5528}
{'loss': 0.8832, 'grad_norm': 2.8039790350522056, 'learning_rate': 4.027777777777779e-06, 'epoch': 2.5532}
{'loss': 0.8795, 'grad_norm': 2.923644463345612, 'learning_rate': 4.026666666666667e-06, 'epoch': 2.5536}
{'loss': 0.8875, 'grad_norm': 2.793006555261373, 'learning_rate': 4.025555555555556e-06, 'epoch': 2.5540000000000003}
{'loss': 0.8931, 'grad_norm': 2.821722740045045, 'learning_rate': 4.024444444444445e-06, 'epoch': 2.5544000000000002}
{'loss': 0.874, 'grad_norm': 2.876520076358065, 'learning_rate': 4.0233333333333335e-06, 'epoch': 2.5548}
{'loss': 0.8784, 'grad_norm': 2.991190134063921, 'learning_rate': 4.022222222222222e-06, 'epoch': 2.5552}
{'loss': 0.8907, 'grad_norm': 3.1816960301267367, 'learning_rate': 4.021111111111111e-06, 'epoch': 2.5556}
{'loss': 0.8846, 'grad_norm': 2.894357838243097, 'learning_rate': 4.0200000000000005e-06, 'epoch': 2.556}
{'eval_valid_loss': 0.85791015625, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.249, 'eval_valid_steps_per_second': 279.062, 'epoch': 2.556}
{'loss': 0.8906, 'grad_norm': 2.9130219347959843, 'learning_rate': 4.018888888888889e-06, 'epoch': 2.5564}
{'loss': 0.8833, 'grad_norm': 2.9783386978381463, 'learning_rate': 4.017777777777778e-06, 'epoch': 2.5568}
{'loss': 0.8858, 'grad_norm': 3.1109881888857873, 'learning_rate': 4.0166666666666675e-06, 'epoch': 2.5572}
{'loss': 0.8825, 'grad_norm': 2.943940984195981, 'learning_rate': 4.015555555555556e-06, 'epoch': 2.5576}
{'loss': 0.891, 'grad_norm': 2.7480162596471414, 'learning_rate': 4.014444444444445e-06, 'epoch': 2.558}
{'loss': 0.8958, 'grad_norm': 3.3196000266003542, 'learning_rate': 4.013333333333334e-06, 'epoch': 2.5584}
{'loss': 0.8849, 'grad_norm': 2.9278104338331747, 'learning_rate': 4.012222222222222e-06, 'epoch': 2.5587999999999997}
{'loss': 0.883, 'grad_norm': 2.9882781782944794, 'learning_rate': 4.011111111111111e-06, 'epoch': 2.5592}
{'loss': 0.8816, 'grad_norm': 3.0474140766288604, 'learning_rate': 4.0100000000000006e-06, 'epoch': 2.5596}
{'loss': 0.8821, 'grad_norm': 2.650798746507163, 'learning_rate': 4.008888888888889e-06, 'epoch': 2.56}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'eval_valid_loss': 0.85693359375, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.668, 'eval_valid_steps_per_second': 278.167, 'epoch': 2.56}
{'loss': 0.8794, 'grad_norm': 3.10116960934096, 'learning_rate': 4.007777777777778e-06, 'epoch': 2.5604}
{'loss': 0.8905, 'grad_norm': 3.116399067765014, 'learning_rate': 4.006666666666667e-06, 'epoch': 2.5608}
{'loss': 0.8767, 'grad_norm': 3.088601037280904, 'learning_rate': 4.005555555555556e-06, 'epoch': 2.5612}
{'loss': 0.89, 'grad_norm': 2.861657456333385, 'learning_rate': 4.004444444444445e-06, 'epoch': 2.5616}
{'loss': 0.8903, 'grad_norm': 3.065080606418097, 'learning_rate': 4.003333333333334e-06, 'epoch': 2.5620000000000003}
{'loss': 0.8781, 'grad_norm': 3.131703482510098, 'learning_rate': 4.002222222222222e-06, 'epoch': 2.5624000000000002}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8742, 'grad_norm': 2.9059978703886817, 'learning_rate': 4.001111111111111e-06, 'epoch': 2.5628}
{'loss': 0.8882, 'grad_norm': 3.0828879245168044, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.5632}
{'loss': 0.8649, 'grad_norm': 2.7636622883660746, 'learning_rate': 3.999e-06, 'epoch': 2.5636}
{'loss': 0.8875, 'grad_norm': 2.881081265455007, 'learning_rate': 3.99788888888889e-06, 'epoch': 2.564}
{'eval_valid_loss': 0.85595703125, 'eval_valid_runtime': 0.0903, 'eval_valid_samples_per_second': 1107.501, 'eval_valid_steps_per_second': 276.875, 'epoch': 2.564}
{'loss': 0.8864, 'grad_norm': 3.1205965966265823, 'learning_rate': 3.9967777777777784e-06, 'epoch': 2.5644}
{'loss': 0.8827, 'grad_norm': 2.8448990711585154, 'learning_rate': 3.995666666666667e-06, 'epoch': 2.5648}
{'loss': 0.8982, 'grad_norm': 3.2494573873768418, 'learning_rate': 3.994555555555556e-06, 'epoch': 2.5652}
{'loss': 0.8855, 'grad_norm': 2.906342279343287, 'learning_rate': 3.9934444444444446e-06, 'epoch': 2.5656}
{'loss': 0.8873, 'grad_norm': 2.908220617729748, 'learning_rate': 3.992333333333333e-06, 'epoch': 2.566}
{'loss': 0.8807, 'grad_norm': 2.9050921215250063, 'learning_rate': 3.991222222222223e-06, 'epoch': 2.5664}
{'loss': 0.8929, 'grad_norm': 2.9827196113746406, 'learning_rate': 3.9901111111111116e-06, 'epoch': 2.5667999999999997}
{'loss': 0.8849, 'grad_norm': 3.007370943628407, 'learning_rate': 3.989e-06, 'epoch': 2.5672}
{'loss': 0.8777, 'grad_norm': 3.0444461335854665, 'learning_rate': 3.98788888888889e-06, 'epoch': 2.5676}
{'loss': 0.889, 'grad_norm': 2.9837138856732643, 'learning_rate': 3.9867777777777785e-06, 'epoch': 2.568}
{'eval_valid_loss': 0.8564453125, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1124.928, 'eval_valid_steps_per_second': 281.232, 'epoch': 2.568}
{'loss': 0.8861, 'grad_norm': 2.748766562892042, 'learning_rate': 3.985666666666667e-06, 'epoch': 2.5684}
{'loss': 0.8983, 'grad_norm': 3.1787796224038165, 'learning_rate': 3.984555555555556e-06, 'epoch': 2.5688}
{'loss': 0.8826, 'grad_norm': 2.9806781260290682, 'learning_rate': 3.983444444444445e-06, 'epoch': 2.5692}
{'loss': 0.8927, 'grad_norm': 2.7439838948409734, 'learning_rate': 3.982333333333333e-06, 'epoch': 2.5696}
{'loss': 0.8868, 'grad_norm': 3.0877246655257293, 'learning_rate': 3.981222222222222e-06, 'epoch': 2.57}
{'loss': 0.8814, 'grad_norm': 3.103201771657556, 'learning_rate': 3.980111111111112e-06, 'epoch': 2.5704000000000002}
{'loss': 0.8924, 'grad_norm': 2.994639067148224, 'learning_rate': 3.979e-06, 'epoch': 2.5708}
{'loss': 0.8752, 'grad_norm': 2.8924161156019834, 'learning_rate': 3.977888888888889e-06, 'epoch': 2.5712}
{'loss': 0.8933, 'grad_norm': 3.1306550166876184, 'learning_rate': 3.976777777777779e-06, 'epoch': 2.5716}
{'loss': 0.8854, 'grad_norm': 2.8137386243667346, 'learning_rate': 3.975666666666667e-06, 'epoch': 2.572}
{'eval_valid_loss': 0.8564453125, 'eval_valid_runtime': 0.0885, 'eval_valid_samples_per_second': 1130.08, 'eval_valid_steps_per_second': 282.52, 'epoch': 2.572}
{'loss': 0.8799, 'grad_norm': 3.0095310107362208, 'learning_rate': 3.974555555555556e-06, 'epoch': 2.5724}
{'loss': 0.89, 'grad_norm': 2.9137591902147286, 'learning_rate': 3.973444444444445e-06, 'epoch': 2.5728}
{'loss': 0.8825, 'grad_norm': 3.2272852072095914, 'learning_rate': 3.9723333333333335e-06, 'epoch': 2.5732}
{'loss': 0.8773, 'grad_norm': 2.8704633942024027, 'learning_rate': 3.971222222222222e-06, 'epoch': 2.5736}
{'loss': 0.8881, 'grad_norm': 2.5952419218586766, 'learning_rate': 3.970111111111111e-06, 'epoch': 2.574}
{'loss': 0.8797, 'grad_norm': 2.612086075521557, 'learning_rate': 3.9690000000000005e-06, 'epoch': 2.5744}
{'loss': 0.8682, 'grad_norm': 2.9589985359281097, 'learning_rate': 3.967888888888889e-06, 'epoch': 2.5747999999999998}
{'loss': 0.8796, 'grad_norm': 2.8143012795396496, 'learning_rate': 3.966777777777778e-06, 'epoch': 2.5752}
{'loss': 0.8911, 'grad_norm': 3.0004391448154513, 'learning_rate': 3.9656666666666674e-06, 'epoch': 2.5756}
{'loss': 0.8892, 'grad_norm': 2.927964174134836, 'learning_rate': 3.964555555555556e-06, 'epoch': 2.576}
{'eval_valid_loss': 0.857421875, 'eval_valid_runtime': 0.0885, 'eval_valid_samples_per_second': 1130.336, 'eval_valid_steps_per_second': 282.584, 'epoch': 2.576}
{'loss': 0.8869, 'grad_norm': 2.942703781574521, 'learning_rate': 3.963444444444445e-06, 'epoch': 2.5764}
{'loss': 0.8927, 'grad_norm': 2.8500827032771725, 'learning_rate': 3.9623333333333336e-06, 'epoch': 2.5768}
{'loss': 0.89, 'grad_norm': 2.822643853948779, 'learning_rate': 3.961222222222222e-06, 'epoch': 2.5772}
{'loss': 0.8742, 'grad_norm': 3.0730907595613925, 'learning_rate': 3.960111111111111e-06, 'epoch': 2.5776}
{'loss': 0.8826, 'grad_norm': 3.184499403495451, 'learning_rate': 3.959e-06, 'epoch': 2.578}
{'loss': 0.8851, 'grad_norm': 3.1584363673136306, 'learning_rate': 3.957888888888889e-06, 'epoch': 2.5784000000000002}
{'loss': 0.8756, 'grad_norm': 2.815832590204844, 'learning_rate': 3.956777777777778e-06, 'epoch': 2.5788}
{'loss': 0.8802, 'grad_norm': 2.9615238384167815, 'learning_rate': 3.955666666666667e-06, 'epoch': 2.5792}
{'loss': 0.8868, 'grad_norm': 2.988156883294653, 'learning_rate': 3.954555555555556e-06, 'epoch': 2.5796}
{'loss': 0.8816, 'grad_norm': 2.7215210177485867, 'learning_rate': 3.953444444444445e-06, 'epoch': 2.58}
{'eval_valid_loss': 0.8564453125, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1124.433, 'eval_valid_steps_per_second': 281.108, 'epoch': 2.58}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.898, 'grad_norm': 2.868085704924149, 'learning_rate': 3.952333333333334e-06, 'epoch': 2.5804}
{'loss': 0.8898, 'grad_norm': 2.985027042279202, 'learning_rate': 3.951222222222222e-06, 'epoch': 2.5808}
{'loss': 0.8863, 'grad_norm': 2.8821797283803634, 'learning_rate': 3.950111111111111e-06, 'epoch': 2.5812}
{'loss': 0.889, 'grad_norm': 3.0000103215198983, 'learning_rate': 3.949e-06, 'epoch': 2.5816}
{'loss': 0.8868, 'grad_norm': 2.9571935692283176, 'learning_rate': 3.947888888888889e-06, 'epoch': 2.582}
{'loss': 0.8828, 'grad_norm': 3.073299517731667, 'learning_rate': 3.946777777777778e-06, 'epoch': 2.5824}
{'loss': 0.8923, 'grad_norm': 3.0599553377033653, 'learning_rate': 3.945666666666667e-06, 'epoch': 2.5827999999999998}
{'loss': 0.8804, 'grad_norm': 3.1610107209742773, 'learning_rate': 3.944555555555556e-06, 'epoch': 2.5832}
{'loss': 0.8821, 'grad_norm': 3.073251147843527, 'learning_rate': 3.943444444444445e-06, 'epoch': 2.5836}
{'loss': 0.8823, 'grad_norm': 2.6948928561613257, 'learning_rate': 3.942333333333334e-06, 'epoch': 2.584}
{'eval_valid_loss': 0.857421875, 'eval_valid_runtime': 0.0967, 'eval_valid_samples_per_second': 1034.198, 'eval_valid_steps_per_second': 258.55, 'epoch': 2.584}
{'loss': 0.8836, 'grad_norm': 2.729203090473199, 'learning_rate': 3.9412222222222225e-06, 'epoch': 2.5844}
{'loss': 0.8759, 'grad_norm': 3.08690680237797, 'learning_rate': 3.940111111111111e-06, 'epoch': 2.5848}
{'loss': 0.8847, 'grad_norm': 3.1668687408788943, 'learning_rate': 3.939e-06, 'epoch': 2.5852}
{'loss': 0.8826, 'grad_norm': 2.9690028434295246, 'learning_rate': 3.9378888888888895e-06, 'epoch': 2.5856}
{'loss': 0.8905, 'grad_norm': 2.637948779257794, 'learning_rate': 3.936777777777778e-06, 'epoch': 2.586}
{'loss': 0.8752, 'grad_norm': 2.839464437766846, 'learning_rate': 3.935666666666667e-06, 'epoch': 2.5864000000000003}
{'loss': 0.8765, 'grad_norm': 2.882558185969439, 'learning_rate': 3.934555555555556e-06, 'epoch': 2.5868}
{'loss': 0.8854, 'grad_norm': 2.7543060715916354, 'learning_rate': 3.933444444444445e-06, 'epoch': 2.5872}
{'loss': 0.8869, 'grad_norm': 2.966598594523902, 'learning_rate': 3.932333333333334e-06, 'epoch': 2.5876}
{'loss': 0.8921, 'grad_norm': 2.787559472933569, 'learning_rate': 3.9312222222222226e-06, 'epoch': 2.588}
{'eval_valid_loss': 0.85546875, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1111.495, 'eval_valid_steps_per_second': 277.874, 'epoch': 2.588}
{'loss': 0.8851, 'grad_norm': 3.318256477615195, 'learning_rate': 3.930111111111111e-06, 'epoch': 2.5884}
{'loss': 0.8808, 'grad_norm': 2.975142433058691, 'learning_rate': 3.929000000000001e-06, 'epoch': 2.5888}
{'loss': 0.8938, 'grad_norm': 2.788697162710693, 'learning_rate': 3.9278888888888895e-06, 'epoch': 2.5892}
{'loss': 0.8879, 'grad_norm': 2.885968308214643, 'learning_rate': 3.926777777777778e-06, 'epoch': 2.5896}
{'loss': 0.8817, 'grad_norm': 2.945718732184725, 'learning_rate': 3.925666666666667e-06, 'epoch': 2.59}
{'loss': 0.8792, 'grad_norm': 2.7517771451412973, 'learning_rate': 3.924555555555556e-06, 'epoch': 2.5904}
{'loss': 0.8862, 'grad_norm': 3.0284427543030086, 'learning_rate': 3.923444444444444e-06, 'epoch': 2.5907999999999998}
{'loss': 0.87, 'grad_norm': 2.525688944438752, 'learning_rate': 3.922333333333334e-06, 'epoch': 2.5911999999999997}
{'loss': 0.8763, 'grad_norm': 2.8588566544773824, 'learning_rate': 3.921222222222223e-06, 'epoch': 2.5916}
{'loss': 0.8774, 'grad_norm': 2.878328055675316, 'learning_rate': 3.920111111111111e-06, 'epoch': 2.592}
{'eval_valid_loss': 0.85546875, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.584, 'eval_valid_steps_per_second': 279.146, 'epoch': 2.592}
{'loss': 0.873, 'grad_norm': 2.528500276115573, 'learning_rate': 3.919000000000001e-06, 'epoch': 2.5924}
{'loss': 0.8881, 'grad_norm': 2.8561581563477954, 'learning_rate': 3.91788888888889e-06, 'epoch': 2.5928}
{'loss': 0.8948, 'grad_norm': 3.197273044659133, 'learning_rate': 3.916777777777778e-06, 'epoch': 2.5932}
{'loss': 0.8839, 'grad_norm': 3.0300959720457272, 'learning_rate': 3.915666666666667e-06, 'epoch': 2.5936}
{'loss': 0.8814, 'grad_norm': 2.8912108987422838, 'learning_rate': 3.914555555555556e-06, 'epoch': 2.594}
{'loss': 0.8839, 'grad_norm': 2.9107177090353065, 'learning_rate': 3.9134444444444445e-06, 'epoch': 2.5944000000000003}
{'loss': 0.8852, 'grad_norm': 2.9484288654696247, 'learning_rate': 3.912333333333333e-06, 'epoch': 2.5948}
{'loss': 0.8705, 'grad_norm': 3.364093766552195, 'learning_rate': 3.911222222222223e-06, 'epoch': 2.5952}
{'loss': 0.8794, 'grad_norm': 2.8570050022170244, 'learning_rate': 3.9101111111111115e-06, 'epoch': 2.5956}
{'loss': 0.8797, 'grad_norm': 2.847441195516286, 'learning_rate': 3.909e-06, 'epoch': 2.596}
{'eval_valid_loss': 0.85546875, 'eval_valid_runtime': 0.0908, 'eval_valid_samples_per_second': 1100.994, 'eval_valid_steps_per_second': 275.249, 'epoch': 2.596}
{'loss': 0.8856, 'grad_norm': 2.78330898150529, 'learning_rate': 3.90788888888889e-06, 'epoch': 2.5964}
{'loss': 0.8888, 'grad_norm': 2.6735238068571, 'learning_rate': 3.9067777777777785e-06, 'epoch': 2.5968}
{'loss': 0.8873, 'grad_norm': 2.9968511924553782, 'learning_rate': 3.905666666666667e-06, 'epoch': 2.5972}
{'loss': 0.8796, 'grad_norm': 2.7383875261653152, 'learning_rate': 3.904555555555556e-06, 'epoch': 2.5976}
{'loss': 0.8763, 'grad_norm': 2.900716846756937, 'learning_rate': 3.903444444444445e-06, 'epoch': 2.598}
{'loss': 0.8819, 'grad_norm': 3.097944585361821, 'learning_rate': 3.902333333333333e-06, 'epoch': 2.5984}
{'loss': 0.8873, 'grad_norm': 2.802330641959135, 'learning_rate': 3.901222222222222e-06, 'epoch': 2.5987999999999998}
{'loss': 0.8764, 'grad_norm': 3.044578009195677, 'learning_rate': 3.9001111111111116e-06, 'epoch': 2.5991999999999997}
{'loss': 0.8817, 'grad_norm': 2.832882225832325, 'learning_rate': 3.899e-06, 'epoch': 2.5996}
{'loss': 0.8871, 'grad_norm': 2.778525201788221, 'learning_rate': 3.897888888888889e-06, 'epoch': 2.6}
{'eval_valid_loss': 0.8564453125, 'eval_valid_runtime': 0.0905, 'eval_valid_samples_per_second': 1104.526, 'eval_valid_steps_per_second': 276.131, 'epoch': 2.6}
{'loss': 0.878, 'grad_norm': 2.9351409814237943, 'learning_rate': 3.8967777777777785e-06, 'epoch': 2.6004}
{'loss': 0.8681, 'grad_norm': 2.8601880090279295, 'learning_rate': 3.895666666666667e-06, 'epoch': 2.6008}
{'loss': 0.8845, 'grad_norm': 3.340413452101846, 'learning_rate': 3.894555555555556e-06, 'epoch': 2.6012}
{'loss': 0.8828, 'grad_norm': 2.983249222290761, 'learning_rate': 3.893444444444445e-06, 'epoch': 2.6016}
{'loss': 0.8718, 'grad_norm': 3.1758155773977417, 'learning_rate': 3.892333333333333e-06, 'epoch': 2.602}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8972, 'grad_norm': 2.7939323363657813, 'learning_rate': 3.891222222222222e-06, 'epoch': 2.6024000000000003}
{'loss': 0.8937, 'grad_norm': 2.9248612285364635, 'learning_rate': 3.890111111111111e-06, 'epoch': 2.6028000000000002}
{'loss': 0.8725, 'grad_norm': 2.9048659197515483, 'learning_rate': 3.889e-06, 'epoch': 2.6032}
{'loss': 0.8811, 'grad_norm': 2.9442958848041463, 'learning_rate': 3.888e-06, 'epoch': 2.6036}
{'loss': 0.8872, 'grad_norm': 3.010995444040643, 'learning_rate': 3.886888888888889e-06, 'epoch': 2.604}
{'eval_valid_loss': 0.85546875, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1109.504, 'eval_valid_steps_per_second': 277.376, 'epoch': 2.604}
{'loss': 0.8843, 'grad_norm': 2.928787205012464, 'learning_rate': 3.885777777777778e-06, 'epoch': 2.6044}
{'loss': 0.8864, 'grad_norm': 2.8357148726752004, 'learning_rate': 3.884666666666667e-06, 'epoch': 2.6048}
{'loss': 0.8895, 'grad_norm': 3.188103291509232, 'learning_rate': 3.8835555555555556e-06, 'epoch': 2.6052}
{'loss': 0.8844, 'grad_norm': 2.882630024538987, 'learning_rate': 3.882444444444445e-06, 'epoch': 2.6056}
{'loss': 0.8699, 'grad_norm': 2.790945982548017, 'learning_rate': 3.881333333333334e-06, 'epoch': 2.606}
{'loss': 0.8845, 'grad_norm': 2.7891982069579098, 'learning_rate': 3.8802222222222225e-06, 'epoch': 2.6064}
{'loss': 0.8813, 'grad_norm': 2.908764202852086, 'learning_rate': 3.879111111111111e-06, 'epoch': 2.6068}
{'loss': 0.89, 'grad_norm': 3.187738129191673, 'learning_rate': 3.878e-06, 'epoch': 2.6071999999999997}
{'loss': 0.8814, 'grad_norm': 2.934031518611492, 'learning_rate': 3.8768888888888895e-06, 'epoch': 2.6076}
{'loss': 0.8798, 'grad_norm': 2.8574260260551165, 'learning_rate': 3.875777777777778e-06, 'epoch': 2.608}
{'eval_valid_loss': 0.85595703125, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1126.051, 'eval_valid_steps_per_second': 281.513, 'epoch': 2.608}
{'loss': 0.8763, 'grad_norm': 3.0915890284624052, 'learning_rate': 3.874666666666667e-06, 'epoch': 2.6084}
{'loss': 0.8765, 'grad_norm': 2.945662996195579, 'learning_rate': 3.873555555555556e-06, 'epoch': 2.6088}
{'loss': 0.8729, 'grad_norm': 2.658806619521045, 'learning_rate': 3.872444444444444e-06, 'epoch': 2.6092}
{'loss': 0.8876, 'grad_norm': 2.8570035470480524, 'learning_rate': 3.871333333333334e-06, 'epoch': 2.6096}
{'loss': 0.8794, 'grad_norm': 3.035662374433282, 'learning_rate': 3.870222222222223e-06, 'epoch': 2.61}
{'loss': 0.8885, 'grad_norm': 3.183921581777476, 'learning_rate': 3.869111111111111e-06, 'epoch': 2.6104000000000003}
{'loss': 0.8977, 'grad_norm': 3.0833382198363752, 'learning_rate': 3.868e-06, 'epoch': 2.6108000000000002}
{'loss': 0.8899, 'grad_norm': 2.765586766555216, 'learning_rate': 3.86688888888889e-06, 'epoch': 2.6112}
{'loss': 0.8874, 'grad_norm': 2.839921291714668, 'learning_rate': 3.865777777777778e-06, 'epoch': 2.6116}
{'loss': 0.8781, 'grad_norm': 2.650839253900371, 'learning_rate': 3.864666666666667e-06, 'epoch': 2.612}
{'eval_valid_loss': 0.8564453125, 'eval_valid_runtime': 0.0908, 'eval_valid_samples_per_second': 1100.853, 'eval_valid_steps_per_second': 275.213, 'epoch': 2.612}
{'loss': 0.8956, 'grad_norm': 3.166428357284102, 'learning_rate': 3.863555555555556e-06, 'epoch': 2.6124}
{'loss': 0.884, 'grad_norm': 2.6323965915700325, 'learning_rate': 3.8624444444444445e-06, 'epoch': 2.6128}
{'loss': 0.8808, 'grad_norm': 3.017731716358176, 'learning_rate': 3.861333333333333e-06, 'epoch': 2.6132}
{'loss': 0.8815, 'grad_norm': 2.9597398253100917, 'learning_rate': 3.860222222222223e-06, 'epoch': 2.6136}
{'loss': 0.8706, 'grad_norm': 2.7639377102195453, 'learning_rate': 3.8591111111111115e-06, 'epoch': 2.614}
{'loss': 0.8652, 'grad_norm': 2.908850900664697, 'learning_rate': 3.858e-06, 'epoch': 2.6144}
{'loss': 0.8908, 'grad_norm': 2.869608467656285, 'learning_rate': 3.85688888888889e-06, 'epoch': 2.6148}
{'loss': 0.8833, 'grad_norm': 2.832648584891097, 'learning_rate': 3.8557777777777784e-06, 'epoch': 2.6151999999999997}
{'loss': 0.8838, 'grad_norm': 2.878507842779284, 'learning_rate': 3.854666666666667e-06, 'epoch': 2.6156}
{'loss': 0.8903, 'grad_norm': 2.721473239806364, 'learning_rate': 3.853555555555556e-06, 'epoch': 2.616}
{'eval_valid_loss': 0.8564453125, 'eval_valid_runtime': 0.0908, 'eval_valid_samples_per_second': 1101.448, 'eval_valid_steps_per_second': 275.362, 'epoch': 2.616}
{'loss': 0.8793, 'grad_norm': 3.284313914913298, 'learning_rate': 3.8524444444444446e-06, 'epoch': 2.6164}
{'loss': 0.8757, 'grad_norm': 2.773143258403297, 'learning_rate': 3.851333333333333e-06, 'epoch': 2.6168}
{'loss': 0.8778, 'grad_norm': 2.8989818092754476, 'learning_rate': 3.850222222222223e-06, 'epoch': 2.6172}
{'loss': 0.8739, 'grad_norm': 2.642539002212794, 'learning_rate': 3.8491111111111115e-06, 'epoch': 2.6176}
{'loss': 0.8836, 'grad_norm': 2.821337600771108, 'learning_rate': 3.848e-06, 'epoch': 2.618}
{'loss': 0.8727, 'grad_norm': 2.856484369516301, 'learning_rate': 3.84688888888889e-06, 'epoch': 2.6184}
{'loss': 0.8954, 'grad_norm': 2.807079018139411, 'learning_rate': 3.8457777777777785e-06, 'epoch': 2.6188000000000002}
{'loss': 0.8875, 'grad_norm': 2.8768156631271005, 'learning_rate': 3.844666666666667e-06, 'epoch': 2.6192}
{'loss': 0.8748, 'grad_norm': 2.685970469966474, 'learning_rate': 3.843555555555556e-06, 'epoch': 2.6196}
{'loss': 0.8712, 'grad_norm': 2.8861401588484297, 'learning_rate': 3.842444444444445e-06, 'epoch': 2.62}
{'eval_valid_loss': 0.85595703125, 'eval_valid_runtime': 0.2392, 'eval_valid_samples_per_second': 417.998, 'eval_valid_steps_per_second': 104.499, 'epoch': 2.62}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.883, 'grad_norm': 2.7211622958403345, 'learning_rate': 3.841333333333333e-06, 'epoch': 2.6204}
{'loss': 0.8812, 'grad_norm': 2.678963805364968, 'learning_rate': 3.840222222222222e-06, 'epoch': 2.6208}
{'loss': 0.8888, 'grad_norm': 2.829346361455628, 'learning_rate': 3.839111111111112e-06, 'epoch': 2.6212}
{'loss': 0.8736, 'grad_norm': 2.5426828543045303, 'learning_rate': 3.838e-06, 'epoch': 2.6216}
{'loss': 0.8761, 'grad_norm': 2.9035559031611538, 'learning_rate': 3.836888888888889e-06, 'epoch': 2.622}
{'loss': 0.895, 'grad_norm': 2.8607911318844224, 'learning_rate': 3.835777777777779e-06, 'epoch': 2.6224}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8781, 'grad_norm': 2.789093706613551, 'learning_rate': 3.834666666666667e-06, 'epoch': 2.6228}
{'loss': 0.8873, 'grad_norm': 3.1150778034529485, 'learning_rate': 3.833555555555556e-06, 'epoch': 2.6231999999999998}
{'loss': 0.885, 'grad_norm': 2.8251025134872334, 'learning_rate': 3.832444444444445e-06, 'epoch': 2.6236}
{'loss': 0.8862, 'grad_norm': 3.0211212788292623, 'learning_rate': 3.8313333333333335e-06, 'epoch': 2.624}
{'eval_valid_loss': 0.85595703125, 'eval_valid_runtime': 0.0969, 'eval_valid_samples_per_second': 1032.312, 'eval_valid_steps_per_second': 258.078, 'epoch': 2.624}
{'loss': 0.8871, 'grad_norm': 2.685909649014945, 'learning_rate': 3.830222222222222e-06, 'epoch': 2.6244}
{'loss': 0.8818, 'grad_norm': 2.8373233782208493, 'learning_rate': 3.829111111111111e-06, 'epoch': 2.6248}
{'loss': 0.8825, 'grad_norm': 2.9683064731763786, 'learning_rate': 3.8280000000000004e-06, 'epoch': 2.6252}
{'loss': 0.8785, 'grad_norm': 2.8752450527576143, 'learning_rate': 3.826888888888889e-06, 'epoch': 2.6256}
{'loss': 0.8761, 'grad_norm': 2.961050439054572, 'learning_rate': 3.825777777777778e-06, 'epoch': 2.626}
{'loss': 0.8941, 'grad_norm': 2.8616596016901243, 'learning_rate': 3.8246666666666674e-06, 'epoch': 2.6264}
{'loss': 0.8852, 'grad_norm': 3.0968322812549367, 'learning_rate': 3.823555555555556e-06, 'epoch': 2.6268000000000002}
{'loss': 0.8853, 'grad_norm': 2.7177974468216544, 'learning_rate': 3.822444444444445e-06, 'epoch': 2.6272}
{'loss': 0.8885, 'grad_norm': 2.877998182959245, 'learning_rate': 3.8213333333333336e-06, 'epoch': 2.6276}
{'loss': 0.8868, 'grad_norm': 2.8957979774146345, 'learning_rate': 3.820222222222222e-06, 'epoch': 2.628}
{'eval_valid_loss': 0.85546875, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1119.141, 'eval_valid_steps_per_second': 279.785, 'epoch': 2.628}
{'loss': 0.8897, 'grad_norm': 3.028945253134354, 'learning_rate': 3.819111111111111e-06, 'epoch': 2.6284}
{'loss': 0.8906, 'grad_norm': 3.1539610881824047, 'learning_rate': 3.818e-06, 'epoch': 2.6288}
{'loss': 0.8941, 'grad_norm': 2.685278845156304, 'learning_rate': 3.816888888888889e-06, 'epoch': 2.6292}
{'loss': 0.889, 'grad_norm': 2.888173357573788, 'learning_rate': 3.815777777777778e-06, 'epoch': 2.6296}
{'loss': 0.8776, 'grad_norm': 2.849866859354469, 'learning_rate': 3.814666666666667e-06, 'epoch': 2.63}
{'loss': 0.8897, 'grad_norm': 3.2377691933937296, 'learning_rate': 3.813555555555556e-06, 'epoch': 2.6304}
{'loss': 0.8827, 'grad_norm': 2.9169516980452026, 'learning_rate': 3.8124444444444445e-06, 'epoch': 2.6308}
{'loss': 0.8891, 'grad_norm': 2.891653682054798, 'learning_rate': 3.8113333333333337e-06, 'epoch': 2.6311999999999998}
{'loss': 0.8907, 'grad_norm': 3.018576882020678, 'learning_rate': 3.8102222222222224e-06, 'epoch': 2.6316}
{'loss': 0.8847, 'grad_norm': 2.866823334395727, 'learning_rate': 3.809111111111111e-06, 'epoch': 2.632}
{'eval_valid_loss': 0.85400390625, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.23, 'eval_valid_steps_per_second': 278.307, 'epoch': 2.632}
{'loss': 0.8848, 'grad_norm': 2.953199365476762, 'learning_rate': 3.8080000000000006e-06, 'epoch': 2.6324}
{'loss': 0.8731, 'grad_norm': 2.8967346062738857, 'learning_rate': 3.8068888888888894e-06, 'epoch': 2.6328}
{'loss': 0.8773, 'grad_norm': 3.054020011151173, 'learning_rate': 3.805777777777778e-06, 'epoch': 2.6332}
{'loss': 0.8742, 'grad_norm': 2.734629466613994, 'learning_rate': 3.804666666666667e-06, 'epoch': 2.6336}
{'loss': 0.8819, 'grad_norm': 2.7586904970639052, 'learning_rate': 3.803555555555556e-06, 'epoch': 2.634}
{'loss': 0.8839, 'grad_norm': 2.6047609217043415, 'learning_rate': 3.8024444444444446e-06, 'epoch': 2.6344}
{'loss': 0.8872, 'grad_norm': 3.058377868516914, 'learning_rate': 3.8013333333333333e-06, 'epoch': 2.6348000000000003}
{'loss': 0.8905, 'grad_norm': 3.145734028585671, 'learning_rate': 3.8002222222222225e-06, 'epoch': 2.6352}
{'loss': 0.8963, 'grad_norm': 2.8452885940444075, 'learning_rate': 3.799111111111111e-06, 'epoch': 2.6356}
{'loss': 0.8904, 'grad_norm': 3.243112785947115, 'learning_rate': 3.7980000000000007e-06, 'epoch': 2.636}
{'eval_valid_loss': 0.85546875, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1126.248, 'eval_valid_steps_per_second': 281.562, 'epoch': 2.636}
{'loss': 0.8844, 'grad_norm': 2.832144846775481, 'learning_rate': 3.7968888888888894e-06, 'epoch': 2.6364}
{'loss': 0.8927, 'grad_norm': 2.7082220378651667, 'learning_rate': 3.795777777777778e-06, 'epoch': 2.6368}
{'loss': 0.8749, 'grad_norm': 2.9713163526682616, 'learning_rate': 3.794666666666667e-06, 'epoch': 2.6372}
{'loss': 0.8762, 'grad_norm': 2.770212883188289, 'learning_rate': 3.793555555555556e-06, 'epoch': 2.6376}
{'loss': 0.8788, 'grad_norm': 2.938872128475698, 'learning_rate': 3.7924444444444447e-06, 'epoch': 2.638}
{'loss': 0.8875, 'grad_norm': 2.914934288710677, 'learning_rate': 3.7913333333333334e-06, 'epoch': 2.6384}
{'loss': 0.8819, 'grad_norm': 2.8265995105452286, 'learning_rate': 3.7902222222222226e-06, 'epoch': 2.6388}
{'loss': 0.8808, 'grad_norm': 3.121879545557877, 'learning_rate': 3.7891111111111113e-06, 'epoch': 2.6391999999999998}
{'loss': 0.8849, 'grad_norm': 3.03866509827764, 'learning_rate': 3.7880000000000004e-06, 'epoch': 2.6395999999999997}
{'loss': 0.8786, 'grad_norm': 2.977022030419231, 'learning_rate': 3.7868888888888895e-06, 'epoch': 2.64}
{'eval_valid_loss': 0.85595703125, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1116.977, 'eval_valid_steps_per_second': 279.244, 'epoch': 2.64}
{'loss': 0.8809, 'grad_norm': 2.780250471507313, 'learning_rate': 3.7857777777777783e-06, 'epoch': 2.6404}
{'loss': 0.8836, 'grad_norm': 2.5739533341093055, 'learning_rate': 3.784666666666667e-06, 'epoch': 2.6408}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8903, 'grad_norm': 2.82804264559891, 'learning_rate': 3.7835555555555557e-06, 'epoch': 2.6412}
{'loss': 0.8852, 'grad_norm': 2.8557340235196036, 'learning_rate': 3.782444444444445e-06, 'epoch': 2.6416}
{'loss': 0.8897, 'grad_norm': 3.000705943892009, 'learning_rate': 3.7813333333333335e-06, 'epoch': 2.642}
{'loss': 0.8727, 'grad_norm': 2.7813945796977375, 'learning_rate': 3.7802222222222222e-06, 'epoch': 2.6424}
{'loss': 0.8823, 'grad_norm': 3.037570155475621, 'learning_rate': 3.7791111111111114e-06, 'epoch': 2.6428000000000003}
{'loss': 0.88, 'grad_norm': 2.8886889959502424, 'learning_rate': 3.7780000000000005e-06, 'epoch': 2.6432}
{'loss': 0.8796, 'grad_norm': 2.7559830214070176, 'learning_rate': 3.777e-06, 'epoch': 2.6436}
{'loss': 0.8746, 'grad_norm': 2.8477095296765453, 'learning_rate': 3.775888888888889e-06, 'epoch': 2.644}
{'eval_valid_loss': 0.85595703125, 'eval_valid_runtime': 0.089, 'eval_valid_samples_per_second': 1124.207, 'eval_valid_steps_per_second': 281.052, 'epoch': 2.644}
{'loss': 0.8802, 'grad_norm': 2.985930421941603, 'learning_rate': 3.7747777777777783e-06, 'epoch': 2.6444}
{'loss': 0.8796, 'grad_norm': 3.0207445241216586, 'learning_rate': 3.773666666666667e-06, 'epoch': 2.6448}
{'loss': 0.8971, 'grad_norm': 2.800979468914237, 'learning_rate': 3.772555555555556e-06, 'epoch': 2.6452}
{'loss': 0.893, 'grad_norm': 3.326469224416757, 'learning_rate': 3.771444444444445e-06, 'epoch': 2.6456}
{'loss': 0.872, 'grad_norm': 2.596043733016831, 'learning_rate': 3.7703333333333335e-06, 'epoch': 2.646}
{'loss': 0.8686, 'grad_norm': 3.1762488983915027, 'learning_rate': 3.7692222222222223e-06, 'epoch': 2.6464}
{'loss': 0.8844, 'grad_norm': 3.1241104767322563, 'learning_rate': 3.7681111111111114e-06, 'epoch': 2.6468}
{'loss': 0.8762, 'grad_norm': 2.8810831067122296, 'learning_rate': 3.767e-06, 'epoch': 2.6471999999999998}
{'loss': 0.8931, 'grad_norm': 2.9962248493741592, 'learning_rate': 3.765888888888889e-06, 'epoch': 2.6475999999999997}
{'loss': 0.8677, 'grad_norm': 3.0380202911357284, 'learning_rate': 3.7647777777777784e-06, 'epoch': 2.648}
{'eval_valid_loss': 0.85498046875, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1124.632, 'eval_valid_steps_per_second': 281.158, 'epoch': 2.648}
{'loss': 0.88, 'grad_norm': 2.8732658629524286, 'learning_rate': 3.763666666666667e-06, 'epoch': 2.6484}
{'loss': 0.8816, 'grad_norm': 3.0346625575832937, 'learning_rate': 3.762555555555556e-06, 'epoch': 2.6488}
{'loss': 0.8919, 'grad_norm': 3.368448335591403, 'learning_rate': 3.761444444444445e-06, 'epoch': 2.6492}
{'loss': 0.8763, 'grad_norm': 2.8361328467563545, 'learning_rate': 3.7603333333333336e-06, 'epoch': 2.6496}
{'loss': 0.8918, 'grad_norm': 2.820083777962054, 'learning_rate': 3.7592222222222224e-06, 'epoch': 2.65}
{'loss': 0.8763, 'grad_norm': 2.940657572307446, 'learning_rate': 3.758111111111111e-06, 'epoch': 2.6504}
{'loss': 0.8884, 'grad_norm': 2.98622804802115, 'learning_rate': 3.757e-06, 'epoch': 2.6508000000000003}
{'loss': 0.8763, 'grad_norm': 3.0292509900669393, 'learning_rate': 3.7558888888888893e-06, 'epoch': 2.6512000000000002}
{'loss': 0.8798, 'grad_norm': 2.661935701253599, 'learning_rate': 3.7547777777777785e-06, 'epoch': 2.6516}
{'loss': 0.8857, 'grad_norm': 3.2915958787251873, 'learning_rate': 3.753666666666667e-06, 'epoch': 2.652}
{'eval_valid_loss': 0.85546875, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.219, 'eval_valid_steps_per_second': 281.805, 'epoch': 2.652}
{'loss': 0.8793, 'grad_norm': 2.795686346677711, 'learning_rate': 3.752555555555556e-06, 'epoch': 2.6524}
{'loss': 0.8833, 'grad_norm': 2.7419849114423727, 'learning_rate': 3.7514444444444446e-06, 'epoch': 2.6528}
{'loss': 0.8714, 'grad_norm': 2.850414912656317, 'learning_rate': 3.7503333333333337e-06, 'epoch': 2.6532}
{'loss': 0.8898, 'grad_norm': 3.074247318896077, 'learning_rate': 3.7492222222222224e-06, 'epoch': 2.6536}
{'loss': 0.868, 'grad_norm': 3.0921593825270017, 'learning_rate': 3.748111111111111e-06, 'epoch': 2.654}
{'loss': 0.8816, 'grad_norm': 2.678429338796375, 'learning_rate': 3.7470000000000003e-06, 'epoch': 2.6544}
{'loss': 0.8893, 'grad_norm': 2.9615749286979707, 'learning_rate': 3.7458888888888894e-06, 'epoch': 2.6548}
{'loss': 0.8834, 'grad_norm': 2.9086184746252113, 'learning_rate': 3.744777777777778e-06, 'epoch': 2.6552}
{'loss': 0.8732, 'grad_norm': 3.2505540192168194, 'learning_rate': 3.7436666666666673e-06, 'epoch': 2.6555999999999997}
{'loss': 0.8835, 'grad_norm': 2.5818266506188206, 'learning_rate': 3.742555555555556e-06, 'epoch': 2.656}
{'eval_valid_loss': 0.8564453125, 'eval_valid_runtime': 0.0884, 'eval_valid_samples_per_second': 1131.223, 'eval_valid_steps_per_second': 282.806, 'epoch': 2.656}
{'loss': 0.8797, 'grad_norm': 2.915311796123049, 'learning_rate': 3.7414444444444447e-06, 'epoch': 2.6564}
{'loss': 0.8865, 'grad_norm': 3.3500748914204537, 'learning_rate': 3.7403333333333334e-06, 'epoch': 2.6568}
{'loss': 0.8741, 'grad_norm': 2.5948846414959243, 'learning_rate': 3.7392222222222225e-06, 'epoch': 2.6572}
{'loss': 0.8893, 'grad_norm': 2.8437231555132803, 'learning_rate': 3.7381111111111113e-06, 'epoch': 2.6576}
{'loss': 0.8841, 'grad_norm': 2.9649868253379803, 'learning_rate': 3.737e-06, 'epoch': 2.658}
{'loss': 0.8825, 'grad_norm': 2.901666318302546, 'learning_rate': 3.7358888888888895e-06, 'epoch': 2.6584}
{'loss': 0.8723, 'grad_norm': 2.8071681663699275, 'learning_rate': 3.7347777777777782e-06, 'epoch': 2.6588000000000003}
{'loss': 0.8824, 'grad_norm': 2.742609007954498, 'learning_rate': 3.733666666666667e-06, 'epoch': 2.6592000000000002}
{'loss': 0.8695, 'grad_norm': 3.189032158340486, 'learning_rate': 3.732555555555556e-06, 'epoch': 2.6596}
{'loss': 0.8836, 'grad_norm': 2.8159448084272314, 'learning_rate': 3.731444444444445e-06, 'epoch': 2.66}
{'eval_valid_loss': 0.85498046875, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.834, 'eval_valid_steps_per_second': 279.459, 'epoch': 2.66}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.8881, 'grad_norm': 2.9842801902850042, 'learning_rate': 3.7303333333333335e-06, 'epoch': 2.6604}
{'loss': 0.8833, 'grad_norm': 2.742428684767892, 'learning_rate': 3.7292222222222222e-06, 'epoch': 2.6608}
{'loss': 0.8793, 'grad_norm': 2.754596396468964, 'learning_rate': 3.7281111111111113e-06, 'epoch': 2.6612}
{'loss': 0.8813, 'grad_norm': 2.795821923992074, 'learning_rate': 3.727e-06, 'epoch': 2.6616}
{'loss': 0.8639, 'grad_norm': 2.87949082276799, 'learning_rate': 3.7258888888888896e-06, 'epoch': 2.662}
{'loss': 0.8903, 'grad_norm': 3.2195609932080758, 'learning_rate': 3.7247777777777783e-06, 'epoch': 2.6624}
{'loss': 0.8766, 'grad_norm': 2.9198761799276447, 'learning_rate': 3.723666666666667e-06, 'epoch': 2.6628}
{'loss': 0.8751, 'grad_norm': 2.798769250797581, 'learning_rate': 3.7225555555555558e-06, 'epoch': 2.6632}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8749, 'grad_norm': 2.91467570176001, 'learning_rate': 3.721444444444445e-06, 'epoch': 2.6635999999999997}
{'loss': 0.8866, 'grad_norm': 2.6658532397103434, 'learning_rate': 3.7203333333333336e-06, 'epoch': 2.664}
{'eval_valid_loss': 0.8544921875, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1111.348, 'eval_valid_steps_per_second': 277.837, 'epoch': 2.664}
{'loss': 0.8987, 'grad_norm': 2.874510707890686, 'learning_rate': 3.7192222222222223e-06, 'epoch': 2.6644}
{'loss': 0.8869, 'grad_norm': 2.9491561781959112, 'learning_rate': 3.718111111111111e-06, 'epoch': 2.6648}
{'loss': 0.8769, 'grad_norm': 2.97548041111186, 'learning_rate': 3.717e-06, 'epoch': 2.6652}
{'loss': 0.8853, 'grad_norm': 2.7491504646616693, 'learning_rate': 3.7158888888888893e-06, 'epoch': 2.6656}
{'loss': 0.8667, 'grad_norm': 3.1784143546835733, 'learning_rate': 3.7147777777777784e-06, 'epoch': 2.666}
{'loss': 0.8835, 'grad_norm': 3.0613589593975568, 'learning_rate': 3.713666666666667e-06, 'epoch': 2.6664}
{'loss': 0.8932, 'grad_norm': 2.9255657345857142, 'learning_rate': 3.712555555555556e-06, 'epoch': 2.6668}
{'loss': 0.8745, 'grad_norm': 3.01396606947602, 'learning_rate': 3.7114444444444446e-06, 'epoch': 2.6672000000000002}
{'loss': 0.8835, 'grad_norm': 2.9487136509341982, 'learning_rate': 3.7103333333333337e-06, 'epoch': 2.6676}
{'loss': 0.8792, 'grad_norm': 2.9669594635400904, 'learning_rate': 3.7092222222222224e-06, 'epoch': 2.668}
{'eval_valid_loss': 0.85546875, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.669, 'eval_valid_steps_per_second': 277.167, 'epoch': 2.668}
{'loss': 0.8809, 'grad_norm': 2.9301922579497384, 'learning_rate': 3.708111111111111e-06, 'epoch': 2.6684}
{'loss': 0.8888, 'grad_norm': 2.8890077035926, 'learning_rate': 3.7070000000000003e-06, 'epoch': 2.6688}
{'loss': 0.8797, 'grad_norm': 2.924436690626966, 'learning_rate': 3.7058888888888894e-06, 'epoch': 2.6692}
{'loss': 0.8723, 'grad_norm': 2.8293631567347965, 'learning_rate': 3.704777777777778e-06, 'epoch': 2.6696}
{'loss': 0.882, 'grad_norm': 3.2401434924145898, 'learning_rate': 3.7036666666666672e-06, 'epoch': 2.67}
{'loss': 0.8738, 'grad_norm': 2.7126435949402703, 'learning_rate': 3.702555555555556e-06, 'epoch': 2.6704}
{'loss': 0.8793, 'grad_norm': 2.665956428066245, 'learning_rate': 3.7014444444444447e-06, 'epoch': 2.6708}
{'loss': 0.8909, 'grad_norm': 3.027725796126928, 'learning_rate': 3.7003333333333334e-06, 'epoch': 2.6712}
{'loss': 0.8749, 'grad_norm': 3.380640455857002, 'learning_rate': 3.6992222222222225e-06, 'epoch': 2.6715999999999998}
{'loss': 0.8678, 'grad_norm': 2.91799925720527, 'learning_rate': 3.6981111111111112e-06, 'epoch': 2.672}
{'eval_valid_loss': 0.8544921875, 'eval_valid_runtime': 0.0911, 'eval_valid_samples_per_second': 1097.195, 'eval_valid_steps_per_second': 274.299, 'epoch': 2.672}
{'loss': 0.8838, 'grad_norm': 2.8572656115490016, 'learning_rate': 3.697e-06, 'epoch': 2.6724}
{'loss': 0.8861, 'grad_norm': 2.897340191522798, 'learning_rate': 3.6958888888888895e-06, 'epoch': 2.6728}
{'loss': 0.8706, 'grad_norm': 2.707627047853966, 'learning_rate': 3.694777777777778e-06, 'epoch': 2.6732}
{'loss': 0.8684, 'grad_norm': 2.98601174534227, 'learning_rate': 3.693666666666667e-06, 'epoch': 2.6736}
{'loss': 0.883, 'grad_norm': 2.9879631521424583, 'learning_rate': 3.692555555555556e-06, 'epoch': 2.674}
{'loss': 0.8841, 'grad_norm': 3.1699517907348667, 'learning_rate': 3.6914444444444448e-06, 'epoch': 2.6744}
{'loss': 0.8796, 'grad_norm': 2.8919896075093643, 'learning_rate': 3.6903333333333335e-06, 'epoch': 2.6748}
{'loss': 0.8896, 'grad_norm': 2.979729426968657, 'learning_rate': 3.689222222222222e-06, 'epoch': 2.6752000000000002}
{'loss': 0.8753, 'grad_norm': 2.849871183504181, 'learning_rate': 3.6881111111111113e-06, 'epoch': 2.6756}
{'loss': 0.8706, 'grad_norm': 2.904562388368164, 'learning_rate': 3.6870000000000004e-06, 'epoch': 2.676}
{'eval_valid_loss': 0.8564453125, 'eval_valid_runtime': 0.097, 'eval_valid_samples_per_second': 1031.0, 'eval_valid_steps_per_second': 257.75, 'epoch': 2.676}
{'loss': 0.8869, 'grad_norm': 2.783663500806715, 'learning_rate': 3.6858888888888896e-06, 'epoch': 2.6764}
{'loss': 0.8861, 'grad_norm': 2.9731692599582087, 'learning_rate': 3.6847777777777783e-06, 'epoch': 2.6768}
{'loss': 0.8879, 'grad_norm': 3.201935551934758, 'learning_rate': 3.683666666666667e-06, 'epoch': 2.6772}
{'loss': 0.8785, 'grad_norm': 3.0238461674145936, 'learning_rate': 3.6825555555555557e-06, 'epoch': 2.6776}
{'loss': 0.8846, 'grad_norm': 2.94902050034081, 'learning_rate': 3.681444444444445e-06, 'epoch': 2.678}
{'loss': 0.8863, 'grad_norm': 3.1945585559882343, 'learning_rate': 3.6803333333333336e-06, 'epoch': 2.6784}
{'loss': 0.8822, 'grad_norm': 2.6977784748598457, 'learning_rate': 3.6792222222222223e-06, 'epoch': 2.6788}
{'loss': 0.889, 'grad_norm': 2.9106423605642173, 'learning_rate': 3.678111111111111e-06, 'epoch': 2.6792}
{'loss': 0.8694, 'grad_norm': 2.8842297768533167, 'learning_rate': 3.6770000000000005e-06, 'epoch': 2.6795999999999998}
{'loss': 0.8829, 'grad_norm': 2.6886980857164553, 'learning_rate': 3.6758888888888893e-06, 'epoch': 2.68}
{'eval_valid_loss': 0.85546875, 'eval_valid_runtime': 0.0908, 'eval_valid_samples_per_second': 1100.934, 'eval_valid_steps_per_second': 275.233, 'epoch': 2.68}
{'loss': 0.8771, 'grad_norm': 3.3026631876054218, 'learning_rate': 3.6747777777777784e-06, 'epoch': 2.6804}
{'loss': 0.8814, 'grad_norm': 2.8123716642981464, 'learning_rate': 3.673666666666667e-06, 'epoch': 2.6808}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8764, 'grad_norm': 2.9745487399818664, 'learning_rate': 3.672555555555556e-06, 'epoch': 2.6812}
{'loss': 0.8796, 'grad_norm': 3.009590584469248, 'learning_rate': 3.6714444444444445e-06, 'epoch': 2.6816}
{'loss': 0.8833, 'grad_norm': 2.777685876690352, 'learning_rate': 3.6703333333333337e-06, 'epoch': 2.682}
{'loss': 0.8788, 'grad_norm': 2.7803702731163287, 'learning_rate': 3.6692222222222224e-06, 'epoch': 2.6824}
{'loss': 0.8828, 'grad_norm': 2.703162799416567, 'learning_rate': 3.668111111111111e-06, 'epoch': 2.6828}
{'loss': 0.8856, 'grad_norm': 3.0253988405021306, 'learning_rate': 3.6670000000000006e-06, 'epoch': 2.6832000000000003}
{'loss': 0.8854, 'grad_norm': 2.777639215046641, 'learning_rate': 3.666e-06, 'epoch': 2.6836}
{'loss': 0.8835, 'grad_norm': 2.8298704335709886, 'learning_rate': 3.664888888888889e-06, 'epoch': 2.684}
{'eval_valid_loss': 0.85400390625, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.068, 'eval_valid_steps_per_second': 281.767, 'epoch': 2.684}
{'loss': 0.888, 'grad_norm': 3.012915535518065, 'learning_rate': 3.6637777777777784e-06, 'epoch': 2.6844}
{'loss': 0.8721, 'grad_norm': 2.831640587531874, 'learning_rate': 3.662666666666667e-06, 'epoch': 2.6848}
{'loss': 0.8891, 'grad_norm': 3.00004307398073, 'learning_rate': 3.661555555555556e-06, 'epoch': 2.6852}
{'loss': 0.8769, 'grad_norm': 3.011146114943764, 'learning_rate': 3.660444444444445e-06, 'epoch': 2.6856}
{'loss': 0.881, 'grad_norm': 2.6868722648278367, 'learning_rate': 3.6593333333333337e-06, 'epoch': 2.686}
{'loss': 0.8831, 'grad_norm': 2.7661931914379987, 'learning_rate': 3.6582222222222224e-06, 'epoch': 2.6864}
{'loss': 0.8764, 'grad_norm': 2.665852395674497, 'learning_rate': 3.657111111111111e-06, 'epoch': 2.6868}
{'loss': 0.883, 'grad_norm': 2.979785575912927, 'learning_rate': 3.6560000000000002e-06, 'epoch': 2.6872}
{'loss': 0.8718, 'grad_norm': 2.984200547397145, 'learning_rate': 3.654888888888889e-06, 'epoch': 2.6875999999999998}
{'loss': 0.8742, 'grad_norm': 2.9615371117102898, 'learning_rate': 3.6537777777777785e-06, 'epoch': 2.6879999999999997}
{'eval_valid_loss': 0.85595703125, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1122.94, 'eval_valid_steps_per_second': 280.735, 'epoch': 2.6879999999999997}
{'loss': 0.873, 'grad_norm': 3.139442401491281, 'learning_rate': 3.6526666666666672e-06, 'epoch': 2.6884}
{'loss': 0.8895, 'grad_norm': 2.746071529162867, 'learning_rate': 3.651555555555556e-06, 'epoch': 2.6888}
{'loss': 0.8747, 'grad_norm': 2.8781348531680693, 'learning_rate': 3.6504444444444446e-06, 'epoch': 2.6892}
{'loss': 0.8786, 'grad_norm': 2.8827661339977673, 'learning_rate': 3.6493333333333338e-06, 'epoch': 2.6896}
{'loss': 0.8857, 'grad_norm': 2.7599020382264956, 'learning_rate': 3.6482222222222225e-06, 'epoch': 2.69}
{'loss': 0.8887, 'grad_norm': 3.3159046849712293, 'learning_rate': 3.647111111111111e-06, 'epoch': 2.6904}
{'loss': 0.8875, 'grad_norm': 2.755902431267944, 'learning_rate': 3.646e-06, 'epoch': 2.6908}
{'loss': 0.8733, 'grad_norm': 2.8579305220855433, 'learning_rate': 3.644888888888889e-06, 'epoch': 2.6912000000000003}
{'loss': 0.8918, 'grad_norm': 2.9291139168328386, 'learning_rate': 3.643777777777778e-06, 'epoch': 2.6916}
{'loss': 0.8837, 'grad_norm': 2.894491125902077, 'learning_rate': 3.6426666666666673e-06, 'epoch': 2.692}
{'eval_valid_loss': 0.85546875, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1125.063, 'eval_valid_steps_per_second': 281.266, 'epoch': 2.692}
{'loss': 0.8729, 'grad_norm': 2.895467784276835, 'learning_rate': 3.641555555555556e-06, 'epoch': 2.6924}
{'loss': 0.888, 'grad_norm': 2.96940929721431, 'learning_rate': 3.6404444444444447e-06, 'epoch': 2.6928}
{'loss': 0.8795, 'grad_norm': 2.978680645483067, 'learning_rate': 3.6393333333333334e-06, 'epoch': 2.6932}
{'loss': 0.8899, 'grad_norm': 2.7744514868312535, 'learning_rate': 3.6382222222222226e-06, 'epoch': 2.6936}
{'loss': 0.8873, 'grad_norm': 3.0814163890428286, 'learning_rate': 3.6371111111111113e-06, 'epoch': 2.694}
{'loss': 0.8816, 'grad_norm': 2.9316613833857117, 'learning_rate': 3.636e-06, 'epoch': 2.6944}
{'loss': 0.8806, 'grad_norm': 3.029412951726534, 'learning_rate': 3.6348888888888887e-06, 'epoch': 2.6948}
{'loss': 0.8839, 'grad_norm': 2.959410361669846, 'learning_rate': 3.6337777777777783e-06, 'epoch': 2.6952}
{'loss': 0.8801, 'grad_norm': 2.8730540946491123, 'learning_rate': 3.632666666666667e-06, 'epoch': 2.6955999999999998}
{'loss': 0.8789, 'grad_norm': 2.8004863163416753, 'learning_rate': 3.631555555555556e-06, 'epoch': 2.6959999999999997}
{'eval_valid_loss': 0.85400390625, 'eval_valid_runtime': 0.089, 'eval_valid_samples_per_second': 1123.984, 'eval_valid_steps_per_second': 280.996, 'epoch': 2.6959999999999997}
{'loss': 0.872, 'grad_norm': 2.5379017920510414, 'learning_rate': 3.630444444444445e-06, 'epoch': 2.6964}
{'loss': 0.8864, 'grad_norm': 2.829489657954601, 'learning_rate': 3.6293333333333335e-06, 'epoch': 2.6968}
{'loss': 0.8678, 'grad_norm': 2.8138300082144845, 'learning_rate': 3.6282222222222223e-06, 'epoch': 2.6972}
{'loss': 0.8789, 'grad_norm': 2.79831192995792, 'learning_rate': 3.6271111111111114e-06, 'epoch': 2.6976}
{'loss': 0.8875, 'grad_norm': 2.756505888999905, 'learning_rate': 3.626e-06, 'epoch': 2.698}
{'loss': 0.8907, 'grad_norm': 2.9620049396965675, 'learning_rate': 3.6248888888888897e-06, 'epoch': 2.6984}
{'loss': 0.8636, 'grad_norm': 2.8934151766795826, 'learning_rate': 3.6237777777777784e-06, 'epoch': 2.6988}
{'loss': 0.8851, 'grad_norm': 2.9056638721369334, 'learning_rate': 3.622666666666667e-06, 'epoch': 2.6992000000000003}
{'loss': 0.8799, 'grad_norm': 3.0303424774477867, 'learning_rate': 3.621555555555556e-06, 'epoch': 2.6996}
{'loss': 0.8795, 'grad_norm': 3.0002193569414106, 'learning_rate': 3.620444444444445e-06, 'epoch': 2.7}
{'eval_valid_loss': 0.85546875, 'eval_valid_runtime': 0.089, 'eval_valid_samples_per_second': 1123.897, 'eval_valid_steps_per_second': 280.974, 'epoch': 2.7}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.8824, 'grad_norm': 2.9114872800767646, 'learning_rate': 3.6193333333333336e-06, 'epoch': 2.7004}
{'loss': 0.8801, 'grad_norm': 2.867100020485128, 'learning_rate': 3.6182222222222223e-06, 'epoch': 2.7008}
{'loss': 0.8826, 'grad_norm': 3.0409727001651996, 'learning_rate': 3.617111111111111e-06, 'epoch': 2.7012}
{'loss': 0.8752, 'grad_norm': 2.8503683751437503, 'learning_rate': 3.616e-06, 'epoch': 2.7016}
{'loss': 0.884, 'grad_norm': 2.8635960467020203, 'learning_rate': 3.6148888888888893e-06, 'epoch': 2.702}
{'loss': 0.8819, 'grad_norm': 2.973492037442869, 'learning_rate': 3.6137777777777785e-06, 'epoch': 2.7024}
{'loss': 0.8889, 'grad_norm': 2.7934831425673847, 'learning_rate': 3.612666666666667e-06, 'epoch': 2.7028}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8844, 'grad_norm': 2.921319541560465, 'learning_rate': 3.611555555555556e-06, 'epoch': 2.7032}
{'loss': 0.883, 'grad_norm': 2.90062751187966, 'learning_rate': 3.6104444444444446e-06, 'epoch': 2.7036}
{'loss': 0.878, 'grad_norm': 2.750796641572439, 'learning_rate': 3.6093333333333337e-06, 'epoch': 2.7039999999999997}
{'eval_valid_loss': 0.8544921875, 'eval_valid_runtime': 0.0913, 'eval_valid_samples_per_second': 1094.73, 'eval_valid_steps_per_second': 273.682, 'epoch': 2.7039999999999997}
{'loss': 0.8961, 'grad_norm': 2.8378181159463054, 'learning_rate': 3.6082222222222224e-06, 'epoch': 2.7044}
{'loss': 0.8839, 'grad_norm': 2.670128586196259, 'learning_rate': 3.607111111111111e-06, 'epoch': 2.7048}
{'loss': 0.8825, 'grad_norm': 3.153470486786039, 'learning_rate': 3.606e-06, 'epoch': 2.7052}
{'loss': 0.8841, 'grad_norm': 3.089424461726644, 'learning_rate': 3.6048888888888894e-06, 'epoch': 2.7056}
{'loss': 0.8818, 'grad_norm': 3.193962548829391, 'learning_rate': 3.603777777777778e-06, 'epoch': 2.706}
{'loss': 0.8721, 'grad_norm': 2.79521922962887, 'learning_rate': 3.6026666666666673e-06, 'epoch': 2.7064}
{'loss': 0.874, 'grad_norm': 3.0515461254706007, 'learning_rate': 3.601555555555556e-06, 'epoch': 2.7068}
{'loss': 0.8852, 'grad_norm': 3.1227175197168857, 'learning_rate': 3.6004444444444447e-06, 'epoch': 2.7072000000000003}
{'loss': 0.8854, 'grad_norm': 3.214865952110336, 'learning_rate': 3.5993333333333334e-06, 'epoch': 2.7076000000000002}
{'loss': 0.8839, 'grad_norm': 2.7634238519345575, 'learning_rate': 3.5982222222222225e-06, 'epoch': 2.708}
{'eval_valid_loss': 0.85498046875, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.415, 'eval_valid_steps_per_second': 279.604, 'epoch': 2.708}
{'loss': 0.8777, 'grad_norm': 2.868883468082893, 'learning_rate': 3.5971111111111112e-06, 'epoch': 2.7084}
{'loss': 0.8668, 'grad_norm': 2.902652636760527, 'learning_rate': 3.596e-06, 'epoch': 2.7088}
{'loss': 0.8818, 'grad_norm': 2.846436792810673, 'learning_rate': 3.5948888888888895e-06, 'epoch': 2.7092}
{'loss': 0.8792, 'grad_norm': 2.774042896228557, 'learning_rate': 3.5937777777777782e-06, 'epoch': 2.7096}
{'loss': 0.8793, 'grad_norm': 2.857713727459052, 'learning_rate': 3.592666666666667e-06, 'epoch': 2.71}
{'loss': 0.8735, 'grad_norm': 2.8835916680902343, 'learning_rate': 3.591555555555556e-06, 'epoch': 2.7104}
{'loss': 0.8682, 'grad_norm': 3.1950763158086297, 'learning_rate': 3.5904444444444448e-06, 'epoch': 2.7108}
{'loss': 0.8801, 'grad_norm': 2.9996689772766736, 'learning_rate': 3.5893333333333335e-06, 'epoch': 2.7112}
{'loss': 0.8736, 'grad_norm': 2.905468989610983, 'learning_rate': 3.588222222222222e-06, 'epoch': 2.7116}
{'loss': 0.8912, 'grad_norm': 3.002882099549889, 'learning_rate': 3.5871111111111113e-06, 'epoch': 2.7119999999999997}
{'eval_valid_loss': 0.85302734375, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1109.898, 'eval_valid_steps_per_second': 277.474, 'epoch': 2.7119999999999997}
{'loss': 0.8786, 'grad_norm': 3.008150334397169, 'learning_rate': 3.586e-06, 'epoch': 2.7124}
{'loss': 0.8819, 'grad_norm': 2.6958445424717006, 'learning_rate': 3.5848888888888896e-06, 'epoch': 2.7128}
{'loss': 0.8836, 'grad_norm': 3.0843112088672857, 'learning_rate': 3.5837777777777783e-06, 'epoch': 2.7132}
{'loss': 0.8881, 'grad_norm': 2.727065042784581, 'learning_rate': 3.582666666666667e-06, 'epoch': 2.7136}
{'loss': 0.8752, 'grad_norm': 2.8431090376651764, 'learning_rate': 3.5815555555555557e-06, 'epoch': 2.714}
{'loss': 0.8782, 'grad_norm': 2.7791477787990706, 'learning_rate': 3.580444444444445e-06, 'epoch': 2.7144}
{'loss': 0.8706, 'grad_norm': 2.7205982886365168, 'learning_rate': 3.5793333333333336e-06, 'epoch': 2.7148}
{'loss': 0.8857, 'grad_norm': 3.0423806225686203, 'learning_rate': 3.5782222222222223e-06, 'epoch': 2.7152}
{'loss': 0.8934, 'grad_norm': 3.0498343059214363, 'learning_rate': 3.577111111111111e-06, 'epoch': 2.7156000000000002}
{'loss': 0.8857, 'grad_norm': 2.9002606089828458, 'learning_rate': 3.576e-06, 'epoch': 2.716}
{'eval_valid_loss': 0.853515625, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1124.644, 'eval_valid_steps_per_second': 281.161, 'epoch': 2.716}
{'loss': 0.8909, 'grad_norm': 2.9782973913070854, 'learning_rate': 3.5748888888888893e-06, 'epoch': 2.7164}
{'loss': 0.8873, 'grad_norm': 2.9515810972468945, 'learning_rate': 3.5737777777777784e-06, 'epoch': 2.7168}
{'loss': 0.8858, 'grad_norm': 2.9388937383017604, 'learning_rate': 3.572666666666667e-06, 'epoch': 2.7172}
{'loss': 0.884, 'grad_norm': 3.154859302560445, 'learning_rate': 3.571555555555556e-06, 'epoch': 2.7176}
{'loss': 0.8814, 'grad_norm': 2.8913460528114685, 'learning_rate': 3.5704444444444446e-06, 'epoch': 2.718}
{'loss': 0.8879, 'grad_norm': 3.1330622147806526, 'learning_rate': 3.5693333333333337e-06, 'epoch': 2.7184}
{'loss': 0.8792, 'grad_norm': 3.0788934687533303, 'learning_rate': 3.5682222222222224e-06, 'epoch': 2.7188}
{'loss': 0.8898, 'grad_norm': 2.951118584889556, 'learning_rate': 3.567111111111111e-06, 'epoch': 2.7192}
{'loss': 0.873, 'grad_norm': 2.757512824659017, 'learning_rate': 3.566e-06, 'epoch': 2.7196}
{'loss': 0.8836, 'grad_norm': 2.816067128921573, 'learning_rate': 3.5648888888888894e-06, 'epoch': 2.7199999999999998}
{'eval_valid_loss': 0.853515625, 'eval_valid_runtime': 0.089, 'eval_valid_samples_per_second': 1123.996, 'eval_valid_steps_per_second': 280.999, 'epoch': 2.7199999999999998}
{'loss': 0.8883, 'grad_norm': 2.688477288581038, 'learning_rate': 3.563777777777778e-06, 'epoch': 2.7204}
{'loss': 0.8739, 'grad_norm': 2.8952277680137932, 'learning_rate': 3.5626666666666672e-06, 'epoch': 2.7208}
{'loss': 0.8757, 'grad_norm': 2.9901024348915044, 'learning_rate': 3.561555555555556e-06, 'epoch': 2.7212}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8622, 'grad_norm': 2.92721361614724, 'learning_rate': 3.5604444444444447e-06, 'epoch': 2.7216}
{'loss': 0.8681, 'grad_norm': 2.8661808044825205, 'learning_rate': 3.5593333333333334e-06, 'epoch': 2.722}
{'loss': 0.8869, 'grad_norm': 2.9525969300602246, 'learning_rate': 3.5582222222222225e-06, 'epoch': 2.7224}
{'loss': 0.8811, 'grad_norm': 2.9056806109106468, 'learning_rate': 3.557111111111111e-06, 'epoch': 2.7228}
{'loss': 0.8901, 'grad_norm': 2.984785660855069, 'learning_rate': 3.5560000000000008e-06, 'epoch': 2.7232}
{'loss': 0.8734, 'grad_norm': 2.8166254516460345, 'learning_rate': 3.5550000000000003e-06, 'epoch': 2.7236000000000002}
{'loss': 0.8826, 'grad_norm': 2.716731336010161, 'learning_rate': 3.553888888888889e-06, 'epoch': 2.724}
{'eval_valid_loss': 0.85302734375, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.093, 'eval_valid_steps_per_second': 279.273, 'epoch': 2.724}
{'loss': 0.8729, 'grad_norm': 3.2917854653823104, 'learning_rate': 3.552777777777778e-06, 'epoch': 2.7244}
{'loss': 0.8928, 'grad_norm': 2.971058439917113, 'learning_rate': 3.5516666666666672e-06, 'epoch': 2.7248}
{'loss': 0.8844, 'grad_norm': 2.8138151378878926, 'learning_rate': 3.550555555555556e-06, 'epoch': 2.7252}
{'loss': 0.8865, 'grad_norm': 3.09557801002838, 'learning_rate': 3.5494444444444447e-06, 'epoch': 2.7256}
{'loss': 0.8763, 'grad_norm': 2.9186578368476273, 'learning_rate': 3.548333333333334e-06, 'epoch': 2.726}
{'loss': 0.8699, 'grad_norm': 2.608898250633597, 'learning_rate': 3.5472222222222225e-06, 'epoch': 2.7264}
{'loss': 0.881, 'grad_norm': 2.9294868909181484, 'learning_rate': 3.5461111111111112e-06, 'epoch': 2.7268}
{'loss': 0.8828, 'grad_norm': 3.117422625630002, 'learning_rate': 3.545e-06, 'epoch': 2.7272}
{'loss': 0.887, 'grad_norm': 3.0306502073265897, 'learning_rate': 3.543888888888889e-06, 'epoch': 2.7276}
{'loss': 0.8695, 'grad_norm': 2.943098323714897, 'learning_rate': 3.542777777777778e-06, 'epoch': 2.7279999999999998}
{'eval_valid_loss': 0.853515625, 'eval_valid_runtime': 0.0885, 'eval_valid_samples_per_second': 1129.834, 'eval_valid_steps_per_second': 282.458, 'epoch': 2.7279999999999998}
{'loss': 0.8871, 'grad_norm': 2.7399003579127816, 'learning_rate': 3.5416666666666673e-06, 'epoch': 2.7284}
{'loss': 0.8852, 'grad_norm': 2.935468052093265, 'learning_rate': 3.540555555555556e-06, 'epoch': 2.7288}
{'loss': 0.8687, 'grad_norm': 2.876124483459206, 'learning_rate': 3.5394444444444448e-06, 'epoch': 2.7292}
{'loss': 0.8868, 'grad_norm': 3.045301451645877, 'learning_rate': 3.5383333333333335e-06, 'epoch': 2.7296}
{'loss': 0.8759, 'grad_norm': 2.7100261453567334, 'learning_rate': 3.5372222222222226e-06, 'epoch': 2.73}
{'loss': 0.8779, 'grad_norm': 2.9830849640479626, 'learning_rate': 3.5361111111111113e-06, 'epoch': 2.7304}
{'loss': 0.875, 'grad_norm': 2.8813040180066145, 'learning_rate': 3.535e-06, 'epoch': 2.7308}
{'loss': 0.8707, 'grad_norm': 2.6057129134996018, 'learning_rate': 3.5338888888888887e-06, 'epoch': 2.7312}
{'loss': 0.8698, 'grad_norm': 3.0848154176768223, 'learning_rate': 3.5327777777777783e-06, 'epoch': 2.7316000000000003}
{'loss': 0.8697, 'grad_norm': 2.871326006363707, 'learning_rate': 3.531666666666667e-06, 'epoch': 2.732}
{'eval_valid_loss': 0.853515625, 'eval_valid_runtime': 0.0882, 'eval_valid_samples_per_second': 1133.645, 'eval_valid_steps_per_second': 283.411, 'epoch': 2.732}
{'loss': 0.8807, 'grad_norm': 2.9264105717153885, 'learning_rate': 3.530555555555556e-06, 'epoch': 2.7324}
{'loss': 0.8814, 'grad_norm': 2.874267588493667, 'learning_rate': 3.529444444444445e-06, 'epoch': 2.7328}
{'loss': 0.8755, 'grad_norm': 2.8353437952958878, 'learning_rate': 3.5283333333333336e-06, 'epoch': 2.7332}
{'loss': 0.8698, 'grad_norm': 2.7935779040623454, 'learning_rate': 3.5272222222222223e-06, 'epoch': 2.7336}
{'loss': 0.8705, 'grad_norm': 2.970459716776719, 'learning_rate': 3.5261111111111114e-06, 'epoch': 2.734}
{'loss': 0.8692, 'grad_norm': 3.198950042432308, 'learning_rate': 3.525e-06, 'epoch': 2.7344}
{'loss': 0.8843, 'grad_norm': 2.8557793256570845, 'learning_rate': 3.523888888888889e-06, 'epoch': 2.7348}
{'loss': 0.8809, 'grad_norm': 2.9879612171612955, 'learning_rate': 3.5227777777777784e-06, 'epoch': 2.7352}
{'loss': 0.8889, 'grad_norm': 2.9372260696438928, 'learning_rate': 3.521666666666667e-06, 'epoch': 2.7356}
{'loss': 0.8695, 'grad_norm': 2.7010233818275817, 'learning_rate': 3.520555555555556e-06, 'epoch': 2.7359999999999998}
{'eval_valid_loss': 0.85302734375, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1120.247, 'eval_valid_steps_per_second': 280.062, 'epoch': 2.7359999999999998}
{'loss': 0.8732, 'grad_norm': 2.5525185421541994, 'learning_rate': 3.519444444444445e-06, 'epoch': 2.7364}
{'loss': 0.8814, 'grad_norm': 2.8322422133170426, 'learning_rate': 3.5183333333333337e-06, 'epoch': 2.7368}
{'loss': 0.862, 'grad_norm': 2.8369785111110644, 'learning_rate': 3.5172222222222224e-06, 'epoch': 2.7372}
{'loss': 0.8781, 'grad_norm': 2.9837341719048944, 'learning_rate': 3.516111111111111e-06, 'epoch': 2.7376}
{'loss': 0.8804, 'grad_norm': 2.784126476580836, 'learning_rate': 3.5150000000000002e-06, 'epoch': 2.738}
{'loss': 0.8827, 'grad_norm': 2.860844250278624, 'learning_rate': 3.513888888888889e-06, 'epoch': 2.7384}
{'loss': 0.8797, 'grad_norm': 2.9760800869315096, 'learning_rate': 3.5127777777777785e-06, 'epoch': 2.7388}
{'loss': 0.8788, 'grad_norm': 2.996001250867111, 'learning_rate': 3.511666666666667e-06, 'epoch': 2.7392}
{'loss': 0.8835, 'grad_norm': 2.8093311737087627, 'learning_rate': 3.510555555555556e-06, 'epoch': 2.7396000000000003}
{'loss': 0.8873, 'grad_norm': 2.8674586529066377, 'learning_rate': 3.5094444444444446e-06, 'epoch': 2.74}
{'eval_valid_loss': 0.85400390625, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1128.323, 'eval_valid_steps_per_second': 282.081, 'epoch': 2.74}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.8895, 'grad_norm': 2.7085116303271493, 'learning_rate': 3.5083333333333338e-06, 'epoch': 2.7404}
{'loss': 0.89, 'grad_norm': 2.7901676349970517, 'learning_rate': 3.5072222222222225e-06, 'epoch': 2.7408}
{'loss': 0.8631, 'grad_norm': 2.733084243210574, 'learning_rate': 3.506111111111111e-06, 'epoch': 2.7412}
{'loss': 0.8685, 'grad_norm': 2.947425509313141, 'learning_rate': 3.505e-06, 'epoch': 2.7416}
{'loss': 0.8689, 'grad_norm': 2.8904921011401927, 'learning_rate': 3.5038888888888895e-06, 'epoch': 2.742}
{'loss': 0.8854, 'grad_norm': 3.1302413758846943, 'learning_rate': 3.502777777777778e-06, 'epoch': 2.7424}
{'loss': 0.8819, 'grad_norm': 3.092006336474002, 'learning_rate': 3.5016666666666673e-06, 'epoch': 2.7428}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8904, 'grad_norm': 2.9753349957067945, 'learning_rate': 3.500555555555556e-06, 'epoch': 2.7432}
{'loss': 0.8814, 'grad_norm': 3.0880674906295433, 'learning_rate': 3.4994444444444447e-06, 'epoch': 2.7436}
{'loss': 0.8888, 'grad_norm': 3.08362308098756, 'learning_rate': 3.4983333333333334e-06, 'epoch': 2.7439999999999998}
{'eval_valid_loss': 0.8525390625, 'eval_valid_runtime': 0.0964, 'eval_valid_samples_per_second': 1037.796, 'eval_valid_steps_per_second': 259.449, 'epoch': 2.7439999999999998}
{'loss': 0.8893, 'grad_norm': 2.8645927579320136, 'learning_rate': 3.4972222222222226e-06, 'epoch': 2.7443999999999997}
{'loss': 0.8789, 'grad_norm': 3.0434971988352366, 'learning_rate': 3.4961111111111113e-06, 'epoch': 2.7448}
{'loss': 0.8608, 'grad_norm': 3.1490607674145243, 'learning_rate': 3.495e-06, 'epoch': 2.7452}
{'loss': 0.8763, 'grad_norm': 2.938796660159539, 'learning_rate': 3.4938888888888896e-06, 'epoch': 2.7456}
{'loss': 0.875, 'grad_norm': 2.7849383489643897, 'learning_rate': 3.4927777777777783e-06, 'epoch': 2.746}
{'loss': 0.8773, 'grad_norm': 2.9337107486480227, 'learning_rate': 3.491666666666667e-06, 'epoch': 2.7464}
{'loss': 0.8901, 'grad_norm': 2.9097218080402487, 'learning_rate': 3.490555555555556e-06, 'epoch': 2.7468}
{'loss': 0.8872, 'grad_norm': 3.263793623438039, 'learning_rate': 3.489444444444445e-06, 'epoch': 2.7472}
{'loss': 0.8902, 'grad_norm': 3.0152850379448406, 'learning_rate': 3.4883333333333335e-06, 'epoch': 2.7476000000000003}
{'loss': 0.8865, 'grad_norm': 3.13026634877669, 'learning_rate': 3.4872222222222222e-06, 'epoch': 2.748}
{'eval_valid_loss': 0.853515625, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.056, 'eval_valid_steps_per_second': 279.014, 'epoch': 2.748}
{'loss': 0.8702, 'grad_norm': 3.1772921488360937, 'learning_rate': 3.4861111111111114e-06, 'epoch': 2.7484}
{'loss': 0.8905, 'grad_norm': 2.8795359995456766, 'learning_rate': 3.485e-06, 'epoch': 2.7488}
{'loss': 0.8806, 'grad_norm': 3.0593727552115415, 'learning_rate': 3.4838888888888892e-06, 'epoch': 2.7492}
{'loss': 0.8792, 'grad_norm': 2.8017978818254634, 'learning_rate': 3.4827777777777784e-06, 'epoch': 2.7496}
{'loss': 0.873, 'grad_norm': 2.5212537392610765, 'learning_rate': 3.481666666666667e-06, 'epoch': 2.75}
{'loss': 0.8666, 'grad_norm': 2.790576833584827, 'learning_rate': 3.4805555555555558e-06, 'epoch': 2.7504}
{'loss': 0.8852, 'grad_norm': 2.8079978087788913, 'learning_rate': 3.479444444444445e-06, 'epoch': 2.7508}
{'loss': 0.8904, 'grad_norm': 2.7739400864957307, 'learning_rate': 3.4783333333333336e-06, 'epoch': 2.7512}
{'loss': 0.8813, 'grad_norm': 3.215696488263274, 'learning_rate': 3.4772222222222223e-06, 'epoch': 2.7516}
{'loss': 0.8791, 'grad_norm': 2.821219021200755, 'learning_rate': 3.476111111111111e-06, 'epoch': 2.752}
{'eval_valid_loss': 0.853515625, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.513, 'eval_valid_steps_per_second': 277.128, 'epoch': 2.752}
{'loss': 0.8811, 'grad_norm': 2.8278866293333795, 'learning_rate': 3.475e-06, 'epoch': 2.7523999999999997}
{'loss': 0.8743, 'grad_norm': 2.872948619580927, 'learning_rate': 3.4738888888888893e-06, 'epoch': 2.7528}
{'loss': 0.8835, 'grad_norm': 2.9874388760335076, 'learning_rate': 3.4727777777777785e-06, 'epoch': 2.7532}
{'loss': 0.8809, 'grad_norm': 2.762083507012637, 'learning_rate': 3.471666666666667e-06, 'epoch': 2.7536}
{'loss': 0.8837, 'grad_norm': 2.7193077600486872, 'learning_rate': 3.470555555555556e-06, 'epoch': 2.754}
{'loss': 0.875, 'grad_norm': 2.9415628459933862, 'learning_rate': 3.4694444444444446e-06, 'epoch': 2.7544}
{'loss': 0.8671, 'grad_norm': 2.9690991748455478, 'learning_rate': 3.4683333333333337e-06, 'epoch': 2.7548}
{'loss': 0.8814, 'grad_norm': 3.0876327107745833, 'learning_rate': 3.4672222222222224e-06, 'epoch': 2.7552}
{'loss': 0.8788, 'grad_norm': 2.846180118545607, 'learning_rate': 3.466111111111111e-06, 'epoch': 2.7556000000000003}
{'loss': 0.8842, 'grad_norm': 2.5336099972881803, 'learning_rate': 3.465e-06, 'epoch': 2.7560000000000002}
{'eval_valid_loss': 0.85302734375, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.023, 'eval_valid_steps_per_second': 278.256, 'epoch': 2.7560000000000002}
{'loss': 0.8747, 'grad_norm': 2.5828044212754975, 'learning_rate': 3.4638888888888894e-06, 'epoch': 2.7564}
{'loss': 0.8764, 'grad_norm': 2.8927178101229267, 'learning_rate': 3.462777777777778e-06, 'epoch': 2.7568}
{'loss': 0.885, 'grad_norm': 2.7918260063892184, 'learning_rate': 3.4616666666666673e-06, 'epoch': 2.7572}
{'loss': 0.8761, 'grad_norm': 3.10976164057706, 'learning_rate': 3.460555555555556e-06, 'epoch': 2.7576}
{'loss': 0.8738, 'grad_norm': 2.6082137887624453, 'learning_rate': 3.4594444444444447e-06, 'epoch': 2.758}
{'loss': 0.8861, 'grad_norm': 2.7839534243429753, 'learning_rate': 3.4583333333333334e-06, 'epoch': 2.7584}
{'loss': 0.8746, 'grad_norm': 2.7794310054193216, 'learning_rate': 3.4572222222222225e-06, 'epoch': 2.7588}
{'loss': 0.8815, 'grad_norm': 3.1902637654558985, 'learning_rate': 3.4561111111111112e-06, 'epoch': 2.7592}
{'loss': 0.8828, 'grad_norm': 2.6066360889762925, 'learning_rate': 3.455e-06, 'epoch': 2.7596}
{'loss': 0.885, 'grad_norm': 2.8574626343454255, 'learning_rate': 3.4538888888888895e-06, 'epoch': 2.76}
{'eval_valid_loss': 0.853515625, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1120.199, 'eval_valid_steps_per_second': 280.05, 'epoch': 2.76}
{'loss': 0.8787, 'grad_norm': 3.1498062622951983, 'learning_rate': 3.4527777777777782e-06, 'epoch': 2.7603999999999997}
{'loss': 0.8711, 'grad_norm': 3.1031945112266395, 'learning_rate': 3.451666666666667e-06, 'epoch': 2.7608}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8808, 'grad_norm': 2.9211671961294194, 'learning_rate': 3.450555555555556e-06, 'epoch': 2.7612}
{'loss': 0.8829, 'grad_norm': 3.0094349040152757, 'learning_rate': 3.4494444444444448e-06, 'epoch': 2.7616}
{'loss': 0.8631, 'grad_norm': 2.817408622742477, 'learning_rate': 3.4483333333333335e-06, 'epoch': 2.762}
{'loss': 0.8715, 'grad_norm': 3.2963065882263964, 'learning_rate': 3.447222222222222e-06, 'epoch': 2.7624}
{'loss': 0.8767, 'grad_norm': 2.6996320968331875, 'learning_rate': 3.4461111111111113e-06, 'epoch': 2.7628}
{'loss': 0.8851, 'grad_norm': 3.0283429668702273, 'learning_rate': 3.445e-06, 'epoch': 2.7632}
{'loss': 0.8901, 'grad_norm': 3.201355775830595, 'learning_rate': 3.444e-06, 'epoch': 2.7636}
{'loss': 0.8793, 'grad_norm': 2.929448181550223, 'learning_rate': 3.442888888888889e-06, 'epoch': 2.7640000000000002}
{'eval_valid_loss': 0.85205078125, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1124.94, 'eval_valid_steps_per_second': 281.235, 'epoch': 2.7640000000000002}
{'loss': 0.8727, 'grad_norm': 2.8912562737765857, 'learning_rate': 3.4417777777777782e-06, 'epoch': 2.7644}
{'loss': 0.8919, 'grad_norm': 2.89733165404967, 'learning_rate': 3.440666666666667e-06, 'epoch': 2.7648}
{'loss': 0.8742, 'grad_norm': 2.9091894809728367, 'learning_rate': 3.439555555555556e-06, 'epoch': 2.7652}
{'loss': 0.8836, 'grad_norm': 2.8606566490771224, 'learning_rate': 3.438444444444445e-06, 'epoch': 2.7656}
{'loss': 0.8724, 'grad_norm': 2.858513999508334, 'learning_rate': 3.4373333333333335e-06, 'epoch': 2.766}
{'loss': 0.8829, 'grad_norm': 3.029239430181101, 'learning_rate': 3.4362222222222226e-06, 'epoch': 2.7664}
{'loss': 0.8887, 'grad_norm': 3.0814673581168615, 'learning_rate': 3.4351111111111114e-06, 'epoch': 2.7668}
{'loss': 0.8735, 'grad_norm': 2.8034755829897073, 'learning_rate': 3.434e-06, 'epoch': 2.7672}
{'loss': 0.8854, 'grad_norm': 2.9898448169587732, 'learning_rate': 3.4328888888888888e-06, 'epoch': 2.7676}
{'loss': 0.8886, 'grad_norm': 2.7858048228251207, 'learning_rate': 3.4317777777777783e-06, 'epoch': 2.768}
{'eval_valid_loss': 0.85302734375, 'eval_valid_runtime': 0.0911, 'eval_valid_samples_per_second': 1097.147, 'eval_valid_steps_per_second': 274.287, 'epoch': 2.768}
{'loss': 0.8753, 'grad_norm': 2.743407513308695, 'learning_rate': 3.430666666666667e-06, 'epoch': 2.7683999999999997}
{'loss': 0.8776, 'grad_norm': 2.617559429330214, 'learning_rate': 3.4295555555555558e-06, 'epoch': 2.7688}
{'loss': 0.8714, 'grad_norm': 2.8103717592995494, 'learning_rate': 3.428444444444445e-06, 'epoch': 2.7692}
{'loss': 0.8874, 'grad_norm': 3.0260227397560975, 'learning_rate': 3.4273333333333336e-06, 'epoch': 2.7696}
{'loss': 0.8775, 'grad_norm': 2.8777554824653597, 'learning_rate': 3.4262222222222223e-06, 'epoch': 2.77}
{'loss': 0.8866, 'grad_norm': 2.916239480115325, 'learning_rate': 3.4251111111111115e-06, 'epoch': 2.7704}
{'loss': 0.878, 'grad_norm': 3.0719646735699575, 'learning_rate': 3.424e-06, 'epoch': 2.7708}
{'loss': 0.8912, 'grad_norm': 2.851752136404569, 'learning_rate': 3.422888888888889e-06, 'epoch': 2.7712}
{'loss': 0.8716, 'grad_norm': 3.031540001436825, 'learning_rate': 3.4217777777777784e-06, 'epoch': 2.7716}
{'loss': 0.8737, 'grad_norm': 2.722812155552035, 'learning_rate': 3.420666666666667e-06, 'epoch': 2.7720000000000002}
{'eval_valid_loss': 0.85302734375, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1106.104, 'eval_valid_steps_per_second': 276.526, 'epoch': 2.7720000000000002}
{'loss': 0.8778, 'grad_norm': 3.080717960099738, 'learning_rate': 3.419555555555556e-06, 'epoch': 2.7724}
{'loss': 0.8827, 'grad_norm': 3.134975057955971, 'learning_rate': 3.418444444444445e-06, 'epoch': 2.7728}
{'loss': 0.8819, 'grad_norm': 2.664207700648625, 'learning_rate': 3.4173333333333337e-06, 'epoch': 2.7732}
{'loss': 0.8754, 'grad_norm': 2.868690720321758, 'learning_rate': 3.4162222222222224e-06, 'epoch': 2.7736}
{'loss': 0.8648, 'grad_norm': 2.809486735310413, 'learning_rate': 3.415111111111111e-06, 'epoch': 2.774}
{'loss': 0.8757, 'grad_norm': 2.599251594255682, 'learning_rate': 3.4140000000000003e-06, 'epoch': 2.7744}
{'loss': 0.8726, 'grad_norm': 2.938201272733596, 'learning_rate': 3.412888888888889e-06, 'epoch': 2.7748}
{'loss': 0.8907, 'grad_norm': 2.876143932815555, 'learning_rate': 3.411777777777778e-06, 'epoch': 2.7752}
{'loss': 0.8789, 'grad_norm': 2.700432014782722, 'learning_rate': 3.4106666666666672e-06, 'epoch': 2.7756}
{'loss': 0.8773, 'grad_norm': 2.8768889814323417, 'learning_rate': 3.409555555555556e-06, 'epoch': 2.776}
{'eval_valid_loss': 0.85302734375, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.947, 'eval_valid_steps_per_second': 279.987, 'epoch': 2.776}
{'loss': 0.8774, 'grad_norm': 2.8834637265590395, 'learning_rate': 3.4084444444444447e-06, 'epoch': 2.7763999999999998}
{'loss': 0.8714, 'grad_norm': 2.6666395135083203, 'learning_rate': 3.407333333333334e-06, 'epoch': 2.7768}
{'loss': 0.882, 'grad_norm': 2.945962100652128, 'learning_rate': 3.4062222222222225e-06, 'epoch': 2.7772}
{'loss': 0.87, 'grad_norm': 2.9283988458176524, 'learning_rate': 3.4051111111111112e-06, 'epoch': 2.7776}
{'loss': 0.8982, 'grad_norm': 3.1619107783099807, 'learning_rate': 3.404e-06, 'epoch': 2.778}
{'loss': 0.8716, 'grad_norm': 2.7938303652134326, 'learning_rate': 3.402888888888889e-06, 'epoch': 2.7784}
{'loss': 0.8722, 'grad_norm': 2.6375488500455444, 'learning_rate': 3.401777777777778e-06, 'epoch': 2.7788}
{'loss': 0.907, 'grad_norm': 3.247707989252793, 'learning_rate': 3.400666666666667e-06, 'epoch': 2.7792}
{'loss': 0.886, 'grad_norm': 2.9950313350585813, 'learning_rate': 3.399555555555556e-06, 'epoch': 2.7796}
{'loss': 0.8812, 'grad_norm': 2.9025091172928588, 'learning_rate': 3.3984444444444448e-06, 'epoch': 2.7800000000000002}
{'eval_valid_loss': 0.8525390625, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.733, 'eval_valid_steps_per_second': 278.183, 'epoch': 2.7800000000000002}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.8744, 'grad_norm': 2.938654297422803, 'learning_rate': 3.3973333333333335e-06, 'epoch': 2.7804}
{'loss': 0.8813, 'grad_norm': 2.845389270951302, 'learning_rate': 3.3962222222222226e-06, 'epoch': 2.7808}
{'loss': 0.8904, 'grad_norm': 2.842385132195674, 'learning_rate': 3.3951111111111113e-06, 'epoch': 2.7812}
{'loss': 0.8706, 'grad_norm': 2.7474307065561407, 'learning_rate': 3.394e-06, 'epoch': 2.7816}
{'loss': 0.8761, 'grad_norm': 2.832523871360785, 'learning_rate': 3.3928888888888887e-06, 'epoch': 2.782}
{'loss': 0.8887, 'grad_norm': 3.0272199150107073, 'learning_rate': 3.3917777777777783e-06, 'epoch': 2.7824}
{'loss': 0.8743, 'grad_norm': 3.0335243187679843, 'learning_rate': 3.390666666666667e-06, 'epoch': 2.7828}
{'loss': 0.8819, 'grad_norm': 2.964037528005727, 'learning_rate': 3.3895555555555557e-06, 'epoch': 2.7832}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.867, 'grad_norm': 2.686618455200075, 'learning_rate': 3.388444444444445e-06, 'epoch': 2.7836}
{'loss': 0.8856, 'grad_norm': 2.8053363644525144, 'learning_rate': 3.3873333333333336e-06, 'epoch': 2.784}
{'eval_valid_loss': 0.85302734375, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.693, 'eval_valid_steps_per_second': 279.673, 'epoch': 2.784}
{'loss': 0.8842, 'grad_norm': 2.829032889215085, 'learning_rate': 3.3862222222222223e-06, 'epoch': 2.7843999999999998}
{'loss': 0.8865, 'grad_norm': 2.8534842403913134, 'learning_rate': 3.3851111111111114e-06, 'epoch': 2.7848}
{'loss': 0.8724, 'grad_norm': 3.1423374745350534, 'learning_rate': 3.384e-06, 'epoch': 2.7852}
{'loss': 0.8825, 'grad_norm': 2.646037080133723, 'learning_rate': 3.382888888888889e-06, 'epoch': 2.7856}
{'loss': 0.8801, 'grad_norm': 3.027981746265189, 'learning_rate': 3.3817777777777784e-06, 'epoch': 2.786}
{'loss': 0.8799, 'grad_norm': 3.1313690513166637, 'learning_rate': 3.380666666666667e-06, 'epoch': 2.7864}
{'loss': 0.8709, 'grad_norm': 2.9481761984375514, 'learning_rate': 3.379555555555556e-06, 'epoch': 2.7868}
{'loss': 0.8708, 'grad_norm': 2.8514851285288123, 'learning_rate': 3.378444444444445e-06, 'epoch': 2.7872}
{'loss': 0.8722, 'grad_norm': 2.888156610203224, 'learning_rate': 3.3773333333333337e-06, 'epoch': 2.7876}
{'loss': 0.8833, 'grad_norm': 3.048147229365251, 'learning_rate': 3.3762222222222224e-06, 'epoch': 2.7880000000000003}
{'eval_valid_loss': 0.8525390625, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.284, 'eval_valid_steps_per_second': 279.071, 'epoch': 2.7880000000000003}
{'loss': 0.8872, 'grad_norm': 2.91345551128931, 'learning_rate': 3.375111111111111e-06, 'epoch': 2.7884}
{'loss': 0.8645, 'grad_norm': 3.317730292223129, 'learning_rate': 3.3740000000000002e-06, 'epoch': 2.7888}
{'loss': 0.9017, 'grad_norm': 3.029993563730183, 'learning_rate': 3.3728888888888894e-06, 'epoch': 2.7892}
{'loss': 0.8744, 'grad_norm': 2.958517560237225, 'learning_rate': 3.371777777777778e-06, 'epoch': 2.7896}
{'loss': 0.8733, 'grad_norm': 2.6463746558386427, 'learning_rate': 3.370666666666667e-06, 'epoch': 2.79}
{'loss': 0.877, 'grad_norm': 2.732501112666344, 'learning_rate': 3.369555555555556e-06, 'epoch': 2.7904}
{'loss': 0.8776, 'grad_norm': 2.77450565668981, 'learning_rate': 3.3684444444444446e-06, 'epoch': 2.7908}
{'loss': 0.8719, 'grad_norm': 2.7809051503474196, 'learning_rate': 3.3673333333333338e-06, 'epoch': 2.7912}
{'loss': 0.8659, 'grad_norm': 2.6722306410807115, 'learning_rate': 3.3662222222222225e-06, 'epoch': 2.7916}
{'loss': 0.8827, 'grad_norm': 2.9812857181880537, 'learning_rate': 3.365111111111111e-06, 'epoch': 2.792}
{'eval_valid_loss': 0.85302734375, 'eval_valid_runtime': 0.091, 'eval_valid_samples_per_second': 1099.131, 'eval_valid_steps_per_second': 274.783, 'epoch': 2.792}
{'loss': 0.8726, 'grad_norm': 3.0842738532088356, 'learning_rate': 3.364e-06, 'epoch': 2.7923999999999998}
{'loss': 0.8932, 'grad_norm': 2.708076187299579, 'learning_rate': 3.3628888888888895e-06, 'epoch': 2.7927999999999997}
{'loss': 0.8869, 'grad_norm': 2.9347386367432278, 'learning_rate': 3.361777777777778e-06, 'epoch': 2.7932}
{'loss': 0.8728, 'grad_norm': 2.877744297856062, 'learning_rate': 3.360666666666667e-06, 'epoch': 2.7936}
{'loss': 0.8867, 'grad_norm': 2.9995601848230677, 'learning_rate': 3.359555555555556e-06, 'epoch': 2.794}
{'loss': 0.8744, 'grad_norm': 2.8661702505729876, 'learning_rate': 3.3584444444444447e-06, 'epoch': 2.7944}
{'loss': 0.8773, 'grad_norm': 2.7179193378725617, 'learning_rate': 3.3573333333333334e-06, 'epoch': 2.7948}
{'loss': 0.8856, 'grad_norm': 3.3357777523769503, 'learning_rate': 3.3562222222222226e-06, 'epoch': 2.7952}
{'loss': 0.8748, 'grad_norm': 2.9454920584992985, 'learning_rate': 3.3551111111111113e-06, 'epoch': 2.7956}
{'loss': 0.8749, 'grad_norm': 3.0071288285512323, 'learning_rate': 3.354e-06, 'epoch': 2.7960000000000003}
{'eval_valid_loss': 0.85302734375, 'eval_valid_runtime': 0.0912, 'eval_valid_samples_per_second': 1095.897, 'eval_valid_steps_per_second': 273.974, 'epoch': 2.7960000000000003}
{'loss': 0.8825, 'grad_norm': 2.788222044714068, 'learning_rate': 3.3528888888888895e-06, 'epoch': 2.7964}
{'loss': 0.8801, 'grad_norm': 3.2182218896145938, 'learning_rate': 3.3517777777777783e-06, 'epoch': 2.7968}
{'loss': 0.8761, 'grad_norm': 2.9379332709872283, 'learning_rate': 3.350666666666667e-06, 'epoch': 2.7972}
{'loss': 0.8766, 'grad_norm': 2.6108221278562493, 'learning_rate': 3.3495555555555557e-06, 'epoch': 2.7976}
{'loss': 0.8721, 'grad_norm': 2.748793570358361, 'learning_rate': 3.348444444444445e-06, 'epoch': 2.798}
{'loss': 0.8729, 'grad_norm': 3.182674551414942, 'learning_rate': 3.3473333333333335e-06, 'epoch': 2.7984}
{'loss': 0.892, 'grad_norm': 2.920358182530408, 'learning_rate': 3.3462222222222222e-06, 'epoch': 2.7988}
{'loss': 0.8899, 'grad_norm': 2.727502633468517, 'learning_rate': 3.3451111111111114e-06, 'epoch': 2.7992}
{'loss': 0.8821, 'grad_norm': 3.00587970015388, 'learning_rate': 3.344e-06, 'epoch': 2.7996}
{'loss': 0.8674, 'grad_norm': 2.9624649677539177, 'learning_rate': 3.3428888888888892e-06, 'epoch': 2.8}
{'eval_valid_loss': 0.85302734375, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.79, 'eval_valid_steps_per_second': 279.447, 'epoch': 2.8}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8822, 'grad_norm': 2.7507771455950287, 'learning_rate': 3.3417777777777784e-06, 'epoch': 2.8004}
{'loss': 0.8672, 'grad_norm': 2.7561508608524377, 'learning_rate': 3.340666666666667e-06, 'epoch': 2.8007999999999997}
{'loss': 0.8711, 'grad_norm': 3.054208420359215, 'learning_rate': 3.3395555555555558e-06, 'epoch': 2.8012}
{'loss': 0.8839, 'grad_norm': 2.7701337344631507, 'learning_rate': 3.338444444444445e-06, 'epoch': 2.8016}
{'loss': 0.8848, 'grad_norm': 2.989332345845243, 'learning_rate': 3.3373333333333336e-06, 'epoch': 2.802}
{'loss': 0.8838, 'grad_norm': 3.058138622266849, 'learning_rate': 3.3362222222222223e-06, 'epoch': 2.8024}
{'loss': 0.8807, 'grad_norm': 2.9871548193867783, 'learning_rate': 3.335111111111111e-06, 'epoch': 2.8028}
{'loss': 0.8854, 'grad_norm': 3.030770696519473, 'learning_rate': 3.334e-06, 'epoch': 2.8032}
{'loss': 0.8739, 'grad_norm': 3.1395273615390074, 'learning_rate': 3.333e-06, 'epoch': 2.8036}
{'loss': 0.8793, 'grad_norm': 2.802676300616823, 'learning_rate': 3.331888888888889e-06, 'epoch': 2.8040000000000003}
{'eval_valid_loss': 0.85205078125, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1121.109, 'eval_valid_steps_per_second': 280.277, 'epoch': 2.8040000000000003}
{'loss': 0.8756, 'grad_norm': 2.9289765677936024, 'learning_rate': 3.330777777777778e-06, 'epoch': 2.8044000000000002}
{'loss': 0.8817, 'grad_norm': 2.9190647768273728, 'learning_rate': 3.329666666666667e-06, 'epoch': 2.8048}
{'loss': 0.8896, 'grad_norm': 2.9074976611763494, 'learning_rate': 3.328555555555556e-06, 'epoch': 2.8052}
{'loss': 0.8792, 'grad_norm': 2.6578136610043166, 'learning_rate': 3.327444444444445e-06, 'epoch': 2.8056}
{'loss': 0.8743, 'grad_norm': 2.794771938180883, 'learning_rate': 3.3263333333333336e-06, 'epoch': 2.806}
{'loss': 0.8768, 'grad_norm': 2.7901891254770397, 'learning_rate': 3.3252222222222224e-06, 'epoch': 2.8064}
{'loss': 0.8823, 'grad_norm': 2.720567085058058, 'learning_rate': 3.3241111111111115e-06, 'epoch': 2.8068}
{'loss': 0.8731, 'grad_norm': 2.624728625893142, 'learning_rate': 3.323e-06, 'epoch': 2.8072}
{'loss': 0.8886, 'grad_norm': 3.111193082265061, 'learning_rate': 3.321888888888889e-06, 'epoch': 2.8076}
{'loss': 0.8728, 'grad_norm': 3.0277723439680173, 'learning_rate': 3.3207777777777785e-06, 'epoch': 2.808}
{'eval_valid_loss': 0.85205078125, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.984, 'eval_valid_steps_per_second': 278.496, 'epoch': 2.808}
{'loss': 0.8762, 'grad_norm': 2.9777393067294193, 'learning_rate': 3.319666666666667e-06, 'epoch': 2.8084}
{'loss': 0.8712, 'grad_norm': 2.9748164084229085, 'learning_rate': 3.318555555555556e-06, 'epoch': 2.8087999999999997}
{'loss': 0.8767, 'grad_norm': 3.1775944074626663, 'learning_rate': 3.3174444444444446e-06, 'epoch': 2.8092}
{'loss': 0.8803, 'grad_norm': 3.054325706735176, 'learning_rate': 3.3163333333333337e-06, 'epoch': 2.8096}
{'loss': 0.8821, 'grad_norm': 2.730237287935204, 'learning_rate': 3.3152222222222225e-06, 'epoch': 2.81}
{'loss': 0.8903, 'grad_norm': 3.213729044038396, 'learning_rate': 3.314111111111111e-06, 'epoch': 2.8104}
{'loss': 0.8887, 'grad_norm': 3.1749877125960353, 'learning_rate': 3.3130000000000003e-06, 'epoch': 2.8108}
{'loss': 0.8644, 'grad_norm': 2.9471092005652944, 'learning_rate': 3.311888888888889e-06, 'epoch': 2.8112}
{'loss': 0.8738, 'grad_norm': 2.829680020571534, 'learning_rate': 3.310777777777778e-06, 'epoch': 2.8116}
{'loss': 0.8794, 'grad_norm': 3.1771913801904756, 'learning_rate': 3.3096666666666673e-06, 'epoch': 2.8120000000000003}
{'eval_valid_loss': 0.85302734375, 'eval_valid_runtime': 0.0911, 'eval_valid_samples_per_second': 1097.316, 'eval_valid_steps_per_second': 274.329, 'epoch': 2.8120000000000003}
{'loss': 0.874, 'grad_norm': 2.9400743303342125, 'learning_rate': 3.308555555555556e-06, 'epoch': 2.8124000000000002}
{'loss': 0.8766, 'grad_norm': 2.9091102717307917, 'learning_rate': 3.3074444444444447e-06, 'epoch': 2.8128}
{'loss': 0.8635, 'grad_norm': 2.8496318290719986, 'learning_rate': 3.3063333333333334e-06, 'epoch': 2.8132}
{'loss': 0.8882, 'grad_norm': 2.739435553946525, 'learning_rate': 3.3052222222222225e-06, 'epoch': 2.8136}
{'loss': 0.8865, 'grad_norm': 2.848316016774211, 'learning_rate': 3.3041111111111113e-06, 'epoch': 2.814}
{'loss': 0.8735, 'grad_norm': 2.6680328897610806, 'learning_rate': 3.303e-06, 'epoch': 2.8144}
{'loss': 0.872, 'grad_norm': 2.729889635005345, 'learning_rate': 3.301888888888889e-06, 'epoch': 2.8148}
{'loss': 0.8838, 'grad_norm': 3.1187511719298215, 'learning_rate': 3.3007777777777782e-06, 'epoch': 2.8152}
{'loss': 0.8792, 'grad_norm': 2.8810393196416566, 'learning_rate': 3.299666666666667e-06, 'epoch': 2.8156}
{'loss': 0.8708, 'grad_norm': 2.580402252837244, 'learning_rate': 3.298555555555556e-06, 'epoch': 2.816}
{'eval_valid_loss': 0.8515625, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.135, 'eval_valid_steps_per_second': 279.534, 'epoch': 2.816}
{'loss': 0.8802, 'grad_norm': 2.625679893092365, 'learning_rate': 3.297444444444445e-06, 'epoch': 2.8164}
{'loss': 0.8697, 'grad_norm': 3.0360012375624277, 'learning_rate': 3.2963333333333335e-06, 'epoch': 2.8167999999999997}
{'loss': 0.8917, 'grad_norm': 2.8481516256755786, 'learning_rate': 3.2952222222222226e-06, 'epoch': 2.8172}
{'loss': 0.8659, 'grad_norm': 2.857583912904758, 'learning_rate': 3.2941111111111114e-06, 'epoch': 2.8176}
{'loss': 0.8725, 'grad_norm': 2.9093749104713895, 'learning_rate': 3.293e-06, 'epoch': 2.818}
{'loss': 0.8829, 'grad_norm': 3.072283919971702, 'learning_rate': 3.2918888888888888e-06, 'epoch': 2.8184}
{'loss': 0.8731, 'grad_norm': 2.7693794919121535, 'learning_rate': 3.2907777777777783e-06, 'epoch': 2.8188}
{'loss': 0.8663, 'grad_norm': 2.642139232020911, 'learning_rate': 3.289666666666667e-06, 'epoch': 2.8192}
{'loss': 0.9, 'grad_norm': 3.022614428639319, 'learning_rate': 3.2885555555555558e-06, 'epoch': 2.8196}
{'loss': 0.8835, 'grad_norm': 2.9844276862961236, 'learning_rate': 3.287444444444445e-06, 'epoch': 2.82}
{'eval_valid_loss': 0.8525390625, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1122.324, 'eval_valid_steps_per_second': 280.581, 'epoch': 2.82}
{'loss': 0.8812, 'grad_norm': 3.099993301584329, 'learning_rate': 3.2863333333333336e-06, 'epoch': 2.8204000000000002}
{'loss': 0.8816, 'grad_norm': 3.212819877239804, 'learning_rate': 3.2852222222222223e-06, 'epoch': 2.8208}
{'loss': 0.8893, 'grad_norm': 3.0966381598328363, 'learning_rate': 3.2841111111111115e-06, 'epoch': 2.8212}
{'loss': 0.8643, 'grad_norm': 2.8294492961481508, 'learning_rate': 3.283e-06, 'epoch': 2.8216}
{'loss': 0.8778, 'grad_norm': 3.1751528560458286, 'learning_rate': 3.281888888888889e-06, 'epoch': 2.822}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8685, 'grad_norm': 2.956826559463003, 'learning_rate': 3.2807777777777784e-06, 'epoch': 2.8224}
{'loss': 0.8784, 'grad_norm': 2.8736194115329305, 'learning_rate': 3.279666666666667e-06, 'epoch': 2.8228}
{'loss': 0.8901, 'grad_norm': 2.9424371014843134, 'learning_rate': 3.278555555555556e-06, 'epoch': 2.8232}
{'loss': 0.8777, 'grad_norm': 2.822713527314752, 'learning_rate': 3.2774444444444446e-06, 'epoch': 2.8236}
{'loss': 0.8872, 'grad_norm': 2.911275415469289, 'learning_rate': 3.2763333333333337e-06, 'epoch': 2.824}
{'eval_valid_loss': 0.8525390625, 'eval_valid_runtime': 0.0905, 'eval_valid_samples_per_second': 1104.401, 'eval_valid_steps_per_second': 276.1, 'epoch': 2.824}
{'loss': 0.8816, 'grad_norm': 2.8223504499207963, 'learning_rate': 3.2752222222222224e-06, 'epoch': 2.8244}
{'loss': 0.8661, 'grad_norm': 2.85884739745322, 'learning_rate': 3.274111111111111e-06, 'epoch': 2.8247999999999998}
{'loss': 0.8775, 'grad_norm': 2.7545894289437345, 'learning_rate': 3.2730000000000003e-06, 'epoch': 2.8252}
{'loss': 0.8762, 'grad_norm': 3.0793753954989915, 'learning_rate': 3.271888888888889e-06, 'epoch': 2.8256}
{'loss': 0.8833, 'grad_norm': 2.8395240845146654, 'learning_rate': 3.270777777777778e-06, 'epoch': 2.826}
{'loss': 0.8887, 'grad_norm': 2.840505095323365, 'learning_rate': 3.2696666666666672e-06, 'epoch': 2.8264}
{'loss': 0.8755, 'grad_norm': 3.0061717921372275, 'learning_rate': 3.268555555555556e-06, 'epoch': 2.8268}
{'loss': 0.8773, 'grad_norm': 2.8034570114601465, 'learning_rate': 3.2674444444444447e-06, 'epoch': 2.8272}
{'loss': 0.8835, 'grad_norm': 2.6892130515353094, 'learning_rate': 3.2663333333333334e-06, 'epoch': 2.8276}
{'loss': 0.8842, 'grad_norm': 2.96390186773432, 'learning_rate': 3.2652222222222225e-06, 'epoch': 2.828}
{'eval_valid_loss': 0.85205078125, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.833, 'eval_valid_steps_per_second': 279.708, 'epoch': 2.828}
{'loss': 0.8656, 'grad_norm': 3.0091657888671253, 'learning_rate': 3.2641111111111112e-06, 'epoch': 2.8284000000000002}
{'loss': 0.8855, 'grad_norm': 2.8961341219141823, 'learning_rate': 3.263e-06, 'epoch': 2.8288}
{'loss': 0.8719, 'grad_norm': 2.6380899325500318, 'learning_rate': 3.261888888888889e-06, 'epoch': 2.8292}
{'loss': 0.8785, 'grad_norm': 3.0756724118642484, 'learning_rate': 3.260777777777778e-06, 'epoch': 2.8296}
{'loss': 0.8667, 'grad_norm': 3.146129339493796, 'learning_rate': 3.259666666666667e-06, 'epoch': 2.83}
{'loss': 0.879, 'grad_norm': 3.031065247008059, 'learning_rate': 3.258555555555556e-06, 'epoch': 2.8304}
{'loss': 0.873, 'grad_norm': 2.9811747803385606, 'learning_rate': 3.2574444444444448e-06, 'epoch': 2.8308}
{'loss': 0.9017, 'grad_norm': 3.062951200684332, 'learning_rate': 3.2563333333333335e-06, 'epoch': 2.8312}
{'loss': 0.8787, 'grad_norm': 2.6529600806588616, 'learning_rate': 3.2552222222222226e-06, 'epoch': 2.8316}
{'loss': 0.8847, 'grad_norm': 2.888367477586813, 'learning_rate': 3.2541111111111113e-06, 'epoch': 2.832}
{'eval_valid_loss': 0.85205078125, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.38, 'eval_valid_steps_per_second': 281.845, 'epoch': 2.832}
{'loss': 0.8821, 'grad_norm': 2.6837089047441527, 'learning_rate': 3.253e-06, 'epoch': 2.8324}
{'loss': 0.8781, 'grad_norm': 2.966090275771926, 'learning_rate': 3.2518888888888896e-06, 'epoch': 2.8327999999999998}
{'loss': 0.8724, 'grad_norm': 3.0275529949108084, 'learning_rate': 3.2507777777777783e-06, 'epoch': 2.8332}
{'loss': 0.8778, 'grad_norm': 2.68767635742671, 'learning_rate': 3.249666666666667e-06, 'epoch': 2.8336}
{'loss': 0.8812, 'grad_norm': 2.92377652802487, 'learning_rate': 3.2485555555555557e-06, 'epoch': 2.834}
{'loss': 0.8842, 'grad_norm': 2.798826059259476, 'learning_rate': 3.247444444444445e-06, 'epoch': 2.8344}
{'loss': 0.8722, 'grad_norm': 2.9074579774028884, 'learning_rate': 3.2463333333333336e-06, 'epoch': 2.8348}
{'loss': 0.8863, 'grad_norm': 2.9010256293702197, 'learning_rate': 3.2452222222222223e-06, 'epoch': 2.8352}
{'loss': 0.8885, 'grad_norm': 3.1757256290684146, 'learning_rate': 3.2441111111111114e-06, 'epoch': 2.8356}
{'loss': 0.8745, 'grad_norm': 2.894084504693326, 'learning_rate': 3.243e-06, 'epoch': 2.836}
{'eval_valid_loss': 0.85107421875, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1120.426, 'eval_valid_steps_per_second': 280.107, 'epoch': 2.836}
{'loss': 0.8865, 'grad_norm': 2.8166519989011727, 'learning_rate': 3.2418888888888893e-06, 'epoch': 2.8364000000000003}
{'loss': 0.8789, 'grad_norm': 3.055353360397891, 'learning_rate': 3.2407777777777784e-06, 'epoch': 2.8368}
{'loss': 0.869, 'grad_norm': 3.0517232712888984, 'learning_rate': 3.239666666666667e-06, 'epoch': 2.8372}
{'loss': 0.8906, 'grad_norm': 2.7504979138008423, 'learning_rate': 3.238555555555556e-06, 'epoch': 2.8376}
{'loss': 0.8732, 'grad_norm': 2.6059219160710723, 'learning_rate': 3.2374444444444445e-06, 'epoch': 2.838}
{'loss': 0.8623, 'grad_norm': 3.0740635989056204, 'learning_rate': 3.2363333333333337e-06, 'epoch': 2.8384}
{'loss': 0.8683, 'grad_norm': 3.0595654713738183, 'learning_rate': 3.2352222222222224e-06, 'epoch': 2.8388}
{'loss': 0.8769, 'grad_norm': 2.9667691398363876, 'learning_rate': 3.234111111111111e-06, 'epoch': 2.8392}
{'loss': 0.8664, 'grad_norm': 2.9254222287989644, 'learning_rate': 3.2330000000000002e-06, 'epoch': 2.8396}
{'loss': 0.8792, 'grad_norm': 3.0046562280480633, 'learning_rate': 3.2318888888888894e-06, 'epoch': 2.84}
{'eval_valid_loss': 0.8525390625, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1106.119, 'eval_valid_steps_per_second': 276.53, 'epoch': 2.84}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8623, 'grad_norm': 3.1888082007581424, 'learning_rate': 3.230777777777778e-06, 'epoch': 2.8404}
{'loss': 0.883, 'grad_norm': 2.9552009088052973, 'learning_rate': 3.229666666666667e-06, 'epoch': 2.8407999999999998}
{'loss': 0.8716, 'grad_norm': 2.758267827038882, 'learning_rate': 3.228555555555556e-06, 'epoch': 2.8411999999999997}
{'loss': 0.8685, 'grad_norm': 2.9465003413587443, 'learning_rate': 3.2274444444444446e-06, 'epoch': 2.8416}
{'loss': 0.8803, 'grad_norm': 3.0443165527948315, 'learning_rate': 3.2263333333333333e-06, 'epoch': 2.842}
{'loss': 0.8847, 'grad_norm': 2.849039053369087, 'learning_rate': 3.2252222222222225e-06, 'epoch': 2.8424}
{'loss': 0.8867, 'grad_norm': 2.7563438989902567, 'learning_rate': 3.224111111111111e-06, 'epoch': 2.8428}
{'loss': 0.8862, 'grad_norm': 2.9036412169187424, 'learning_rate': 3.223e-06, 'epoch': 2.8432}
{'loss': 0.8675, 'grad_norm': 2.8377817635813978, 'learning_rate': 3.2220000000000002e-06, 'epoch': 2.8436}
{'loss': 0.8778, 'grad_norm': 2.994318967361439, 'learning_rate': 3.220888888888889e-06, 'epoch': 2.844}
{'eval_valid_loss': 0.85205078125, 'eval_valid_runtime': 0.0915, 'eval_valid_samples_per_second': 1092.628, 'eval_valid_steps_per_second': 273.157, 'epoch': 2.844}
{'loss': 0.8835, 'grad_norm': 2.8384533538845984, 'learning_rate': 3.2197777777777777e-06, 'epoch': 2.8444000000000003}
{'loss': 0.8769, 'grad_norm': 2.6899961921955655, 'learning_rate': 3.2186666666666672e-06, 'epoch': 2.8448}
{'loss': 0.8692, 'grad_norm': 2.915594598176199, 'learning_rate': 3.217555555555556e-06, 'epoch': 2.8452}
{'loss': 0.8804, 'grad_norm': 3.0433191335213823, 'learning_rate': 3.2164444444444446e-06, 'epoch': 2.8456}
{'loss': 0.8857, 'grad_norm': 2.992236642458877, 'learning_rate': 3.2153333333333338e-06, 'epoch': 2.846}
{'loss': 0.883, 'grad_norm': 2.8611788974425068, 'learning_rate': 3.2142222222222225e-06, 'epoch': 2.8464}
{'loss': 0.8803, 'grad_norm': 2.859273231237864, 'learning_rate': 3.213111111111111e-06, 'epoch': 2.8468}
{'loss': 0.8823, 'grad_norm': 2.966970432370737, 'learning_rate': 3.212e-06, 'epoch': 2.8472}
{'loss': 0.8747, 'grad_norm': 3.2095110896896983, 'learning_rate': 3.210888888888889e-06, 'epoch': 2.8476}
{'loss': 0.8738, 'grad_norm': 3.0540495789337605, 'learning_rate': 3.2097777777777778e-06, 'epoch': 2.848}
{'eval_valid_loss': 0.8515625, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.465, 'eval_valid_steps_per_second': 278.866, 'epoch': 2.848}
{'loss': 0.8801, 'grad_norm': 3.0288842594744647, 'learning_rate': 3.2086666666666673e-06, 'epoch': 2.8484}
{'loss': 0.8691, 'grad_norm': 2.8730685338929103, 'learning_rate': 3.207555555555556e-06, 'epoch': 2.8487999999999998}
{'loss': 0.8752, 'grad_norm': 2.8744084019056673, 'learning_rate': 3.2064444444444447e-06, 'epoch': 2.8491999999999997}
{'loss': 0.8625, 'grad_norm': 2.801756216948232, 'learning_rate': 3.2053333333333334e-06, 'epoch': 2.8496}
{'loss': 0.8835, 'grad_norm': 2.7645371609051486, 'learning_rate': 3.2042222222222226e-06, 'epoch': 2.85}
{'loss': 0.8874, 'grad_norm': 2.943760536361272, 'learning_rate': 3.2031111111111113e-06, 'epoch': 2.8504}
{'loss': 0.8821, 'grad_norm': 3.1696700996824694, 'learning_rate': 3.202e-06, 'epoch': 2.8508}
{'loss': 0.8776, 'grad_norm': 3.0275412021356485, 'learning_rate': 3.200888888888889e-06, 'epoch': 2.8512}
{'loss': 0.8871, 'grad_norm': 2.9360773618637737, 'learning_rate': 3.1997777777777783e-06, 'epoch': 2.8516}
{'loss': 0.8722, 'grad_norm': 2.9333680290821746, 'learning_rate': 3.198666666666667e-06, 'epoch': 2.852}
{'eval_valid_loss': 0.85107421875, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.754, 'eval_valid_steps_per_second': 277.188, 'epoch': 2.852}
{'loss': 0.8745, 'grad_norm': 2.894239475303929, 'learning_rate': 3.197555555555556e-06, 'epoch': 2.8524000000000003}
{'loss': 0.8793, 'grad_norm': 2.865881786260366, 'learning_rate': 3.196444444444445e-06, 'epoch': 2.8528000000000002}
{'loss': 0.8745, 'grad_norm': 3.244081775698781, 'learning_rate': 3.1953333333333335e-06, 'epoch': 2.8532}
{'loss': 0.8788, 'grad_norm': 2.87914710933007, 'learning_rate': 3.1942222222222223e-06, 'epoch': 2.8536}
{'loss': 0.8723, 'grad_norm': 2.906482564173318, 'learning_rate': 3.1931111111111114e-06, 'epoch': 2.854}
{'loss': 0.8719, 'grad_norm': 3.0288385060438547, 'learning_rate': 3.192e-06, 'epoch': 2.8544}
{'loss': 0.8828, 'grad_norm': 3.0543098996825453, 'learning_rate': 3.190888888888889e-06, 'epoch': 2.8548}
{'loss': 0.874, 'grad_norm': 2.857325392558926, 'learning_rate': 3.1897777777777784e-06, 'epoch': 2.8552}
{'loss': 0.8817, 'grad_norm': 3.047013294309278, 'learning_rate': 3.188666666666667e-06, 'epoch': 2.8556}
{'loss': 0.8748, 'grad_norm': 2.8848517940309715, 'learning_rate': 3.187555555555556e-06, 'epoch': 2.856}
{'eval_valid_loss': 0.8515625, 'eval_valid_runtime': 0.091, 'eval_valid_samples_per_second': 1098.978, 'eval_valid_steps_per_second': 274.744, 'epoch': 2.856}
{'loss': 0.8866, 'grad_norm': 2.755835929787098, 'learning_rate': 3.186444444444445e-06, 'epoch': 2.8564}
{'loss': 0.8818, 'grad_norm': 2.9928810072912144, 'learning_rate': 3.1853333333333336e-06, 'epoch': 2.8568}
{'loss': 0.8746, 'grad_norm': 2.8021551457436003, 'learning_rate': 3.1842222222222224e-06, 'epoch': 2.8571999999999997}
{'loss': 0.8862, 'grad_norm': 2.760504125013346, 'learning_rate': 3.183111111111111e-06, 'epoch': 2.8576}
{'loss': 0.8685, 'grad_norm': 2.833729554385688, 'learning_rate': 3.182e-06, 'epoch': 2.858}
{'loss': 0.8867, 'grad_norm': 3.0898207942143663, 'learning_rate': 3.180888888888889e-06, 'epoch': 2.8584}
{'loss': 0.8804, 'grad_norm': 2.9998047486980854, 'learning_rate': 3.1797777777777785e-06, 'epoch': 2.8588}
{'loss': 0.875, 'grad_norm': 3.0190367793464423, 'learning_rate': 3.178666666666667e-06, 'epoch': 2.8592}
{'loss': 0.8894, 'grad_norm': 2.719613491700788, 'learning_rate': 3.177555555555556e-06, 'epoch': 2.8596}
{'loss': 0.8741, 'grad_norm': 2.8848267111641555, 'learning_rate': 3.1764444444444446e-06, 'epoch': 2.86}
{'eval_valid_loss': 0.85205078125, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1116.76, 'eval_valid_steps_per_second': 279.19, 'epoch': 2.86}
{'loss': 0.8789, 'grad_norm': 3.0249574571565656, 'learning_rate': 3.1753333333333337e-06, 'epoch': 2.8604000000000003}
{'loss': 0.879, 'grad_norm': 2.8155490718938725, 'learning_rate': 3.1742222222222224e-06, 'epoch': 2.8608000000000002}
{'loss': 0.8757, 'grad_norm': 2.975212291548126, 'learning_rate': 3.173111111111111e-06, 'epoch': 2.8612}
{'loss': 0.8855, 'grad_norm': 3.1155023345981374, 'learning_rate': 3.172e-06, 'epoch': 2.8616}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.88, 'grad_norm': 2.8297624431693427, 'learning_rate': 3.170888888888889e-06, 'epoch': 2.862}
{'loss': 0.8812, 'grad_norm': 2.946440270842407, 'learning_rate': 3.169777777777778e-06, 'epoch': 2.8624}
{'loss': 0.8748, 'grad_norm': 2.9016297438907346, 'learning_rate': 3.1686666666666673e-06, 'epoch': 2.8628}
{'loss': 0.8771, 'grad_norm': 2.919691994079835, 'learning_rate': 3.167555555555556e-06, 'epoch': 2.8632}
{'loss': 0.8855, 'grad_norm': 2.874073661964197, 'learning_rate': 3.1664444444444447e-06, 'epoch': 2.8636}
{'loss': 0.8803, 'grad_norm': 3.116008114766853, 'learning_rate': 3.1653333333333334e-06, 'epoch': 2.864}
{'eval_valid_loss': 0.85205078125, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1128.639, 'eval_valid_steps_per_second': 282.16, 'epoch': 2.864}
{'loss': 0.8871, 'grad_norm': 2.7628397629083845, 'learning_rate': 3.1642222222222225e-06, 'epoch': 2.8644}
{'loss': 0.8719, 'grad_norm': 2.6059796634496832, 'learning_rate': 3.1631111111111113e-06, 'epoch': 2.8648}
{'loss': 0.8803, 'grad_norm': 2.6885225213802944, 'learning_rate': 3.162e-06, 'epoch': 2.8651999999999997}
{'loss': 0.8742, 'grad_norm': 2.9254209859410722, 'learning_rate': 3.160888888888889e-06, 'epoch': 2.8656}
{'loss': 0.8741, 'grad_norm': 2.6816879179382958, 'learning_rate': 3.1597777777777782e-06, 'epoch': 2.866}
{'loss': 0.8775, 'grad_norm': 2.9715030553738817, 'learning_rate': 3.158666666666667e-06, 'epoch': 2.8664}
{'loss': 0.8709, 'grad_norm': 2.9855829161101024, 'learning_rate': 3.157555555555556e-06, 'epoch': 2.8668}
{'loss': 0.881, 'grad_norm': 2.6995861007753574, 'learning_rate': 3.156444444444445e-06, 'epoch': 2.8672}
{'loss': 0.8841, 'grad_norm': 2.739714586113815, 'learning_rate': 3.1553333333333335e-06, 'epoch': 2.8676}
{'loss': 0.8721, 'grad_norm': 3.006859725060177, 'learning_rate': 3.1542222222222222e-06, 'epoch': 2.868}
{'eval_valid_loss': 0.85009765625, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1125.589, 'eval_valid_steps_per_second': 281.397, 'epoch': 2.868}
{'loss': 0.8764, 'grad_norm': 2.7867031215681863, 'learning_rate': 3.1531111111111113e-06, 'epoch': 2.8684}
{'loss': 0.8706, 'grad_norm': 2.6408478569477225, 'learning_rate': 3.152e-06, 'epoch': 2.8688000000000002}
{'loss': 0.8782, 'grad_norm': 2.897838057560905, 'learning_rate': 3.1508888888888888e-06, 'epoch': 2.8692}
{'loss': 0.8833, 'grad_norm': 2.8433353939268806, 'learning_rate': 3.1497777777777783e-06, 'epoch': 2.8696}
{'loss': 0.8828, 'grad_norm': 2.989861194090221, 'learning_rate': 3.148666666666667e-06, 'epoch': 2.87}
{'loss': 0.8813, 'grad_norm': 2.853407955031343, 'learning_rate': 3.1475555555555558e-06, 'epoch': 2.8704}
{'loss': 0.8708, 'grad_norm': 2.8059739055623356, 'learning_rate': 3.146444444444445e-06, 'epoch': 2.8708}
{'loss': 0.8769, 'grad_norm': 2.872861289559576, 'learning_rate': 3.1453333333333336e-06, 'epoch': 2.8712}
{'loss': 0.8764, 'grad_norm': 2.9097020351606924, 'learning_rate': 3.1442222222222223e-06, 'epoch': 2.8716}
{'loss': 0.8864, 'grad_norm': 3.2245271003342646, 'learning_rate': 3.143111111111111e-06, 'epoch': 2.872}
{'eval_valid_loss': 0.85009765625, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.583, 'eval_valid_steps_per_second': 278.146, 'epoch': 2.872}
{'loss': 0.8688, 'grad_norm': 2.911135423026311, 'learning_rate': 3.142e-06, 'epoch': 2.8724}
{'loss': 0.8773, 'grad_norm': 2.989623303207118, 'learning_rate': 3.140888888888889e-06, 'epoch': 2.8728}
{'loss': 0.8795, 'grad_norm': 3.0573058845583674, 'learning_rate': 3.1397777777777784e-06, 'epoch': 2.8731999999999998}
{'loss': 0.8797, 'grad_norm': 3.49918157511351, 'learning_rate': 3.138666666666667e-06, 'epoch': 2.8736}
{'loss': 0.8819, 'grad_norm': 3.18639435011262, 'learning_rate': 3.137555555555556e-06, 'epoch': 2.874}
{'loss': 0.8811, 'grad_norm': 2.815671541885669, 'learning_rate': 3.1364444444444446e-06, 'epoch': 2.8744}
{'loss': 0.875, 'grad_norm': 2.8802426263148457, 'learning_rate': 3.1353333333333337e-06, 'epoch': 2.8748}
{'loss': 0.8785, 'grad_norm': 3.091305923279998, 'learning_rate': 3.1342222222222224e-06, 'epoch': 2.8752}
{'loss': 0.8691, 'grad_norm': 2.7597805057888465, 'learning_rate': 3.133111111111111e-06, 'epoch': 2.8756}
{'loss': 0.8722, 'grad_norm': 2.927184009291318, 'learning_rate': 3.132e-06, 'epoch': 2.876}
{'eval_valid_loss': 0.8505859375, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1119.185, 'eval_valid_steps_per_second': 279.796, 'epoch': 2.876}
{'loss': 0.8937, 'grad_norm': 2.9488805513803893, 'learning_rate': 3.1308888888888894e-06, 'epoch': 2.8764}
{'loss': 0.8688, 'grad_norm': 2.7968851440927684, 'learning_rate': 3.129777777777778e-06, 'epoch': 2.8768000000000002}
{'loss': 0.8824, 'grad_norm': 2.9043914738469656, 'learning_rate': 3.1286666666666672e-06, 'epoch': 2.8772}
{'loss': 0.8822, 'grad_norm': 3.104072266408445, 'learning_rate': 3.127555555555556e-06, 'epoch': 2.8776}
{'loss': 0.8783, 'grad_norm': 3.119379061935309, 'learning_rate': 3.1264444444444447e-06, 'epoch': 2.878}
{'loss': 0.8719, 'grad_norm': 2.823187305995052, 'learning_rate': 3.1253333333333334e-06, 'epoch': 2.8784}
{'loss': 0.8696, 'grad_norm': 2.9679988513132014, 'learning_rate': 3.1242222222222225e-06, 'epoch': 2.8788}
{'loss': 0.8743, 'grad_norm': 3.1844007908428997, 'learning_rate': 3.1231111111111112e-06, 'epoch': 2.8792}
{'loss': 0.8856, 'grad_norm': 2.804568699947937, 'learning_rate': 3.122e-06, 'epoch': 2.8796}
{'loss': 0.8641, 'grad_norm': 2.9719260844143194, 'learning_rate': 3.1208888888888895e-06, 'epoch': 2.88}
{'eval_valid_loss': 0.8505859375, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1120.321, 'eval_valid_steps_per_second': 280.08, 'epoch': 2.88}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.8658, 'grad_norm': 2.9810945946197216, 'learning_rate': 3.119777777777778e-06, 'epoch': 2.8804}
{'loss': 0.8795, 'grad_norm': 2.9358168304640757, 'learning_rate': 3.118666666666667e-06, 'epoch': 2.8808}
{'loss': 0.8698, 'grad_norm': 2.9952594236985504, 'learning_rate': 3.117555555555556e-06, 'epoch': 2.8811999999999998}
{'loss': 0.8771, 'grad_norm': 2.9521410377604034, 'learning_rate': 3.1164444444444448e-06, 'epoch': 2.8816}
{'loss': 0.8882, 'grad_norm': 3.066068244333833, 'learning_rate': 3.1153333333333335e-06, 'epoch': 2.882}
{'loss': 0.8875, 'grad_norm': 3.3087513243208173, 'learning_rate': 3.114222222222222e-06, 'epoch': 2.8824}
{'loss': 0.8838, 'grad_norm': 2.813922268149738, 'learning_rate': 3.1131111111111113e-06, 'epoch': 2.8828}
{'loss': 0.8795, 'grad_norm': 2.917168503685649, 'learning_rate': 3.112e-06, 'epoch': 2.8832}
{'loss': 0.8894, 'grad_norm': 2.920638368405768, 'learning_rate': 3.111e-06, 'epoch': 2.8836}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8758, 'grad_norm': 3.0579706680333154, 'learning_rate': 3.109888888888889e-06, 'epoch': 2.884}
{'eval_valid_loss': 0.85009765625, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1121.349, 'eval_valid_steps_per_second': 280.337, 'epoch': 2.884}
{'loss': 0.8718, 'grad_norm': 2.9271638605754173, 'learning_rate': 3.108777777777778e-06, 'epoch': 2.8844}
{'loss': 0.8738, 'grad_norm': 2.5860181144757, 'learning_rate': 3.1076666666666673e-06, 'epoch': 2.8848000000000003}
{'loss': 0.8742, 'grad_norm': 3.0760445026488408, 'learning_rate': 3.106555555555556e-06, 'epoch': 2.8852}
{'loss': 0.8849, 'grad_norm': 2.9572059650282037, 'learning_rate': 3.1054444444444448e-06, 'epoch': 2.8856}
{'loss': 0.8768, 'grad_norm': 2.773740843181443, 'learning_rate': 3.1043333333333335e-06, 'epoch': 2.886}
{'loss': 0.8895, 'grad_norm': 3.0866305961176574, 'learning_rate': 3.1032222222222226e-06, 'epoch': 2.8864}
{'loss': 0.8828, 'grad_norm': 2.7986192219324537, 'learning_rate': 3.1021111111111113e-06, 'epoch': 2.8868}
{'loss': 0.8759, 'grad_norm': 2.750413841839801, 'learning_rate': 3.101e-06, 'epoch': 2.8872}
{'loss': 0.9007, 'grad_norm': 2.9772876747233963, 'learning_rate': 3.0998888888888888e-06, 'epoch': 2.8876}
{'loss': 0.8714, 'grad_norm': 2.798518449218497, 'learning_rate': 3.098777777777778e-06, 'epoch': 2.888}
{'eval_valid_loss': 0.85009765625, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1106.787, 'eval_valid_steps_per_second': 276.697, 'epoch': 2.888}
{'loss': 0.8766, 'grad_norm': 2.8079323978118325, 'learning_rate': 3.097666666666667e-06, 'epoch': 2.8884}
{'loss': 0.876, 'grad_norm': 2.9886447261583866, 'learning_rate': 3.096555555555556e-06, 'epoch': 2.8888}
{'loss': 0.8833, 'grad_norm': 2.922702437553103, 'learning_rate': 3.095444444444445e-06, 'epoch': 2.8891999999999998}
{'loss': 0.8652, 'grad_norm': 2.8636767126002707, 'learning_rate': 3.0943333333333336e-06, 'epoch': 2.8895999999999997}
{'loss': 0.8712, 'grad_norm': 3.1130769535746157, 'learning_rate': 3.0932222222222223e-06, 'epoch': 2.89}
{'loss': 0.8691, 'grad_norm': 2.664222058079432, 'learning_rate': 3.0921111111111114e-06, 'epoch': 2.8904}
{'loss': 0.8781, 'grad_norm': 2.6770149839460697, 'learning_rate': 3.091e-06, 'epoch': 2.8908}
{'loss': 0.8885, 'grad_norm': 2.870855972927907, 'learning_rate': 3.089888888888889e-06, 'epoch': 2.8912}
{'loss': 0.8668, 'grad_norm': 2.6528570945281795, 'learning_rate': 3.0887777777777776e-06, 'epoch': 2.8916}
{'loss': 0.8774, 'grad_norm': 3.0225904889913693, 'learning_rate': 3.087666666666667e-06, 'epoch': 2.892}
{'eval_valid_loss': 0.8515625, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.863, 'eval_valid_steps_per_second': 278.966, 'epoch': 2.892}
{'loss': 0.8893, 'grad_norm': 2.9853457924060507, 'learning_rate': 3.086555555555556e-06, 'epoch': 2.8924}
{'loss': 0.869, 'grad_norm': 2.9295309813333152, 'learning_rate': 3.085444444444445e-06, 'epoch': 2.8928000000000003}
{'loss': 0.8663, 'grad_norm': 3.051086732855627, 'learning_rate': 3.0843333333333337e-06, 'epoch': 2.8932}
{'loss': 0.8819, 'grad_norm': 2.7261725696147967, 'learning_rate': 3.0832222222222224e-06, 'epoch': 2.8936}
{'loss': 0.8764, 'grad_norm': 2.831495705315514, 'learning_rate': 3.082111111111111e-06, 'epoch': 2.894}
{'loss': 0.8732, 'grad_norm': 2.8471754581337865, 'learning_rate': 3.0810000000000002e-06, 'epoch': 2.8944}
{'loss': 0.8728, 'grad_norm': 2.9391434617276135, 'learning_rate': 3.079888888888889e-06, 'epoch': 2.8948}
{'loss': 0.8714, 'grad_norm': 2.7644042052387134, 'learning_rate': 3.0787777777777777e-06, 'epoch': 2.8952}
{'loss': 0.8795, 'grad_norm': 2.839411659373529, 'learning_rate': 3.0776666666666672e-06, 'epoch': 2.8956}
{'loss': 0.8761, 'grad_norm': 2.8779197980065545, 'learning_rate': 3.076555555555556e-06, 'epoch': 2.896}
{'eval_valid_loss': 0.85107421875, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1119.159, 'eval_valid_steps_per_second': 279.79, 'epoch': 2.896}
{'loss': 0.8724, 'grad_norm': 3.088165975475246, 'learning_rate': 3.0754444444444446e-06, 'epoch': 2.8964}
{'loss': 0.8945, 'grad_norm': 2.863957542498318, 'learning_rate': 3.0743333333333338e-06, 'epoch': 2.8968}
{'loss': 0.8825, 'grad_norm': 3.014375477189088, 'learning_rate': 3.0732222222222225e-06, 'epoch': 2.8971999999999998}
{'loss': 0.8864, 'grad_norm': 2.8788334696251483, 'learning_rate': 3.072111111111111e-06, 'epoch': 2.8975999999999997}
{'loss': 0.8673, 'grad_norm': 2.765801499548386, 'learning_rate': 3.071e-06, 'epoch': 2.898}
{'loss': 0.8864, 'grad_norm': 2.969891399009928, 'learning_rate': 3.069888888888889e-06, 'epoch': 2.8984}
{'loss': 0.8746, 'grad_norm': 3.1668879950290445, 'learning_rate': 3.068777777777778e-06, 'epoch': 2.8988}
{'loss': 0.8626, 'grad_norm': 2.801248147054023, 'learning_rate': 3.0676666666666673e-06, 'epoch': 2.8992}
{'loss': 0.8721, 'grad_norm': 2.6207674110432486, 'learning_rate': 3.066555555555556e-06, 'epoch': 2.8996}
{'loss': 0.8824, 'grad_norm': 3.1042870150455477, 'learning_rate': 3.0654444444444447e-06, 'epoch': 2.9}
{'eval_valid_loss': 0.85107421875, 'eval_valid_runtime': 0.1786, 'eval_valid_samples_per_second': 560.053, 'eval_valid_steps_per_second': 140.013, 'epoch': 2.9}
{'loss': 0.8795, 'grad_norm': 2.876159133686037, 'learning_rate': 3.0643333333333334e-06, 'epoch': 2.9004}
{'loss': 0.8703, 'grad_norm': 2.983092227086817, 'learning_rate': 3.0632222222222226e-06, 'epoch': 2.9008000000000003}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8786, 'grad_norm': 2.9119863963880306, 'learning_rate': 3.0621111111111113e-06, 'epoch': 2.9012000000000002}
{'loss': 0.868, 'grad_norm': 3.1129298376772234, 'learning_rate': 3.061e-06, 'epoch': 2.9016}
{'loss': 0.8745, 'grad_norm': 3.2306420534598557, 'learning_rate': 3.0598888888888887e-06, 'epoch': 2.902}
{'loss': 0.8755, 'grad_norm': 3.106225215498295, 'learning_rate': 3.0587777777777783e-06, 'epoch': 2.9024}
{'loss': 0.8707, 'grad_norm': 2.9712208253505796, 'learning_rate': 3.057666666666667e-06, 'epoch': 2.9028}
{'loss': 0.8821, 'grad_norm': 3.33866579575865, 'learning_rate': 3.056555555555556e-06, 'epoch': 2.9032}
{'loss': 0.8564, 'grad_norm': 2.7330028851487236, 'learning_rate': 3.055444444444445e-06, 'epoch': 2.9036}
{'loss': 0.865, 'grad_norm': 3.071670591008961, 'learning_rate': 3.0543333333333335e-06, 'epoch': 2.904}
{'eval_valid_loss': 0.85107421875, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.031, 'eval_valid_steps_per_second': 278.008, 'epoch': 2.904}
{'loss': 0.8834, 'grad_norm': 2.6019053591226844, 'learning_rate': 3.0532222222222222e-06, 'epoch': 2.9044}
{'loss': 0.867, 'grad_norm': 3.053157530565425, 'learning_rate': 3.0521111111111114e-06, 'epoch': 2.9048}
{'loss': 0.8893, 'grad_norm': 2.997203546758911, 'learning_rate': 3.051e-06, 'epoch': 2.9052}
{'loss': 0.8839, 'grad_norm': 2.6649860951327495, 'learning_rate': 3.049888888888889e-06, 'epoch': 2.9055999999999997}
{'loss': 0.8662, 'grad_norm': 3.184765905358948, 'learning_rate': 3.0487777777777784e-06, 'epoch': 2.906}
{'loss': 0.8838, 'grad_norm': 2.9966605892392044, 'learning_rate': 3.047666666666667e-06, 'epoch': 2.9064}
{'loss': 0.8719, 'grad_norm': 3.1226674148172915, 'learning_rate': 3.0465555555555558e-06, 'epoch': 2.9068}
{'loss': 0.8792, 'grad_norm': 3.1036178543221813, 'learning_rate': 3.045444444444445e-06, 'epoch': 2.9072}
{'loss': 0.8813, 'grad_norm': 2.8148272793108235, 'learning_rate': 3.0443333333333336e-06, 'epoch': 2.9076}
{'loss': 0.8758, 'grad_norm': 2.8848462878025316, 'learning_rate': 3.0432222222222223e-06, 'epoch': 2.908}
{'eval_valid_loss': 0.85009765625, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1120.049, 'eval_valid_steps_per_second': 280.012, 'epoch': 2.908}
{'loss': 0.871, 'grad_norm': 2.6954931005150082, 'learning_rate': 3.042111111111111e-06, 'epoch': 2.9084}
{'loss': 0.8859, 'grad_norm': 2.857868297901912, 'learning_rate': 3.041e-06, 'epoch': 2.9088000000000003}
{'loss': 0.878, 'grad_norm': 2.949283078736289, 'learning_rate': 3.039888888888889e-06, 'epoch': 2.9092000000000002}
{'loss': 0.8751, 'grad_norm': 2.8406674110175794, 'learning_rate': 3.0387777777777785e-06, 'epoch': 2.9096}
{'loss': 0.8861, 'grad_norm': 2.9108382277215945, 'learning_rate': 3.037666666666667e-06, 'epoch': 2.91}
{'loss': 0.8864, 'grad_norm': 3.037820773200252, 'learning_rate': 3.036555555555556e-06, 'epoch': 2.9104}
{'loss': 0.8635, 'grad_norm': 2.8428719915291825, 'learning_rate': 3.0354444444444446e-06, 'epoch': 2.9108}
{'loss': 0.8801, 'grad_norm': 3.0736940704444886, 'learning_rate': 3.0343333333333337e-06, 'epoch': 2.9112}
{'loss': 0.8747, 'grad_norm': 3.085380194837165, 'learning_rate': 3.0332222222222224e-06, 'epoch': 2.9116}
{'loss': 0.8774, 'grad_norm': 3.1820923617437487, 'learning_rate': 3.032111111111111e-06, 'epoch': 2.912}
{'eval_valid_loss': 0.85107421875, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.611, 'eval_valid_steps_per_second': 278.653, 'epoch': 2.912}
{'loss': 0.8734, 'grad_norm': 2.7791741478765433, 'learning_rate': 3.031e-06, 'epoch': 2.9124}
{'loss': 0.8825, 'grad_norm': 3.024497504310379, 'learning_rate': 3.029888888888889e-06, 'epoch': 2.9128}
{'loss': 0.8763, 'grad_norm': 2.940002937477616, 'learning_rate': 3.028777777777778e-06, 'epoch': 2.9132}
{'loss': 0.8694, 'grad_norm': 3.204002446272508, 'learning_rate': 3.0276666666666673e-06, 'epoch': 2.9135999999999997}
{'loss': 0.8796, 'grad_norm': 3.102464472684852, 'learning_rate': 3.026555555555556e-06, 'epoch': 2.914}
{'loss': 0.8868, 'grad_norm': 2.8241909864651786, 'learning_rate': 3.0254444444444447e-06, 'epoch': 2.9144}
{'loss': 0.8735, 'grad_norm': 2.777040644763785, 'learning_rate': 3.0243333333333334e-06, 'epoch': 2.9148}
{'loss': 0.8746, 'grad_norm': 3.2262485249784603, 'learning_rate': 3.0232222222222225e-06, 'epoch': 2.9152}
{'loss': 0.8734, 'grad_norm': 2.915121566141662, 'learning_rate': 3.0221111111111112e-06, 'epoch': 2.9156}
{'loss': 0.888, 'grad_norm': 3.0969972715288807, 'learning_rate': 3.021e-06, 'epoch': 2.916}
{'eval_valid_loss': 0.8505859375, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.614, 'eval_valid_steps_per_second': 279.154, 'epoch': 2.916}
{'loss': 0.8762, 'grad_norm': 2.9518478800836068, 'learning_rate': 3.0198888888888887e-06, 'epoch': 2.9164}
{'loss': 0.8829, 'grad_norm': 3.3706176174032745, 'learning_rate': 3.0187777777777782e-06, 'epoch': 2.9168}
{'loss': 0.8779, 'grad_norm': 2.741465117463069, 'learning_rate': 3.017666666666667e-06, 'epoch': 2.9172000000000002}
{'loss': 0.878, 'grad_norm': 3.229457766219977, 'learning_rate': 3.016555555555556e-06, 'epoch': 2.9176}
{'loss': 0.8743, 'grad_norm': 2.908491664737472, 'learning_rate': 3.0154444444444448e-06, 'epoch': 2.918}
{'loss': 0.8839, 'grad_norm': 2.9163739852285464, 'learning_rate': 3.0143333333333335e-06, 'epoch': 2.9184}
{'loss': 0.8836, 'grad_norm': 2.8531210102333344, 'learning_rate': 3.013222222222222e-06, 'epoch': 2.9188}
{'loss': 0.8786, 'grad_norm': 2.7667495036267353, 'learning_rate': 3.0121111111111113e-06, 'epoch': 2.9192}
{'loss': 0.8726, 'grad_norm': 2.9502374973709418, 'learning_rate': 3.011e-06, 'epoch': 2.9196}
{'loss': 0.8671, 'grad_norm': 2.8468776522811448, 'learning_rate': 3.0098888888888888e-06, 'epoch': 2.92}
{'eval_valid_loss': 0.85009765625, 'eval_valid_runtime': 0.0908, 'eval_valid_samples_per_second': 1100.893, 'eval_valid_steps_per_second': 275.223, 'epoch': 2.92}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.8754, 'grad_norm': 2.7698914437591986, 'learning_rate': 3.0087777777777783e-06, 'epoch': 2.9204}
{'loss': 0.8846, 'grad_norm': 2.933442946213888, 'learning_rate': 3.007666666666667e-06, 'epoch': 2.9208}
{'loss': 0.8739, 'grad_norm': 2.7149910811370845, 'learning_rate': 3.0065555555555557e-06, 'epoch': 2.9212}
{'loss': 0.8737, 'grad_norm': 2.957300987690292, 'learning_rate': 3.005444444444445e-06, 'epoch': 2.9215999999999998}
{'loss': 0.8747, 'grad_norm': 2.741061677800014, 'learning_rate': 3.0043333333333336e-06, 'epoch': 2.922}
{'loss': 0.8629, 'grad_norm': 3.123748776762663, 'learning_rate': 3.0032222222222223e-06, 'epoch': 2.9224}
{'loss': 0.8739, 'grad_norm': 3.0817558349240963, 'learning_rate': 3.002111111111111e-06, 'epoch': 2.9228}
{'loss': 0.8583, 'grad_norm': 2.6620731141383893, 'learning_rate': 3.001e-06, 'epoch': 2.9232}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8777, 'grad_norm': 2.7850613999233076, 'learning_rate': 3e-06, 'epoch': 2.9236}
{'loss': 0.8731, 'grad_norm': 2.830006416207376, 'learning_rate': 2.9988888888888888e-06, 'epoch': 2.924}
{'eval_valid_loss': 0.85009765625, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.871, 'eval_valid_steps_per_second': 278.468, 'epoch': 2.924}
{'loss': 0.8739, 'grad_norm': 3.0970670469230397, 'learning_rate': 2.997777777777778e-06, 'epoch': 2.9244}
{'loss': 0.8701, 'grad_norm': 2.9163580333812478, 'learning_rate': 2.996666666666667e-06, 'epoch': 2.9248}
{'loss': 0.8702, 'grad_norm': 3.0488152903861567, 'learning_rate': 2.995555555555556e-06, 'epoch': 2.9252000000000002}
{'loss': 0.8606, 'grad_norm': 2.920536754768074, 'learning_rate': 2.994444444444445e-06, 'epoch': 2.9256}
{'loss': 0.8758, 'grad_norm': 2.721624258166113, 'learning_rate': 2.9933333333333336e-06, 'epoch': 2.926}
{'loss': 0.8596, 'grad_norm': 2.887440540890529, 'learning_rate': 2.9922222222222223e-06, 'epoch': 2.9264}
{'loss': 0.8816, 'grad_norm': 2.9718642012984575, 'learning_rate': 2.9911111111111115e-06, 'epoch': 2.9268}
{'loss': 0.8904, 'grad_norm': 3.0391401511424085, 'learning_rate': 2.99e-06, 'epoch': 2.9272}
{'loss': 0.8695, 'grad_norm': 2.770822097760321, 'learning_rate': 2.988888888888889e-06, 'epoch': 2.9276}
{'loss': 0.8758, 'grad_norm': 2.830462275009898, 'learning_rate': 2.9877777777777776e-06, 'epoch': 2.928}
{'eval_valid_loss': 0.85009765625, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.188, 'eval_valid_steps_per_second': 279.297, 'epoch': 2.928}
{'loss': 0.8671, 'grad_norm': 2.614381829256736, 'learning_rate': 2.986666666666667e-06, 'epoch': 2.9284}
{'loss': 0.8735, 'grad_norm': 2.996641534217943, 'learning_rate': 2.985555555555556e-06, 'epoch': 2.9288}
{'loss': 0.8801, 'grad_norm': 3.1301973895972828, 'learning_rate': 2.984444444444445e-06, 'epoch': 2.9292}
{'loss': 0.8928, 'grad_norm': 2.870796240099565, 'learning_rate': 2.9833333333333337e-06, 'epoch': 2.9295999999999998}
{'loss': 0.892, 'grad_norm': 3.057124304969784, 'learning_rate': 2.9822222222222224e-06, 'epoch': 2.93}
{'loss': 0.8631, 'grad_norm': 2.8046160133623506, 'learning_rate': 2.981111111111111e-06, 'epoch': 2.9304}
{'loss': 0.8863, 'grad_norm': 3.2240215206989817, 'learning_rate': 2.9800000000000003e-06, 'epoch': 2.9308}
{'loss': 0.8687, 'grad_norm': 2.7581070696716665, 'learning_rate': 2.978888888888889e-06, 'epoch': 2.9312}
{'loss': 0.8797, 'grad_norm': 2.76631890767457, 'learning_rate': 2.9777777777777777e-06, 'epoch': 2.9316}
{'loss': 0.8611, 'grad_norm': 2.984457743830797, 'learning_rate': 2.9766666666666672e-06, 'epoch': 2.932}
{'eval_valid_loss': 0.85009765625, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.455, 'eval_valid_steps_per_second': 277.114, 'epoch': 2.932}
{'loss': 0.8747, 'grad_norm': 2.6960479506938753, 'learning_rate': 2.975555555555556e-06, 'epoch': 2.9324}
{'loss': 0.8674, 'grad_norm': 2.677477033340303, 'learning_rate': 2.9744444444444447e-06, 'epoch': 2.9328}
{'loss': 0.883, 'grad_norm': 2.876960800999736, 'learning_rate': 2.973333333333334e-06, 'epoch': 2.9332000000000003}
{'loss': 0.8737, 'grad_norm': 2.9101296340442104, 'learning_rate': 2.9722222222222225e-06, 'epoch': 2.9336}
{'loss': 0.8744, 'grad_norm': 3.0503755561025554, 'learning_rate': 2.9711111111111112e-06, 'epoch': 2.934}
{'loss': 0.87, 'grad_norm': 3.1036403239641337, 'learning_rate': 2.97e-06, 'epoch': 2.9344}
{'loss': 0.8724, 'grad_norm': 2.8824831766727756, 'learning_rate': 2.968888888888889e-06, 'epoch': 2.9348}
{'loss': 0.8808, 'grad_norm': 3.2509372533482583, 'learning_rate': 2.9677777777777778e-06, 'epoch': 2.9352}
{'loss': 0.8773, 'grad_norm': 2.9360196055919254, 'learning_rate': 2.9666666666666673e-06, 'epoch': 2.9356}
{'loss': 0.8771, 'grad_norm': 2.966893720019314, 'learning_rate': 2.965555555555556e-06, 'epoch': 2.936}
{'eval_valid_loss': 0.85009765625, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1120.965, 'eval_valid_steps_per_second': 280.241, 'epoch': 2.936}
{'loss': 0.8871, 'grad_norm': 2.926777760358463, 'learning_rate': 2.9644444444444448e-06, 'epoch': 2.9364}
{'loss': 0.8693, 'grad_norm': 3.071082351108339, 'learning_rate': 2.9633333333333335e-06, 'epoch': 2.9368}
{'loss': 0.8776, 'grad_norm': 2.8968303676981666, 'learning_rate': 2.9622222222222226e-06, 'epoch': 2.9372}
{'loss': 0.877, 'grad_norm': 2.96713146460133, 'learning_rate': 2.9611111111111113e-06, 'epoch': 2.9375999999999998}
{'loss': 0.8801, 'grad_norm': 2.9492994082707438, 'learning_rate': 2.96e-06, 'epoch': 2.9379999999999997}
{'loss': 0.883, 'grad_norm': 2.8843011191547223, 'learning_rate': 2.9588888888888887e-06, 'epoch': 2.9384}
{'loss': 0.8815, 'grad_norm': 2.857364124645369, 'learning_rate': 2.957777777777778e-06, 'epoch': 2.9388}
{'loss': 0.8763, 'grad_norm': 2.976741745325191, 'learning_rate': 2.956666666666667e-06, 'epoch': 2.9392}
{'loss': 0.8765, 'grad_norm': 2.802643309918118, 'learning_rate': 2.955555555555556e-06, 'epoch': 2.9396}
{'loss': 0.887, 'grad_norm': 2.9510631023687277, 'learning_rate': 2.954444444444445e-06, 'epoch': 2.94}
{'eval_valid_loss': 0.849609375, 'eval_valid_runtime': 0.0908, 'eval_valid_samples_per_second': 1101.59, 'eval_valid_steps_per_second': 275.398, 'epoch': 2.94}
{'loss': 0.8798, 'grad_norm': 2.8493116927932425, 'learning_rate': 2.9533333333333336e-06, 'epoch': 2.9404}
{'loss': 0.8754, 'grad_norm': 2.625267287678836, 'learning_rate': 2.9522222222222223e-06, 'epoch': 2.9408}
{'loss': 0.8817, 'grad_norm': 2.879435528118378, 'learning_rate': 2.9511111111111114e-06, 'epoch': 2.9412000000000003}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8892, 'grad_norm': 2.9554785578799687, 'learning_rate': 2.95e-06, 'epoch': 2.9416}
{'loss': 0.8798, 'grad_norm': 2.9474765810792443, 'learning_rate': 2.948888888888889e-06, 'epoch': 2.942}
{'loss': 0.8716, 'grad_norm': 3.0900169835194875, 'learning_rate': 2.9477777777777784e-06, 'epoch': 2.9424}
{'loss': 0.8777, 'grad_norm': 3.0365418127677657, 'learning_rate': 2.946666666666667e-06, 'epoch': 2.9428}
{'loss': 0.8646, 'grad_norm': 2.8425337946953753, 'learning_rate': 2.945555555555556e-06, 'epoch': 2.9432}
{'loss': 0.8529, 'grad_norm': 3.0520674354651276, 'learning_rate': 2.944444444444445e-06, 'epoch': 2.9436}
{'loss': 0.8812, 'grad_norm': 3.1544319997844847, 'learning_rate': 2.9433333333333337e-06, 'epoch': 2.944}
{'eval_valid_loss': 0.84912109375, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1106.516, 'eval_valid_steps_per_second': 276.629, 'epoch': 2.944}
{'loss': 0.8894, 'grad_norm': 2.966245392805043, 'learning_rate': 2.9422222222222224e-06, 'epoch': 2.9444}
{'loss': 0.8882, 'grad_norm': 3.0321817833977187, 'learning_rate': 2.941111111111111e-06, 'epoch': 2.9448}
{'loss': 0.8783, 'grad_norm': 3.0377982679947433, 'learning_rate': 2.9400000000000002e-06, 'epoch': 2.9452}
{'loss': 0.8828, 'grad_norm': 2.863241281269468, 'learning_rate': 2.938888888888889e-06, 'epoch': 2.9455999999999998}
{'loss': 0.8871, 'grad_norm': 2.903869571818487, 'learning_rate': 2.937777777777778e-06, 'epoch': 2.9459999999999997}
{'loss': 0.8878, 'grad_norm': 2.767530474525147, 'learning_rate': 2.936666666666667e-06, 'epoch': 2.9464}
{'loss': 0.8713, 'grad_norm': 2.7963325624465636, 'learning_rate': 2.935555555555556e-06, 'epoch': 2.9468}
{'loss': 0.8683, 'grad_norm': 2.8545412052212096, 'learning_rate': 2.9344444444444446e-06, 'epoch': 2.9472}
{'loss': 0.8749, 'grad_norm': 2.9288731676676925, 'learning_rate': 2.9333333333333338e-06, 'epoch': 2.9476}
{'loss': 0.8712, 'grad_norm': 2.8161305730448682, 'learning_rate': 2.9322222222222225e-06, 'epoch': 2.948}
{'eval_valid_loss': 0.84912109375, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1116.766, 'eval_valid_steps_per_second': 279.191, 'epoch': 2.948}
{'loss': 0.888, 'grad_norm': 2.860246963125263, 'learning_rate': 2.931111111111111e-06, 'epoch': 2.9484}
{'loss': 0.8792, 'grad_norm': 3.0510734193520803, 'learning_rate': 2.93e-06, 'epoch': 2.9488}
{'loss': 0.8764, 'grad_norm': 2.6773752184516515, 'learning_rate': 2.928888888888889e-06, 'epoch': 2.9492000000000003}
{'loss': 0.8845, 'grad_norm': 2.729281165849097, 'learning_rate': 2.927777777777778e-06, 'epoch': 2.9496}
{'loss': 0.8848, 'grad_norm': 2.8571503849867015, 'learning_rate': 2.9266666666666673e-06, 'epoch': 2.95}
{'loss': 0.8745, 'grad_norm': 2.810129496510948, 'learning_rate': 2.925555555555556e-06, 'epoch': 2.9504}
{'loss': 0.8765, 'grad_norm': 2.7110270339735867, 'learning_rate': 2.9244444444444447e-06, 'epoch': 2.9508}
{'loss': 0.8766, 'grad_norm': 2.7678321178025733, 'learning_rate': 2.9233333333333334e-06, 'epoch': 2.9512}
{'loss': 0.8672, 'grad_norm': 2.546521460679586, 'learning_rate': 2.9222222222222226e-06, 'epoch': 2.9516}
{'loss': 0.8816, 'grad_norm': 3.0919116174223307, 'learning_rate': 2.9211111111111113e-06, 'epoch': 2.952}
{'eval_valid_loss': 0.849609375, 'eval_valid_runtime': 0.089, 'eval_valid_samples_per_second': 1122.988, 'eval_valid_steps_per_second': 280.747, 'epoch': 2.952}
{'loss': 0.8807, 'grad_norm': 2.9385402440542414, 'learning_rate': 2.92e-06, 'epoch': 2.9524}
{'loss': 0.8768, 'grad_norm': 2.9090512117033054, 'learning_rate': 2.9188888888888887e-06, 'epoch': 2.9528}
{'loss': 0.8942, 'grad_norm': 3.0601493221824905, 'learning_rate': 2.9177777777777783e-06, 'epoch': 2.9532}
{'loss': 0.8819, 'grad_norm': 2.757202232206474, 'learning_rate': 2.916666666666667e-06, 'epoch': 2.9536}
{'loss': 0.875, 'grad_norm': 2.8931305825885634, 'learning_rate': 2.915555555555556e-06, 'epoch': 2.9539999999999997}
{'loss': 0.8784, 'grad_norm': 2.895580189203987, 'learning_rate': 2.914444444444445e-06, 'epoch': 2.9544}
{'loss': 0.8833, 'grad_norm': 2.5974199345303455, 'learning_rate': 2.9133333333333335e-06, 'epoch': 2.9548}
{'loss': 0.878, 'grad_norm': 2.989714295091849, 'learning_rate': 2.9122222222222222e-06, 'epoch': 2.9552}
{'loss': 0.8851, 'grad_norm': 2.8355347318451027, 'learning_rate': 2.9111111111111114e-06, 'epoch': 2.9556}
{'loss': 0.8687, 'grad_norm': 2.911458444794372, 'learning_rate': 2.91e-06, 'epoch': 2.956}
{'eval_valid_loss': 0.84814453125, 'eval_valid_runtime': 0.0885, 'eval_valid_samples_per_second': 1129.493, 'eval_valid_steps_per_second': 282.373, 'epoch': 2.956}
{'loss': 0.8629, 'grad_norm': 3.1239902772422448, 'learning_rate': 2.908888888888889e-06, 'epoch': 2.9564}
{'loss': 0.8846, 'grad_norm': 2.9916343177615556, 'learning_rate': 2.9077777777777784e-06, 'epoch': 2.9568}
{'loss': 0.8683, 'grad_norm': 3.0810369089039855, 'learning_rate': 2.906666666666667e-06, 'epoch': 2.9572000000000003}
{'loss': 0.8849, 'grad_norm': 2.885535920544368, 'learning_rate': 2.9055555555555558e-06, 'epoch': 2.9576000000000002}
{'loss': 0.8862, 'grad_norm': 2.99550754228501, 'learning_rate': 2.904444444444445e-06, 'epoch': 2.958}
{'loss': 0.8729, 'grad_norm': 3.0563780260477107, 'learning_rate': 2.9033333333333336e-06, 'epoch': 2.9584}
{'loss': 0.8694, 'grad_norm': 2.7776403040776114, 'learning_rate': 2.9022222222222223e-06, 'epoch': 2.9588}
{'loss': 0.8725, 'grad_norm': 3.1510109051340462, 'learning_rate': 2.901111111111111e-06, 'epoch': 2.9592}
{'loss': 0.8755, 'grad_norm': 2.6819393827826405, 'learning_rate': 2.9e-06, 'epoch': 2.9596}
{'loss': 0.8734, 'grad_norm': 3.0332053750073404, 'learning_rate': 2.898888888888889e-06, 'epoch': 2.96}
{'eval_valid_loss': 0.849609375, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1111.236, 'eval_valid_steps_per_second': 277.809, 'epoch': 2.96}
{'loss': 0.8713, 'grad_norm': 3.1477934318183833, 'learning_rate': 2.8977777777777785e-06, 'epoch': 2.9604}
{'loss': 0.8625, 'grad_norm': 2.9512572967264594, 'learning_rate': 2.896666666666667e-06, 'epoch': 2.9608}
{'loss': 0.8692, 'grad_norm': 3.045355902294511, 'learning_rate': 2.895555555555556e-06, 'epoch': 2.9612}
{'loss': 0.8787, 'grad_norm': 2.648471179399176, 'learning_rate': 2.8944444444444446e-06, 'epoch': 2.9616}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8789, 'grad_norm': 3.030429965127772, 'learning_rate': 2.8933333333333337e-06, 'epoch': 2.9619999999999997}
{'loss': 0.8639, 'grad_norm': 2.943109654880521, 'learning_rate': 2.8922222222222224e-06, 'epoch': 2.9624}
{'loss': 0.8797, 'grad_norm': 3.2211074343993666, 'learning_rate': 2.891111111111111e-06, 'epoch': 2.9628}
{'loss': 0.874, 'grad_norm': 2.7252875082337398, 'learning_rate': 2.89e-06, 'epoch': 2.9632}
{'loss': 0.8769, 'grad_norm': 2.903098730824582, 'learning_rate': 2.888888888888889e-06, 'epoch': 2.9636}
{'loss': 0.8799, 'grad_norm': 2.839432824377525, 'learning_rate': 2.887888888888889e-06, 'epoch': 2.964}
{'eval_valid_loss': 0.849609375, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1109.669, 'eval_valid_steps_per_second': 277.417, 'epoch': 2.964}
{'loss': 0.8742, 'grad_norm': 3.1393365731793375, 'learning_rate': 2.8867777777777776e-06, 'epoch': 2.9644}
{'loss': 0.8807, 'grad_norm': 2.842908210665409, 'learning_rate': 2.885666666666667e-06, 'epoch': 2.9648}
{'loss': 0.8575, 'grad_norm': 2.975384035888203, 'learning_rate': 2.884555555555556e-06, 'epoch': 2.9652}
{'loss': 0.8638, 'grad_norm': 3.1775307241315525, 'learning_rate': 2.883444444444445e-06, 'epoch': 2.9656000000000002}
{'loss': 0.8767, 'grad_norm': 2.7048304034671724, 'learning_rate': 2.8823333333333337e-06, 'epoch': 2.966}
{'loss': 0.8557, 'grad_norm': 2.817664173694487, 'learning_rate': 2.8812222222222225e-06, 'epoch': 2.9664}
{'loss': 0.878, 'grad_norm': 3.0145190291190866, 'learning_rate': 2.880111111111111e-06, 'epoch': 2.9668}
{'loss': 0.8731, 'grad_norm': 2.8004104975387567, 'learning_rate': 2.8790000000000003e-06, 'epoch': 2.9672}
{'loss': 0.874, 'grad_norm': 3.1574196867042668, 'learning_rate': 2.877888888888889e-06, 'epoch': 2.9676}
{'loss': 0.8858, 'grad_norm': 2.8867953939419357, 'learning_rate': 2.8767777777777777e-06, 'epoch': 2.968}
{'eval_valid_loss': 0.849609375, 'eval_valid_runtime': 0.0905, 'eval_valid_samples_per_second': 1104.383, 'eval_valid_steps_per_second': 276.096, 'epoch': 2.968}
{'loss': 0.8928, 'grad_norm': 2.8475255949295146, 'learning_rate': 2.8756666666666673e-06, 'epoch': 2.9684}
{'loss': 0.8766, 'grad_norm': 3.023780763839402, 'learning_rate': 2.874555555555556e-06, 'epoch': 2.9688}
{'loss': 0.8734, 'grad_norm': 2.7882203291856285, 'learning_rate': 2.8734444444444447e-06, 'epoch': 2.9692}
{'loss': 0.8794, 'grad_norm': 3.0214471303053556, 'learning_rate': 2.872333333333334e-06, 'epoch': 2.9696}
{'loss': 0.8799, 'grad_norm': 3.1636439576420328, 'learning_rate': 2.8712222222222226e-06, 'epoch': 2.9699999999999998}
{'loss': 0.8704, 'grad_norm': 3.1277487300821134, 'learning_rate': 2.8701111111111113e-06, 'epoch': 2.9704}
{'loss': 0.8859, 'grad_norm': 3.0288752662740754, 'learning_rate': 2.869e-06, 'epoch': 2.9708}
{'loss': 0.8812, 'grad_norm': 2.796946322208348, 'learning_rate': 2.867888888888889e-06, 'epoch': 2.9712}
{'loss': 0.8691, 'grad_norm': 2.7849983020739626, 'learning_rate': 2.866777777777778e-06, 'epoch': 2.9716}
{'loss': 0.8755, 'grad_norm': 2.80543377970542, 'learning_rate': 2.865666666666667e-06, 'epoch': 2.972}
{'eval_valid_loss': 0.84912109375, 'eval_valid_runtime': 0.0915, 'eval_valid_samples_per_second': 1093.32, 'eval_valid_steps_per_second': 273.33, 'epoch': 2.972}
{'loss': 0.863, 'grad_norm': 2.893864306423006, 'learning_rate': 2.864555555555556e-06, 'epoch': 2.9724}
{'loss': 0.8651, 'grad_norm': 3.0484431119600277, 'learning_rate': 2.863444444444445e-06, 'epoch': 2.9728}
{'loss': 0.875, 'grad_norm': 3.075177102687655, 'learning_rate': 2.8623333333333335e-06, 'epoch': 2.9732}
{'loss': 0.8685, 'grad_norm': 2.9169426611471962, 'learning_rate': 2.8612222222222226e-06, 'epoch': 2.9736000000000002}
{'loss': 0.8734, 'grad_norm': 2.84117202968158, 'learning_rate': 2.8601111111111114e-06, 'epoch': 2.974}
{'loss': 0.863, 'grad_norm': 2.9811799287058682, 'learning_rate': 2.859e-06, 'epoch': 2.9744}
{'loss': 0.8882, 'grad_norm': 2.815741324271075, 'learning_rate': 2.8578888888888888e-06, 'epoch': 2.9748}
{'loss': 0.8669, 'grad_norm': 3.007956791178709, 'learning_rate': 2.856777777777778e-06, 'epoch': 2.9752}
{'loss': 0.8785, 'grad_norm': 2.9500435995662397, 'learning_rate': 2.855666666666667e-06, 'epoch': 2.9756}
{'loss': 0.8795, 'grad_norm': 2.8977681335587446, 'learning_rate': 2.8545555555555558e-06, 'epoch': 2.976}
{'eval_valid_loss': 0.849609375, 'eval_valid_runtime': 0.0912, 'eval_valid_samples_per_second': 1096.473, 'eval_valid_steps_per_second': 274.118, 'epoch': 2.976}
{'loss': 0.8813, 'grad_norm': 3.060118703098097, 'learning_rate': 2.853444444444445e-06, 'epoch': 2.9764}
{'loss': 0.8828, 'grad_norm': 2.647981317442711, 'learning_rate': 2.8523333333333336e-06, 'epoch': 2.9768}
{'loss': 0.872, 'grad_norm': 2.8239876638080896, 'learning_rate': 2.8512222222222223e-06, 'epoch': 2.9772}
{'loss': 0.8796, 'grad_norm': 2.7786683573550284, 'learning_rate': 2.8501111111111115e-06, 'epoch': 2.9776}
{'loss': 0.868, 'grad_norm': 2.788174757736863, 'learning_rate': 2.849e-06, 'epoch': 2.9779999999999998}
{'loss': 0.872, 'grad_norm': 3.194931772396057, 'learning_rate': 2.847888888888889e-06, 'epoch': 2.9784}
{'loss': 0.879, 'grad_norm': 2.899596067570467, 'learning_rate': 2.8467777777777776e-06, 'epoch': 2.9788}
{'loss': 0.8728, 'grad_norm': 3.0138250721173048, 'learning_rate': 2.845666666666667e-06, 'epoch': 2.9792}
{'loss': 0.8747, 'grad_norm': 2.9387571302949698, 'learning_rate': 2.844555555555556e-06, 'epoch': 2.9796}
{'loss': 0.8771, 'grad_norm': 2.9076009502253473, 'learning_rate': 2.843444444444445e-06, 'epoch': 2.98}
{'eval_valid_loss': 0.8486328125, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1122.051, 'eval_valid_steps_per_second': 280.513, 'epoch': 2.98}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8774, 'grad_norm': 2.94018958108168, 'learning_rate': 2.8423333333333337e-06, 'epoch': 2.9804}
{'loss': 0.8693, 'grad_norm': 2.84676590489579, 'learning_rate': 2.8412222222222224e-06, 'epoch': 2.9808}
{'loss': 0.87, 'grad_norm': 2.877489903570669, 'learning_rate': 2.840111111111111e-06, 'epoch': 2.9812}
{'loss': 0.8855, 'grad_norm': 2.952166840864709, 'learning_rate': 2.8390000000000003e-06, 'epoch': 2.9816000000000003}
{'loss': 0.8847, 'grad_norm': 3.03266719251867, 'learning_rate': 2.837888888888889e-06, 'epoch': 2.982}
{'loss': 0.8761, 'grad_norm': 2.8756403002697755, 'learning_rate': 2.8367777777777777e-06, 'epoch': 2.9824}
{'loss': 0.8829, 'grad_norm': 2.8548709332308424, 'learning_rate': 2.8356666666666672e-06, 'epoch': 2.9828}
{'loss': 0.872, 'grad_norm': 2.9225002476616226, 'learning_rate': 2.834555555555556e-06, 'epoch': 2.9832}
{'loss': 0.8801, 'grad_norm': 2.9532797132059896, 'learning_rate': 2.8334444444444447e-06, 'epoch': 2.9836}
{'loss': 0.8774, 'grad_norm': 2.7883004288081947, 'learning_rate': 2.832333333333334e-06, 'epoch': 2.984}
{'eval_valid_loss': 0.8486328125, 'eval_valid_runtime': 0.0908, 'eval_valid_samples_per_second': 1101.512, 'eval_valid_steps_per_second': 275.378, 'epoch': 2.984}
{'loss': 0.8854, 'grad_norm': 2.8164117736338974, 'learning_rate': 2.8312222222222225e-06, 'epoch': 2.9844}
{'loss': 0.8663, 'grad_norm': 2.648698225663003, 'learning_rate': 2.8301111111111112e-06, 'epoch': 2.9848}
{'loss': 0.8758, 'grad_norm': 3.0069277267099004, 'learning_rate': 2.829e-06, 'epoch': 2.9852}
{'loss': 0.8752, 'grad_norm': 2.9995917698149417, 'learning_rate': 2.827888888888889e-06, 'epoch': 2.9856}
{'loss': 0.8838, 'grad_norm': 2.9975168958797025, 'learning_rate': 2.8267777777777778e-06, 'epoch': 2.9859999999999998}
{'loss': 0.8675, 'grad_norm': 2.841812343489768, 'learning_rate': 2.825666666666667e-06, 'epoch': 2.9864}
{'loss': 0.8814, 'grad_norm': 3.099174021846957, 'learning_rate': 2.824555555555556e-06, 'epoch': 2.9868}
{'loss': 0.8711, 'grad_norm': 2.868671771044589, 'learning_rate': 2.8234444444444448e-06, 'epoch': 2.9872}
{'loss': 0.8733, 'grad_norm': 3.077991589206287, 'learning_rate': 2.8223333333333335e-06, 'epoch': 2.9876}
{'loss': 0.8774, 'grad_norm': 2.706667227441592, 'learning_rate': 2.8212222222222226e-06, 'epoch': 2.988}
{'eval_valid_loss': 0.849609375, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1106.317, 'eval_valid_steps_per_second': 276.579, 'epoch': 2.988}
{'loss': 0.8651, 'grad_norm': 2.547920848976086, 'learning_rate': 2.8201111111111113e-06, 'epoch': 2.9884}
{'loss': 0.8823, 'grad_norm': 3.062389537220429, 'learning_rate': 2.819e-06, 'epoch': 2.9888}
{'loss': 0.869, 'grad_norm': 2.91749292091681, 'learning_rate': 2.8178888888888887e-06, 'epoch': 2.9892}
{'loss': 0.8661, 'grad_norm': 2.7823677924246337, 'learning_rate': 2.8167777777777783e-06, 'epoch': 2.9896000000000003}
{'loss': 0.8914, 'grad_norm': 2.917616336483925, 'learning_rate': 2.815666666666667e-06, 'epoch': 2.99}
{'loss': 0.8651, 'grad_norm': 2.955721415146846, 'learning_rate': 2.8145555555555557e-06, 'epoch': 2.9904}
{'loss': 0.8752, 'grad_norm': 2.84200302372583, 'learning_rate': 2.813444444444445e-06, 'epoch': 2.9908}
{'loss': 0.8932, 'grad_norm': 3.0140455191110664, 'learning_rate': 2.8123333333333336e-06, 'epoch': 2.9912}
{'loss': 0.866, 'grad_norm': 2.9820749896756573, 'learning_rate': 2.8112222222222223e-06, 'epoch': 2.9916}
{'loss': 0.8721, 'grad_norm': 2.910253433550587, 'learning_rate': 2.8101111111111114e-06, 'epoch': 2.992}
{'eval_valid_loss': 0.84912109375, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.941, 'eval_valid_steps_per_second': 279.485, 'epoch': 2.992}
{'loss': 0.8779, 'grad_norm': 2.9099883267899345, 'learning_rate': 2.809e-06, 'epoch': 2.9924}
{'loss': 0.8752, 'grad_norm': 3.022032250984602, 'learning_rate': 2.807888888888889e-06, 'epoch': 2.9928}
{'loss': 0.8708, 'grad_norm': 2.883719118215016, 'learning_rate': 2.8067777777777784e-06, 'epoch': 2.9932}
{'loss': 0.8816, 'grad_norm': 2.8627188886218318, 'learning_rate': 2.805666666666667e-06, 'epoch': 2.9936}
{'loss': 0.8796, 'grad_norm': 3.011679999155392, 'learning_rate': 2.804555555555556e-06, 'epoch': 2.9939999999999998}
{'loss': 0.9021, 'grad_norm': 2.9824810109649142, 'learning_rate': 2.803444444444445e-06, 'epoch': 2.9943999999999997}
{'loss': 0.8748, 'grad_norm': 2.9478117864618616, 'learning_rate': 2.8023333333333337e-06, 'epoch': 2.9948}
{'loss': 0.8643, 'grad_norm': 2.8978177253694954, 'learning_rate': 2.8012222222222224e-06, 'epoch': 2.9952}
{'loss': 0.8736, 'grad_norm': 2.7748018374441505, 'learning_rate': 2.800111111111111e-06, 'epoch': 2.9956}
{'loss': 0.8761, 'grad_norm': 2.685733919045645, 'learning_rate': 2.7990000000000002e-06, 'epoch': 2.996}
{'eval_valid_loss': 0.8486328125, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1115.34, 'eval_valid_steps_per_second': 278.835, 'epoch': 2.996}
{'loss': 0.8668, 'grad_norm': 2.8650921121239796, 'learning_rate': 2.797888888888889e-06, 'epoch': 2.9964}
{'loss': 0.866, 'grad_norm': 2.8027326683544316, 'learning_rate': 2.796777777777778e-06, 'epoch': 2.9968}
{'loss': 0.8621, 'grad_norm': 2.763954261398584, 'learning_rate': 2.795666666666667e-06, 'epoch': 2.9972}
{'loss': 0.874, 'grad_norm': 2.959818636180479, 'learning_rate': 2.794555555555556e-06, 'epoch': 2.9976000000000003}
{'loss': 0.8803, 'grad_norm': 2.899582700877763, 'learning_rate': 2.7934444444444446e-06, 'epoch': 2.998}
{'loss': 0.8717, 'grad_norm': 2.6623839742605178, 'learning_rate': 2.7923333333333338e-06, 'epoch': 2.9984}
{'loss': 0.8793, 'grad_norm': 3.0179953178719567, 'learning_rate': 2.7912222222222225e-06, 'epoch': 2.9988}
{'loss': 0.8822, 'grad_norm': 2.7306481167405665, 'learning_rate': 2.790111111111111e-06, 'epoch': 2.9992}
{'loss': 0.8677, 'grad_norm': 2.731247661205486, 'learning_rate': 2.789e-06, 'epoch': 2.9996}
{'loss': 0.8834, 'grad_norm': 3.1880796690789666, 'learning_rate': 2.787888888888889e-06, 'epoch': 3.0}
{'eval_valid_loss': 0.8486328125, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.654, 'eval_valid_steps_per_second': 277.163, 'epoch': 3.0}
{'loss': 0.8715, 'grad_norm': 2.791135519774703, 'learning_rate': 2.786777777777778e-06, 'epoch': 3.0004}
{'loss': 0.8768, 'grad_norm': 3.124512252891031, 'learning_rate': 2.785666666666667e-06, 'epoch': 3.0008}
{'loss': 0.8735, 'grad_norm': 3.231627509017771, 'learning_rate': 2.784555555555556e-06, 'epoch': 3.0012}
{'loss': 0.8667, 'grad_norm': 3.0077238391838437, 'learning_rate': 2.7834444444444447e-06, 'epoch': 3.0016}
{'loss': 0.8654, 'grad_norm': 2.6993801107356026, 'learning_rate': 2.7823333333333334e-06, 'epoch': 3.002}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8727, 'grad_norm': 2.903277276544884, 'learning_rate': 2.7812222222222226e-06, 'epoch': 3.0024}
{'loss': 0.8723, 'grad_norm': 2.9173044165958513, 'learning_rate': 2.7801111111111113e-06, 'epoch': 3.0028}
{'loss': 0.8727, 'grad_norm': 3.1216655966452582, 'learning_rate': 2.779e-06, 'epoch': 3.0032}
{'loss': 0.8704, 'grad_norm': 2.99336017507577, 'learning_rate': 2.7778888888888887e-06, 'epoch': 3.0036}
{'loss': 0.8733, 'grad_norm': 2.888322284170707, 'learning_rate': 2.776888888888889e-06, 'epoch': 3.004}
{'eval_valid_loss': 0.8486328125, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1126.115, 'eval_valid_steps_per_second': 281.529, 'epoch': 3.004}
{'loss': 0.8765, 'grad_norm': 2.854590070718671, 'learning_rate': 2.7757777777777778e-06, 'epoch': 3.0044}
{'loss': 0.8747, 'grad_norm': 2.913594297784502, 'learning_rate': 2.7746666666666665e-06, 'epoch': 3.0048}
{'loss': 0.8838, 'grad_norm': 3.134213094057232, 'learning_rate': 2.773555555555556e-06, 'epoch': 3.0052}
{'loss': 0.8647, 'grad_norm': 2.765150444351019, 'learning_rate': 2.7724444444444447e-06, 'epoch': 3.0056}
{'loss': 0.8729, 'grad_norm': 2.9713823995271404, 'learning_rate': 2.7713333333333335e-06, 'epoch': 3.006}
{'loss': 0.8633, 'grad_norm': 3.078161307183707, 'learning_rate': 2.7702222222222226e-06, 'epoch': 3.0064}
{'loss': 0.8651, 'grad_norm': 3.0488045769123597, 'learning_rate': 2.7691111111111113e-06, 'epoch': 3.0068}
{'loss': 0.8911, 'grad_norm': 3.0853556024010946, 'learning_rate': 2.768e-06, 'epoch': 3.0072}
{'loss': 0.8766, 'grad_norm': 3.0878396943903046, 'learning_rate': 2.766888888888889e-06, 'epoch': 3.0076}
{'loss': 0.8721, 'grad_norm': 3.126241904488095, 'learning_rate': 2.765777777777778e-06, 'epoch': 3.008}
{'eval_valid_loss': 0.84912109375, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1106.51, 'eval_valid_steps_per_second': 276.628, 'epoch': 3.008}
{'loss': 0.8728, 'grad_norm': 2.859303499588989, 'learning_rate': 2.764666666666667e-06, 'epoch': 3.0084}
{'loss': 0.8792, 'grad_norm': 2.9234919128573607, 'learning_rate': 2.763555555555556e-06, 'epoch': 3.0088}
{'loss': 0.8869, 'grad_norm': 2.780901522718226, 'learning_rate': 2.762444444444445e-06, 'epoch': 3.0092}
{'loss': 0.8737, 'grad_norm': 2.714506658030868, 'learning_rate': 2.7613333333333335e-06, 'epoch': 3.0096}
{'loss': 0.8769, 'grad_norm': 2.890700138249577, 'learning_rate': 2.7602222222222223e-06, 'epoch': 3.01}
{'loss': 0.8639, 'grad_norm': 3.073145173623181, 'learning_rate': 2.7591111111111114e-06, 'epoch': 3.0104}
{'loss': 0.867, 'grad_norm': 2.947550785736964, 'learning_rate': 2.758e-06, 'epoch': 3.0108}
{'loss': 0.8778, 'grad_norm': 3.0884321921354196, 'learning_rate': 2.756888888888889e-06, 'epoch': 3.0112}
{'loss': 0.8718, 'grad_norm': 2.9600096182248072, 'learning_rate': 2.755777777777778e-06, 'epoch': 3.0116}
{'loss': 0.8803, 'grad_norm': 3.0426868606866666, 'learning_rate': 2.754666666666667e-06, 'epoch': 3.012}
{'eval_valid_loss': 0.8486328125, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1110.974, 'eval_valid_steps_per_second': 277.743, 'epoch': 3.012}
{'loss': 0.8696, 'grad_norm': 2.9829484915568694, 'learning_rate': 2.753555555555556e-06, 'epoch': 3.0124}
{'loss': 0.8745, 'grad_norm': 2.957716646973864, 'learning_rate': 2.752444444444445e-06, 'epoch': 3.0128}
{'loss': 0.8732, 'grad_norm': 3.116025234751321, 'learning_rate': 2.7513333333333336e-06, 'epoch': 3.0132}
{'loss': 0.8812, 'grad_norm': 2.747573659966384, 'learning_rate': 2.7502222222222224e-06, 'epoch': 3.0136}
{'loss': 0.8756, 'grad_norm': 2.813532592046048, 'learning_rate': 2.7491111111111115e-06, 'epoch': 3.014}
{'loss': 0.8757, 'grad_norm': 3.14617513963879, 'learning_rate': 2.748e-06, 'epoch': 3.0144}
{'loss': 0.8742, 'grad_norm': 2.8781692979864175, 'learning_rate': 2.746888888888889e-06, 'epoch': 3.0148}
{'loss': 0.869, 'grad_norm': 2.7574898690463847, 'learning_rate': 2.7457777777777776e-06, 'epoch': 3.0152}
{'loss': 0.8893, 'grad_norm': 3.0189186155722347, 'learning_rate': 2.744666666666667e-06, 'epoch': 3.0156}
{'loss': 0.8741, 'grad_norm': 2.7718361704535464, 'learning_rate': 2.743555555555556e-06, 'epoch': 3.016}
{'eval_valid_loss': 0.84814453125, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.944, 'eval_valid_steps_per_second': 279.736, 'epoch': 3.016}
{'loss': 0.8711, 'grad_norm': 2.9698018670060233, 'learning_rate': 2.7424444444444446e-06, 'epoch': 3.0164}
{'loss': 0.8783, 'grad_norm': 2.71445877309561, 'learning_rate': 2.7413333333333337e-06, 'epoch': 3.0168}
{'loss': 0.8763, 'grad_norm': 3.084844584020272, 'learning_rate': 2.7402222222222225e-06, 'epoch': 3.0172}
{'loss': 0.8569, 'grad_norm': 2.7655331763417466, 'learning_rate': 2.739111111111111e-06, 'epoch': 3.0176}
{'loss': 0.8794, 'grad_norm': 3.0053791854836236, 'learning_rate': 2.7380000000000003e-06, 'epoch': 3.018}
{'loss': 0.8664, 'grad_norm': 2.782118452762781, 'learning_rate': 2.736888888888889e-06, 'epoch': 3.0184}
{'loss': 0.8731, 'grad_norm': 2.7466454202754593, 'learning_rate': 2.7357777777777777e-06, 'epoch': 3.0188}
{'loss': 0.8854, 'grad_norm': 3.022016462389769, 'learning_rate': 2.7346666666666673e-06, 'epoch': 3.0192}
{'loss': 0.8656, 'grad_norm': 2.982589247328318, 'learning_rate': 2.733555555555556e-06, 'epoch': 3.0196}
{'loss': 0.8768, 'grad_norm': 2.845000510835853, 'learning_rate': 2.7324444444444447e-06, 'epoch': 3.02}
{'eval_valid_loss': 0.84814453125, 'eval_valid_runtime': 0.089, 'eval_valid_samples_per_second': 1123.451, 'eval_valid_steps_per_second': 280.863, 'epoch': 3.02}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.8746, 'grad_norm': 2.9904516977064874, 'learning_rate': 2.7313333333333334e-06, 'epoch': 3.0204}
{'loss': 0.872, 'grad_norm': 2.9807628221537605, 'learning_rate': 2.7302222222222225e-06, 'epoch': 3.0208}
{'loss': 0.8766, 'grad_norm': 3.0465952916013714, 'learning_rate': 2.7291111111111113e-06, 'epoch': 3.0212}
{'loss': 0.8745, 'grad_norm': 3.0719834554004453, 'learning_rate': 2.728e-06, 'epoch': 3.0216}
{'loss': 0.8672, 'grad_norm': 2.795635374899805, 'learning_rate': 2.726888888888889e-06, 'epoch': 3.022}
{'loss': 0.8761, 'grad_norm': 2.9649912379072645, 'learning_rate': 2.725777777777778e-06, 'epoch': 3.0224}
{'loss': 0.8692, 'grad_norm': 3.0468280886437715, 'learning_rate': 2.724666666666667e-06, 'epoch': 3.0228}
{'loss': 0.8747, 'grad_norm': 2.804531890037651, 'learning_rate': 2.723555555555556e-06, 'epoch': 3.0232}
{'loss': 0.8804, 'grad_norm': 2.8347223373962143, 'learning_rate': 2.722444444444445e-06, 'epoch': 3.0236}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8737, 'grad_norm': 2.7955250972122028, 'learning_rate': 2.7213333333333335e-06, 'epoch': 3.024}
{'eval_valid_loss': 0.84912109375, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.08, 'eval_valid_steps_per_second': 281.77, 'epoch': 3.024}
{'loss': 0.8876, 'grad_norm': 3.03923183253799, 'learning_rate': 2.7202222222222222e-06, 'epoch': 3.0244}
{'loss': 0.8812, 'grad_norm': 2.9738080153191837, 'learning_rate': 2.7191111111111114e-06, 'epoch': 3.0248}
{'loss': 0.8664, 'grad_norm': 3.290621073359133, 'learning_rate': 2.718e-06, 'epoch': 3.0252}
{'loss': 0.8629, 'grad_norm': 2.680686036177941, 'learning_rate': 2.7168888888888888e-06, 'epoch': 3.0256}
{'loss': 0.8794, 'grad_norm': 2.8526204432887416, 'learning_rate': 2.715777777777778e-06, 'epoch': 3.026}
{'loss': 0.8721, 'grad_norm': 3.0193475171407593, 'learning_rate': 2.714666666666667e-06, 'epoch': 3.0264}
{'loss': 0.8713, 'grad_norm': 3.3539354420370504, 'learning_rate': 2.7135555555555558e-06, 'epoch': 3.0268}
{'loss': 0.887, 'grad_norm': 2.9691682119434186, 'learning_rate': 2.712444444444445e-06, 'epoch': 3.0272}
{'loss': 0.8654, 'grad_norm': 3.0550059667176264, 'learning_rate': 2.7113333333333336e-06, 'epoch': 3.0276}
{'loss': 0.8599, 'grad_norm': 2.8844164389695055, 'learning_rate': 2.7102222222222223e-06, 'epoch': 3.028}
{'eval_valid_loss': 0.84814453125, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1106.317, 'eval_valid_steps_per_second': 276.579, 'epoch': 3.028}
{'loss': 0.8899, 'grad_norm': 2.81241490447269, 'learning_rate': 2.7091111111111114e-06, 'epoch': 3.0284}
{'loss': 0.8826, 'grad_norm': 3.2500400907537985, 'learning_rate': 2.708e-06, 'epoch': 3.0288}
{'loss': 0.8772, 'grad_norm': 2.910910325632499, 'learning_rate': 2.706888888888889e-06, 'epoch': 3.0292}
{'loss': 0.8634, 'grad_norm': 3.077850852415177, 'learning_rate': 2.7057777777777776e-06, 'epoch': 3.0296}
{'loss': 0.8759, 'grad_norm': 2.835474538714684, 'learning_rate': 2.704666666666667e-06, 'epoch': 3.03}
{'loss': 0.8744, 'grad_norm': 2.82499696765163, 'learning_rate': 2.703555555555556e-06, 'epoch': 3.0304}
{'loss': 0.8847, 'grad_norm': 3.1288455380991986, 'learning_rate': 2.7024444444444446e-06, 'epoch': 3.0308}
{'loss': 0.8649, 'grad_norm': 2.9656272224924423, 'learning_rate': 2.7013333333333337e-06, 'epoch': 3.0312}
{'loss': 0.8709, 'grad_norm': 2.6654546741391445, 'learning_rate': 2.7002222222222224e-06, 'epoch': 3.0316}
{'loss': 0.8634, 'grad_norm': 2.8884658482080927, 'learning_rate': 2.699111111111111e-06, 'epoch': 3.032}
{'eval_valid_loss': 0.8486328125, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1125.03, 'eval_valid_steps_per_second': 281.258, 'epoch': 3.032}
{'loss': 0.8697, 'grad_norm': 2.6597561584384866, 'learning_rate': 2.6980000000000003e-06, 'epoch': 3.0324}
{'loss': 0.8746, 'grad_norm': 3.272489351236315, 'learning_rate': 2.696888888888889e-06, 'epoch': 3.0328}
{'loss': 0.8756, 'grad_norm': 3.042647093788153, 'learning_rate': 2.695777777777778e-06, 'epoch': 3.0332}
{'loss': 0.8717, 'grad_norm': 2.831891128756785, 'learning_rate': 2.6946666666666672e-06, 'epoch': 3.0336}
{'loss': 0.8591, 'grad_norm': 2.949964335597776, 'learning_rate': 2.693555555555556e-06, 'epoch': 3.034}
{'loss': 0.8691, 'grad_norm': 3.129563594732675, 'learning_rate': 2.6924444444444447e-06, 'epoch': 3.0344}
{'loss': 0.8654, 'grad_norm': 3.0027793883880927, 'learning_rate': 2.6913333333333334e-06, 'epoch': 3.0348}
{'loss': 0.8667, 'grad_norm': 2.985101531504882, 'learning_rate': 2.6902222222222225e-06, 'epoch': 3.0352}
{'loss': 0.8725, 'grad_norm': 2.8412838712050346, 'learning_rate': 2.6891111111111112e-06, 'epoch': 3.0356}
{'loss': 0.8776, 'grad_norm': 3.1309110526557404, 'learning_rate': 2.688e-06, 'epoch': 3.036}
{'eval_valid_loss': 0.84912109375, 'eval_valid_runtime': 0.0885, 'eval_valid_samples_per_second': 1130.126, 'eval_valid_steps_per_second': 282.531, 'epoch': 3.036}
{'loss': 0.8658, 'grad_norm': 3.0636703337510216, 'learning_rate': 2.686888888888889e-06, 'epoch': 3.0364}
{'loss': 0.8699, 'grad_norm': 2.969548228515645, 'learning_rate': 2.685777777777778e-06, 'epoch': 3.0368}
{'loss': 0.8739, 'grad_norm': 3.301023539843173, 'learning_rate': 2.684666666666667e-06, 'epoch': 3.0372}
{'loss': 0.8857, 'grad_norm': 2.985689313186683, 'learning_rate': 2.683555555555556e-06, 'epoch': 3.0376}
{'loss': 0.8709, 'grad_norm': 2.7932715725292327, 'learning_rate': 2.6824444444444448e-06, 'epoch': 3.038}
{'loss': 0.8725, 'grad_norm': 2.669043049591265, 'learning_rate': 2.6813333333333335e-06, 'epoch': 3.0384}
{'loss': 0.8746, 'grad_norm': 2.9460307290299697, 'learning_rate': 2.6802222222222226e-06, 'epoch': 3.0388}
{'loss': 0.8731, 'grad_norm': 2.929834580862042, 'learning_rate': 2.6791111111111113e-06, 'epoch': 3.0392}
{'loss': 0.8713, 'grad_norm': 2.869866483835807, 'learning_rate': 2.678e-06, 'epoch': 3.0396}
{'loss': 0.8674, 'grad_norm': 2.943923207672472, 'learning_rate': 2.6768888888888887e-06, 'epoch': 3.04}
{'eval_valid_loss': 0.84716796875, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1122.69, 'eval_valid_steps_per_second': 280.673, 'epoch': 3.04}
{'loss': 0.8682, 'grad_norm': 2.8127032259526543, 'learning_rate': 2.6757777777777783e-06, 'epoch': 3.0404}
{'loss': 0.88, 'grad_norm': 3.135433499025975, 'learning_rate': 2.674666666666667e-06, 'epoch': 3.0408}
{'loss': 0.8604, 'grad_norm': 3.226545920340958, 'learning_rate': 2.6735555555555557e-06, 'epoch': 3.0412}
{'loss': 0.8727, 'grad_norm': 2.9617435394916027, 'learning_rate': 2.672444444444445e-06, 'epoch': 3.0416}
{'loss': 0.8832, 'grad_norm': 3.0644448391454064, 'learning_rate': 2.6713333333333336e-06, 'epoch': 3.042}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8764, 'grad_norm': 2.8649593915607876, 'learning_rate': 2.6702222222222223e-06, 'epoch': 3.0424}
{'loss': 0.874, 'grad_norm': 2.977212099094015, 'learning_rate': 2.6691111111111114e-06, 'epoch': 3.0428}
{'loss': 0.8794, 'grad_norm': 2.6603338095493636, 'learning_rate': 2.668e-06, 'epoch': 3.0432}
{'loss': 0.8747, 'grad_norm': 2.9056974213835236, 'learning_rate': 2.666888888888889e-06, 'epoch': 3.0436}
{'loss': 0.8698, 'grad_norm': 2.873668020024047, 'learning_rate': 2.665888888888889e-06, 'epoch': 3.044}
{'eval_valid_loss': 0.84765625, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1124.747, 'eval_valid_steps_per_second': 281.187, 'epoch': 3.044}
{'loss': 0.8862, 'grad_norm': 3.0372765301889517, 'learning_rate': 2.664777777777778e-06, 'epoch': 3.0444}
{'loss': 0.8637, 'grad_norm': 3.0036061190316894, 'learning_rate': 2.6636666666666666e-06, 'epoch': 3.0448}
{'loss': 0.8669, 'grad_norm': 2.811464558839825, 'learning_rate': 2.662555555555556e-06, 'epoch': 3.0452}
{'loss': 0.8686, 'grad_norm': 2.923465285788466, 'learning_rate': 2.661444444444445e-06, 'epoch': 3.0456}
{'loss': 0.8618, 'grad_norm': 2.8622750860249924, 'learning_rate': 2.6603333333333336e-06, 'epoch': 3.046}
{'loss': 0.8673, 'grad_norm': 2.828322482722353, 'learning_rate': 2.6592222222222223e-06, 'epoch': 3.0464}
{'loss': 0.8783, 'grad_norm': 2.8602132923363777, 'learning_rate': 2.6581111111111114e-06, 'epoch': 3.0468}
{'loss': 0.8862, 'grad_norm': 2.8491598170769845, 'learning_rate': 2.657e-06, 'epoch': 3.0472}
{'loss': 0.8712, 'grad_norm': 2.817565266801843, 'learning_rate': 2.655888888888889e-06, 'epoch': 3.0476}
{'loss': 0.8724, 'grad_norm': 2.7681239201708765, 'learning_rate': 2.654777777777778e-06, 'epoch': 3.048}
{'eval_valid_loss': 0.8466796875, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.65, 'eval_valid_steps_per_second': 278.662, 'epoch': 3.048}
{'loss': 0.8787, 'grad_norm': 2.9970530698696023, 'learning_rate': 2.6536666666666667e-06, 'epoch': 3.0484}
{'loss': 0.8882, 'grad_norm': 3.169704577883161, 'learning_rate': 2.652555555555556e-06, 'epoch': 3.0488}
{'loss': 0.8699, 'grad_norm': 3.022730525354655, 'learning_rate': 2.651444444444445e-06, 'epoch': 3.0492}
{'loss': 0.8692, 'grad_norm': 3.281408006405963, 'learning_rate': 2.6503333333333337e-06, 'epoch': 3.0496}
{'loss': 0.8623, 'grad_norm': 2.7077530838423414, 'learning_rate': 2.6492222222222224e-06, 'epoch': 3.05}
{'loss': 0.8669, 'grad_norm': 3.096249582092725, 'learning_rate': 2.648111111111111e-06, 'epoch': 3.0504}
{'loss': 0.8796, 'grad_norm': 3.3678868091997627, 'learning_rate': 2.6470000000000002e-06, 'epoch': 3.0508}
{'loss': 0.8683, 'grad_norm': 3.09897459437274, 'learning_rate': 2.645888888888889e-06, 'epoch': 3.0512}
{'loss': 0.8861, 'grad_norm': 3.0360221953070745, 'learning_rate': 2.6447777777777777e-06, 'epoch': 3.0516}
{'loss': 0.8761, 'grad_norm': 2.8822265691176225, 'learning_rate': 2.6436666666666672e-06, 'epoch': 3.052}
{'eval_valid_loss': 0.84765625, 'eval_valid_runtime': 0.1239, 'eval_valid_samples_per_second': 807.348, 'eval_valid_steps_per_second': 201.837, 'epoch': 3.052}
{'loss': 0.8726, 'grad_norm': 2.9262331625035625, 'learning_rate': 2.642555555555556e-06, 'epoch': 3.0524}
{'loss': 0.882, 'grad_norm': 3.0138734958966573, 'learning_rate': 2.6414444444444446e-06, 'epoch': 3.0528}
{'loss': 0.8727, 'grad_norm': 3.284060564242936, 'learning_rate': 2.6403333333333338e-06, 'epoch': 3.0532}
{'loss': 0.8601, 'grad_norm': 2.794677169185237, 'learning_rate': 2.6392222222222225e-06, 'epoch': 3.0536}
{'loss': 0.8756, 'grad_norm': 3.024550555865645, 'learning_rate': 2.638111111111111e-06, 'epoch': 3.054}
{'loss': 0.8616, 'grad_norm': 2.8810925767553677, 'learning_rate': 2.637e-06, 'epoch': 3.0544}
{'loss': 0.8682, 'grad_norm': 2.7899497835364073, 'learning_rate': 2.635888888888889e-06, 'epoch': 3.0548}
{'loss': 0.8654, 'grad_norm': 2.6709164690319125, 'learning_rate': 2.6347777777777778e-06, 'epoch': 3.0552}
{'loss': 0.8764, 'grad_norm': 3.057264933341976, 'learning_rate': 2.6336666666666673e-06, 'epoch': 3.0556}
{'loss': 0.8675, 'grad_norm': 3.054520302413905, 'learning_rate': 2.632555555555556e-06, 'epoch': 3.056}
{'eval_valid_loss': 0.84765625, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.174, 'eval_valid_steps_per_second': 277.043, 'epoch': 3.056}
{'loss': 0.8765, 'grad_norm': 2.6137057930315244, 'learning_rate': 2.6314444444444447e-06, 'epoch': 3.0564}
{'loss': 0.8781, 'grad_norm': 2.8392500328896295, 'learning_rate': 2.6303333333333334e-06, 'epoch': 3.0568}
{'loss': 0.8742, 'grad_norm': 2.832417702451491, 'learning_rate': 2.6292222222222226e-06, 'epoch': 3.0572}
{'loss': 0.8807, 'grad_norm': 3.0467283947226256, 'learning_rate': 2.6281111111111113e-06, 'epoch': 3.0576}
{'loss': 0.8743, 'grad_norm': 2.8802225941494095, 'learning_rate': 2.627e-06, 'epoch': 3.058}
{'loss': 0.8724, 'grad_norm': 2.6877833095083554, 'learning_rate': 2.625888888888889e-06, 'epoch': 3.0584}
{'loss': 0.8729, 'grad_norm': 2.9323968490156713, 'learning_rate': 2.624777777777778e-06, 'epoch': 3.0588}
{'loss': 0.8581, 'grad_norm': 2.668965623975807, 'learning_rate': 2.623666666666667e-06, 'epoch': 3.0592}
{'loss': 0.8762, 'grad_norm': 3.059918775540101, 'learning_rate': 2.622555555555556e-06, 'epoch': 3.0596}
{'loss': 0.8672, 'grad_norm': 2.954319808086121, 'learning_rate': 2.621444444444445e-06, 'epoch': 3.06}
{'eval_valid_loss': 0.84814453125, 'eval_valid_runtime': 0.0883, 'eval_valid_samples_per_second': 1132.732, 'eval_valid_steps_per_second': 283.183, 'epoch': 3.06}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8784, 'grad_norm': 2.991559891579564, 'learning_rate': 2.6203333333333335e-06, 'epoch': 3.0604}
{'loss': 0.8787, 'grad_norm': 2.915517080912162, 'learning_rate': 2.6192222222222223e-06, 'epoch': 3.0608}
{'loss': 0.8725, 'grad_norm': 2.9632647384396753, 'learning_rate': 2.6181111111111114e-06, 'epoch': 3.0612}
{'loss': 0.8681, 'grad_norm': 2.8565377766568303, 'learning_rate': 2.617e-06, 'epoch': 3.0616}
{'loss': 0.8622, 'grad_norm': 2.7714215164637275, 'learning_rate': 2.615888888888889e-06, 'epoch': 3.062}
{'loss': 0.8684, 'grad_norm': 2.756286582574253, 'learning_rate': 2.614777777777778e-06, 'epoch': 3.0624}
{'loss': 0.8707, 'grad_norm': 2.9074705493711384, 'learning_rate': 2.613666666666667e-06, 'epoch': 3.0628}
{'loss': 0.871, 'grad_norm': 3.0349797574331316, 'learning_rate': 2.612555555555556e-06, 'epoch': 3.0632}
{'loss': 0.8719, 'grad_norm': 2.8480411054934445, 'learning_rate': 2.611444444444445e-06, 'epoch': 3.0636}
{'loss': 0.8729, 'grad_norm': 2.716460848224758, 'learning_rate': 2.6103333333333336e-06, 'epoch': 3.064}
{'eval_valid_loss': 0.84765625, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.32, 'eval_valid_steps_per_second': 279.08, 'epoch': 3.064}
{'loss': 0.8758, 'grad_norm': 2.7784645190634443, 'learning_rate': 2.6092222222222224e-06, 'epoch': 3.0644}
{'loss': 0.8636, 'grad_norm': 2.691826880241699, 'learning_rate': 2.608111111111111e-06, 'epoch': 3.0648}
{'loss': 0.8742, 'grad_norm': 2.848219429913465, 'learning_rate': 2.607e-06, 'epoch': 3.0652}
{'loss': 0.8629, 'grad_norm': 3.013337310198323, 'learning_rate': 2.605888888888889e-06, 'epoch': 3.0656}
{'loss': 0.8872, 'grad_norm': 2.927051558723712, 'learning_rate': 2.6047777777777776e-06, 'epoch': 3.066}
{'loss': 0.8793, 'grad_norm': 2.8175342856269903, 'learning_rate': 2.603666666666667e-06, 'epoch': 3.0664}
{'loss': 0.8752, 'grad_norm': 2.804397738054022, 'learning_rate': 2.602555555555556e-06, 'epoch': 3.0668}
{'loss': 0.864, 'grad_norm': 2.845100103303303, 'learning_rate': 2.6014444444444446e-06, 'epoch': 3.0672}
{'loss': 0.8625, 'grad_norm': 2.8337562306204784, 'learning_rate': 2.6003333333333337e-06, 'epoch': 3.0676}
{'loss': 0.8778, 'grad_norm': 3.0382572568425235, 'learning_rate': 2.5992222222222224e-06, 'epoch': 3.068}
{'eval_valid_loss': 0.84716796875, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1128.035, 'eval_valid_steps_per_second': 282.009, 'epoch': 3.068}
{'loss': 0.8666, 'grad_norm': 2.8839656053676594, 'learning_rate': 2.598111111111111e-06, 'epoch': 3.0684}
{'loss': 0.8683, 'grad_norm': 2.9819662949021564, 'learning_rate': 2.597e-06, 'epoch': 3.0688}
{'loss': 0.8656, 'grad_norm': 2.725606056502689, 'learning_rate': 2.595888888888889e-06, 'epoch': 3.0692}
{'loss': 0.87, 'grad_norm': 2.979282478046655, 'learning_rate': 2.5947777777777777e-06, 'epoch': 3.0696}
{'loss': 0.8707, 'grad_norm': 2.8786958594289347, 'learning_rate': 2.5936666666666673e-06, 'epoch': 3.07}
{'loss': 0.8613, 'grad_norm': 3.0347691390957117, 'learning_rate': 2.592555555555556e-06, 'epoch': 3.0704}
{'loss': 0.8741, 'grad_norm': 3.1547537174615585, 'learning_rate': 2.5914444444444447e-06, 'epoch': 3.0708}
{'loss': 0.8626, 'grad_norm': 2.8221408064013027, 'learning_rate': 2.5903333333333334e-06, 'epoch': 3.0712}
{'loss': 0.858, 'grad_norm': 2.916875774971136, 'learning_rate': 2.5892222222222225e-06, 'epoch': 3.0716}
{'loss': 0.8641, 'grad_norm': 2.8782433532088736, 'learning_rate': 2.5881111111111113e-06, 'epoch': 3.072}
{'eval_valid_loss': 0.8486328125, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.687, 'eval_valid_steps_per_second': 278.922, 'epoch': 3.072}
{'loss': 0.8726, 'grad_norm': 3.3470617810301038, 'learning_rate': 2.587e-06, 'epoch': 3.0724}
{'loss': 0.8708, 'grad_norm': 3.1667153528302285, 'learning_rate': 2.585888888888889e-06, 'epoch': 3.0728}
{'loss': 0.8676, 'grad_norm': 3.080508534046123, 'learning_rate': 2.584777777777778e-06, 'epoch': 3.0732}
{'loss': 0.8698, 'grad_norm': 2.543036001461758, 'learning_rate': 2.583666666666667e-06, 'epoch': 3.0736}
{'loss': 0.8695, 'grad_norm': 2.805368500174281, 'learning_rate': 2.582555555555556e-06, 'epoch': 3.074}
{'loss': 0.8605, 'grad_norm': 3.1917832711004226, 'learning_rate': 2.581444444444445e-06, 'epoch': 3.0744}
{'loss': 0.874, 'grad_norm': 2.7761548873877584, 'learning_rate': 2.5803333333333335e-06, 'epoch': 3.0748}
{'loss': 0.8681, 'grad_norm': 2.9324343354710076, 'learning_rate': 2.5792222222222222e-06, 'epoch': 3.0752}
{'loss': 0.8678, 'grad_norm': 2.9066224423559954, 'learning_rate': 2.5781111111111113e-06, 'epoch': 3.0756}
{'loss': 0.8756, 'grad_norm': 2.78985550770329, 'learning_rate': 2.577e-06, 'epoch': 3.076}
{'eval_valid_loss': 0.8486328125, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.903, 'eval_valid_steps_per_second': 279.476, 'epoch': 3.076}
{'loss': 0.8755, 'grad_norm': 2.915090057591535, 'learning_rate': 2.5758888888888888e-06, 'epoch': 3.0764}
{'loss': 0.865, 'grad_norm': 2.8001072230840993, 'learning_rate': 2.5747777777777783e-06, 'epoch': 3.0768}
{'loss': 0.8747, 'grad_norm': 2.708562310638481, 'learning_rate': 2.573666666666667e-06, 'epoch': 3.0772}
{'loss': 0.8845, 'grad_norm': 2.917709757840727, 'learning_rate': 2.5725555555555558e-06, 'epoch': 3.0776}
{'loss': 0.8716, 'grad_norm': 2.999699428521518, 'learning_rate': 2.571444444444445e-06, 'epoch': 3.078}
{'loss': 0.8718, 'grad_norm': 2.8063223491764417, 'learning_rate': 2.5703333333333336e-06, 'epoch': 3.0784}
{'loss': 0.8641, 'grad_norm': 3.1551172282147886, 'learning_rate': 2.5692222222222223e-06, 'epoch': 3.0788}
{'loss': 0.8721, 'grad_norm': 2.877668044588117, 'learning_rate': 2.568111111111111e-06, 'epoch': 3.0792}
{'loss': 0.8645, 'grad_norm': 2.832108668880517, 'learning_rate': 2.567e-06, 'epoch': 3.0796}
{'loss': 0.8762, 'grad_norm': 2.8400805504467197, 'learning_rate': 2.565888888888889e-06, 'epoch': 3.08}
{'eval_valid_loss': 0.84765625, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1110.371, 'eval_valid_steps_per_second': 277.593, 'epoch': 3.08}
{'loss': 0.8597, 'grad_norm': 2.8381407300004535, 'learning_rate': 2.5647777777777784e-06, 'epoch': 3.0804}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.871, 'grad_norm': 2.6573257063287103, 'learning_rate': 2.563666666666667e-06, 'epoch': 3.0808}
{'loss': 0.8829, 'grad_norm': 2.7423744681324913, 'learning_rate': 2.562555555555556e-06, 'epoch': 3.0812}
{'loss': 0.8741, 'grad_norm': 2.914914331371442, 'learning_rate': 2.5614444444444446e-06, 'epoch': 3.0816}
{'loss': 0.8653, 'grad_norm': 2.8761798003166663, 'learning_rate': 2.5603333333333337e-06, 'epoch': 3.082}
{'loss': 0.8778, 'grad_norm': 2.9803722860942075, 'learning_rate': 2.5592222222222224e-06, 'epoch': 3.0824}
{'loss': 0.8761, 'grad_norm': 2.964448864841152, 'learning_rate': 2.558111111111111e-06, 'epoch': 3.0828}
{'loss': 0.87, 'grad_norm': 2.903686336126509, 'learning_rate': 2.557e-06, 'epoch': 3.0832}
{'loss': 0.8591, 'grad_norm': 2.6531649962826407, 'learning_rate': 2.555888888888889e-06, 'epoch': 3.0836}
{'loss': 0.8622, 'grad_norm': 2.9406907375256495, 'learning_rate': 2.554888888888889e-06, 'epoch': 3.084}
{'eval_valid_loss': 0.84814453125, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1119.167, 'eval_valid_steps_per_second': 279.792, 'epoch': 3.084}
{'loss': 0.8877, 'grad_norm': 3.3743631327166583, 'learning_rate': 2.5537777777777776e-06, 'epoch': 3.0844}
{'loss': 0.8731, 'grad_norm': 3.058789905869261, 'learning_rate': 2.5526666666666667e-06, 'epoch': 3.0848}
{'loss': 0.863, 'grad_norm': 2.8713866103925123, 'learning_rate': 2.551555555555556e-06, 'epoch': 3.0852}
{'loss': 0.8661, 'grad_norm': 2.835312440784529, 'learning_rate': 2.550444444444445e-06, 'epoch': 3.0856}
{'loss': 0.8663, 'grad_norm': 3.094828167322072, 'learning_rate': 2.5493333333333337e-06, 'epoch': 3.086}
{'loss': 0.8717, 'grad_norm': 2.796512420424146, 'learning_rate': 2.5482222222222224e-06, 'epoch': 3.0864}
{'loss': 0.8706, 'grad_norm': 3.1645416556286947, 'learning_rate': 2.547111111111111e-06, 'epoch': 3.0868}
{'loss': 0.8791, 'grad_norm': 3.0503273796044557, 'learning_rate': 2.5460000000000003e-06, 'epoch': 3.0872}
{'loss': 0.8705, 'grad_norm': 3.010098646233046, 'learning_rate': 2.544888888888889e-06, 'epoch': 3.0876}
{'loss': 0.8663, 'grad_norm': 3.0050390858099725, 'learning_rate': 2.5437777777777777e-06, 'epoch': 3.088}
{'eval_valid_loss': 0.84814453125, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1110.818, 'eval_valid_steps_per_second': 277.704, 'epoch': 3.088}
{'loss': 0.8758, 'grad_norm': 3.216224207161269, 'learning_rate': 2.5426666666666664e-06, 'epoch': 3.0884}
{'loss': 0.8785, 'grad_norm': 2.751243960188909, 'learning_rate': 2.541555555555556e-06, 'epoch': 3.0888}
{'loss': 0.8666, 'grad_norm': 2.73653725281696, 'learning_rate': 2.5404444444444447e-06, 'epoch': 3.0892}
{'loss': 0.8846, 'grad_norm': 3.0037663381309208, 'learning_rate': 2.539333333333334e-06, 'epoch': 3.0896}
{'loss': 0.8764, 'grad_norm': 3.04775688906825, 'learning_rate': 2.5382222222222225e-06, 'epoch': 3.09}
{'loss': 0.8768, 'grad_norm': 2.825146318334559, 'learning_rate': 2.5371111111111112e-06, 'epoch': 3.0904}
{'loss': 0.8831, 'grad_norm': 3.094038410422408, 'learning_rate': 2.536e-06, 'epoch': 3.0908}
{'loss': 0.8751, 'grad_norm': 2.9114291844191187, 'learning_rate': 2.534888888888889e-06, 'epoch': 3.0912}
{'loss': 0.8858, 'grad_norm': 3.124726025969243, 'learning_rate': 2.533777777777778e-06, 'epoch': 3.0916}
{'loss': 0.8679, 'grad_norm': 2.9866422658871525, 'learning_rate': 2.5326666666666665e-06, 'epoch': 3.092}
{'eval_valid_loss': 0.84619140625, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1128.196, 'eval_valid_steps_per_second': 282.049, 'epoch': 3.092}
{'loss': 0.8774, 'grad_norm': 2.9587722450487006, 'learning_rate': 2.531555555555556e-06, 'epoch': 3.0924}
{'loss': 0.8783, 'grad_norm': 2.9821421374001136, 'learning_rate': 2.5304444444444448e-06, 'epoch': 3.0928}
{'loss': 0.8784, 'grad_norm': 3.139676354077993, 'learning_rate': 2.5293333333333335e-06, 'epoch': 3.0932}
{'loss': 0.8725, 'grad_norm': 3.1140550348218223, 'learning_rate': 2.5282222222222226e-06, 'epoch': 3.0936}
{'loss': 0.8795, 'grad_norm': 3.0077324200163114, 'learning_rate': 2.5271111111111113e-06, 'epoch': 3.094}
{'loss': 0.8725, 'grad_norm': 2.609902716857038, 'learning_rate': 2.526e-06, 'epoch': 3.0944}
{'loss': 0.8734, 'grad_norm': 2.9691159876044773, 'learning_rate': 2.5248888888888888e-06, 'epoch': 3.0948}
{'loss': 0.8813, 'grad_norm': 2.8793931236218366, 'learning_rate': 2.523777777777778e-06, 'epoch': 3.0952}
{'loss': 0.8726, 'grad_norm': 3.0753068270774873, 'learning_rate': 2.5226666666666666e-06, 'epoch': 3.0956}
{'loss': 0.8795, 'grad_norm': 2.8900481911538463, 'learning_rate': 2.521555555555556e-06, 'epoch': 3.096}
{'eval_valid_loss': 0.84814453125, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1128.94, 'eval_valid_steps_per_second': 282.235, 'epoch': 3.096}
{'loss': 0.8692, 'grad_norm': 3.239448217454719, 'learning_rate': 2.520444444444445e-06, 'epoch': 3.0964}
{'loss': 0.8633, 'grad_norm': 3.2564843120949707, 'learning_rate': 2.5193333333333336e-06, 'epoch': 3.0968}
{'loss': 0.8699, 'grad_norm': 3.050167046334319, 'learning_rate': 2.5182222222222223e-06, 'epoch': 3.0972}
{'loss': 0.8616, 'grad_norm': 2.7449003912271044, 'learning_rate': 2.5171111111111114e-06, 'epoch': 3.0976}
{'loss': 0.872, 'grad_norm': 2.772524965627899, 'learning_rate': 2.516e-06, 'epoch': 3.098}
{'loss': 0.8762, 'grad_norm': 3.064486754447455, 'learning_rate': 2.514888888888889e-06, 'epoch': 3.0984}
{'loss': 0.8852, 'grad_norm': 2.902399619631364, 'learning_rate': 2.5137777777777776e-06, 'epoch': 3.0987999999999998}
{'loss': 0.8628, 'grad_norm': 2.978391590975321, 'learning_rate': 2.512666666666667e-06, 'epoch': 3.0992}
{'loss': 0.8703, 'grad_norm': 3.116543830059071, 'learning_rate': 2.511555555555556e-06, 'epoch': 3.0996}
{'loss': 0.8769, 'grad_norm': 2.81659392050377, 'learning_rate': 2.510444444444445e-06, 'epoch': 3.1}
{'eval_valid_loss': 0.84765625, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1126.596, 'eval_valid_steps_per_second': 281.649, 'epoch': 3.1}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.8784, 'grad_norm': 2.9749370852798114, 'learning_rate': 2.5093333333333337e-06, 'epoch': 3.1004}
{'loss': 0.8782, 'grad_norm': 3.054518107134685, 'learning_rate': 2.5082222222222224e-06, 'epoch': 3.1008}
{'loss': 0.875, 'grad_norm': 2.8272017188972756, 'learning_rate': 2.507111111111111e-06, 'epoch': 3.1012}
{'loss': 0.8722, 'grad_norm': 2.9273717094493135, 'learning_rate': 2.5060000000000002e-06, 'epoch': 3.1016}
{'loss': 0.8722, 'grad_norm': 2.6905406562088423, 'learning_rate': 2.504888888888889e-06, 'epoch': 3.102}
{'loss': 0.8786, 'grad_norm': 3.0209149130405684, 'learning_rate': 2.5037777777777777e-06, 'epoch': 3.1024}
{'loss': 0.868, 'grad_norm': 2.8063919498760397, 'learning_rate': 2.5026666666666672e-06, 'epoch': 3.1028000000000002}
{'loss': 0.8886, 'grad_norm': 2.9050423615185337, 'learning_rate': 2.501555555555556e-06, 'epoch': 3.1032}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8662, 'grad_norm': 2.740995566479682, 'learning_rate': 2.5004444444444446e-06, 'epoch': 3.1036}
{'loss': 0.8649, 'grad_norm': 3.009251663912999, 'learning_rate': 2.4993333333333338e-06, 'epoch': 3.104}
{'eval_valid_loss': 0.84814453125, 'eval_valid_runtime': 0.0915, 'eval_valid_samples_per_second': 1092.378, 'eval_valid_steps_per_second': 273.094, 'epoch': 3.104}
{'loss': 0.8722, 'grad_norm': 2.8949776329731347, 'learning_rate': 2.4982222222222225e-06, 'epoch': 3.1044}
{'loss': 0.8675, 'grad_norm': 3.13613343626958, 'learning_rate': 2.497111111111111e-06, 'epoch': 3.1048}
{'loss': 0.88, 'grad_norm': 3.0549894705589384, 'learning_rate': 2.496e-06, 'epoch': 3.1052}
{'loss': 0.8733, 'grad_norm': 3.2005203449858293, 'learning_rate': 2.494888888888889e-06, 'epoch': 3.1056}
{'loss': 0.8607, 'grad_norm': 3.3229097597455, 'learning_rate': 2.493777777777778e-06, 'epoch': 3.106}
{'loss': 0.8837, 'grad_norm': 3.391506704491691, 'learning_rate': 2.492666666666667e-06, 'epoch': 3.1064}
{'loss': 0.8654, 'grad_norm': 2.9620439731722876, 'learning_rate': 2.4915555555555556e-06, 'epoch': 3.1068}
{'loss': 0.8794, 'grad_norm': 2.9643080658406467, 'learning_rate': 2.4904444444444447e-06, 'epoch': 3.1072}
{'loss': 0.8755, 'grad_norm': 2.9983825694465036, 'learning_rate': 2.4893333333333334e-06, 'epoch': 3.1076}
{'loss': 0.8762, 'grad_norm': 3.0639361011631183, 'learning_rate': 2.4882222222222226e-06, 'epoch': 3.108}
{'eval_valid_loss': 0.849609375, 'eval_valid_runtime': 0.0903, 'eval_valid_samples_per_second': 1107.732, 'eval_valid_steps_per_second': 276.933, 'epoch': 3.108}
{'loss': 0.8643, 'grad_norm': 2.9405421169328356, 'learning_rate': 2.4871111111111113e-06, 'epoch': 3.1084}
{'loss': 0.8651, 'grad_norm': 3.1211390199588904, 'learning_rate': 2.486e-06, 'epoch': 3.1088}
{'loss': 0.8702, 'grad_norm': 2.8768450165718678, 'learning_rate': 2.484888888888889e-06, 'epoch': 3.1092}
{'loss': 0.8688, 'grad_norm': 3.0714792941664038, 'learning_rate': 2.483777777777778e-06, 'epoch': 3.1096}
{'loss': 0.8634, 'grad_norm': 2.8479517402712324, 'learning_rate': 2.482666666666667e-06, 'epoch': 3.11}
{'loss': 0.8632, 'grad_norm': 2.833761951806669, 'learning_rate': 2.4815555555555557e-06, 'epoch': 3.1104}
{'loss': 0.8706, 'grad_norm': 2.729272113587813, 'learning_rate': 2.480444444444445e-06, 'epoch': 3.1108}
{'loss': 0.8683, 'grad_norm': 2.9545141716948224, 'learning_rate': 2.4793333333333335e-06, 'epoch': 3.1112}
{'loss': 0.8815, 'grad_norm': 3.0463179739379727, 'learning_rate': 2.4782222222222222e-06, 'epoch': 3.1116}
{'loss': 0.865, 'grad_norm': 2.832206089279081, 'learning_rate': 2.4771111111111114e-06, 'epoch': 3.112}
{'eval_valid_loss': 0.84765625, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1110.383, 'eval_valid_steps_per_second': 277.596, 'epoch': 3.112}
{'loss': 0.8714, 'grad_norm': 2.720378766027865, 'learning_rate': 2.476e-06, 'epoch': 3.1124}
{'loss': 0.8804, 'grad_norm': 2.9199921972680025, 'learning_rate': 2.4748888888888892e-06, 'epoch': 3.1128}
{'loss': 0.8645, 'grad_norm': 3.0681360698911324, 'learning_rate': 2.473777777777778e-06, 'epoch': 3.1132}
{'loss': 0.8751, 'grad_norm': 2.9453704281885478, 'learning_rate': 2.4726666666666667e-06, 'epoch': 3.1136}
{'loss': 0.8698, 'grad_norm': 2.68840036724182, 'learning_rate': 2.4715555555555558e-06, 'epoch': 3.114}
{'loss': 0.8758, 'grad_norm': 2.9698687603721705, 'learning_rate': 2.470444444444445e-06, 'epoch': 3.1144}
{'loss': 0.865, 'grad_norm': 2.7614059972582172, 'learning_rate': 2.4693333333333336e-06, 'epoch': 3.1148}
{'loss': 0.8681, 'grad_norm': 2.6302229090875313, 'learning_rate': 2.4682222222222223e-06, 'epoch': 3.1152}
{'loss': 0.877, 'grad_norm': 3.285636095707099, 'learning_rate': 2.467111111111111e-06, 'epoch': 3.1156}
{'loss': 0.8578, 'grad_norm': 2.812608907498339, 'learning_rate': 2.466e-06, 'epoch': 3.116}
{'eval_valid_loss': 0.84765625, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.073, 'eval_valid_steps_per_second': 279.018, 'epoch': 3.116}
{'loss': 0.8834, 'grad_norm': 2.818440907357103, 'learning_rate': 2.4648888888888893e-06, 'epoch': 3.1164}
{'loss': 0.8821, 'grad_norm': 3.0923967195254893, 'learning_rate': 2.463777777777778e-06, 'epoch': 3.1168}
{'loss': 0.8829, 'grad_norm': 3.0520405337790524, 'learning_rate': 2.4626666666666667e-06, 'epoch': 3.1172}
{'loss': 0.8872, 'grad_norm': 2.9248331160907104, 'learning_rate': 2.4615555555555555e-06, 'epoch': 3.1176}
{'loss': 0.86, 'grad_norm': 2.958668909158268, 'learning_rate': 2.4604444444444446e-06, 'epoch': 3.118}
{'loss': 0.876, 'grad_norm': 3.090241619974882, 'learning_rate': 2.4593333333333337e-06, 'epoch': 3.1184}
{'loss': 0.8604, 'grad_norm': 3.045050823229135, 'learning_rate': 2.4582222222222224e-06, 'epoch': 3.1188}
{'loss': 0.8685, 'grad_norm': 2.8038283389606207, 'learning_rate': 2.457111111111111e-06, 'epoch': 3.1192}
{'loss': 0.8752, 'grad_norm': 2.837283416708229, 'learning_rate': 2.4560000000000003e-06, 'epoch': 3.1196}
{'loss': 0.8866, 'grad_norm': 2.9889103545149203, 'learning_rate': 2.454888888888889e-06, 'epoch': 3.12}
{'eval_valid_loss': 0.845703125, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.573, 'eval_valid_steps_per_second': 278.643, 'epoch': 3.12}
{'loss': 0.8692, 'grad_norm': 2.906745929858543, 'learning_rate': 2.453777777777778e-06, 'epoch': 3.1204}
{'loss': 0.8838, 'grad_norm': 3.0641924502270332, 'learning_rate': 2.452666666666667e-06, 'epoch': 3.1208}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8911, 'grad_norm': 2.935632786918125, 'learning_rate': 2.4515555555555556e-06, 'epoch': 3.1212}
{'loss': 0.8763, 'grad_norm': 2.710194933987172, 'learning_rate': 2.4504444444444447e-06, 'epoch': 3.1216}
{'loss': 0.8697, 'grad_norm': 2.8028072334416154, 'learning_rate': 2.4493333333333334e-06, 'epoch': 3.122}
{'loss': 0.8678, 'grad_norm': 2.840353871406317, 'learning_rate': 2.4482222222222225e-06, 'epoch': 3.1224}
{'loss': 0.8687, 'grad_norm': 2.755489532721125, 'learning_rate': 2.4471111111111112e-06, 'epoch': 3.1228}
{'loss': 0.8645, 'grad_norm': 2.925703131698563, 'learning_rate': 2.4460000000000004e-06, 'epoch': 3.1232}
{'loss': 0.8812, 'grad_norm': 2.9281728669348683, 'learning_rate': 2.444888888888889e-06, 'epoch': 3.1236}
{'loss': 0.8793, 'grad_norm': 3.1067440845055305, 'learning_rate': 2.443888888888889e-06, 'epoch': 3.124}
{'eval_valid_loss': 0.8466796875, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1121.451, 'eval_valid_steps_per_second': 280.363, 'epoch': 3.124}
{'loss': 0.878, 'grad_norm': 2.867979153474224, 'learning_rate': 2.442777777777778e-06, 'epoch': 3.1244}
{'loss': 0.8709, 'grad_norm': 2.7993435701163154, 'learning_rate': 2.441666666666667e-06, 'epoch': 3.1248}
{'loss': 0.8783, 'grad_norm': 3.0523285599099426, 'learning_rate': 2.4405555555555556e-06, 'epoch': 3.1252}
{'loss': 0.8759, 'grad_norm': 2.943916597126953, 'learning_rate': 2.4394444444444447e-06, 'epoch': 3.1256}
{'loss': 0.8669, 'grad_norm': 2.9294143752895936, 'learning_rate': 2.438333333333334e-06, 'epoch': 3.126}
{'loss': 0.8664, 'grad_norm': 2.9167250366273976, 'learning_rate': 2.4372222222222226e-06, 'epoch': 3.1264}
{'loss': 0.8618, 'grad_norm': 3.1300775095449755, 'learning_rate': 2.4361111111111113e-06, 'epoch': 3.1268}
{'loss': 0.8896, 'grad_norm': 2.9090943261537614, 'learning_rate': 2.435e-06, 'epoch': 3.1272}
{'loss': 0.8711, 'grad_norm': 2.927432868604792, 'learning_rate': 2.433888888888889e-06, 'epoch': 3.1276}
{'loss': 0.8707, 'grad_norm': 2.969456839488509, 'learning_rate': 2.4327777777777782e-06, 'epoch': 3.128}
{'eval_valid_loss': 0.8466796875, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1116.879, 'eval_valid_steps_per_second': 279.22, 'epoch': 3.128}
{'loss': 0.8651, 'grad_norm': 3.073760961937332, 'learning_rate': 2.431666666666667e-06, 'epoch': 3.1284}
{'loss': 0.8645, 'grad_norm': 3.132008589721827, 'learning_rate': 2.4305555555555557e-06, 'epoch': 3.1288}
{'loss': 0.8615, 'grad_norm': 3.0143838215793175, 'learning_rate': 2.4294444444444444e-06, 'epoch': 3.1292}
{'loss': 0.874, 'grad_norm': 3.032715079596674, 'learning_rate': 2.4283333333333335e-06, 'epoch': 3.1296}
{'loss': 0.8832, 'grad_norm': 3.20193928427856, 'learning_rate': 2.4272222222222227e-06, 'epoch': 3.13}
{'loss': 0.8836, 'grad_norm': 2.9169092974333246, 'learning_rate': 2.4261111111111114e-06, 'epoch': 3.1304}
{'loss': 0.8732, 'grad_norm': 2.7760859456685036, 'learning_rate': 2.425e-06, 'epoch': 3.1308}
{'loss': 0.8721, 'grad_norm': 2.9218745563119155, 'learning_rate': 2.4238888888888888e-06, 'epoch': 3.1312}
{'loss': 0.8724, 'grad_norm': 2.7135214899427127, 'learning_rate': 2.422777777777778e-06, 'epoch': 3.1316}
{'loss': 0.8645, 'grad_norm': 3.198543742865439, 'learning_rate': 2.421666666666667e-06, 'epoch': 3.132}
{'eval_valid_loss': 0.8466796875, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.126, 'eval_valid_steps_per_second': 278.282, 'epoch': 3.132}
{'loss': 0.8588, 'grad_norm': 3.100665359080617, 'learning_rate': 2.4205555555555558e-06, 'epoch': 3.1324}
{'loss': 0.874, 'grad_norm': 2.794506807249702, 'learning_rate': 2.4194444444444445e-06, 'epoch': 3.1328}
{'loss': 0.8691, 'grad_norm': 3.174148064553242, 'learning_rate': 2.4183333333333336e-06, 'epoch': 3.1332}
{'loss': 0.8608, 'grad_norm': 2.854691515764691, 'learning_rate': 2.4172222222222223e-06, 'epoch': 3.1336}
{'loss': 0.8661, 'grad_norm': 3.0856441020316354, 'learning_rate': 2.4161111111111115e-06, 'epoch': 3.134}
{'loss': 0.8719, 'grad_norm': 2.775988418445352, 'learning_rate': 2.415e-06, 'epoch': 3.1344}
{'loss': 0.8757, 'grad_norm': 2.9220621997615326, 'learning_rate': 2.413888888888889e-06, 'epoch': 3.1348}
{'loss': 0.8717, 'grad_norm': 2.8982216788087585, 'learning_rate': 2.412777777777778e-06, 'epoch': 3.1352}
{'loss': 0.8801, 'grad_norm': 2.644764409570215, 'learning_rate': 2.4116666666666667e-06, 'epoch': 3.1356}
{'loss': 0.8815, 'grad_norm': 2.8520812150702586, 'learning_rate': 2.410555555555556e-06, 'epoch': 3.136}
{'eval_valid_loss': 0.84619140625, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.09, 'eval_valid_steps_per_second': 279.272, 'epoch': 3.136}
{'loss': 0.8692, 'grad_norm': 3.043704667429387, 'learning_rate': 2.4094444444444446e-06, 'epoch': 3.1364}
{'loss': 0.8577, 'grad_norm': 2.7063746516128084, 'learning_rate': 2.4083333333333337e-06, 'epoch': 3.1368}
{'loss': 0.8742, 'grad_norm': 2.955546118267866, 'learning_rate': 2.4072222222222224e-06, 'epoch': 3.1372}
{'loss': 0.8722, 'grad_norm': 3.0879815490577855, 'learning_rate': 2.406111111111111e-06, 'epoch': 3.1376}
{'loss': 0.8662, 'grad_norm': 2.897033557029889, 'learning_rate': 2.4050000000000003e-06, 'epoch': 3.138}
{'loss': 0.8723, 'grad_norm': 2.8079975646713486, 'learning_rate': 2.4038888888888894e-06, 'epoch': 3.1384}
{'loss': 0.8693, 'grad_norm': 3.177114087357221, 'learning_rate': 2.402777777777778e-06, 'epoch': 3.1388}
{'loss': 0.8788, 'grad_norm': 2.834918078007569, 'learning_rate': 2.401666666666667e-06, 'epoch': 3.1391999999999998}
{'loss': 0.873, 'grad_norm': 2.8147827103704097, 'learning_rate': 2.4005555555555555e-06, 'epoch': 3.1396}
{'loss': 0.8677, 'grad_norm': 2.7849521963478727, 'learning_rate': 2.3994444444444447e-06, 'epoch': 3.14}
{'eval_valid_loss': 0.84716796875, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.729, 'eval_valid_steps_per_second': 278.932, 'epoch': 3.14}
{'loss': 0.861, 'grad_norm': 3.144931952493878, 'learning_rate': 2.398333333333334e-06, 'epoch': 3.1404}
{'loss': 0.8859, 'grad_norm': 3.1719217813734524, 'learning_rate': 2.3972222222222225e-06, 'epoch': 3.1408}
{'loss': 0.8688, 'grad_norm': 3.116951706460464, 'learning_rate': 2.3961111111111112e-06, 'epoch': 3.1412}
{'loss': 0.8679, 'grad_norm': 3.0534304303048465, 'learning_rate': 2.395e-06, 'epoch': 3.1416}
{'loss': 0.864, 'grad_norm': 3.068627261409014, 'learning_rate': 2.393888888888889e-06, 'epoch': 3.142}
{'loss': 0.8721, 'grad_norm': 2.9909841941605904, 'learning_rate': 2.392777777777778e-06, 'epoch': 3.1424}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8785, 'grad_norm': 3.1740213188646464, 'learning_rate': 2.391666666666667e-06, 'epoch': 3.1428}
{'loss': 0.873, 'grad_norm': 3.1295756220520468, 'learning_rate': 2.3905555555555556e-06, 'epoch': 3.1432}
{'loss': 0.8794, 'grad_norm': 3.04917751267756, 'learning_rate': 2.3894444444444443e-06, 'epoch': 3.1436}
{'loss': 0.8644, 'grad_norm': 2.9227554810214578, 'learning_rate': 2.3883333333333335e-06, 'epoch': 3.144}
{'eval_valid_loss': 0.84716796875, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.284, 'eval_valid_steps_per_second': 279.821, 'epoch': 3.144}
{'loss': 0.8732, 'grad_norm': 3.114685919144961, 'learning_rate': 2.3872222222222226e-06, 'epoch': 3.1444}
{'loss': 0.8652, 'grad_norm': 2.8900635766919773, 'learning_rate': 2.3861111111111113e-06, 'epoch': 3.1448}
{'loss': 0.8623, 'grad_norm': 2.690200574365373, 'learning_rate': 2.385e-06, 'epoch': 3.1452}
{'loss': 0.8617, 'grad_norm': 3.0317858586116233, 'learning_rate': 2.383888888888889e-06, 'epoch': 3.1456}
{'loss': 0.8764, 'grad_norm': 2.8105052019731787, 'learning_rate': 2.382777777777778e-06, 'epoch': 3.146}
{'loss': 0.8745, 'grad_norm': 2.8032078359690864, 'learning_rate': 2.381666666666667e-06, 'epoch': 3.1464}
{'loss': 0.8717, 'grad_norm': 2.87839480739396, 'learning_rate': 2.3805555555555557e-06, 'epoch': 3.1468}
{'loss': 0.8716, 'grad_norm': 2.7452378003938684, 'learning_rate': 2.3794444444444444e-06, 'epoch': 3.1471999999999998}
{'loss': 0.876, 'grad_norm': 3.0567574670659394, 'learning_rate': 2.3783333333333336e-06, 'epoch': 3.1476}
{'loss': 0.8703, 'grad_norm': 2.986350119929527, 'learning_rate': 2.3772222222222223e-06, 'epoch': 3.148}
{'eval_valid_loss': 0.84716796875, 'eval_valid_runtime': 0.0908, 'eval_valid_samples_per_second': 1101.654, 'eval_valid_steps_per_second': 275.413, 'epoch': 3.148}
{'loss': 0.8724, 'grad_norm': 3.0744770624971154, 'learning_rate': 2.3761111111111114e-06, 'epoch': 3.1484}
{'loss': 0.8823, 'grad_norm': 2.847988883686271, 'learning_rate': 2.375e-06, 'epoch': 3.1488}
{'loss': 0.882, 'grad_norm': 2.9124306003340905, 'learning_rate': 2.3738888888888893e-06, 'epoch': 3.1492}
{'loss': 0.8688, 'grad_norm': 2.924916637488002, 'learning_rate': 2.372777777777778e-06, 'epoch': 3.1496}
{'loss': 0.8658, 'grad_norm': 2.653979424924014, 'learning_rate': 2.3716666666666667e-06, 'epoch': 3.15}
{'loss': 0.879, 'grad_norm': 2.9150666048675293, 'learning_rate': 2.370555555555556e-06, 'epoch': 3.1504}
{'loss': 0.8651, 'grad_norm': 2.9489716382110003, 'learning_rate': 2.369444444444445e-06, 'epoch': 3.1508}
{'loss': 0.8778, 'grad_norm': 2.880735765811799, 'learning_rate': 2.3683333333333337e-06, 'epoch': 3.1512000000000002}
{'loss': 0.8636, 'grad_norm': 3.0952273319086934, 'learning_rate': 2.3672222222222224e-06, 'epoch': 3.1516}
{'loss': 0.8717, 'grad_norm': 2.7636480647163366, 'learning_rate': 2.366111111111111e-06, 'epoch': 3.152}
{'eval_valid_loss': 0.845703125, 'eval_valid_runtime': 0.0905, 'eval_valid_samples_per_second': 1105.285, 'eval_valid_steps_per_second': 276.321, 'epoch': 3.152}
{'loss': 0.8696, 'grad_norm': 2.871955301160427, 'learning_rate': 2.3650000000000002e-06, 'epoch': 3.1524}
{'loss': 0.8777, 'grad_norm': 2.9185834285737995, 'learning_rate': 2.3638888888888894e-06, 'epoch': 3.1528}
{'loss': 0.8716, 'grad_norm': 2.855454440935554, 'learning_rate': 2.362777777777778e-06, 'epoch': 3.1532}
{'loss': 0.8669, 'grad_norm': 2.8495160323523834, 'learning_rate': 2.3616666666666668e-06, 'epoch': 3.1536}
{'loss': 0.8732, 'grad_norm': 2.888251933742798, 'learning_rate': 2.3605555555555555e-06, 'epoch': 3.154}
{'loss': 0.8639, 'grad_norm': 3.061237084535556, 'learning_rate': 2.3594444444444446e-06, 'epoch': 3.1544}
{'loss': 0.8658, 'grad_norm': 2.5993996699676605, 'learning_rate': 2.3583333333333338e-06, 'epoch': 3.1548}
{'loss': 0.8586, 'grad_norm': 2.775067368969781, 'learning_rate': 2.3572222222222225e-06, 'epoch': 3.1552}
{'loss': 0.8634, 'grad_norm': 3.336375062641585, 'learning_rate': 2.356111111111111e-06, 'epoch': 3.1556}
{'loss': 0.8819, 'grad_norm': 3.01826501824472, 'learning_rate': 2.355e-06, 'epoch': 3.156}
{'eval_valid_loss': 0.84619140625, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.947, 'eval_valid_steps_per_second': 281.987, 'epoch': 3.156}
{'loss': 0.8784, 'grad_norm': 3.200386765695501, 'learning_rate': 2.353888888888889e-06, 'epoch': 3.1564}
{'loss': 0.8604, 'grad_norm': 3.059530852715063, 'learning_rate': 2.352777777777778e-06, 'epoch': 3.1568}
{'loss': 0.852, 'grad_norm': 2.7446424792364197, 'learning_rate': 2.351666666666667e-06, 'epoch': 3.1572}
{'loss': 0.8628, 'grad_norm': 2.9134052036438476, 'learning_rate': 2.3505555555555556e-06, 'epoch': 3.1576}
{'loss': 0.8665, 'grad_norm': 2.9433189394374986, 'learning_rate': 2.3494444444444447e-06, 'epoch': 3.158}
{'loss': 0.8717, 'grad_norm': 2.884489693226339, 'learning_rate': 2.3483333333333334e-06, 'epoch': 3.1584}
{'loss': 0.876, 'grad_norm': 3.10895498113041, 'learning_rate': 2.3472222222222226e-06, 'epoch': 3.1588}
{'loss': 0.866, 'grad_norm': 2.7474786730867944, 'learning_rate': 2.3461111111111113e-06, 'epoch': 3.1592000000000002}
{'loss': 0.8625, 'grad_norm': 2.6118198123038905, 'learning_rate': 2.345e-06, 'epoch': 3.1596}
{'loss': 0.8633, 'grad_norm': 3.0011561867868544, 'learning_rate': 2.343888888888889e-06, 'epoch': 3.16}
{'eval_valid_loss': 0.84619140625, 'eval_valid_runtime': 0.1358, 'eval_valid_samples_per_second': 736.547, 'eval_valid_steps_per_second': 184.137, 'epoch': 3.16}
{'loss': 0.8647, 'grad_norm': 2.7963408167811634, 'learning_rate': 2.342777777777778e-06, 'epoch': 3.1604}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8595, 'grad_norm': 2.619978740047145, 'learning_rate': 2.341666666666667e-06, 'epoch': 3.1608}
{'loss': 0.8773, 'grad_norm': 3.21941448500719, 'learning_rate': 2.3405555555555557e-06, 'epoch': 3.1612}
{'loss': 0.8612, 'grad_norm': 2.835857106480106, 'learning_rate': 2.339444444444445e-06, 'epoch': 3.1616}
{'loss': 0.8663, 'grad_norm': 2.8637198909740227, 'learning_rate': 2.3383333333333335e-06, 'epoch': 3.162}
{'loss': 0.8775, 'grad_norm': 2.8394258761020135, 'learning_rate': 2.3372222222222222e-06, 'epoch': 3.1624}
{'loss': 0.8674, 'grad_norm': 2.859423084584314, 'learning_rate': 2.3361111111111114e-06, 'epoch': 3.1628}
{'loss': 0.8533, 'grad_norm': 3.1420291779181357, 'learning_rate': 2.3350000000000005e-06, 'epoch': 3.1632}
{'loss': 0.8744, 'grad_norm': 3.0531752177142892, 'learning_rate': 2.3338888888888892e-06, 'epoch': 3.1636}
{'loss': 0.8806, 'grad_norm': 3.04431071825008, 'learning_rate': 2.332888888888889e-06, 'epoch': 3.164}
{'eval_valid_loss': 0.84619140625, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.664, 'eval_valid_steps_per_second': 278.666, 'epoch': 3.164}
{'loss': 0.862, 'grad_norm': 2.991761797310715, 'learning_rate': 2.3317777777777783e-06, 'epoch': 3.1644}
{'loss': 0.8731, 'grad_norm': 2.798876594927692, 'learning_rate': 2.330666666666667e-06, 'epoch': 3.1648}
{'loss': 0.875, 'grad_norm': 2.8520420037101704, 'learning_rate': 2.3295555555555557e-06, 'epoch': 3.1652}
{'loss': 0.8736, 'grad_norm': 3.0922780051149448, 'learning_rate': 2.3284444444444444e-06, 'epoch': 3.1656}
{'loss': 0.8705, 'grad_norm': 2.635295365046133, 'learning_rate': 2.3273333333333336e-06, 'epoch': 3.166}
{'loss': 0.8695, 'grad_norm': 2.9436199272883448, 'learning_rate': 2.3262222222222227e-06, 'epoch': 3.1664}
{'loss': 0.8666, 'grad_norm': 3.279434900974498, 'learning_rate': 2.3251111111111114e-06, 'epoch': 3.1668}
{'loss': 0.8648, 'grad_norm': 3.077530537243831, 'learning_rate': 2.324e-06, 'epoch': 3.1672}
{'loss': 0.8606, 'grad_norm': 2.9632481539723807, 'learning_rate': 2.322888888888889e-06, 'epoch': 3.1676}
{'loss': 0.8743, 'grad_norm': 2.881282658919392, 'learning_rate': 2.321777777777778e-06, 'epoch': 3.168}
{'eval_valid_loss': 0.84619140625, 'eval_valid_runtime': 0.0885, 'eval_valid_samples_per_second': 1129.511, 'eval_valid_steps_per_second': 282.378, 'epoch': 3.168}
{'loss': 0.8684, 'grad_norm': 2.944376718418994, 'learning_rate': 2.320666666666667e-06, 'epoch': 3.1684}
{'loss': 0.8774, 'grad_norm': 2.8755342878540393, 'learning_rate': 2.319555555555556e-06, 'epoch': 3.1688}
{'loss': 0.8589, 'grad_norm': 2.9486705043081947, 'learning_rate': 2.3184444444444445e-06, 'epoch': 3.1692}
{'loss': 0.8652, 'grad_norm': 2.947008797974814, 'learning_rate': 2.3173333333333336e-06, 'epoch': 3.1696}
{'loss': 0.8699, 'grad_norm': 2.9542857316041657, 'learning_rate': 2.3162222222222224e-06, 'epoch': 3.17}
{'loss': 0.8689, 'grad_norm': 2.9058421474294303, 'learning_rate': 2.3151111111111115e-06, 'epoch': 3.1704}
{'loss': 0.8577, 'grad_norm': 2.830069658412866, 'learning_rate': 2.314e-06, 'epoch': 3.1708}
{'loss': 0.8743, 'grad_norm': 2.695618078628453, 'learning_rate': 2.312888888888889e-06, 'epoch': 3.1712}
{'loss': 0.8636, 'grad_norm': 2.717568562852564, 'learning_rate': 2.311777777777778e-06, 'epoch': 3.1716}
{'loss': 0.8809, 'grad_norm': 2.8623567574488327, 'learning_rate': 2.3106666666666668e-06, 'epoch': 3.172}
{'eval_valid_loss': 0.84619140625, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.995, 'eval_valid_steps_per_second': 279.999, 'epoch': 3.172}
{'loss': 0.8686, 'grad_norm': 3.102709858257316, 'learning_rate': 2.309555555555556e-06, 'epoch': 3.1724}
{'loss': 0.8683, 'grad_norm': 3.3098633097029393, 'learning_rate': 2.3084444444444446e-06, 'epoch': 3.1728}
{'loss': 0.867, 'grad_norm': 2.9159853321093756, 'learning_rate': 2.3073333333333337e-06, 'epoch': 3.1732}
{'loss': 0.876, 'grad_norm': 2.6953853431478088, 'learning_rate': 2.3062222222222225e-06, 'epoch': 3.1736}
{'loss': 0.8808, 'grad_norm': 2.7291412018662937, 'learning_rate': 2.305111111111111e-06, 'epoch': 3.174}
{'loss': 0.8729, 'grad_norm': 3.0518828978271286, 'learning_rate': 2.3040000000000003e-06, 'epoch': 3.1744}
{'loss': 0.8701, 'grad_norm': 2.8302997741884854, 'learning_rate': 2.302888888888889e-06, 'epoch': 3.1748}
{'loss': 0.8713, 'grad_norm': 2.603592112419979, 'learning_rate': 2.301777777777778e-06, 'epoch': 3.1752}
{'loss': 0.8721, 'grad_norm': 3.0924341987171755, 'learning_rate': 2.300666666666667e-06, 'epoch': 3.1756}
{'loss': 0.8585, 'grad_norm': 2.908854804157038, 'learning_rate': 2.2995555555555556e-06, 'epoch': 3.176}
{'eval_valid_loss': 0.84619140625, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.748, 'eval_valid_steps_per_second': 277.187, 'epoch': 3.176}
{'loss': 0.8867, 'grad_norm': 3.163764242549385, 'learning_rate': 2.2984444444444447e-06, 'epoch': 3.1764}
{'loss': 0.8619, 'grad_norm': 2.973851047769758, 'learning_rate': 2.297333333333334e-06, 'epoch': 3.1768}
{'loss': 0.8711, 'grad_norm': 3.367833927352205, 'learning_rate': 2.2962222222222226e-06, 'epoch': 3.1772}
{'loss': 0.8597, 'grad_norm': 2.936567031541839, 'learning_rate': 2.2951111111111113e-06, 'epoch': 3.1776}
{'loss': 0.8702, 'grad_norm': 3.136543858119185, 'learning_rate': 2.294e-06, 'epoch': 3.178}
{'loss': 0.8773, 'grad_norm': 3.013441322986457, 'learning_rate': 2.292888888888889e-06, 'epoch': 3.1784}
{'loss': 0.8762, 'grad_norm': 3.023074746856776, 'learning_rate': 2.2917777777777782e-06, 'epoch': 3.1788}
{'loss': 0.8757, 'grad_norm': 2.875052150999676, 'learning_rate': 2.290666666666667e-06, 'epoch': 3.1792}
{'loss': 0.8803, 'grad_norm': 2.7679792933165004, 'learning_rate': 2.2895555555555557e-06, 'epoch': 3.1796}
{'loss': 0.8568, 'grad_norm': 2.797249688991523, 'learning_rate': 2.2884444444444444e-06, 'epoch': 3.18}
{'eval_valid_loss': 0.84521484375, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1128.384, 'eval_valid_steps_per_second': 282.096, 'epoch': 3.18}
{'loss': 0.8735, 'grad_norm': 2.9980347077063767, 'learning_rate': 2.2873333333333335e-06, 'epoch': 3.1804}
{'loss': 0.8871, 'grad_norm': 2.9780747284303817, 'learning_rate': 2.2862222222222226e-06, 'epoch': 3.1808}
{'loss': 0.8735, 'grad_norm': 3.0199452026397804, 'learning_rate': 2.2851111111111114e-06, 'epoch': 3.1812}
{'loss': 0.8706, 'grad_norm': 3.256175190071738, 'learning_rate': 2.284e-06, 'epoch': 3.1816}
{'loss': 0.8699, 'grad_norm': 2.872682668913531, 'learning_rate': 2.282888888888889e-06, 'epoch': 3.182}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8647, 'grad_norm': 3.210385797258544, 'learning_rate': 2.281777777777778e-06, 'epoch': 3.1824}
{'loss': 0.8686, 'grad_norm': 2.828346902350736, 'learning_rate': 2.280666666666667e-06, 'epoch': 3.1828}
{'loss': 0.8754, 'grad_norm': 2.9595990439564623, 'learning_rate': 2.2795555555555558e-06, 'epoch': 3.1832}
{'loss': 0.867, 'grad_norm': 3.0083099053035074, 'learning_rate': 2.2784444444444445e-06, 'epoch': 3.1836}
{'loss': 0.8721, 'grad_norm': 3.0042972483849857, 'learning_rate': 2.2773333333333336e-06, 'epoch': 3.184}
{'eval_valid_loss': 0.8466796875, 'eval_valid_runtime': 0.0918, 'eval_valid_samples_per_second': 1088.802, 'eval_valid_steps_per_second': 272.2, 'epoch': 3.184}
{'loss': 0.863, 'grad_norm': 2.7306745721740144, 'learning_rate': 2.2762222222222223e-06, 'epoch': 3.1844}
{'loss': 0.8789, 'grad_norm': 2.9981746385420185, 'learning_rate': 2.2751111111111115e-06, 'epoch': 3.1848}
{'loss': 0.8746, 'grad_norm': 2.9723973806905364, 'learning_rate': 2.274e-06, 'epoch': 3.1852}
{'loss': 0.8671, 'grad_norm': 3.0307684938709363, 'learning_rate': 2.2728888888888893e-06, 'epoch': 3.1856}
{'loss': 0.8716, 'grad_norm': 2.8255230436301777, 'learning_rate': 2.271777777777778e-06, 'epoch': 3.186}
{'loss': 0.8554, 'grad_norm': 2.7552917999047257, 'learning_rate': 2.2706666666666667e-06, 'epoch': 3.1864}
{'loss': 0.8707, 'grad_norm': 3.0706976243772837, 'learning_rate': 2.269555555555556e-06, 'epoch': 3.1868}
{'loss': 0.8683, 'grad_norm': 2.9236209671134263, 'learning_rate': 2.2684444444444446e-06, 'epoch': 3.1872}
{'loss': 0.8585, 'grad_norm': 2.79544361610877, 'learning_rate': 2.2673333333333337e-06, 'epoch': 3.1875999999999998}
{'loss': 0.8724, 'grad_norm': 2.913731860509809, 'learning_rate': 2.2662222222222224e-06, 'epoch': 3.188}
{'eval_valid_loss': 0.8466796875, 'eval_valid_runtime': 0.0909, 'eval_valid_samples_per_second': 1100.633, 'eval_valid_steps_per_second': 275.158, 'epoch': 3.188}
{'loss': 0.8824, 'grad_norm': 2.889637226728871, 'learning_rate': 2.265111111111111e-06, 'epoch': 3.1884}
{'loss': 0.8774, 'grad_norm': 3.679630546372039, 'learning_rate': 2.2640000000000003e-06, 'epoch': 3.1888}
{'loss': 0.868, 'grad_norm': 2.917053712413755, 'learning_rate': 2.2628888888888894e-06, 'epoch': 3.1892}
{'loss': 0.8704, 'grad_norm': 2.7940847021410513, 'learning_rate': 2.261777777777778e-06, 'epoch': 3.1896}
{'loss': 0.8791, 'grad_norm': 2.680112816113905, 'learning_rate': 2.260666666666667e-06, 'epoch': 3.19}
{'loss': 0.8634, 'grad_norm': 3.167198934285056, 'learning_rate': 2.2595555555555555e-06, 'epoch': 3.1904}
{'loss': 0.8755, 'grad_norm': 2.9201544117272427, 'learning_rate': 2.2584444444444447e-06, 'epoch': 3.1908}
{'loss': 0.8765, 'grad_norm': 2.903973990515678, 'learning_rate': 2.257333333333334e-06, 'epoch': 3.1912}
{'loss': 0.873, 'grad_norm': 2.7392436797241535, 'learning_rate': 2.2562222222222225e-06, 'epoch': 3.1916}
{'loss': 0.8774, 'grad_norm': 2.9009761848570172, 'learning_rate': 2.2551111111111112e-06, 'epoch': 3.192}
{'eval_valid_loss': 0.84521484375, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1110.841, 'eval_valid_steps_per_second': 277.71, 'epoch': 3.192}
{'loss': 0.8683, 'grad_norm': 3.0281565897802603, 'learning_rate': 2.254e-06, 'epoch': 3.1924}
{'loss': 0.8649, 'grad_norm': 3.0790820788085544, 'learning_rate': 2.252888888888889e-06, 'epoch': 3.1928}
{'loss': 0.8731, 'grad_norm': 3.1213906997909726, 'learning_rate': 2.251777777777778e-06, 'epoch': 3.1932}
{'loss': 0.8688, 'grad_norm': 2.7474024978966654, 'learning_rate': 2.250666666666667e-06, 'epoch': 3.1936}
{'loss': 0.8643, 'grad_norm': 2.8647897553485504, 'learning_rate': 2.2495555555555556e-06, 'epoch': 3.194}
{'loss': 0.8725, 'grad_norm': 3.139850350112224, 'learning_rate': 2.2484444444444448e-06, 'epoch': 3.1944}
{'loss': 0.868, 'grad_norm': 3.1927080335922886, 'learning_rate': 2.2473333333333335e-06, 'epoch': 3.1948}
{'loss': 0.8648, 'grad_norm': 2.986878628134894, 'learning_rate': 2.2462222222222226e-06, 'epoch': 3.1952}
{'loss': 0.8794, 'grad_norm': 2.9447157581604526, 'learning_rate': 2.2451111111111113e-06, 'epoch': 3.1955999999999998}
{'loss': 0.8854, 'grad_norm': 3.0478643816159194, 'learning_rate': 2.244e-06, 'epoch': 3.196}
{'eval_valid_loss': 0.84521484375, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1111.28, 'eval_valid_steps_per_second': 277.82, 'epoch': 3.196}
{'loss': 0.8527, 'grad_norm': 2.7959473632624885, 'learning_rate': 2.242888888888889e-06, 'epoch': 3.1964}
{'loss': 0.8808, 'grad_norm': 2.8677749239922847, 'learning_rate': 2.241777777777778e-06, 'epoch': 3.1968}
{'loss': 0.87, 'grad_norm': 3.0808383678301934, 'learning_rate': 2.240666666666667e-06, 'epoch': 3.1972}
{'loss': 0.8708, 'grad_norm': 3.23446953676803, 'learning_rate': 2.2395555555555557e-06, 'epoch': 3.1976}
{'loss': 0.8753, 'grad_norm': 2.977123057529956, 'learning_rate': 2.238444444444445e-06, 'epoch': 3.198}
{'loss': 0.864, 'grad_norm': 2.813931037505155, 'learning_rate': 2.2373333333333336e-06, 'epoch': 3.1984}
{'loss': 0.8657, 'grad_norm': 2.98949639042204, 'learning_rate': 2.2362222222222223e-06, 'epoch': 3.1988}
{'loss': 0.859, 'grad_norm': 3.1599590867146348, 'learning_rate': 2.2351111111111114e-06, 'epoch': 3.1992}
{'loss': 0.876, 'grad_norm': 2.794778000429351, 'learning_rate': 2.234e-06, 'epoch': 3.1996}
{'loss': 0.862, 'grad_norm': 3.0652737998638915, 'learning_rate': 2.2328888888888893e-06, 'epoch': 3.2}
{'eval_valid_loss': 0.84521484375, 'eval_valid_runtime': 0.0885, 'eval_valid_samples_per_second': 1129.916, 'eval_valid_steps_per_second': 282.479, 'epoch': 3.2}
{'loss': 0.8696, 'grad_norm': 2.7620080475313595, 'learning_rate': 2.231777777777778e-06, 'epoch': 3.2004}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8733, 'grad_norm': 2.6376268589101266, 'learning_rate': 2.2306666666666667e-06, 'epoch': 3.2008}
{'loss': 0.8656, 'grad_norm': 2.9794080654235624, 'learning_rate': 2.229555555555556e-06, 'epoch': 3.2012}
{'loss': 0.8516, 'grad_norm': 2.9069475649378345, 'learning_rate': 2.228444444444445e-06, 'epoch': 3.2016}
{'loss': 0.8708, 'grad_norm': 3.1594522113968044, 'learning_rate': 2.2273333333333337e-06, 'epoch': 3.202}
{'loss': 0.8711, 'grad_norm': 2.815992391074803, 'learning_rate': 2.2262222222222224e-06, 'epoch': 3.2024}
{'loss': 0.8653, 'grad_norm': 2.7336834441736, 'learning_rate': 2.225111111111111e-06, 'epoch': 3.2028}
{'loss': 0.8568, 'grad_norm': 2.91438733967282, 'learning_rate': 2.2240000000000002e-06, 'epoch': 3.2032}
{'loss': 0.8732, 'grad_norm': 2.943685851440927, 'learning_rate': 2.2228888888888894e-06, 'epoch': 3.2036}
{'loss': 0.8701, 'grad_norm': 2.84624251448862, 'learning_rate': 2.221888888888889e-06, 'epoch': 3.204}
{'eval_valid_loss': 0.8466796875, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1105.804, 'eval_valid_steps_per_second': 276.451, 'epoch': 3.204}
{'loss': 0.8687, 'grad_norm': 3.134046221268705, 'learning_rate': 2.220777777777778e-06, 'epoch': 3.2044}
{'loss': 0.8537, 'grad_norm': 2.8230860854028466, 'learning_rate': 2.2196666666666667e-06, 'epoch': 3.2048}
{'loss': 0.868, 'grad_norm': 3.113784912033103, 'learning_rate': 2.218555555555556e-06, 'epoch': 3.2052}
{'loss': 0.8623, 'grad_norm': 2.6269848109039615, 'learning_rate': 2.2174444444444445e-06, 'epoch': 3.2056}
{'loss': 0.865, 'grad_norm': 2.909211106401205, 'learning_rate': 2.2163333333333333e-06, 'epoch': 3.206}
{'loss': 0.8731, 'grad_norm': 2.7177089418961553, 'learning_rate': 2.2152222222222224e-06, 'epoch': 3.2064}
{'loss': 0.8645, 'grad_norm': 2.7659080166592713, 'learning_rate': 2.2141111111111115e-06, 'epoch': 3.2068}
{'loss': 0.8578, 'grad_norm': 2.9597395635099124, 'learning_rate': 2.2130000000000002e-06, 'epoch': 3.2072}
{'loss': 0.8725, 'grad_norm': 2.6194363853053737, 'learning_rate': 2.211888888888889e-06, 'epoch': 3.2076000000000002}
{'loss': 0.8724, 'grad_norm': 2.783265439055362, 'learning_rate': 2.210777777777778e-06, 'epoch': 3.208}
{'eval_valid_loss': 0.845703125, 'eval_valid_runtime': 0.0912, 'eval_valid_samples_per_second': 1096.135, 'eval_valid_steps_per_second': 274.034, 'epoch': 3.208}
{'loss': 0.8718, 'grad_norm': 2.757071614368567, 'learning_rate': 2.209666666666667e-06, 'epoch': 3.2084}
{'loss': 0.8701, 'grad_norm': 2.9113467501627146, 'learning_rate': 2.208555555555556e-06, 'epoch': 3.2088}
{'loss': 0.8704, 'grad_norm': 2.6576349126208303, 'learning_rate': 2.2074444444444446e-06, 'epoch': 3.2092}
{'loss': 0.8651, 'grad_norm': 2.915507257584338, 'learning_rate': 2.2063333333333334e-06, 'epoch': 3.2096}
{'loss': 0.8706, 'grad_norm': 3.1216970918078695, 'learning_rate': 2.2052222222222225e-06, 'epoch': 3.21}
{'loss': 0.859, 'grad_norm': 2.7210534357002794, 'learning_rate': 2.204111111111111e-06, 'epoch': 3.2104}
{'loss': 0.8737, 'grad_norm': 2.704857871663465, 'learning_rate': 2.2030000000000003e-06, 'epoch': 3.2108}
{'loss': 0.8624, 'grad_norm': 3.0457179684758575, 'learning_rate': 2.201888888888889e-06, 'epoch': 3.2112}
{'loss': 0.8612, 'grad_norm': 2.850821877020444, 'learning_rate': 2.200777777777778e-06, 'epoch': 3.2116}
{'loss': 0.8841, 'grad_norm': 2.999146737508302, 'learning_rate': 2.199666666666667e-06, 'epoch': 3.212}
{'eval_valid_loss': 0.845703125, 'eval_valid_runtime': 0.0905, 'eval_valid_samples_per_second': 1104.817, 'eval_valid_steps_per_second': 276.204, 'epoch': 3.212}
{'loss': 0.8624, 'grad_norm': 2.866987850295855, 'learning_rate': 2.1985555555555556e-06, 'epoch': 3.2124}
{'loss': 0.8688, 'grad_norm': 3.0283925657685193, 'learning_rate': 2.1974444444444447e-06, 'epoch': 3.2128}
{'loss': 0.8651, 'grad_norm': 3.046477042398581, 'learning_rate': 2.1963333333333335e-06, 'epoch': 3.2132}
{'loss': 0.8688, 'grad_norm': 2.9762455028033736, 'learning_rate': 2.1952222222222226e-06, 'epoch': 3.2136}
{'loss': 0.8733, 'grad_norm': 2.8721406753569836, 'learning_rate': 2.1941111111111113e-06, 'epoch': 3.214}
{'loss': 0.8696, 'grad_norm': 2.8028578568215723, 'learning_rate': 2.193e-06, 'epoch': 3.2144}
{'loss': 0.8729, 'grad_norm': 3.196653108176899, 'learning_rate': 2.191888888888889e-06, 'epoch': 3.2148}
{'loss': 0.8571, 'grad_norm': 2.8376598014870233, 'learning_rate': 2.190777777777778e-06, 'epoch': 3.2152}
{'loss': 0.8695, 'grad_norm': 3.294143530555872, 'learning_rate': 2.189666666666667e-06, 'epoch': 3.2156000000000002}
{'loss': 0.8686, 'grad_norm': 2.8371724889185663, 'learning_rate': 2.1885555555555557e-06, 'epoch': 3.216}
{'eval_valid_loss': 0.845703125, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.828, 'eval_valid_steps_per_second': 279.957, 'epoch': 3.216}
{'loss': 0.8768, 'grad_norm': 3.049370521327891, 'learning_rate': 2.1874444444444444e-06, 'epoch': 3.2164}
{'loss': 0.8758, 'grad_norm': 2.765578215682881, 'learning_rate': 2.1863333333333335e-06, 'epoch': 3.2168}
{'loss': 0.87, 'grad_norm': 3.6501076283981937, 'learning_rate': 2.1852222222222223e-06, 'epoch': 3.2172}
{'loss': 0.8677, 'grad_norm': 3.4270255461732178, 'learning_rate': 2.1841111111111114e-06, 'epoch': 3.2176}
{'loss': 0.877, 'grad_norm': 3.1143442538475967, 'learning_rate': 2.183e-06, 'epoch': 3.218}
{'loss': 0.8742, 'grad_norm': 2.9484064057069554, 'learning_rate': 2.181888888888889e-06, 'epoch': 3.2184}
{'loss': 0.8713, 'grad_norm': 2.9031782118701344, 'learning_rate': 2.180777777777778e-06, 'epoch': 3.2188}
{'loss': 0.8786, 'grad_norm': 2.847546108297888, 'learning_rate': 2.179666666666667e-06, 'epoch': 3.2192}
{'loss': 0.8625, 'grad_norm': 2.89223193274134, 'learning_rate': 2.178555555555556e-06, 'epoch': 3.2196}
{'loss': 0.876, 'grad_norm': 2.930133933067415, 'learning_rate': 2.1774444444444445e-06, 'epoch': 3.22}
{'eval_valid_loss': 0.8447265625, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.241, 'eval_valid_steps_per_second': 277.06, 'epoch': 3.22}
{'loss': 0.8712, 'grad_norm': 3.0480500425219383, 'learning_rate': 2.1763333333333336e-06, 'epoch': 3.2204}
{'loss': 0.8701, 'grad_norm': 2.9264103578531238, 'learning_rate': 2.1752222222222224e-06, 'epoch': 3.2208}
{'loss': 0.8678, 'grad_norm': 3.0093288909295226, 'learning_rate': 2.1741111111111115e-06, 'epoch': 3.2212}
{'loss': 0.8747, 'grad_norm': 3.1347400509750716, 'learning_rate': 2.173e-06, 'epoch': 3.2216}
{'loss': 0.8688, 'grad_norm': 3.22485097422526, 'learning_rate': 2.171888888888889e-06, 'epoch': 3.222}
{'loss': 0.8775, 'grad_norm': 3.080331147323131, 'learning_rate': 2.170777777777778e-06, 'epoch': 3.2224}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8842, 'grad_norm': 3.266653656365962, 'learning_rate': 2.1696666666666668e-06, 'epoch': 3.2228}
{'loss': 0.8692, 'grad_norm': 2.9604248468678422, 'learning_rate': 2.168555555555556e-06, 'epoch': 3.2232}
{'loss': 0.8655, 'grad_norm': 2.932761885826629, 'learning_rate': 2.1674444444444446e-06, 'epoch': 3.2236}
{'loss': 0.8786, 'grad_norm': 2.9337354999130265, 'learning_rate': 2.1663333333333337e-06, 'epoch': 3.224}
{'eval_valid_loss': 0.84521484375, 'eval_valid_runtime': 0.0958, 'eval_valid_samples_per_second': 1044.349, 'eval_valid_steps_per_second': 261.087, 'epoch': 3.224}
{'loss': 0.8755, 'grad_norm': 3.1300022715417097, 'learning_rate': 2.1652222222222225e-06, 'epoch': 3.2244}
{'loss': 0.8609, 'grad_norm': 3.1206254094477113, 'learning_rate': 2.164111111111111e-06, 'epoch': 3.2248}
{'loss': 0.8521, 'grad_norm': 2.752653586462187, 'learning_rate': 2.1630000000000003e-06, 'epoch': 3.2252}
{'loss': 0.868, 'grad_norm': 2.871074837343374, 'learning_rate': 2.161888888888889e-06, 'epoch': 3.2256}
{'loss': 0.8674, 'grad_norm': 3.308459300582545, 'learning_rate': 2.160777777777778e-06, 'epoch': 3.226}
{'loss': 0.865, 'grad_norm': 2.893228770661463, 'learning_rate': 2.159666666666667e-06, 'epoch': 3.2264}
{'loss': 0.8692, 'grad_norm': 2.8835056371049994, 'learning_rate': 2.1585555555555556e-06, 'epoch': 3.2268}
{'loss': 0.8604, 'grad_norm': 3.055959685409223, 'learning_rate': 2.1574444444444447e-06, 'epoch': 3.2272}
{'loss': 0.8551, 'grad_norm': 3.070212122317386, 'learning_rate': 2.1563333333333334e-06, 'epoch': 3.2276}
{'loss': 0.8594, 'grad_norm': 2.965845092227781, 'learning_rate': 2.1552222222222225e-06, 'epoch': 3.228}
{'eval_valid_loss': 0.8447265625, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1105.83, 'eval_valid_steps_per_second': 276.458, 'epoch': 3.228}
{'loss': 0.8649, 'grad_norm': 2.961496738171846, 'learning_rate': 2.1541111111111113e-06, 'epoch': 3.2284}
{'loss': 0.8698, 'grad_norm': 3.045560103127591, 'learning_rate': 2.153e-06, 'epoch': 3.2288}
{'loss': 0.8669, 'grad_norm': 2.7101722978527514, 'learning_rate': 2.151888888888889e-06, 'epoch': 3.2292}
{'loss': 0.8724, 'grad_norm': 3.0860377041929414, 'learning_rate': 2.150777777777778e-06, 'epoch': 3.2296}
{'loss': 0.8702, 'grad_norm': 2.703750383656581, 'learning_rate': 2.149666666666667e-06, 'epoch': 3.23}
{'loss': 0.8702, 'grad_norm': 3.0311100032765297, 'learning_rate': 2.1485555555555557e-06, 'epoch': 3.2304}
{'loss': 0.8731, 'grad_norm': 3.065051427023919, 'learning_rate': 2.1474444444444444e-06, 'epoch': 3.2308}
{'loss': 0.8784, 'grad_norm': 2.8430571812532546, 'learning_rate': 2.1463333333333335e-06, 'epoch': 3.2312}
{'loss': 0.8688, 'grad_norm': 3.063131004643621, 'learning_rate': 2.1452222222222222e-06, 'epoch': 3.2316}
{'loss': 0.8662, 'grad_norm': 2.8483432417281125, 'learning_rate': 2.1441111111111114e-06, 'epoch': 3.232}
{'eval_valid_loss': 0.84423828125, 'eval_valid_runtime': 0.0916, 'eval_valid_samples_per_second': 1091.173, 'eval_valid_steps_per_second': 272.793, 'epoch': 3.232}
{'loss': 0.8587, 'grad_norm': 3.064528776150417, 'learning_rate': 2.143e-06, 'epoch': 3.2324}
{'loss': 0.8708, 'grad_norm': 3.04581097339367, 'learning_rate': 2.141888888888889e-06, 'epoch': 3.2328}
{'loss': 0.8631, 'grad_norm': 2.8547355138356187, 'learning_rate': 2.140777777777778e-06, 'epoch': 3.2332}
{'loss': 0.8637, 'grad_norm': 2.937232411148068, 'learning_rate': 2.139666666666667e-06, 'epoch': 3.2336}
{'loss': 0.8697, 'grad_norm': 3.1388405907036803, 'learning_rate': 2.1385555555555558e-06, 'epoch': 3.234}
{'loss': 0.8615, 'grad_norm': 3.1243370020418295, 'learning_rate': 2.1374444444444445e-06, 'epoch': 3.2344}
{'loss': 0.8794, 'grad_norm': 2.8746660650204525, 'learning_rate': 2.1363333333333336e-06, 'epoch': 3.2348}
{'loss': 0.8852, 'grad_norm': 2.904656270614936, 'learning_rate': 2.1352222222222223e-06, 'epoch': 3.2352}
{'loss': 0.8734, 'grad_norm': 3.090233046437724, 'learning_rate': 2.1341111111111114e-06, 'epoch': 3.2356}
{'loss': 0.8818, 'grad_norm': 3.203399264991539, 'learning_rate': 2.133e-06, 'epoch': 3.2359999999999998}
{'eval_valid_loss': 0.84423828125, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.008, 'eval_valid_steps_per_second': 279.002, 'epoch': 3.2359999999999998}
{'loss': 0.8675, 'grad_norm': 3.088517165877549, 'learning_rate': 2.1318888888888893e-06, 'epoch': 3.2364}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.8621, 'grad_norm': 3.101047463497595, 'learning_rate': 2.130777777777778e-06, 'epoch': 3.2368}
{'loss': 0.8685, 'grad_norm': 2.8016915059213026, 'learning_rate': 2.1296666666666667e-06, 'epoch': 3.2372}
{'loss': 0.8785, 'grad_norm': 3.057488311127647, 'learning_rate': 2.128555555555556e-06, 'epoch': 3.2376}
{'loss': 0.8775, 'grad_norm': 2.955314601555789, 'learning_rate': 2.1274444444444446e-06, 'epoch': 3.238}
{'loss': 0.8683, 'grad_norm': 3.0163266151426615, 'learning_rate': 2.1263333333333337e-06, 'epoch': 3.2384}
{'loss': 0.8639, 'grad_norm': 3.062431646090259, 'learning_rate': 2.1252222222222224e-06, 'epoch': 3.2388}
{'loss': 0.871, 'grad_norm': 2.521475563654437, 'learning_rate': 2.124111111111111e-06, 'epoch': 3.2392}
{'loss': 0.8743, 'grad_norm': 2.993893657335522, 'learning_rate': 2.1230000000000003e-06, 'epoch': 3.2396}
{'loss': 0.8654, 'grad_norm': 3.083614972285328, 'learning_rate': 2.121888888888889e-06, 'epoch': 3.24}
{'eval_valid_loss': 0.8447265625, 'eval_valid_runtime': 0.089, 'eval_valid_samples_per_second': 1123.159, 'eval_valid_steps_per_second': 280.79, 'epoch': 3.24}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8749, 'grad_norm': 2.8535948421858635, 'learning_rate': 2.120777777777778e-06, 'epoch': 3.2404}
{'loss': 0.8642, 'grad_norm': 2.8469178455114252, 'learning_rate': 2.119666666666667e-06, 'epoch': 3.2408}
{'loss': 0.8658, 'grad_norm': 2.6231168338538544, 'learning_rate': 2.1185555555555555e-06, 'epoch': 3.2412}
{'loss': 0.8679, 'grad_norm': 2.7975498589358585, 'learning_rate': 2.1174444444444447e-06, 'epoch': 3.2416}
{'loss': 0.8791, 'grad_norm': 2.9551827159457473, 'learning_rate': 2.1163333333333334e-06, 'epoch': 3.242}
{'loss': 0.8707, 'grad_norm': 2.8401227601477634, 'learning_rate': 2.1152222222222225e-06, 'epoch': 3.2424}
{'loss': 0.8711, 'grad_norm': 3.154429108768026, 'learning_rate': 2.1141111111111112e-06, 'epoch': 3.2428}
{'loss': 0.8711, 'grad_norm': 3.323675727645635, 'learning_rate': 2.113e-06, 'epoch': 3.2432}
{'loss': 0.871, 'grad_norm': 2.82533030709924, 'learning_rate': 2.111888888888889e-06, 'epoch': 3.2436}
{'loss': 0.8737, 'grad_norm': 2.9111445137820176, 'learning_rate': 2.110888888888889e-06, 'epoch': 3.2439999999999998}
{'eval_valid_loss': 0.8447265625, 'eval_valid_runtime': 0.0911, 'eval_valid_samples_per_second': 1098.184, 'eval_valid_steps_per_second': 274.546, 'epoch': 3.2439999999999998}
{'loss': 0.866, 'grad_norm': 3.0398522456552124, 'learning_rate': 2.1097777777777777e-06, 'epoch': 3.2444}
{'loss': 0.8567, 'grad_norm': 2.7040584914833405, 'learning_rate': 2.108666666666667e-06, 'epoch': 3.2448}
{'loss': 0.8768, 'grad_norm': 3.3862214652823934, 'learning_rate': 2.1075555555555555e-06, 'epoch': 3.2452}
{'loss': 0.8683, 'grad_norm': 2.9149619956508115, 'learning_rate': 2.1064444444444447e-06, 'epoch': 3.2456}
{'loss': 0.878, 'grad_norm': 3.0414820260133424, 'learning_rate': 2.1053333333333334e-06, 'epoch': 3.246}
{'loss': 0.8766, 'grad_norm': 2.913300197188822, 'learning_rate': 2.1042222222222225e-06, 'epoch': 3.2464}
{'loss': 0.8672, 'grad_norm': 3.0532372976245177, 'learning_rate': 2.1031111111111112e-06, 'epoch': 3.2468}
{'loss': 0.8631, 'grad_norm': 2.4670150491215264, 'learning_rate': 2.102e-06, 'epoch': 3.2472}
{'loss': 0.8608, 'grad_norm': 2.7472692065715294, 'learning_rate': 2.100888888888889e-06, 'epoch': 3.2476}
{'loss': 0.8659, 'grad_norm': 3.1309060172392256, 'learning_rate': 2.0997777777777782e-06, 'epoch': 3.248}
{'eval_valid_loss': 0.8447265625, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1110.677, 'eval_valid_steps_per_second': 277.669, 'epoch': 3.248}
{'loss': 0.8688, 'grad_norm': 2.7204189220560706, 'learning_rate': 2.098666666666667e-06, 'epoch': 3.2484}
{'loss': 0.8586, 'grad_norm': 2.9629214844074183, 'learning_rate': 2.0975555555555556e-06, 'epoch': 3.2488}
{'loss': 0.8714, 'grad_norm': 3.0097804183645915, 'learning_rate': 2.0964444444444444e-06, 'epoch': 3.2492}
{'loss': 0.8909, 'grad_norm': 3.0001889805080957, 'learning_rate': 2.0953333333333335e-06, 'epoch': 3.2496}
{'loss': 0.8739, 'grad_norm': 2.8805177026928748, 'learning_rate': 2.0942222222222226e-06, 'epoch': 3.25}
{'loss': 0.8737, 'grad_norm': 2.9986029690104883, 'learning_rate': 2.0931111111111113e-06, 'epoch': 3.2504}
{'loss': 0.86, 'grad_norm': 2.749584231591592, 'learning_rate': 2.092e-06, 'epoch': 3.2508}
{'loss': 0.883, 'grad_norm': 3.110886709506594, 'learning_rate': 2.0908888888888888e-06, 'epoch': 3.2512}
{'loss': 0.8682, 'grad_norm': 2.566597790588668, 'learning_rate': 2.089777777777778e-06, 'epoch': 3.2516}
{'loss': 0.8658, 'grad_norm': 3.1226471912977023, 'learning_rate': 2.088666666666667e-06, 'epoch': 3.252}
{'eval_valid_loss': 0.8447265625, 'eval_valid_runtime': 0.1094, 'eval_valid_samples_per_second': 913.802, 'eval_valid_steps_per_second': 228.45, 'epoch': 3.252}
{'loss': 0.8669, 'grad_norm': 2.8401352524200245, 'learning_rate': 2.0875555555555557e-06, 'epoch': 3.2524}
{'loss': 0.8697, 'grad_norm': 2.8554006273568886, 'learning_rate': 2.0864444444444444e-06, 'epoch': 3.2528}
{'loss': 0.8681, 'grad_norm': 2.8633693145490344, 'learning_rate': 2.0853333333333336e-06, 'epoch': 3.2532}
{'loss': 0.857, 'grad_norm': 2.6955751152915317, 'learning_rate': 2.0842222222222223e-06, 'epoch': 3.2536}
{'loss': 0.8668, 'grad_norm': 2.986580308504048, 'learning_rate': 2.0831111111111114e-06, 'epoch': 3.254}
{'loss': 0.8688, 'grad_norm': 2.9853253074841666, 'learning_rate': 2.082e-06, 'epoch': 3.2544}
{'loss': 0.8747, 'grad_norm': 2.916062453673875, 'learning_rate': 2.080888888888889e-06, 'epoch': 3.2548}
{'loss': 0.8638, 'grad_norm': 3.034604242074624, 'learning_rate': 2.079777777777778e-06, 'epoch': 3.2552}
{'loss': 0.8629, 'grad_norm': 3.189572268408069, 'learning_rate': 2.0786666666666667e-06, 'epoch': 3.2556}
{'loss': 0.8554, 'grad_norm': 2.901544792108933, 'learning_rate': 2.077555555555556e-06, 'epoch': 3.2560000000000002}
{'eval_valid_loss': 0.8447265625, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1126.511, 'eval_valid_steps_per_second': 281.628, 'epoch': 3.2560000000000002}
{'loss': 0.885, 'grad_norm': 2.8292373924844125, 'learning_rate': 2.0764444444444445e-06, 'epoch': 3.2564}
{'loss': 0.8687, 'grad_norm': 3.069629234018073, 'learning_rate': 2.0753333333333333e-06, 'epoch': 3.2568}
{'loss': 0.8635, 'grad_norm': 2.927809410838691, 'learning_rate': 2.0742222222222224e-06, 'epoch': 3.2572}
{'loss': 0.8642, 'grad_norm': 2.8278107284100242, 'learning_rate': 2.073111111111111e-06, 'epoch': 3.2576}
{'loss': 0.8687, 'grad_norm': 3.1205439268277377, 'learning_rate': 2.0720000000000002e-06, 'epoch': 3.258}
{'loss': 0.8808, 'grad_norm': 2.7463713732148514, 'learning_rate': 2.070888888888889e-06, 'epoch': 3.2584}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8734, 'grad_norm': 3.220353578900563, 'learning_rate': 2.069777777777778e-06, 'epoch': 3.2588}
{'loss': 0.8717, 'grad_norm': 3.1220793236204805, 'learning_rate': 2.068666666666667e-06, 'epoch': 3.2592}
{'loss': 0.8631, 'grad_norm': 3.1992112044360703, 'learning_rate': 2.0675555555555555e-06, 'epoch': 3.2596}
{'loss': 0.8702, 'grad_norm': 2.841518448761202, 'learning_rate': 2.0664444444444446e-06, 'epoch': 3.26}
{'eval_valid_loss': 0.84375, 'eval_valid_runtime': 0.0883, 'eval_valid_samples_per_second': 1132.943, 'eval_valid_steps_per_second': 283.236, 'epoch': 3.26}
{'loss': 0.8687, 'grad_norm': 2.934657060353393, 'learning_rate': 2.0653333333333338e-06, 'epoch': 3.2604}
{'loss': 0.88, 'grad_norm': 3.0389475862013104, 'learning_rate': 2.0642222222222225e-06, 'epoch': 3.2608}
{'loss': 0.8669, 'grad_norm': 2.8127206291975595, 'learning_rate': 2.063111111111111e-06, 'epoch': 3.2612}
{'loss': 0.8699, 'grad_norm': 3.0720562339090023, 'learning_rate': 2.062e-06, 'epoch': 3.2616}
{'loss': 0.8713, 'grad_norm': 2.9560173745599685, 'learning_rate': 2.060888888888889e-06, 'epoch': 3.262}
{'loss': 0.8781, 'grad_norm': 2.872693193710093, 'learning_rate': 2.059777777777778e-06, 'epoch': 3.2624}
{'loss': 0.8709, 'grad_norm': 3.017135775343497, 'learning_rate': 2.058666666666667e-06, 'epoch': 3.2628}
{'loss': 0.8555, 'grad_norm': 2.9093951158002285, 'learning_rate': 2.0575555555555556e-06, 'epoch': 3.2632}
{'loss': 0.8711, 'grad_norm': 2.8133204429076537, 'learning_rate': 2.0564444444444443e-06, 'epoch': 3.2636}
{'loss': 0.8731, 'grad_norm': 2.911347973437399, 'learning_rate': 2.0553333333333334e-06, 'epoch': 3.2640000000000002}
{'eval_valid_loss': 0.84375, 'eval_valid_runtime': 0.0883, 'eval_valid_samples_per_second': 1132.228, 'eval_valid_steps_per_second': 283.057, 'epoch': 3.2640000000000002}
{'loss': 0.8778, 'grad_norm': 3.070637528005519, 'learning_rate': 2.0542222222222226e-06, 'epoch': 3.2644}
{'loss': 0.8627, 'grad_norm': 2.6895118712503465, 'learning_rate': 2.0531111111111113e-06, 'epoch': 3.2648}
{'loss': 0.8745, 'grad_norm': 3.1368637051467347, 'learning_rate': 2.052e-06, 'epoch': 3.2652}
{'loss': 0.8767, 'grad_norm': 2.877337029371821, 'learning_rate': 2.050888888888889e-06, 'epoch': 3.2656}
{'loss': 0.8693, 'grad_norm': 3.0325329611953245, 'learning_rate': 2.049777777777778e-06, 'epoch': 3.266}
{'loss': 0.8749, 'grad_norm': 3.020757991026607, 'learning_rate': 2.048666666666667e-06, 'epoch': 3.2664}
{'loss': 0.8729, 'grad_norm': 2.9804784092177914, 'learning_rate': 2.0475555555555557e-06, 'epoch': 3.2668}
{'loss': 0.8727, 'grad_norm': 3.4296196689719305, 'learning_rate': 2.0464444444444444e-06, 'epoch': 3.2672}
{'loss': 0.877, 'grad_norm': 3.16582182020996, 'learning_rate': 2.0453333333333335e-06, 'epoch': 3.2676}
{'loss': 0.866, 'grad_norm': 2.707710373587555, 'learning_rate': 2.0442222222222223e-06, 'epoch': 3.268}
{'eval_valid_loss': 0.84423828125, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1109.469, 'eval_valid_steps_per_second': 277.367, 'epoch': 3.268}
{'loss': 0.8587, 'grad_norm': 2.993266854691403, 'learning_rate': 2.0431111111111114e-06, 'epoch': 3.2684}
{'loss': 0.8613, 'grad_norm': 2.970041686690264, 'learning_rate': 2.042e-06, 'epoch': 3.2688}
{'loss': 0.872, 'grad_norm': 3.0798477251167418, 'learning_rate': 2.040888888888889e-06, 'epoch': 3.2692}
{'loss': 0.8715, 'grad_norm': 3.1066316070867037, 'learning_rate': 2.039777777777778e-06, 'epoch': 3.2696}
{'loss': 0.8649, 'grad_norm': 2.9620260638182394, 'learning_rate': 2.0386666666666667e-06, 'epoch': 3.27}
{'loss': 0.8723, 'grad_norm': 3.021856264137168, 'learning_rate': 2.037555555555556e-06, 'epoch': 3.2704}
{'loss': 0.8729, 'grad_norm': 2.9040357912330195, 'learning_rate': 2.0364444444444445e-06, 'epoch': 3.2708}
{'loss': 0.8684, 'grad_norm': 2.9015005588853064, 'learning_rate': 2.0353333333333336e-06, 'epoch': 3.2712}
{'loss': 0.8679, 'grad_norm': 3.0791316347017803, 'learning_rate': 2.0342222222222223e-06, 'epoch': 3.2716}
{'loss': 0.8731, 'grad_norm': 3.057981125241941, 'learning_rate': 2.033111111111111e-06, 'epoch': 3.2720000000000002}
{'eval_valid_loss': 0.84423828125, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1110.562, 'eval_valid_steps_per_second': 277.641, 'epoch': 3.2720000000000002}
{'loss': 0.8697, 'grad_norm': 3.5954811239010978, 'learning_rate': 2.032e-06, 'epoch': 3.2724}
{'loss': 0.8708, 'grad_norm': 2.90556733512152, 'learning_rate': 2.0308888888888893e-06, 'epoch': 3.2728}
{'loss': 0.8677, 'grad_norm': 3.0147017118036548, 'learning_rate': 2.029777777777778e-06, 'epoch': 3.2732}
{'loss': 0.8667, 'grad_norm': 2.884932712682609, 'learning_rate': 2.0286666666666668e-06, 'epoch': 3.2736}
{'loss': 0.8724, 'grad_norm': 2.767533845082874, 'learning_rate': 2.0275555555555555e-06, 'epoch': 3.274}
{'loss': 0.8712, 'grad_norm': 3.2449641827339097, 'learning_rate': 2.0264444444444446e-06, 'epoch': 3.2744}
{'loss': 0.872, 'grad_norm': 2.8723816143570478, 'learning_rate': 2.0253333333333337e-06, 'epoch': 3.2748}
{'loss': 0.87, 'grad_norm': 3.0742666587344116, 'learning_rate': 2.0242222222222224e-06, 'epoch': 3.2752}
{'loss': 0.876, 'grad_norm': 2.609430603759772, 'learning_rate': 2.023111111111111e-06, 'epoch': 3.2756}
{'loss': 0.8789, 'grad_norm': 2.8937943687909886, 'learning_rate': 2.022e-06, 'epoch': 3.276}
{'eval_valid_loss': 0.8447265625, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1124.723, 'eval_valid_steps_per_second': 281.181, 'epoch': 3.276}
{'loss': 0.8736, 'grad_norm': 3.068840722415347, 'learning_rate': 2.020888888888889e-06, 'epoch': 3.2763999999999998}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.8693, 'grad_norm': 2.939673948389535, 'learning_rate': 2.019777777777778e-06, 'epoch': 3.2768}
{'loss': 0.8677, 'grad_norm': 2.8293984428228365, 'learning_rate': 2.018666666666667e-06, 'epoch': 3.2772}
{'loss': 0.8667, 'grad_norm': 2.9587639452722874, 'learning_rate': 2.0175555555555556e-06, 'epoch': 3.2776}
{'loss': 0.8689, 'grad_norm': 2.7700455622506475, 'learning_rate': 2.0164444444444443e-06, 'epoch': 3.278}
{'loss': 0.8799, 'grad_norm': 2.8150415540019975, 'learning_rate': 2.0153333333333334e-06, 'epoch': 3.2784}
{'loss': 0.8743, 'grad_norm': 2.710120141104134, 'learning_rate': 2.0142222222222225e-06, 'epoch': 3.2788}
{'loss': 0.8678, 'grad_norm': 2.7371094192062815, 'learning_rate': 2.0131111111111113e-06, 'epoch': 3.2792}
{'loss': 0.8728, 'grad_norm': 2.7076969016395562, 'learning_rate': 2.012e-06, 'epoch': 3.2796}
{'loss': 0.8611, 'grad_norm': 3.084875604959392, 'learning_rate': 2.010888888888889e-06, 'epoch': 3.2800000000000002}
{'eval_valid_loss': 0.84375, 'eval_valid_runtime': 0.0881, 'eval_valid_samples_per_second': 1135.179, 'eval_valid_steps_per_second': 283.795, 'epoch': 3.2800000000000002}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8723, 'grad_norm': 3.169737777017867, 'learning_rate': 2.009777777777778e-06, 'epoch': 3.2804}
{'loss': 0.8821, 'grad_norm': 3.051160849816374, 'learning_rate': 2.008666666666667e-06, 'epoch': 3.2808}
{'loss': 0.8729, 'grad_norm': 2.9370471727065732, 'learning_rate': 2.0075555555555557e-06, 'epoch': 3.2812}
{'loss': 0.8636, 'grad_norm': 2.79299882992639, 'learning_rate': 2.0064444444444444e-06, 'epoch': 3.2816}
{'loss': 0.8798, 'grad_norm': 3.084564338059461, 'learning_rate': 2.0053333333333335e-06, 'epoch': 3.282}
{'loss': 0.8543, 'grad_norm': 2.6571403133325955, 'learning_rate': 2.0042222222222222e-06, 'epoch': 3.2824}
{'loss': 0.8741, 'grad_norm': 2.7856389305600056, 'learning_rate': 2.0031111111111113e-06, 'epoch': 3.2828}
{'loss': 0.8632, 'grad_norm': 2.9047193085322665, 'learning_rate': 2.002e-06, 'epoch': 3.2832}
{'loss': 0.8699, 'grad_norm': 2.8291950414486586, 'learning_rate': 2.000888888888889e-06, 'epoch': 3.2836}
{'loss': 0.8671, 'grad_norm': 2.8197879673332653, 'learning_rate': 1.999888888888889e-06, 'epoch': 3.284}
{'eval_valid_loss': 0.84423828125, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.682, 'eval_valid_steps_per_second': 279.421, 'epoch': 3.284}
{'loss': 0.87, 'grad_norm': 3.0460618034305798, 'learning_rate': 1.998777777777778e-06, 'epoch': 3.2843999999999998}
{'loss': 0.8615, 'grad_norm': 2.7202735227611314, 'learning_rate': 1.997666666666667e-06, 'epoch': 3.2848}
{'loss': 0.8759, 'grad_norm': 3.2432023723559977, 'learning_rate': 1.9965555555555557e-06, 'epoch': 3.2852}
{'loss': 0.8749, 'grad_norm': 3.1266163555884847, 'learning_rate': 1.9954444444444444e-06, 'epoch': 3.2856}
{'loss': 0.8658, 'grad_norm': 2.8084810099038675, 'learning_rate': 1.9943333333333335e-06, 'epoch': 3.286}
{'loss': 0.8759, 'grad_norm': 2.8628710756117166, 'learning_rate': 1.9932222222222227e-06, 'epoch': 3.2864}
{'loss': 0.8681, 'grad_norm': 3.085054198736473, 'learning_rate': 1.9921111111111114e-06, 'epoch': 3.2868}
{'loss': 0.8791, 'grad_norm': 3.087345806317871, 'learning_rate': 1.991e-06, 'epoch': 3.2872}
{'loss': 0.8693, 'grad_norm': 2.985451399332032, 'learning_rate': 1.989888888888889e-06, 'epoch': 3.2876}
{'loss': 0.8683, 'grad_norm': 2.74690607897435, 'learning_rate': 1.988777777777778e-06, 'epoch': 3.288}
{'eval_valid_loss': 0.84375, 'eval_valid_runtime': 0.089, 'eval_valid_samples_per_second': 1123.259, 'eval_valid_steps_per_second': 280.815, 'epoch': 3.288}
{'loss': 0.8734, 'grad_norm': 2.954245400197849, 'learning_rate': 1.987666666666667e-06, 'epoch': 3.2884}
{'loss': 0.8767, 'grad_norm': 3.0591020807216576, 'learning_rate': 1.9865555555555558e-06, 'epoch': 3.2888}
{'loss': 0.8713, 'grad_norm': 2.8075063454682816, 'learning_rate': 1.9854444444444445e-06, 'epoch': 3.2892}
{'loss': 0.8683, 'grad_norm': 3.036920807801693, 'learning_rate': 1.984333333333333e-06, 'epoch': 3.2896}
{'loss': 0.86, 'grad_norm': 2.742546465600929, 'learning_rate': 1.9832222222222223e-06, 'epoch': 3.29}
{'loss': 0.8674, 'grad_norm': 3.3080073876286895, 'learning_rate': 1.9821111111111115e-06, 'epoch': 3.2904}
{'loss': 0.8751, 'grad_norm': 3.007905864505066, 'learning_rate': 1.981e-06, 'epoch': 3.2908}
{'loss': 0.8709, 'grad_norm': 2.6317392533188793, 'learning_rate': 1.979888888888889e-06, 'epoch': 3.2912}
{'loss': 0.8595, 'grad_norm': 3.22804285408371, 'learning_rate': 1.978777777777778e-06, 'epoch': 3.2916}
{'loss': 0.864, 'grad_norm': 2.935178316067403, 'learning_rate': 1.9776666666666667e-06, 'epoch': 3.292}
{'eval_valid_loss': 0.84423828125, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.362, 'eval_valid_steps_per_second': 278.591, 'epoch': 3.292}
{'loss': 0.8711, 'grad_norm': 2.8865185411374163, 'learning_rate': 1.976555555555556e-06, 'epoch': 3.2923999999999998}
{'loss': 0.8696, 'grad_norm': 2.7936956833459887, 'learning_rate': 1.9754444444444446e-06, 'epoch': 3.2928}
{'loss': 0.8626, 'grad_norm': 2.859095533711541, 'learning_rate': 1.9743333333333333e-06, 'epoch': 3.2932}
{'loss': 0.8676, 'grad_norm': 3.2638206151265057, 'learning_rate': 1.9732222222222224e-06, 'epoch': 3.2936}
{'loss': 0.8764, 'grad_norm': 3.0768481816005044, 'learning_rate': 1.972111111111111e-06, 'epoch': 3.294}
{'loss': 0.8721, 'grad_norm': 3.021337504352738, 'learning_rate': 1.9710000000000003e-06, 'epoch': 3.2944}
{'loss': 0.867, 'grad_norm': 3.092378177324931, 'learning_rate': 1.969888888888889e-06, 'epoch': 3.2948}
{'loss': 0.8777, 'grad_norm': 2.9541439185770946, 'learning_rate': 1.968777777777778e-06, 'epoch': 3.2952}
{'loss': 0.8646, 'grad_norm': 2.986180762777307, 'learning_rate': 1.967666666666667e-06, 'epoch': 3.2956}
{'loss': 0.8763, 'grad_norm': 3.2321628247783107, 'learning_rate': 1.9665555555555555e-06, 'epoch': 3.296}
{'eval_valid_loss': 0.84326171875, 'eval_valid_runtime': 0.0903, 'eval_valid_samples_per_second': 1107.659, 'eval_valid_steps_per_second': 276.915, 'epoch': 3.296}
{'loss': 0.8557, 'grad_norm': 2.939047770891694, 'learning_rate': 1.9654444444444447e-06, 'epoch': 3.2964}
{'loss': 0.8725, 'grad_norm': 2.879315019880812, 'learning_rate': 1.9643333333333334e-06, 'epoch': 3.2968}
{'loss': 0.8577, 'grad_norm': 2.7659060125311736, 'learning_rate': 1.9632222222222225e-06, 'epoch': 3.2972}
{'loss': 0.8676, 'grad_norm': 3.22578731212014, 'learning_rate': 1.9621111111111112e-06, 'epoch': 3.2976}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8681, 'grad_norm': 2.853652647933626, 'learning_rate': 1.961e-06, 'epoch': 3.298}
{'loss': 0.8609, 'grad_norm': 2.9330797316550976, 'learning_rate': 1.959888888888889e-06, 'epoch': 3.2984}
{'loss': 0.8704, 'grad_norm': 2.912984428721649, 'learning_rate': 1.9587777777777782e-06, 'epoch': 3.2988}
{'loss': 0.8794, 'grad_norm': 2.8976361434210136, 'learning_rate': 1.957666666666667e-06, 'epoch': 3.2992}
{'loss': 0.867, 'grad_norm': 2.8987444794747446, 'learning_rate': 1.9565555555555556e-06, 'epoch': 3.2996}
{'loss': 0.8657, 'grad_norm': 2.7588329318449243, 'learning_rate': 1.9554444444444443e-06, 'epoch': 3.3}
{'eval_valid_loss': 0.84326171875, 'eval_valid_runtime': 0.0925, 'eval_valid_samples_per_second': 1080.755, 'eval_valid_steps_per_second': 270.189, 'epoch': 3.3}
{'loss': 0.8699, 'grad_norm': 3.0833131664922537, 'learning_rate': 1.9543333333333335e-06, 'epoch': 3.3004}
{'loss': 0.8626, 'grad_norm': 2.912314189664883, 'learning_rate': 1.9532222222222226e-06, 'epoch': 3.3008}
{'loss': 0.8748, 'grad_norm': 2.884446567461718, 'learning_rate': 1.9521111111111113e-06, 'epoch': 3.3012}
{'loss': 0.8664, 'grad_norm': 2.7667270339753043, 'learning_rate': 1.951e-06, 'epoch': 3.3016}
{'loss': 0.865, 'grad_norm': 2.831333321698971, 'learning_rate': 1.9498888888888888e-06, 'epoch': 3.302}
{'loss': 0.8688, 'grad_norm': 3.1943521898649814, 'learning_rate': 1.948777777777778e-06, 'epoch': 3.3024}
{'loss': 0.8627, 'grad_norm': 2.8666051330357036, 'learning_rate': 1.947666666666667e-06, 'epoch': 3.3028}
{'loss': 0.862, 'grad_norm': 2.559322556225014, 'learning_rate': 1.9465555555555557e-06, 'epoch': 3.3032}
{'loss': 0.8624, 'grad_norm': 2.887632924690534, 'learning_rate': 1.9454444444444444e-06, 'epoch': 3.3036}
{'loss': 0.8683, 'grad_norm': 3.020960086048693, 'learning_rate': 1.9443333333333336e-06, 'epoch': 3.304}
{'eval_valid_loss': 0.8447265625, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.1, 'eval_valid_steps_per_second': 279.025, 'epoch': 3.304}
{'loss': 0.8656, 'grad_norm': 3.070491911715495, 'learning_rate': 1.9432222222222223e-06, 'epoch': 3.3044000000000002}
{'loss': 0.8726, 'grad_norm': 2.689178569135124, 'learning_rate': 1.9421111111111114e-06, 'epoch': 3.3048}
{'loss': 0.8773, 'grad_norm': 2.8680119693318797, 'learning_rate': 1.941e-06, 'epoch': 3.3052}
{'loss': 0.8708, 'grad_norm': 2.730117475100792, 'learning_rate': 1.939888888888889e-06, 'epoch': 3.3056}
{'loss': 0.876, 'grad_norm': 3.059615684292181, 'learning_rate': 1.938777777777778e-06, 'epoch': 3.306}
{'loss': 0.8509, 'grad_norm': 2.7456074171417235, 'learning_rate': 1.9376666666666667e-06, 'epoch': 3.3064}
{'loss': 0.8656, 'grad_norm': 3.040104331968032, 'learning_rate': 1.936555555555556e-06, 'epoch': 3.3068}
{'loss': 0.8713, 'grad_norm': 2.9434927825968846, 'learning_rate': 1.9354444444444445e-06, 'epoch': 3.3072}
{'loss': 0.8556, 'grad_norm': 3.2711756045256526, 'learning_rate': 1.9343333333333337e-06, 'epoch': 3.3076}
{'loss': 0.8764, 'grad_norm': 3.2306373487617694, 'learning_rate': 1.9332222222222224e-06, 'epoch': 3.308}
{'eval_valid_loss': 0.84423828125, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1121.853, 'eval_valid_steps_per_second': 280.463, 'epoch': 3.308}
{'loss': 0.871, 'grad_norm': 2.9007538282278245, 'learning_rate': 1.932111111111111e-06, 'epoch': 3.3084}
{'loss': 0.8661, 'grad_norm': 3.0008612926229046, 'learning_rate': 1.9310000000000002e-06, 'epoch': 3.3088}
{'loss': 0.8602, 'grad_norm': 2.8935856477933077, 'learning_rate': 1.929888888888889e-06, 'epoch': 3.3092}
{'loss': 0.8645, 'grad_norm': 2.6809026337515287, 'learning_rate': 1.928777777777778e-06, 'epoch': 3.3096}
{'loss': 0.8725, 'grad_norm': 2.762997650830626, 'learning_rate': 1.927666666666667e-06, 'epoch': 3.31}
{'loss': 0.8627, 'grad_norm': 2.771031644483885, 'learning_rate': 1.9265555555555555e-06, 'epoch': 3.3104}
{'loss': 0.8621, 'grad_norm': 3.161404338461466, 'learning_rate': 1.9254444444444446e-06, 'epoch': 3.3108}
{'loss': 0.8638, 'grad_norm': 2.53235871558252, 'learning_rate': 1.9243333333333338e-06, 'epoch': 3.3112}
{'loss': 0.8656, 'grad_norm': 2.7916373220486155, 'learning_rate': 1.9232222222222225e-06, 'epoch': 3.3116}
{'loss': 0.8568, 'grad_norm': 2.761087957966217, 'learning_rate': 1.922111111111111e-06, 'epoch': 3.312}
{'eval_valid_loss': 0.84326171875, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1115.038, 'eval_valid_steps_per_second': 278.759, 'epoch': 3.312}
{'loss': 0.875, 'grad_norm': 2.726140068798965, 'learning_rate': 1.921e-06, 'epoch': 3.3124000000000002}
{'loss': 0.8746, 'grad_norm': 3.1134294405217107, 'learning_rate': 1.919888888888889e-06, 'epoch': 3.3128}
{'loss': 0.8735, 'grad_norm': 2.882922550364856, 'learning_rate': 1.918777777777778e-06, 'epoch': 3.3132}
{'loss': 0.8719, 'grad_norm': 2.966087492563592, 'learning_rate': 1.917666666666667e-06, 'epoch': 3.3136}
{'loss': 0.8747, 'grad_norm': 3.1683770891422656, 'learning_rate': 1.9165555555555556e-06, 'epoch': 3.314}
{'loss': 0.8752, 'grad_norm': 3.0088893597559587, 'learning_rate': 1.9154444444444443e-06, 'epoch': 3.3144}
{'loss': 0.8853, 'grad_norm': 2.803558866335603, 'learning_rate': 1.9143333333333334e-06, 'epoch': 3.3148}
{'loss': 0.8699, 'grad_norm': 2.675941139511846, 'learning_rate': 1.9132222222222226e-06, 'epoch': 3.3152}
{'loss': 0.8654, 'grad_norm': 3.185785954693778, 'learning_rate': 1.9121111111111113e-06, 'epoch': 3.3156}
{'loss': 0.8598, 'grad_norm': 2.843355172383855, 'learning_rate': 1.911e-06, 'epoch': 3.316}
{'eval_valid_loss': 0.84375, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1122.045, 'eval_valid_steps_per_second': 280.511, 'epoch': 3.316}
{'loss': 0.8648, 'grad_norm': 2.8242246065830905, 'learning_rate': 1.909888888888889e-06, 'epoch': 3.3164}
{'loss': 0.8772, 'grad_norm': 3.0546991782196895, 'learning_rate': 1.908777777777778e-06, 'epoch': 3.3168}
{'loss': 0.8641, 'grad_norm': 2.700640975844863, 'learning_rate': 1.907666666666667e-06, 'epoch': 3.3172}
{'loss': 0.8654, 'grad_norm': 2.8184168407267975, 'learning_rate': 1.9065555555555557e-06, 'epoch': 3.3176}
{'loss': 0.8677, 'grad_norm': 2.9691359921542073, 'learning_rate': 1.9054444444444444e-06, 'epoch': 3.318}
{'loss': 0.8744, 'grad_norm': 3.0644456268860676, 'learning_rate': 1.9043333333333335e-06, 'epoch': 3.3184}
{'loss': 0.862, 'grad_norm': 2.956225649549916, 'learning_rate': 1.9032222222222225e-06, 'epoch': 3.3188}
{'loss': 0.8721, 'grad_norm': 2.8532343209455133, 'learning_rate': 1.9021111111111112e-06, 'epoch': 3.3192}
{'loss': 0.8648, 'grad_norm': 3.1987532858106844, 'learning_rate': 1.901e-06, 'epoch': 3.3196}
{'loss': 0.8676, 'grad_norm': 2.856434931287893, 'learning_rate': 1.8998888888888892e-06, 'epoch': 3.32}
{'eval_valid_loss': 0.84326171875, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1124.421, 'eval_valid_steps_per_second': 281.105, 'epoch': 3.32}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8787, 'grad_norm': 2.9737841036790464, 'learning_rate': 1.898777777777778e-06, 'epoch': 3.3204000000000002}
{'loss': 0.867, 'grad_norm': 2.9218994384396395, 'learning_rate': 1.8976666666666669e-06, 'epoch': 3.3208}
{'loss': 0.8751, 'grad_norm': 3.1594635778399334, 'learning_rate': 1.8965555555555556e-06, 'epoch': 3.3212}
{'loss': 0.8661, 'grad_norm': 2.9681889455577743, 'learning_rate': 1.8954444444444445e-06, 'epoch': 3.3216}
{'loss': 0.8651, 'grad_norm': 2.6101616627312505, 'learning_rate': 1.8943333333333336e-06, 'epoch': 3.322}
{'loss': 0.875, 'grad_norm': 2.978713217262865, 'learning_rate': 1.8932222222222223e-06, 'epoch': 3.3224}
{'loss': 0.8773, 'grad_norm': 2.8023937749425594, 'learning_rate': 1.8921111111111113e-06, 'epoch': 3.3228}
{'loss': 0.8694, 'grad_norm': 3.0037996548514343, 'learning_rate': 1.891e-06, 'epoch': 3.3232}
{'loss': 0.8688, 'grad_norm': 2.8444279816197864, 'learning_rate': 1.8898888888888891e-06, 'epoch': 3.3236}
{'loss': 0.8655, 'grad_norm': 3.0130624803958, 'learning_rate': 1.888888888888889e-06, 'epoch': 3.324}
{'eval_valid_loss': 0.84326171875, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1126.472, 'eval_valid_steps_per_second': 281.618, 'epoch': 3.324}
{'loss': 0.8629, 'grad_norm': 2.9453075317826602, 'learning_rate': 1.8877777777777777e-06, 'epoch': 3.3244}
{'loss': 0.8722, 'grad_norm': 2.7601253331793463, 'learning_rate': 1.8866666666666669e-06, 'epoch': 3.3247999999999998}
{'loss': 0.8772, 'grad_norm': 3.1004284243486127, 'learning_rate': 1.8855555555555558e-06, 'epoch': 3.3252}
{'loss': 0.8719, 'grad_norm': 2.9393590563553205, 'learning_rate': 1.8844444444444445e-06, 'epoch': 3.3256}
{'loss': 0.8666, 'grad_norm': 3.2802147640013, 'learning_rate': 1.8833333333333334e-06, 'epoch': 3.326}
{'loss': 0.8721, 'grad_norm': 3.0933458468852755, 'learning_rate': 1.8822222222222226e-06, 'epoch': 3.3264}
{'loss': 0.866, 'grad_norm': 2.9423479900458194, 'learning_rate': 1.8811111111111113e-06, 'epoch': 3.3268}
{'loss': 0.8676, 'grad_norm': 2.959126958150026, 'learning_rate': 1.8800000000000002e-06, 'epoch': 3.3272}
{'loss': 0.876, 'grad_norm': 2.783104497935545, 'learning_rate': 1.878888888888889e-06, 'epoch': 3.3276}
{'loss': 0.8712, 'grad_norm': 2.7928580628983473, 'learning_rate': 1.8777777777777778e-06, 'epoch': 3.328}
{'eval_valid_loss': 0.84375, 'eval_valid_runtime': 0.089, 'eval_valid_samples_per_second': 1124.087, 'eval_valid_steps_per_second': 281.022, 'epoch': 3.328}
{'loss': 0.8749, 'grad_norm': 2.9226819979204715, 'learning_rate': 1.876666666666667e-06, 'epoch': 3.3284000000000002}
{'loss': 0.862, 'grad_norm': 3.1584576166270395, 'learning_rate': 1.8755555555555557e-06, 'epoch': 3.3288}
{'loss': 0.8724, 'grad_norm': 2.8797133309524794, 'learning_rate': 1.8744444444444446e-06, 'epoch': 3.3292}
{'loss': 0.8745, 'grad_norm': 3.38806072077727, 'learning_rate': 1.8733333333333333e-06, 'epoch': 3.3296}
{'loss': 0.8719, 'grad_norm': 2.7868428841361763, 'learning_rate': 1.8722222222222225e-06, 'epoch': 3.33}
{'loss': 0.8616, 'grad_norm': 2.969762117623987, 'learning_rate': 1.8711111111111114e-06, 'epoch': 3.3304}
{'loss': 0.8719, 'grad_norm': 2.7593620528957468, 'learning_rate': 1.87e-06, 'epoch': 3.3308}
{'loss': 0.873, 'grad_norm': 3.13463413996664, 'learning_rate': 1.868888888888889e-06, 'epoch': 3.3312}
{'loss': 0.8724, 'grad_norm': 2.8968902427000724, 'learning_rate': 1.8677777777777777e-06, 'epoch': 3.3316}
{'loss': 0.8858, 'grad_norm': 2.904269127883174, 'learning_rate': 1.8666666666666669e-06, 'epoch': 3.332}
{'eval_valid_loss': 0.84326171875, 'eval_valid_runtime': 0.0883, 'eval_valid_samples_per_second': 1132.2, 'eval_valid_steps_per_second': 283.05, 'epoch': 3.332}
{'loss': 0.8693, 'grad_norm': 2.799638719965124, 'learning_rate': 1.8655555555555558e-06, 'epoch': 3.3324}
{'loss': 0.8605, 'grad_norm': 2.82290704427583, 'learning_rate': 1.8644444444444445e-06, 'epoch': 3.3327999999999998}
{'loss': 0.8643, 'grad_norm': 2.7815744339382964, 'learning_rate': 1.8633333333333334e-06, 'epoch': 3.3332}
{'loss': 0.8673, 'grad_norm': 2.781588105213618, 'learning_rate': 1.8622222222222226e-06, 'epoch': 3.3336}
{'loss': 0.8687, 'grad_norm': 2.6057302981388233, 'learning_rate': 1.8611111111111113e-06, 'epoch': 3.334}
{'loss': 0.8759, 'grad_norm': 3.0557734225128454, 'learning_rate': 1.8600000000000002e-06, 'epoch': 3.3344}
{'loss': 0.8663, 'grad_norm': 2.769371248689081, 'learning_rate': 1.858888888888889e-06, 'epoch': 3.3348}
{'loss': 0.8765, 'grad_norm': 2.7295234528253918, 'learning_rate': 1.8577777777777778e-06, 'epoch': 3.3352}
{'loss': 0.8735, 'grad_norm': 2.7844747696714887, 'learning_rate': 1.856666666666667e-06, 'epoch': 3.3356}
{'loss': 0.8693, 'grad_norm': 3.0253082717315882, 'learning_rate': 1.8555555555555557e-06, 'epoch': 3.336}
{'eval_valid_loss': 0.8427734375, 'eval_valid_runtime': 0.0909, 'eval_valid_samples_per_second': 1100.154, 'eval_valid_steps_per_second': 275.038, 'epoch': 3.336}
{'loss': 0.8693, 'grad_norm': 2.8157703935805385, 'learning_rate': 1.8544444444444446e-06, 'epoch': 3.3364}
{'loss': 0.8659, 'grad_norm': 2.935727853075737, 'learning_rate': 1.8533333333333333e-06, 'epoch': 3.3368}
{'loss': 0.8649, 'grad_norm': 2.882663976259995, 'learning_rate': 1.8522222222222224e-06, 'epoch': 3.3372}
{'loss': 0.8688, 'grad_norm': 2.8350941215765415, 'learning_rate': 1.8511111111111114e-06, 'epoch': 3.3376}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.8723, 'grad_norm': 2.8600930422373794, 'learning_rate': 1.85e-06, 'epoch': 3.338}
{'loss': 0.8619, 'grad_norm': 2.882046667581519, 'learning_rate': 1.848888888888889e-06, 'epoch': 3.3384}
{'loss': 0.861, 'grad_norm': 2.9390836920777796, 'learning_rate': 1.8477777777777781e-06, 'epoch': 3.3388}
{'loss': 0.868, 'grad_norm': 3.1243270149369207, 'learning_rate': 1.8466666666666668e-06, 'epoch': 3.3392}
{'loss': 0.8751, 'grad_norm': 2.8170837878338912, 'learning_rate': 1.8455555555555558e-06, 'epoch': 3.3396}
{'loss': 0.8697, 'grad_norm': 3.2169777611446126, 'learning_rate': 1.8444444444444445e-06, 'epoch': 3.34}
{'eval_valid_loss': 0.84326171875, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1120.546, 'eval_valid_steps_per_second': 280.136, 'epoch': 3.34}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8595, 'grad_norm': 3.02480629190405, 'learning_rate': 1.8433333333333334e-06, 'epoch': 3.3404}
{'loss': 0.8845, 'grad_norm': 3.1142065953719973, 'learning_rate': 1.8422222222222225e-06, 'epoch': 3.3407999999999998}
{'loss': 0.8603, 'grad_norm': 2.9066636088574653, 'learning_rate': 1.8411111111111112e-06, 'epoch': 3.3412}
{'loss': 0.8654, 'grad_norm': 2.7905966121857246, 'learning_rate': 1.8400000000000002e-06, 'epoch': 3.3416}
{'loss': 0.8581, 'grad_norm': 2.874653556931035, 'learning_rate': 1.8388888888888889e-06, 'epoch': 3.342}
{'loss': 0.869, 'grad_norm': 2.8101100834057395, 'learning_rate': 1.837777777777778e-06, 'epoch': 3.3424}
{'loss': 0.8679, 'grad_norm': 2.875591901303894, 'learning_rate': 1.836666666666667e-06, 'epoch': 3.3428}
{'loss': 0.8558, 'grad_norm': 2.9470148908920417, 'learning_rate': 1.8355555555555557e-06, 'epoch': 3.3432}
{'loss': 0.8595, 'grad_norm': 2.8551598419632813, 'learning_rate': 1.8344444444444446e-06, 'epoch': 3.3436}
{'loss': 0.8677, 'grad_norm': 2.913980344785251, 'learning_rate': 1.8333333333333333e-06, 'epoch': 3.344}
{'eval_valid_loss': 0.84326171875, 'eval_valid_runtime': 0.2081, 'eval_valid_samples_per_second': 480.541, 'eval_valid_steps_per_second': 120.135, 'epoch': 3.344}
{'loss': 0.8675, 'grad_norm': 2.7603449878143325, 'learning_rate': 1.8322222222222224e-06, 'epoch': 3.3444}
{'loss': 0.859, 'grad_norm': 3.1000442440198124, 'learning_rate': 1.8311111111111113e-06, 'epoch': 3.3448}
{'loss': 0.8751, 'grad_norm': 2.845432758251351, 'learning_rate': 1.83e-06, 'epoch': 3.3452}
{'loss': 0.8638, 'grad_norm': 2.960245222625666, 'learning_rate': 1.828888888888889e-06, 'epoch': 3.3456}
{'loss': 0.8729, 'grad_norm': 3.077583759184466, 'learning_rate': 1.8277777777777781e-06, 'epoch': 3.346}
{'loss': 0.8638, 'grad_norm': 3.017009772938903, 'learning_rate': 1.8266666666666668e-06, 'epoch': 3.3464}
{'loss': 0.8708, 'grad_norm': 2.8937770566231547, 'learning_rate': 1.8255555555555557e-06, 'epoch': 3.3468}
{'loss': 0.8766, 'grad_norm': 2.88123394098999, 'learning_rate': 1.8244444444444445e-06, 'epoch': 3.3472}
{'loss': 0.8657, 'grad_norm': 2.7399803419086686, 'learning_rate': 1.8233333333333334e-06, 'epoch': 3.3476}
{'loss': 0.8698, 'grad_norm': 3.1470701689368847, 'learning_rate': 1.8222222222222225e-06, 'epoch': 3.348}
{'eval_valid_loss': 0.84326171875, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.314, 'eval_valid_steps_per_second': 279.579, 'epoch': 3.348}
{'loss': 0.8697, 'grad_norm': 2.7812371146514385, 'learning_rate': 1.8211111111111112e-06, 'epoch': 3.3484}
{'loss': 0.8728, 'grad_norm': 3.0215137285460067, 'learning_rate': 1.8200000000000002e-06, 'epoch': 3.3487999999999998}
{'loss': 0.8612, 'grad_norm': 2.990840837907309, 'learning_rate': 1.8188888888888889e-06, 'epoch': 3.3492}
{'loss': 0.8677, 'grad_norm': 3.0986161879253724, 'learning_rate': 1.817777777777778e-06, 'epoch': 3.3496}
{'loss': 0.8613, 'grad_norm': 2.8865595813689797, 'learning_rate': 1.816666666666667e-06, 'epoch': 3.35}
{'loss': 0.8737, 'grad_norm': 2.96502101001404, 'learning_rate': 1.8155555555555556e-06, 'epoch': 3.3504}
{'loss': 0.8761, 'grad_norm': 3.2466263225148686, 'learning_rate': 1.8144444444444446e-06, 'epoch': 3.3508}
{'loss': 0.8597, 'grad_norm': 2.8428512347684403, 'learning_rate': 1.8133333333333337e-06, 'epoch': 3.3512}
{'loss': 0.8775, 'grad_norm': 3.109051510262973, 'learning_rate': 1.8122222222222224e-06, 'epoch': 3.3516}
{'loss': 0.8707, 'grad_norm': 2.8557940922662457, 'learning_rate': 1.8111111111111113e-06, 'epoch': 3.352}
{'eval_valid_loss': 0.8427734375, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1128.126, 'eval_valid_steps_per_second': 282.031, 'epoch': 3.352}
{'loss': 0.8633, 'grad_norm': 2.7948199132931935, 'learning_rate': 1.81e-06, 'epoch': 3.3524}
{'loss': 0.8706, 'grad_norm': 2.9689078038086447, 'learning_rate': 1.808888888888889e-06, 'epoch': 3.3528000000000002}
{'loss': 0.87, 'grad_norm': 2.9821897365158345, 'learning_rate': 1.807777777777778e-06, 'epoch': 3.3532}
{'loss': 0.8775, 'grad_norm': 2.854388318307442, 'learning_rate': 1.8066666666666668e-06, 'epoch': 3.3536}
{'loss': 0.8542, 'grad_norm': 2.8755136062198674, 'learning_rate': 1.8055555555555557e-06, 'epoch': 3.354}
{'loss': 0.8549, 'grad_norm': 2.8490838448018954, 'learning_rate': 1.8044444444444444e-06, 'epoch': 3.3544}
{'loss': 0.8571, 'grad_norm': 2.956734872979757, 'learning_rate': 1.8033333333333336e-06, 'epoch': 3.3548}
{'loss': 0.8678, 'grad_norm': 2.9107722302552586, 'learning_rate': 1.8022222222222225e-06, 'epoch': 3.3552}
{'loss': 0.8727, 'grad_norm': 2.8349981775311597, 'learning_rate': 1.8011111111111112e-06, 'epoch': 3.3556}
{'loss': 0.8759, 'grad_norm': 2.795135351274883, 'learning_rate': 1.8000000000000001e-06, 'epoch': 3.356}
{'eval_valid_loss': 0.84326171875, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1115.183, 'eval_valid_steps_per_second': 278.796, 'epoch': 3.356}
{'loss': 0.8671, 'grad_norm': 2.914063134078297, 'learning_rate': 1.7988888888888888e-06, 'epoch': 3.3564}
{'loss': 0.8625, 'grad_norm': 2.802210742270806, 'learning_rate': 1.797777777777778e-06, 'epoch': 3.3568}
{'loss': 0.8793, 'grad_norm': 3.078976663414934, 'learning_rate': 1.796666666666667e-06, 'epoch': 3.3572}
{'loss': 0.8865, 'grad_norm': 3.1125800708444777, 'learning_rate': 1.7955555555555556e-06, 'epoch': 3.3576}
{'loss': 0.8662, 'grad_norm': 3.0904592679815273, 'learning_rate': 1.7944444444444445e-06, 'epoch': 3.358}
{'loss': 0.8608, 'grad_norm': 2.6828632498791634, 'learning_rate': 1.7933333333333337e-06, 'epoch': 3.3584}
{'loss': 0.8697, 'grad_norm': 2.9257727955185446, 'learning_rate': 1.7922222222222224e-06, 'epoch': 3.3588}
{'loss': 0.8667, 'grad_norm': 3.1082119515746607, 'learning_rate': 1.7911111111111113e-06, 'epoch': 3.3592}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8644, 'grad_norm': 2.951036602886221, 'learning_rate': 1.79e-06, 'epoch': 3.3596}
{'loss': 0.8707, 'grad_norm': 2.940128864672271, 'learning_rate': 1.788888888888889e-06, 'epoch': 3.36}
{'eval_valid_loss': 0.8427734375, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.315, 'eval_valid_steps_per_second': 278.579, 'epoch': 3.36}
{'loss': 0.8542, 'grad_norm': 3.0273431199596117, 'learning_rate': 1.787777777777778e-06, 'epoch': 3.3604}
{'loss': 0.8657, 'grad_norm': 3.0273529249482944, 'learning_rate': 1.7866666666666668e-06, 'epoch': 3.3608000000000002}
{'loss': 0.8743, 'grad_norm': 2.9045907072671278, 'learning_rate': 1.7855555555555557e-06, 'epoch': 3.3612}
{'loss': 0.8482, 'grad_norm': 2.863245413479187, 'learning_rate': 1.7844444444444444e-06, 'epoch': 3.3616}
{'loss': 0.8716, 'grad_norm': 2.8804995916674523, 'learning_rate': 1.7833333333333336e-06, 'epoch': 3.362}
{'loss': 0.8683, 'grad_norm': 2.934404284053845, 'learning_rate': 1.7822222222222225e-06, 'epoch': 3.3624}
{'loss': 0.8667, 'grad_norm': 3.190290669289838, 'learning_rate': 1.7811111111111112e-06, 'epoch': 3.3628}
{'loss': 0.857, 'grad_norm': 2.8010774506344553, 'learning_rate': 1.7800000000000001e-06, 'epoch': 3.3632}
{'loss': 0.8797, 'grad_norm': 3.135437215482137, 'learning_rate': 1.7788888888888892e-06, 'epoch': 3.3636}
{'loss': 0.8692, 'grad_norm': 2.855138381236561, 'learning_rate': 1.777777777777778e-06, 'epoch': 3.364}
{'eval_valid_loss': 0.841796875, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1122.108, 'eval_valid_steps_per_second': 280.527, 'epoch': 3.364}
{'loss': 0.8759, 'grad_norm': 2.844664510462009, 'learning_rate': 1.7767777777777779e-06, 'epoch': 3.3644}
{'loss': 0.8641, 'grad_norm': 3.160076041317716, 'learning_rate': 1.775666666666667e-06, 'epoch': 3.3648}
{'loss': 0.8607, 'grad_norm': 3.0370581913894474, 'learning_rate': 1.7745555555555557e-06, 'epoch': 3.3652}
{'loss': 0.8701, 'grad_norm': 2.9736847570733738, 'learning_rate': 1.7734444444444447e-06, 'epoch': 3.3656}
{'loss': 0.8624, 'grad_norm': 3.0698299465547705, 'learning_rate': 1.7723333333333334e-06, 'epoch': 3.366}
{'loss': 0.866, 'grad_norm': 2.9141082862345575, 'learning_rate': 1.7712222222222223e-06, 'epoch': 3.3664}
{'loss': 0.8656, 'grad_norm': 3.0027156945872386, 'learning_rate': 1.7701111111111114e-06, 'epoch': 3.3668}
{'loss': 0.853, 'grad_norm': 2.572478648339015, 'learning_rate': 1.7690000000000001e-06, 'epoch': 3.3672}
{'loss': 0.8602, 'grad_norm': 2.79320984980182, 'learning_rate': 1.767888888888889e-06, 'epoch': 3.3676}
{'loss': 0.8671, 'grad_norm': 2.895847921997939, 'learning_rate': 1.7667777777777778e-06, 'epoch': 3.368}
{'eval_valid_loss': 0.8427734375, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.677, 'eval_valid_steps_per_second': 278.169, 'epoch': 3.368}
{'loss': 0.8535, 'grad_norm': 3.1822863275428013, 'learning_rate': 1.765666666666667e-06, 'epoch': 3.3684}
{'loss': 0.878, 'grad_norm': 2.8023854639943657, 'learning_rate': 1.7645555555555558e-06, 'epoch': 3.3688000000000002}
{'loss': 0.8603, 'grad_norm': 2.812758094813246, 'learning_rate': 1.7634444444444445e-06, 'epoch': 3.3692}
{'loss': 0.8668, 'grad_norm': 2.9661051663945317, 'learning_rate': 1.7623333333333335e-06, 'epoch': 3.3696}
{'loss': 0.8628, 'grad_norm': 3.332735199709207, 'learning_rate': 1.7612222222222226e-06, 'epoch': 3.37}
{'loss': 0.8718, 'grad_norm': 2.827972155071046, 'learning_rate': 1.7601111111111113e-06, 'epoch': 3.3704}
{'loss': 0.8617, 'grad_norm': 3.0558218252358706, 'learning_rate': 1.7590000000000002e-06, 'epoch': 3.3708}
{'loss': 0.8668, 'grad_norm': 2.7056763025479347, 'learning_rate': 1.757888888888889e-06, 'epoch': 3.3712}
{'loss': 0.8669, 'grad_norm': 3.1253144106054744, 'learning_rate': 1.7567777777777779e-06, 'epoch': 3.3716}
{'loss': 0.869, 'grad_norm': 3.0737905434334856, 'learning_rate': 1.755666666666667e-06, 'epoch': 3.372}
{'eval_valid_loss': 0.84228515625, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.874, 'eval_valid_steps_per_second': 281.969, 'epoch': 3.372}
{'loss': 0.8651, 'grad_norm': 2.8794883646755616, 'learning_rate': 1.7545555555555557e-06, 'epoch': 3.3724}
{'loss': 0.8694, 'grad_norm': 3.1013891714558803, 'learning_rate': 1.7534444444444446e-06, 'epoch': 3.3728}
{'loss': 0.8626, 'grad_norm': 2.5276701361201717, 'learning_rate': 1.7523333333333333e-06, 'epoch': 3.3731999999999998}
{'loss': 0.8699, 'grad_norm': 3.0932651582983293, 'learning_rate': 1.7512222222222225e-06, 'epoch': 3.3736}
{'loss': 0.8604, 'grad_norm': 3.0656217409791258, 'learning_rate': 1.7501111111111114e-06, 'epoch': 3.374}
{'loss': 0.8695, 'grad_norm': 3.14422702620828, 'learning_rate': 1.7490000000000001e-06, 'epoch': 3.3744}
{'loss': 0.8666, 'grad_norm': 3.164909833313736, 'learning_rate': 1.747888888888889e-06, 'epoch': 3.3748}
{'loss': 0.8663, 'grad_norm': 2.984226662509318, 'learning_rate': 1.7467777777777777e-06, 'epoch': 3.3752}
{'loss': 0.8741, 'grad_norm': 2.908407692154492, 'learning_rate': 1.7456666666666669e-06, 'epoch': 3.3756}
{'loss': 0.8643, 'grad_norm': 2.97173813465619, 'learning_rate': 1.7445555555555558e-06, 'epoch': 3.376}
{'eval_valid_loss': 0.84228515625, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1106.548, 'eval_valid_steps_per_second': 276.637, 'epoch': 3.376}
{'loss': 0.8791, 'grad_norm': 3.1413834068911717, 'learning_rate': 1.7434444444444445e-06, 'epoch': 3.3764}
{'loss': 0.8763, 'grad_norm': 2.951232495496348, 'learning_rate': 1.7423333333333334e-06, 'epoch': 3.3768000000000002}
{'loss': 0.8665, 'grad_norm': 2.9105142462794116, 'learning_rate': 1.7412222222222226e-06, 'epoch': 3.3772}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.8608, 'grad_norm': 2.941990777811628, 'learning_rate': 1.7401111111111113e-06, 'epoch': 3.3776}
{'loss': 0.8742, 'grad_norm': 2.831685043746459, 'learning_rate': 1.7390000000000002e-06, 'epoch': 3.378}
{'loss': 0.8695, 'grad_norm': 3.1607696069997697, 'learning_rate': 1.737888888888889e-06, 'epoch': 3.3784}
{'loss': 0.8781, 'grad_norm': 2.876999605402888, 'learning_rate': 1.7367777777777778e-06, 'epoch': 3.3788}
{'loss': 0.8725, 'grad_norm': 2.9450643318513663, 'learning_rate': 1.735666666666667e-06, 'epoch': 3.3792}
{'loss': 0.8576, 'grad_norm': 2.7685425934704018, 'learning_rate': 1.7345555555555557e-06, 'epoch': 3.3796}
{'loss': 0.8732, 'grad_norm': 2.9970985329232533, 'learning_rate': 1.7334444444444446e-06, 'epoch': 3.38}
{'eval_valid_loss': 0.8427734375, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.481, 'eval_valid_steps_per_second': 279.87, 'epoch': 3.38}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8602, 'grad_norm': 3.084261272386011, 'learning_rate': 1.7323333333333333e-06, 'epoch': 3.3804}
{'loss': 0.8687, 'grad_norm': 2.7215311634722625, 'learning_rate': 1.7312222222222225e-06, 'epoch': 3.3808}
{'loss': 0.8666, 'grad_norm': 3.1611274763974984, 'learning_rate': 1.7301111111111114e-06, 'epoch': 3.3811999999999998}
{'loss': 0.8657, 'grad_norm': 3.199940061007857, 'learning_rate': 1.729e-06, 'epoch': 3.3816}
{'loss': 0.875, 'grad_norm': 3.304815132075281, 'learning_rate': 1.727888888888889e-06, 'epoch': 3.382}
{'loss': 0.8644, 'grad_norm': 3.2406314654823776, 'learning_rate': 1.7267777777777781e-06, 'epoch': 3.3824}
{'loss': 0.8562, 'grad_norm': 2.777513233276898, 'learning_rate': 1.7256666666666669e-06, 'epoch': 3.3828}
{'loss': 0.8704, 'grad_norm': 2.9858490863448015, 'learning_rate': 1.7245555555555558e-06, 'epoch': 3.3832}
{'loss': 0.87, 'grad_norm': 2.9624389827255118, 'learning_rate': 1.7234444444444445e-06, 'epoch': 3.3836}
{'loss': 0.8722, 'grad_norm': 2.7753010305161867, 'learning_rate': 1.7223333333333334e-06, 'epoch': 3.384}
{'eval_valid_loss': 0.84228515625, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.399, 'eval_valid_steps_per_second': 279.35, 'epoch': 3.384}
{'loss': 0.8639, 'grad_norm': 3.069512765410767, 'learning_rate': 1.7212222222222226e-06, 'epoch': 3.3844}
{'loss': 0.8634, 'grad_norm': 2.9906567076593618, 'learning_rate': 1.7201111111111113e-06, 'epoch': 3.3848}
{'loss': 0.876, 'grad_norm': 3.02401249861468, 'learning_rate': 1.7190000000000002e-06, 'epoch': 3.3852}
{'loss': 0.8783, 'grad_norm': 3.027965240673329, 'learning_rate': 1.717888888888889e-06, 'epoch': 3.3856}
{'loss': 0.862, 'grad_norm': 3.110140164795335, 'learning_rate': 1.716777777777778e-06, 'epoch': 3.386}
{'loss': 0.8739, 'grad_norm': 2.693831288334827, 'learning_rate': 1.715666666666667e-06, 'epoch': 3.3864}
{'loss': 0.865, 'grad_norm': 2.9259519234395186, 'learning_rate': 1.7145555555555557e-06, 'epoch': 3.3868}
{'loss': 0.8698, 'grad_norm': 2.9194128096689713, 'learning_rate': 1.7134444444444446e-06, 'epoch': 3.3872}
{'loss': 0.863, 'grad_norm': 2.8476267264971984, 'learning_rate': 1.7123333333333333e-06, 'epoch': 3.3876}
{'loss': 0.8693, 'grad_norm': 2.8745881231336, 'learning_rate': 1.7112222222222224e-06, 'epoch': 3.388}
{'eval_valid_loss': 0.8427734375, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1111.286, 'eval_valid_steps_per_second': 277.821, 'epoch': 3.388}
{'loss': 0.8662, 'grad_norm': 2.8526191425934484, 'learning_rate': 1.7101111111111114e-06, 'epoch': 3.3884}
{'loss': 0.8682, 'grad_norm': 2.682766360650971, 'learning_rate': 1.709e-06, 'epoch': 3.3888}
{'loss': 0.8634, 'grad_norm': 3.000965201557774, 'learning_rate': 1.707888888888889e-06, 'epoch': 3.3891999999999998}
{'loss': 0.8696, 'grad_norm': 3.002960254544631, 'learning_rate': 1.7067777777777781e-06, 'epoch': 3.3896}
{'loss': 0.8655, 'grad_norm': 2.8410848925121543, 'learning_rate': 1.7056666666666668e-06, 'epoch': 3.39}
{'loss': 0.8584, 'grad_norm': 3.05945837045605, 'learning_rate': 1.7045555555555558e-06, 'epoch': 3.3904}
{'loss': 0.8734, 'grad_norm': 3.015629931431034, 'learning_rate': 1.7034444444444445e-06, 'epoch': 3.3908}
{'loss': 0.8649, 'grad_norm': 3.036989372801942, 'learning_rate': 1.7023333333333334e-06, 'epoch': 3.3912}
{'loss': 0.8614, 'grad_norm': 3.1331618725417876, 'learning_rate': 1.7012222222222225e-06, 'epoch': 3.3916}
{'loss': 0.8743, 'grad_norm': 2.9501827559192866, 'learning_rate': 1.7001111111111112e-06, 'epoch': 3.392}
{'eval_valid_loss': 0.8427734375, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1124.569, 'eval_valid_steps_per_second': 281.142, 'epoch': 3.392}
{'loss': 0.8659, 'grad_norm': 2.929000468759704, 'learning_rate': 1.6990000000000002e-06, 'epoch': 3.3924}
{'loss': 0.8545, 'grad_norm': 3.0407360832882575, 'learning_rate': 1.6978888888888889e-06, 'epoch': 3.3928}
{'loss': 0.8724, 'grad_norm': 2.7881377687997717, 'learning_rate': 1.696777777777778e-06, 'epoch': 3.3932}
{'loss': 0.8779, 'grad_norm': 2.7824149370943774, 'learning_rate': 1.695666666666667e-06, 'epoch': 3.3936}
{'loss': 0.8715, 'grad_norm': 2.986573732504059, 'learning_rate': 1.6945555555555556e-06, 'epoch': 3.394}
{'loss': 0.8643, 'grad_norm': 2.663113668400207, 'learning_rate': 1.6934444444444446e-06, 'epoch': 3.3944}
{'loss': 0.8701, 'grad_norm': 2.8219055685286536, 'learning_rate': 1.6923333333333337e-06, 'epoch': 3.3948}
{'loss': 0.8686, 'grad_norm': 2.929996128356381, 'learning_rate': 1.6912222222222224e-06, 'epoch': 3.3952}
{'loss': 0.8647, 'grad_norm': 2.802484018661368, 'learning_rate': 1.6901111111111113e-06, 'epoch': 3.3956}
{'loss': 0.8626, 'grad_norm': 3.025106348974133, 'learning_rate': 1.689e-06, 'epoch': 3.396}
{'eval_valid_loss': 0.84228515625, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1129.098, 'eval_valid_steps_per_second': 282.274, 'epoch': 3.396}
{'loss': 0.8683, 'grad_norm': 3.133211124772828, 'learning_rate': 1.687888888888889e-06, 'epoch': 3.3964}
{'loss': 0.8679, 'grad_norm': 2.512245093410414, 'learning_rate': 1.6867777777777781e-06, 'epoch': 3.3968}
{'loss': 0.8712, 'grad_norm': 2.930341144123573, 'learning_rate': 1.6856666666666668e-06, 'epoch': 3.3971999999999998}
{'loss': 0.8691, 'grad_norm': 2.862216204938609, 'learning_rate': 1.6845555555555557e-06, 'epoch': 3.3976}
{'loss': 0.8719, 'grad_norm': 3.128233004943404, 'learning_rate': 1.6834444444444445e-06, 'epoch': 3.398}
{'loss': 0.8545, 'grad_norm': 2.876432290101025, 'learning_rate': 1.6823333333333336e-06, 'epoch': 3.3984}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.857, 'grad_norm': 3.056198780434126, 'learning_rate': 1.6812222222222225e-06, 'epoch': 3.3988}
{'loss': 0.8752, 'grad_norm': 2.866086566572697, 'learning_rate': 1.6801111111111112e-06, 'epoch': 3.3992}
{'loss': 0.8464, 'grad_norm': 2.9403774797814832, 'learning_rate': 1.6790000000000001e-06, 'epoch': 3.3996}
{'loss': 0.8657, 'grad_norm': 3.4257340205035005, 'learning_rate': 1.6778888888888889e-06, 'epoch': 3.4}
{'eval_valid_loss': 0.8427734375, 'eval_valid_runtime': 0.1071, 'eval_valid_samples_per_second': 933.601, 'eval_valid_steps_per_second': 233.4, 'epoch': 3.4}
{'loss': 0.8634, 'grad_norm': 2.978877011247431, 'learning_rate': 1.676777777777778e-06, 'epoch': 3.4004}
{'loss': 0.8649, 'grad_norm': 3.1315314606126687, 'learning_rate': 1.675666666666667e-06, 'epoch': 3.4008}
{'loss': 0.8751, 'grad_norm': 3.0261100076299856, 'learning_rate': 1.6745555555555556e-06, 'epoch': 3.4012000000000002}
{'loss': 0.8606, 'grad_norm': 3.1389197374523414, 'learning_rate': 1.6734444444444445e-06, 'epoch': 3.4016}
{'loss': 0.8621, 'grad_norm': 2.9547873465513548, 'learning_rate': 1.6723333333333337e-06, 'epoch': 3.402}
{'loss': 0.8804, 'grad_norm': 2.9670776676301918, 'learning_rate': 1.6712222222222224e-06, 'epoch': 3.4024}
{'loss': 0.8756, 'grad_norm': 2.899467321300528, 'learning_rate': 1.6701111111111113e-06, 'epoch': 3.4028}
{'loss': 0.8666, 'grad_norm': 3.0188956634255306, 'learning_rate': 1.669e-06, 'epoch': 3.4032}
{'loss': 0.8695, 'grad_norm': 2.752583525600413, 'learning_rate': 1.667888888888889e-06, 'epoch': 3.4036}
{'loss': 0.8668, 'grad_norm': 3.0474807528356616, 'learning_rate': 1.666777777777778e-06, 'epoch': 3.404}
{'eval_valid_loss': 0.84228515625, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.916, 'eval_valid_steps_per_second': 278.979, 'epoch': 3.404}
{'loss': 0.8576, 'grad_norm': 2.8171940097321055, 'learning_rate': 1.6657777777777778e-06, 'epoch': 3.4044}
{'loss': 0.8649, 'grad_norm': 2.8638139257868094, 'learning_rate': 1.664666666666667e-06, 'epoch': 3.4048}
{'loss': 0.8557, 'grad_norm': 2.8655690294829634, 'learning_rate': 1.6635555555555559e-06, 'epoch': 3.4052}
{'loss': 0.8617, 'grad_norm': 3.0125494181271657, 'learning_rate': 1.6624444444444446e-06, 'epoch': 3.4056}
{'loss': 0.8679, 'grad_norm': 3.171378533732341, 'learning_rate': 1.6613333333333335e-06, 'epoch': 3.406}
{'loss': 0.8672, 'grad_norm': 2.770138726375619, 'learning_rate': 1.6602222222222222e-06, 'epoch': 3.4064}
{'loss': 0.8552, 'grad_norm': 2.7815890052011722, 'learning_rate': 1.6591111111111113e-06, 'epoch': 3.4068}
{'loss': 0.8725, 'grad_norm': 3.245930967139812, 'learning_rate': 1.6580000000000003e-06, 'epoch': 3.4072}
{'loss': 0.8733, 'grad_norm': 3.2698140540465, 'learning_rate': 1.656888888888889e-06, 'epoch': 3.4076}
{'loss': 0.8612, 'grad_norm': 2.8046949487443795, 'learning_rate': 1.655777777777778e-06, 'epoch': 3.408}
{'eval_valid_loss': 0.841796875, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1115.032, 'eval_valid_steps_per_second': 278.758, 'epoch': 3.408}
{'loss': 0.8634, 'grad_norm': 2.9677471575212047, 'learning_rate': 1.6546666666666668e-06, 'epoch': 3.4084}
{'loss': 0.8693, 'grad_norm': 3.105206498882712, 'learning_rate': 1.6535555555555557e-06, 'epoch': 3.4088}
{'loss': 0.8659, 'grad_norm': 3.042962413384594, 'learning_rate': 1.6524444444444447e-06, 'epoch': 3.4092000000000002}
{'loss': 0.869, 'grad_norm': 2.902486774519591, 'learning_rate': 1.6513333333333334e-06, 'epoch': 3.4096}
{'loss': 0.88, 'grad_norm': 2.9718387998668607, 'learning_rate': 1.6502222222222223e-06, 'epoch': 3.41}
{'loss': 0.8726, 'grad_norm': 2.8935061452954263, 'learning_rate': 1.6491111111111114e-06, 'epoch': 3.4104}
{'loss': 0.8719, 'grad_norm': 2.997985700051506, 'learning_rate': 1.6480000000000001e-06, 'epoch': 3.4108}
{'loss': 0.8652, 'grad_norm': 2.808020967058821, 'learning_rate': 1.646888888888889e-06, 'epoch': 3.4112}
{'loss': 0.8877, 'grad_norm': 2.8451855777291284, 'learning_rate': 1.6457777777777778e-06, 'epoch': 3.4116}
{'loss': 0.8627, 'grad_norm': 3.435549590178561, 'learning_rate': 1.644666666666667e-06, 'epoch': 3.412}
{'eval_valid_loss': 0.84228515625, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.268, 'eval_valid_steps_per_second': 279.317, 'epoch': 3.412}
{'loss': 0.8569, 'grad_norm': 2.8719885074155083, 'learning_rate': 1.6435555555555558e-06, 'epoch': 3.4124}
{'loss': 0.8801, 'grad_norm': 3.1163342868991033, 'learning_rate': 1.6424444444444445e-06, 'epoch': 3.4128}
{'loss': 0.8761, 'grad_norm': 3.1010553632405324, 'learning_rate': 1.6413333333333335e-06, 'epoch': 3.4132}
{'loss': 0.8807, 'grad_norm': 3.0824922378856883, 'learning_rate': 1.6402222222222224e-06, 'epoch': 3.4136}
{'loss': 0.863, 'grad_norm': 2.90165932390561, 'learning_rate': 1.6391111111111113e-06, 'epoch': 3.414}
{'loss': 0.8686, 'grad_norm': 2.9535060011100605, 'learning_rate': 1.6380000000000002e-06, 'epoch': 3.4144}
{'loss': 0.8695, 'grad_norm': 3.1155300849070513, 'learning_rate': 1.636888888888889e-06, 'epoch': 3.4148}
{'loss': 0.8695, 'grad_norm': 2.829495493095059, 'learning_rate': 1.6357777777777779e-06, 'epoch': 3.4152}
{'loss': 0.8595, 'grad_norm': 2.990935021159187, 'learning_rate': 1.6346666666666668e-06, 'epoch': 3.4156}
{'loss': 0.872, 'grad_norm': 2.8484019387641557, 'learning_rate': 1.6335555555555557e-06, 'epoch': 3.416}
{'eval_valid_loss': 0.84228515625, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.277, 'eval_valid_steps_per_second': 279.319, 'epoch': 3.416}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.8698, 'grad_norm': 2.9937297043878925, 'learning_rate': 1.6324444444444446e-06, 'epoch': 3.4164}
{'loss': 0.8657, 'grad_norm': 3.0701727993076617, 'learning_rate': 1.6313333333333334e-06, 'epoch': 3.4168}
{'loss': 0.8787, 'grad_norm': 2.922573205073449, 'learning_rate': 1.6302222222222225e-06, 'epoch': 3.4172000000000002}
{'loss': 0.8507, 'grad_norm': 3.0359560822111087, 'learning_rate': 1.6291111111111114e-06, 'epoch': 3.4176}
{'loss': 0.8672, 'grad_norm': 3.059904429105765, 'learning_rate': 1.6280000000000001e-06, 'epoch': 3.418}
{'loss': 0.8636, 'grad_norm': 2.8488571816200543, 'learning_rate': 1.626888888888889e-06, 'epoch': 3.4184}
{'loss': 0.8602, 'grad_norm': 3.203926246727633, 'learning_rate': 1.6257777777777778e-06, 'epoch': 3.4188}
{'loss': 0.8675, 'grad_norm': 3.1894011624665013, 'learning_rate': 1.624666666666667e-06, 'epoch': 3.4192}
{'loss': 0.86, 'grad_norm': 3.039384104130465, 'learning_rate': 1.6235555555555558e-06, 'epoch': 3.4196}
{'loss': 0.8568, 'grad_norm': 2.8681228213905676, 'learning_rate': 1.6224444444444445e-06, 'epoch': 3.42}
{'eval_valid_loss': 0.8427734375, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1120.043, 'eval_valid_steps_per_second': 280.011, 'epoch': 3.42}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8715, 'grad_norm': 3.0452777001323894, 'learning_rate': 1.6213333333333335e-06, 'epoch': 3.4204}
{'loss': 0.8715, 'grad_norm': 2.7461617516987644, 'learning_rate': 1.6202222222222224e-06, 'epoch': 3.4208}
{'loss': 0.8594, 'grad_norm': 2.8158569011810193, 'learning_rate': 1.6191111111111113e-06, 'epoch': 3.4212}
{'loss': 0.8678, 'grad_norm': 2.8712735385785204, 'learning_rate': 1.6180000000000002e-06, 'epoch': 3.4215999999999998}
{'loss': 0.8666, 'grad_norm': 2.8520634929606854, 'learning_rate': 1.616888888888889e-06, 'epoch': 3.422}
{'loss': 0.864, 'grad_norm': 2.8425670930229767, 'learning_rate': 1.6157777777777779e-06, 'epoch': 3.4224}
{'loss': 0.8755, 'grad_norm': 3.0528166815363713, 'learning_rate': 1.6146666666666668e-06, 'epoch': 3.4228}
{'loss': 0.8631, 'grad_norm': 2.8377671447900057, 'learning_rate': 1.6135555555555557e-06, 'epoch': 3.4232}
{'loss': 0.8597, 'grad_norm': 2.8207838451847094, 'learning_rate': 1.6124444444444446e-06, 'epoch': 3.4236}
{'loss': 0.8669, 'grad_norm': 2.989707516650768, 'learning_rate': 1.6113333333333333e-06, 'epoch': 3.424}
{'eval_valid_loss': 0.84228515625, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.546, 'eval_valid_steps_per_second': 277.136, 'epoch': 3.424}
{'loss': 0.875, 'grad_norm': 2.9533029532933073, 'learning_rate': 1.6102222222222225e-06, 'epoch': 3.4244}
{'loss': 0.8702, 'grad_norm': 2.9480240833720193, 'learning_rate': 1.6091111111111114e-06, 'epoch': 3.4248}
{'loss': 0.8538, 'grad_norm': 3.0862130054189465, 'learning_rate': 1.608e-06, 'epoch': 3.4252000000000002}
{'loss': 0.8552, 'grad_norm': 3.0626670830749454, 'learning_rate': 1.606888888888889e-06, 'epoch': 3.4256}
{'loss': 0.8617, 'grad_norm': 3.0199982058759756, 'learning_rate': 1.605777777777778e-06, 'epoch': 3.426}
{'loss': 0.872, 'grad_norm': 2.9096266091741305, 'learning_rate': 1.6046666666666669e-06, 'epoch': 3.4264}
{'loss': 0.8621, 'grad_norm': 3.024068140700901, 'learning_rate': 1.6035555555555558e-06, 'epoch': 3.4268}
{'loss': 0.8603, 'grad_norm': 3.031547462969526, 'learning_rate': 1.6024444444444445e-06, 'epoch': 3.4272}
{'loss': 0.8615, 'grad_norm': 3.1410557086210114, 'learning_rate': 1.6013333333333334e-06, 'epoch': 3.4276}
{'loss': 0.8581, 'grad_norm': 3.040885339394844, 'learning_rate': 1.6002222222222224e-06, 'epoch': 3.428}
{'eval_valid_loss': 0.8427734375, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1114.043, 'eval_valid_steps_per_second': 278.511, 'epoch': 3.428}
{'loss': 0.8588, 'grad_norm': 3.0211174118816047, 'learning_rate': 1.5991111111111113e-06, 'epoch': 3.4284}
{'loss': 0.8717, 'grad_norm': 3.190025273599627, 'learning_rate': 1.5980000000000002e-06, 'epoch': 3.4288}
{'loss': 0.8785, 'grad_norm': 3.118024555912182, 'learning_rate': 1.596888888888889e-06, 'epoch': 3.4292}
{'loss': 0.8604, 'grad_norm': 3.011630243764777, 'learning_rate': 1.595777777777778e-06, 'epoch': 3.4295999999999998}
{'loss': 0.8543, 'grad_norm': 2.65914939949979, 'learning_rate': 1.5946666666666668e-06, 'epoch': 3.43}
{'loss': 0.8624, 'grad_norm': 2.7850058837362766, 'learning_rate': 1.5935555555555557e-06, 'epoch': 3.4304}
{'loss': 0.8748, 'grad_norm': 3.0341774382151656, 'learning_rate': 1.5924444444444446e-06, 'epoch': 3.4308}
{'loss': 0.8638, 'grad_norm': 2.9172737693187685, 'learning_rate': 1.5913333333333333e-06, 'epoch': 3.4312}
{'loss': 0.8694, 'grad_norm': 2.9241394713017654, 'learning_rate': 1.5902222222222225e-06, 'epoch': 3.4316}
{'loss': 0.8663, 'grad_norm': 2.8027127095624533, 'learning_rate': 1.5891111111111114e-06, 'epoch': 3.432}
{'eval_valid_loss': 0.84228515625, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.916, 'eval_valid_steps_per_second': 278.229, 'epoch': 3.432}
{'loss': 0.8808, 'grad_norm': 3.157749452485342, 'learning_rate': 1.588e-06, 'epoch': 3.4324}
{'loss': 0.8554, 'grad_norm': 2.927801399914305, 'learning_rate': 1.586888888888889e-06, 'epoch': 3.4328}
{'loss': 0.8619, 'grad_norm': 3.1134488145637396, 'learning_rate': 1.585777777777778e-06, 'epoch': 3.4332}
{'loss': 0.8449, 'grad_norm': 2.8250700800566912, 'learning_rate': 1.5846666666666669e-06, 'epoch': 3.4336}
{'loss': 0.8667, 'grad_norm': 2.991382271408292, 'learning_rate': 1.5835555555555558e-06, 'epoch': 3.434}
{'loss': 0.86, 'grad_norm': 2.7068920402457315, 'learning_rate': 1.5824444444444445e-06, 'epoch': 3.4344}
{'loss': 0.8699, 'grad_norm': 2.880405253162356, 'learning_rate': 1.5813333333333334e-06, 'epoch': 3.4348}
{'loss': 0.8631, 'grad_norm': 2.6609659461104216, 'learning_rate': 1.5802222222222223e-06, 'epoch': 3.4352}
{'loss': 0.8629, 'grad_norm': 2.9609088342733436, 'learning_rate': 1.5791111111111113e-06, 'epoch': 3.4356}
{'loss': 0.8648, 'grad_norm': 2.8001356138868854, 'learning_rate': 1.5780000000000002e-06, 'epoch': 3.436}
{'eval_valid_loss': 0.84228515625, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1122.66, 'eval_valid_steps_per_second': 280.665, 'epoch': 3.436}
{'loss': 0.8662, 'grad_norm': 3.076958304499284, 'learning_rate': 1.5768888888888889e-06, 'epoch': 3.4364}
{'loss': 0.8714, 'grad_norm': 2.9329791485565164, 'learning_rate': 1.575777777777778e-06, 'epoch': 3.4368}
{'loss': 0.8707, 'grad_norm': 3.2565246433260993, 'learning_rate': 1.5746666666666667e-06, 'epoch': 3.4372}
{'loss': 0.8719, 'grad_norm': 3.1697192265388474, 'learning_rate': 1.5735555555555557e-06, 'epoch': 3.4375999999999998}
{'loss': 0.8605, 'grad_norm': 2.893088152196094, 'learning_rate': 1.5724444444444446e-06, 'epoch': 3.438}
{'loss': 0.8701, 'grad_norm': 3.127019935099094, 'learning_rate': 1.5713333333333335e-06, 'epoch': 3.4384}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8678, 'grad_norm': 2.895708871586407, 'learning_rate': 1.5702222222222224e-06, 'epoch': 3.4388}
{'loss': 0.8596, 'grad_norm': 3.2102333373383054, 'learning_rate': 1.5691111111111114e-06, 'epoch': 3.4392}
{'loss': 0.854, 'grad_norm': 2.839138468144301, 'learning_rate': 1.568e-06, 'epoch': 3.4396}
{'loss': 0.8687, 'grad_norm': 3.075182859287141, 'learning_rate': 1.566888888888889e-06, 'epoch': 3.44}
{'eval_valid_loss': 0.84130859375, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.742, 'eval_valid_steps_per_second': 279.435, 'epoch': 3.44}
{'loss': 0.8682, 'grad_norm': 2.8757276391688222, 'learning_rate': 1.565777777777778e-06, 'epoch': 3.4404}
{'loss': 0.863, 'grad_norm': 3.190505032427642, 'learning_rate': 1.5646666666666668e-06, 'epoch': 3.4408}
{'loss': 0.8721, 'grad_norm': 2.988023175832703, 'learning_rate': 1.5635555555555558e-06, 'epoch': 3.4412}
{'loss': 0.8621, 'grad_norm': 3.122328856415069, 'learning_rate': 1.5624444444444445e-06, 'epoch': 3.4416}
{'loss': 0.8685, 'grad_norm': 2.9212159112125584, 'learning_rate': 1.5613333333333336e-06, 'epoch': 3.442}
{'loss': 0.8569, 'grad_norm': 3.0228687609016744, 'learning_rate': 1.5602222222222223e-06, 'epoch': 3.4424}
{'loss': 0.8664, 'grad_norm': 3.236539043029544, 'learning_rate': 1.5591111111111112e-06, 'epoch': 3.4428}
{'loss': 0.8672, 'grad_norm': 2.76609143133195, 'learning_rate': 1.5580000000000002e-06, 'epoch': 3.4432}
{'loss': 0.864, 'grad_norm': 3.229562219368553, 'learning_rate': 1.5568888888888889e-06, 'epoch': 3.4436}
{'loss': 0.8691, 'grad_norm': 3.0771789065020503, 'learning_rate': 1.555777777777778e-06, 'epoch': 3.444}
{'eval_valid_loss': 0.841796875, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1124.454, 'eval_valid_steps_per_second': 281.114, 'epoch': 3.444}
{'loss': 0.8757, 'grad_norm': 3.1676360119022746, 'learning_rate': 1.554777777777778e-06, 'epoch': 3.4444}
{'loss': 0.8687, 'grad_norm': 2.657916090671636, 'learning_rate': 1.5536666666666666e-06, 'epoch': 3.4448}
{'loss': 0.8553, 'grad_norm': 2.9046289424729124, 'learning_rate': 1.5525555555555558e-06, 'epoch': 3.4452}
{'loss': 0.8706, 'grad_norm': 2.704429146489309, 'learning_rate': 1.5514444444444445e-06, 'epoch': 3.4455999999999998}
{'loss': 0.8795, 'grad_norm': 2.8922681520037337, 'learning_rate': 1.5503333333333334e-06, 'epoch': 3.446}
{'loss': 0.8663, 'grad_norm': 2.872212561775168, 'learning_rate': 1.5492222222222223e-06, 'epoch': 3.4464}
{'loss': 0.8703, 'grad_norm': 3.064502898007151, 'learning_rate': 1.5481111111111113e-06, 'epoch': 3.4468}
{'loss': 0.8592, 'grad_norm': 3.1562789122513086, 'learning_rate': 1.5470000000000002e-06, 'epoch': 3.4472}
{'loss': 0.8717, 'grad_norm': 3.6437094723785197, 'learning_rate': 1.5458888888888889e-06, 'epoch': 3.4476}
{'loss': 0.8658, 'grad_norm': 2.85157456591418, 'learning_rate': 1.5447777777777778e-06, 'epoch': 3.448}
{'eval_valid_loss': 0.84326171875, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.581, 'eval_valid_steps_per_second': 279.145, 'epoch': 3.448}
{'loss': 0.8591, 'grad_norm': 2.887022257994898, 'learning_rate': 1.543666666666667e-06, 'epoch': 3.4484}
{'loss': 0.8774, 'grad_norm': 2.87399437151154, 'learning_rate': 1.5425555555555557e-06, 'epoch': 3.4488}
{'loss': 0.8662, 'grad_norm': 2.8837665850224847, 'learning_rate': 1.5414444444444446e-06, 'epoch': 3.4492}
{'loss': 0.8745, 'grad_norm': 3.175027999604284, 'learning_rate': 1.5403333333333335e-06, 'epoch': 3.4496}
{'loss': 0.8646, 'grad_norm': 2.8850859746320294, 'learning_rate': 1.5392222222222222e-06, 'epoch': 3.45}
{'loss': 0.875, 'grad_norm': 3.1300894587197456, 'learning_rate': 1.5381111111111114e-06, 'epoch': 3.4504}
{'loss': 0.8588, 'grad_norm': 2.894885434039856, 'learning_rate': 1.537e-06, 'epoch': 3.4508}
{'loss': 0.8685, 'grad_norm': 3.1955011214193285, 'learning_rate': 1.535888888888889e-06, 'epoch': 3.4512}
{'loss': 0.8814, 'grad_norm': 2.948733985925312, 'learning_rate': 1.534777777777778e-06, 'epoch': 3.4516}
{'loss': 0.8615, 'grad_norm': 3.093281893525122, 'learning_rate': 1.5336666666666668e-06, 'epoch': 3.452}
{'eval_valid_loss': 0.84130859375, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1109.179, 'eval_valid_steps_per_second': 277.295, 'epoch': 3.452}
{'loss': 0.8713, 'grad_norm': 3.0885384909670854, 'learning_rate': 1.5325555555555558e-06, 'epoch': 3.4524}
{'loss': 0.8614, 'grad_norm': 3.2751300960646863, 'learning_rate': 1.5314444444444445e-06, 'epoch': 3.4528}
{'loss': 0.8685, 'grad_norm': 3.068719523447516, 'learning_rate': 1.5303333333333334e-06, 'epoch': 3.4532}
{'loss': 0.8514, 'grad_norm': 2.742131680955654, 'learning_rate': 1.5292222222222223e-06, 'epoch': 3.4536}
{'loss': 0.863, 'grad_norm': 2.972204392086057, 'learning_rate': 1.5281111111111112e-06, 'epoch': 3.454}
{'loss': 0.87, 'grad_norm': 3.025485308786587, 'learning_rate': 1.5270000000000002e-06, 'epoch': 3.4544}
{'loss': 0.8755, 'grad_norm': 2.880222687274468, 'learning_rate': 1.5258888888888889e-06, 'epoch': 3.4548}
{'loss': 0.8649, 'grad_norm': 3.0018938960906887, 'learning_rate': 1.5247777777777778e-06, 'epoch': 3.4552}
{'loss': 0.868, 'grad_norm': 3.036245202272346, 'learning_rate': 1.523666666666667e-06, 'epoch': 3.4556}
{'loss': 0.8537, 'grad_norm': 2.994109480186369, 'learning_rate': 1.5225555555555556e-06, 'epoch': 3.456}
{'eval_valid_loss': 0.84326171875, 'eval_valid_runtime': 0.1521, 'eval_valid_samples_per_second': 657.596, 'eval_valid_steps_per_second': 164.399, 'epoch': 3.456}
{'loss': 0.8694, 'grad_norm': 2.975044634428104, 'learning_rate': 1.5214444444444446e-06, 'epoch': 3.4564}
{'loss': 0.8621, 'grad_norm': 2.57707562909302, 'learning_rate': 1.5203333333333335e-06, 'epoch': 3.4568}
{'loss': 0.8562, 'grad_norm': 2.8350980793169427, 'learning_rate': 1.5192222222222222e-06, 'epoch': 3.4572}
{'loss': 0.8604, 'grad_norm': 2.8775507040221364, 'learning_rate': 1.5181111111111113e-06, 'epoch': 3.4576000000000002}
{'loss': 0.862, 'grad_norm': 3.3246159837289784, 'learning_rate': 1.517e-06, 'epoch': 3.458}
{'loss': 0.868, 'grad_norm': 2.8130038763725405, 'learning_rate': 1.515888888888889e-06, 'epoch': 3.4584}
{'loss': 0.8684, 'grad_norm': 2.7458602241642653, 'learning_rate': 1.5147777777777779e-06, 'epoch': 3.4588}
{'loss': 0.8618, 'grad_norm': 2.8095745448616616, 'learning_rate': 1.5136666666666668e-06, 'epoch': 3.4592}
{'loss': 0.8669, 'grad_norm': 2.669161595536138, 'learning_rate': 1.5125555555555557e-06, 'epoch': 3.4596}
{'loss': 0.8619, 'grad_norm': 2.7313707087137717, 'learning_rate': 1.5114444444444444e-06, 'epoch': 3.46}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'eval_valid_loss': 0.84228515625, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1106.493, 'eval_valid_steps_per_second': 276.623, 'epoch': 3.46}
{'loss': 0.87, 'grad_norm': 2.9237707485361395, 'learning_rate': 1.5103333333333334e-06, 'epoch': 3.4604}
{'loss': 0.862, 'grad_norm': 2.9359858749976264, 'learning_rate': 1.5092222222222225e-06, 'epoch': 3.4608}
{'loss': 0.8738, 'grad_norm': 3.0860472647545056, 'learning_rate': 1.5081111111111112e-06, 'epoch': 3.4612}
{'loss': 0.8642, 'grad_norm': 3.0087853976149685, 'learning_rate': 1.5070000000000001e-06, 'epoch': 3.4616}
{'loss': 0.8744, 'grad_norm': 3.265732649347581, 'learning_rate': 1.5058888888888889e-06, 'epoch': 3.462}
{'loss': 0.8772, 'grad_norm': 3.3570510455305365, 'learning_rate': 1.5047777777777778e-06, 'epoch': 3.4624}
{'loss': 0.8615, 'grad_norm': 3.1715948986533964, 'learning_rate': 1.503666666666667e-06, 'epoch': 3.4628}
{'loss': 0.8608, 'grad_norm': 2.904370397420856, 'learning_rate': 1.5025555555555556e-06, 'epoch': 3.4632}
{'loss': 0.884, 'grad_norm': 3.3243728225451297, 'learning_rate': 1.5014444444444445e-06, 'epoch': 3.4636}
{'loss': 0.866, 'grad_norm': 2.9943961017703526, 'learning_rate': 1.5003333333333335e-06, 'epoch': 3.464}
{'eval_valid_loss': 0.84130859375, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1109.968, 'eval_valid_steps_per_second': 277.492, 'epoch': 3.464}
{'loss': 0.8679, 'grad_norm': 3.019529887960307, 'learning_rate': 1.4992222222222224e-06, 'epoch': 3.4644}
{'loss': 0.869, 'grad_norm': 2.889114335679783, 'learning_rate': 1.4981111111111113e-06, 'epoch': 3.4648}
{'loss': 0.8674, 'grad_norm': 3.0390267946161025, 'learning_rate': 1.497e-06, 'epoch': 3.4652}
{'loss': 0.8701, 'grad_norm': 2.980783578386161, 'learning_rate': 1.495888888888889e-06, 'epoch': 3.4656000000000002}
{'loss': 0.8724, 'grad_norm': 2.943261117693687, 'learning_rate': 1.4947777777777779e-06, 'epoch': 3.466}
{'loss': 0.8676, 'grad_norm': 2.8090985710602667, 'learning_rate': 1.4936666666666668e-06, 'epoch': 3.4664}
{'loss': 0.8813, 'grad_norm': 2.9706336119254284, 'learning_rate': 1.4925555555555557e-06, 'epoch': 3.4668}
{'loss': 0.8662, 'grad_norm': 3.0194977910210126, 'learning_rate': 1.4914444444444444e-06, 'epoch': 3.4672}
{'loss': 0.8727, 'grad_norm': 3.0758793961279656, 'learning_rate': 1.4903333333333334e-06, 'epoch': 3.4676}
{'loss': 0.881, 'grad_norm': 2.9008177523033347, 'learning_rate': 1.4892222222222225e-06, 'epoch': 3.468}
{'eval_valid_loss': 0.84130859375, 'eval_valid_runtime': 0.0966, 'eval_valid_samples_per_second': 1035.702, 'eval_valid_steps_per_second': 258.926, 'epoch': 3.468}
{'loss': 0.8676, 'grad_norm': 3.365007097270787, 'learning_rate': 1.4881111111111112e-06, 'epoch': 3.4684}
{'loss': 0.8724, 'grad_norm': 2.8563021162608133, 'learning_rate': 1.4870000000000001e-06, 'epoch': 3.4688}
{'loss': 0.8701, 'grad_norm': 3.286111744874076, 'learning_rate': 1.4858888888888888e-06, 'epoch': 3.4692}
{'loss': 0.8747, 'grad_norm': 2.915352523026844, 'learning_rate': 1.4847777777777778e-06, 'epoch': 3.4696}
{'loss': 0.8685, 'grad_norm': 2.981027403304671, 'learning_rate': 1.4836666666666669e-06, 'epoch': 3.4699999999999998}
{'loss': 0.8769, 'grad_norm': 3.1667795548060074, 'learning_rate': 1.4825555555555556e-06, 'epoch': 3.4704}
{'loss': 0.8647, 'grad_norm': 2.909908744988656, 'learning_rate': 1.4814444444444445e-06, 'epoch': 3.4708}
{'loss': 0.8683, 'grad_norm': 3.1276567133359885, 'learning_rate': 1.4803333333333334e-06, 'epoch': 3.4712}
{'loss': 0.856, 'grad_norm': 3.2890079611562735, 'learning_rate': 1.4792222222222224e-06, 'epoch': 3.4716}
{'loss': 0.8735, 'grad_norm': 3.000746008386224, 'learning_rate': 1.4781111111111113e-06, 'epoch': 3.472}
{'eval_valid_loss': 0.8408203125, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.045, 'eval_valid_steps_per_second': 277.011, 'epoch': 3.472}
{'loss': 0.8696, 'grad_norm': 2.947123610688126, 'learning_rate': 1.477e-06, 'epoch': 3.4724}
{'loss': 0.8756, 'grad_norm': 3.3194897069775755, 'learning_rate': 1.475888888888889e-06, 'epoch': 3.4728}
{'loss': 0.8742, 'grad_norm': 2.968172418726054, 'learning_rate': 1.474777777777778e-06, 'epoch': 3.4732}
{'loss': 0.8647, 'grad_norm': 2.943109867529325, 'learning_rate': 1.4736666666666668e-06, 'epoch': 3.4736000000000002}
{'loss': 0.8627, 'grad_norm': 3.0326167102881394, 'learning_rate': 1.4725555555555557e-06, 'epoch': 3.474}
{'loss': 0.8655, 'grad_norm': 2.886288622452754, 'learning_rate': 1.4714444444444444e-06, 'epoch': 3.4744}
{'loss': 0.8677, 'grad_norm': 2.8526820193013385, 'learning_rate': 1.4703333333333333e-06, 'epoch': 3.4748}
{'loss': 0.8632, 'grad_norm': 2.888617658847865, 'learning_rate': 1.4692222222222225e-06, 'epoch': 3.4752}
{'loss': 0.8732, 'grad_norm': 2.9597010584929397, 'learning_rate': 1.4681111111111112e-06, 'epoch': 3.4756}
{'loss': 0.8643, 'grad_norm': 2.971833063706186, 'learning_rate': 1.467e-06, 'epoch': 3.476}
{'eval_valid_loss': 0.84130859375, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.543, 'eval_valid_steps_per_second': 278.636, 'epoch': 3.476}
{'loss': 0.869, 'grad_norm': 2.9160834047139823, 'learning_rate': 1.4658888888888888e-06, 'epoch': 3.4764}
{'loss': 0.8686, 'grad_norm': 2.7530098125682647, 'learning_rate': 1.464777777777778e-06, 'epoch': 3.4768}
{'loss': 0.8722, 'grad_norm': 3.032388080220407, 'learning_rate': 1.4636666666666669e-06, 'epoch': 3.4772}
{'loss': 0.8664, 'grad_norm': 2.8866812118959655, 'learning_rate': 1.4625555555555556e-06, 'epoch': 3.4776}
{'loss': 0.8578, 'grad_norm': 2.868648053105169, 'learning_rate': 1.4614444444444445e-06, 'epoch': 3.4779999999999998}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8642, 'grad_norm': 2.8961214132662554, 'learning_rate': 1.4603333333333334e-06, 'epoch': 3.4784}
{'loss': 0.8691, 'grad_norm': 3.217776642075652, 'learning_rate': 1.4592222222222223e-06, 'epoch': 3.4788}
{'loss': 0.8702, 'grad_norm': 2.7493914396831216, 'learning_rate': 1.4581111111111113e-06, 'epoch': 3.4792}
{'loss': 0.8789, 'grad_norm': 3.4636660082685546, 'learning_rate': 1.457e-06, 'epoch': 3.4796}
{'loss': 0.8613, 'grad_norm': 2.8065527820578198, 'learning_rate': 1.455888888888889e-06, 'epoch': 3.48}
{'eval_valid_loss': 0.841796875, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.501, 'eval_valid_steps_per_second': 279.375, 'epoch': 3.48}
{'loss': 0.8579, 'grad_norm': 2.7055244490738923, 'learning_rate': 1.454777777777778e-06, 'epoch': 3.4804}
{'loss': 0.8753, 'grad_norm': 3.064269547224507, 'learning_rate': 1.4536666666666668e-06, 'epoch': 3.4808}
{'loss': 0.8723, 'grad_norm': 2.9242934555551843, 'learning_rate': 1.4525555555555557e-06, 'epoch': 3.4812}
{'loss': 0.872, 'grad_norm': 2.675694240074261, 'learning_rate': 1.4514444444444444e-06, 'epoch': 3.4816}
{'loss': 0.8681, 'grad_norm': 3.156380471280594, 'learning_rate': 1.4503333333333333e-06, 'epoch': 3.482}
{'loss': 0.8649, 'grad_norm': 2.77668592385838, 'learning_rate': 1.4492222222222224e-06, 'epoch': 3.4824}
{'loss': 0.8546, 'grad_norm': 2.8806492201882934, 'learning_rate': 1.4481111111111112e-06, 'epoch': 3.4828}
{'loss': 0.8731, 'grad_norm': 2.8505367367593144, 'learning_rate': 1.447e-06, 'epoch': 3.4832}
{'loss': 0.8512, 'grad_norm': 2.9526390351982994, 'learning_rate': 1.445888888888889e-06, 'epoch': 3.4836}
{'loss': 0.8744, 'grad_norm': 3.2214102168386494, 'learning_rate': 1.444777777777778e-06, 'epoch': 3.484}
{'eval_valid_loss': 0.84130859375, 'eval_valid_runtime': 0.0885, 'eval_valid_samples_per_second': 1129.812, 'eval_valid_steps_per_second': 282.453, 'epoch': 3.484}
{'loss': 0.8684, 'grad_norm': 2.9332640522303497, 'learning_rate': 1.4437777777777779e-06, 'epoch': 3.4844}
{'loss': 0.8614, 'grad_norm': 2.919922579251718, 'learning_rate': 1.4426666666666666e-06, 'epoch': 3.4848}
{'loss': 0.8712, 'grad_norm': 2.73324183280853, 'learning_rate': 1.4415555555555557e-06, 'epoch': 3.4852}
{'loss': 0.8657, 'grad_norm': 2.83666325049712, 'learning_rate': 1.4404444444444446e-06, 'epoch': 3.4856}
{'loss': 0.8602, 'grad_norm': 2.885173957247695, 'learning_rate': 1.4393333333333333e-06, 'epoch': 3.4859999999999998}
{'loss': 0.8754, 'grad_norm': 2.969003997778843, 'learning_rate': 1.4382222222222223e-06, 'epoch': 3.4864}
{'loss': 0.876, 'grad_norm': 2.991288615334973, 'learning_rate': 1.4371111111111114e-06, 'epoch': 3.4868}
{'loss': 0.8641, 'grad_norm': 2.8883202927550555, 'learning_rate': 1.436e-06, 'epoch': 3.4872}
{'loss': 0.8552, 'grad_norm': 2.8832784653461396, 'learning_rate': 1.434888888888889e-06, 'epoch': 3.4876}
{'loss': 0.8604, 'grad_norm': 2.65050403461966, 'learning_rate': 1.4337777777777777e-06, 'epoch': 3.488}
{'eval_valid_loss': 0.84130859375, 'eval_valid_runtime': 0.112, 'eval_valid_samples_per_second': 893.078, 'eval_valid_steps_per_second': 223.269, 'epoch': 3.488}
{'loss': 0.8701, 'grad_norm': 2.945632239271528, 'learning_rate': 1.4326666666666667e-06, 'epoch': 3.4884}
{'loss': 0.8686, 'grad_norm': 2.9802841889319787, 'learning_rate': 1.4315555555555558e-06, 'epoch': 3.4888}
{'loss': 0.8788, 'grad_norm': 2.9405392588683483, 'learning_rate': 1.4304444444444445e-06, 'epoch': 3.4892}
{'loss': 0.8551, 'grad_norm': 2.8271038625098757, 'learning_rate': 1.4293333333333334e-06, 'epoch': 3.4896}
{'loss': 0.8712, 'grad_norm': 2.8202400145369566, 'learning_rate': 1.4282222222222221e-06, 'epoch': 3.49}
{'loss': 0.8571, 'grad_norm': 2.596769293321835, 'learning_rate': 1.4271111111111113e-06, 'epoch': 3.4904}
{'loss': 0.8578, 'grad_norm': 3.26270495826919, 'learning_rate': 1.4260000000000002e-06, 'epoch': 3.4908}
{'loss': 0.8711, 'grad_norm': 2.700478057130158, 'learning_rate': 1.424888888888889e-06, 'epoch': 3.4912}
{'loss': 0.8668, 'grad_norm': 2.7449758487170524, 'learning_rate': 1.4237777777777778e-06, 'epoch': 3.4916}
{'loss': 0.8711, 'grad_norm': 3.0517673730318995, 'learning_rate': 1.422666666666667e-06, 'epoch': 3.492}
{'eval_valid_loss': 0.84130859375, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1122.168, 'eval_valid_steps_per_second': 280.542, 'epoch': 3.492}
{'loss': 0.8553, 'grad_norm': 2.878513728677076, 'learning_rate': 1.4215555555555557e-06, 'epoch': 3.4924}
{'loss': 0.8662, 'grad_norm': 2.8171857318671627, 'learning_rate': 1.4204444444444446e-06, 'epoch': 3.4928}
{'loss': 0.8651, 'grad_norm': 2.8890227026748843, 'learning_rate': 1.4193333333333333e-06, 'epoch': 3.4932}
{'loss': 0.8686, 'grad_norm': 3.059428971840277, 'learning_rate': 1.4182222222222222e-06, 'epoch': 3.4936}
{'loss': 0.8715, 'grad_norm': 2.703304565948273, 'learning_rate': 1.4171111111111114e-06, 'epoch': 3.4939999999999998}
{'loss': 0.8604, 'grad_norm': 2.8058431470620233, 'learning_rate': 1.416e-06, 'epoch': 3.4944}
{'loss': 0.8739, 'grad_norm': 3.0891874076816546, 'learning_rate': 1.414888888888889e-06, 'epoch': 3.4948}
{'loss': 0.8761, 'grad_norm': 3.142711980326788, 'learning_rate': 1.4137777777777777e-06, 'epoch': 3.4952}
{'loss': 0.878, 'grad_norm': 3.025798880882518, 'learning_rate': 1.4126666666666668e-06, 'epoch': 3.4956}
{'loss': 0.878, 'grad_norm': 2.7813222747155733, 'learning_rate': 1.4115555555555558e-06, 'epoch': 3.496}
{'eval_valid_loss': 0.84130859375, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1114.069, 'eval_valid_steps_per_second': 278.517, 'epoch': 3.496}
{'loss': 0.8654, 'grad_norm': 2.8899771196291204, 'learning_rate': 1.4104444444444445e-06, 'epoch': 3.4964}
{'loss': 0.8613, 'grad_norm': 3.056165557116023, 'learning_rate': 1.4093333333333334e-06, 'epoch': 3.4968}
{'loss': 0.8722, 'grad_norm': 2.877438816941341, 'learning_rate': 1.4082222222222221e-06, 'epoch': 3.4972}
{'loss': 0.8615, 'grad_norm': 2.8688082362893947, 'learning_rate': 1.4071111111111113e-06, 'epoch': 3.4976}
{'loss': 0.8642, 'grad_norm': 2.8649158833839405, 'learning_rate': 1.4060000000000002e-06, 'epoch': 3.498}
{'loss': 0.8553, 'grad_norm': 2.9159170493080357, 'learning_rate': 1.4048888888888889e-06, 'epoch': 3.4984}
{'loss': 0.8644, 'grad_norm': 3.2527205561187094, 'learning_rate': 1.4037777777777778e-06, 'epoch': 3.4988}
{'loss': 0.8668, 'grad_norm': 3.059505263488451, 'learning_rate': 1.402666666666667e-06, 'epoch': 3.4992}
{'loss': 0.8753, 'grad_norm': 3.171321285004457, 'learning_rate': 1.4015555555555557e-06, 'epoch': 3.4996}
{'loss': 0.8698, 'grad_norm': 2.9739269894063294, 'learning_rate': 1.4004444444444446e-06, 'epoch': 3.5}
{'eval_valid_loss': 0.84130859375, 'eval_valid_runtime': 0.0884, 'eval_valid_samples_per_second': 1131.531, 'eval_valid_steps_per_second': 282.883, 'epoch': 3.5}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.86, 'grad_norm': 2.894999446918257, 'learning_rate': 1.3993333333333333e-06, 'epoch': 3.5004}
{'loss': 0.8635, 'grad_norm': 2.911897770583218, 'learning_rate': 1.3982222222222222e-06, 'epoch': 3.5008}
{'loss': 0.8583, 'grad_norm': 2.8532986881581173, 'learning_rate': 1.3971111111111113e-06, 'epoch': 3.5012}
{'loss': 0.869, 'grad_norm': 3.0085854267965098, 'learning_rate': 1.396e-06, 'epoch': 3.5016}
{'loss': 0.8685, 'grad_norm': 3.0266650909697708, 'learning_rate': 1.394888888888889e-06, 'epoch': 3.502}
{'loss': 0.8792, 'grad_norm': 3.0882905704696366, 'learning_rate': 1.3937777777777777e-06, 'epoch': 3.5023999999999997}
{'loss': 0.8761, 'grad_norm': 2.9277778810573065, 'learning_rate': 1.3926666666666668e-06, 'epoch': 3.5028}
{'loss': 0.8679, 'grad_norm': 3.0339405074815353, 'learning_rate': 1.3915555555555558e-06, 'epoch': 3.5032}
{'loss': 0.8589, 'grad_norm': 3.1491327583006345, 'learning_rate': 1.3904444444444445e-06, 'epoch': 3.5036}
{'loss': 0.8576, 'grad_norm': 2.7881255192126586, 'learning_rate': 1.3893333333333334e-06, 'epoch': 3.504}
{'eval_valid_loss': 0.84130859375, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.573, 'eval_valid_steps_per_second': 278.643, 'epoch': 3.504}
{'loss': 0.8672, 'grad_norm': 2.7484273965637938, 'learning_rate': 1.3882222222222225e-06, 'epoch': 3.5044}
{'loss': 0.8564, 'grad_norm': 3.0045449578668193, 'learning_rate': 1.3871111111111112e-06, 'epoch': 3.5048}
{'loss': 0.8676, 'grad_norm': 3.096509704447747, 'learning_rate': 1.3860000000000002e-06, 'epoch': 3.5052}
{'loss': 0.8707, 'grad_norm': 2.8476783322497186, 'learning_rate': 1.3848888888888889e-06, 'epoch': 3.5056000000000003}
{'loss': 0.8709, 'grad_norm': 3.3709219560576873, 'learning_rate': 1.3837777777777778e-06, 'epoch': 3.5060000000000002}
{'loss': 0.8729, 'grad_norm': 2.870850522903854, 'learning_rate': 1.382666666666667e-06, 'epoch': 3.5064}
{'loss': 0.8626, 'grad_norm': 2.7737317426165746, 'learning_rate': 1.3815555555555556e-06, 'epoch': 3.5068}
{'loss': 0.8764, 'grad_norm': 2.8701810463014836, 'learning_rate': 1.3804444444444446e-06, 'epoch': 3.5072}
{'loss': 0.8688, 'grad_norm': 2.832503506977304, 'learning_rate': 1.3793333333333333e-06, 'epoch': 3.5076}
{'loss': 0.8622, 'grad_norm': 3.034130782367705, 'learning_rate': 1.3782222222222224e-06, 'epoch': 3.508}
{'eval_valid_loss': 0.8408203125, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.765, 'eval_valid_steps_per_second': 279.941, 'epoch': 3.508}
{'loss': 0.8667, 'grad_norm': 2.989501105760522, 'learning_rate': 1.3771111111111113e-06, 'epoch': 3.5084}
{'loss': 0.8483, 'grad_norm': 2.9777689914124976, 'learning_rate': 1.376e-06, 'epoch': 3.5088}
{'loss': 0.88, 'grad_norm': 3.061851607990788, 'learning_rate': 1.374888888888889e-06, 'epoch': 3.5092}
{'loss': 0.8623, 'grad_norm': 3.0988049007013374, 'learning_rate': 1.3737777777777777e-06, 'epoch': 3.5096}
{'loss': 0.8562, 'grad_norm': 2.9712633837317015, 'learning_rate': 1.3726666666666668e-06, 'epoch': 3.51}
{'loss': 0.8735, 'grad_norm': 2.768605770332787, 'learning_rate': 1.3715555555555557e-06, 'epoch': 3.5103999999999997}
{'loss': 0.8804, 'grad_norm': 2.850305332383524, 'learning_rate': 1.3704444444444444e-06, 'epoch': 3.5108}
{'loss': 0.8672, 'grad_norm': 2.8388515192471466, 'learning_rate': 1.3693333333333334e-06, 'epoch': 3.5112}
{'loss': 0.8723, 'grad_norm': 2.854915111218376, 'learning_rate': 1.3682222222222225e-06, 'epoch': 3.5116}
{'loss': 0.8661, 'grad_norm': 3.0924729109975404, 'learning_rate': 1.3671111111111112e-06, 'epoch': 3.512}
{'eval_valid_loss': 0.84130859375, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.484, 'eval_valid_steps_per_second': 279.621, 'epoch': 3.512}
{'loss': 0.8751, 'grad_norm': 3.258536617388225, 'learning_rate': 1.3660000000000001e-06, 'epoch': 3.5124}
{'loss': 0.8736, 'grad_norm': 3.0721420874375953, 'learning_rate': 1.3648888888888888e-06, 'epoch': 3.5128}
{'loss': 0.8781, 'grad_norm': 2.942557688320163, 'learning_rate': 1.3637777777777778e-06, 'epoch': 3.5132}
{'loss': 0.8819, 'grad_norm': 2.6584212012602397, 'learning_rate': 1.362666666666667e-06, 'epoch': 3.5136}
{'loss': 0.8636, 'grad_norm': 2.8877627352292623, 'learning_rate': 1.3615555555555556e-06, 'epoch': 3.5140000000000002}
{'loss': 0.8706, 'grad_norm': 3.003177757871535, 'learning_rate': 1.3604444444444445e-06, 'epoch': 3.5144}
{'loss': 0.8544, 'grad_norm': 3.013581952615703, 'learning_rate': 1.3593333333333332e-06, 'epoch': 3.5148}
{'loss': 0.8797, 'grad_norm': 2.8633041120711944, 'learning_rate': 1.3582222222222224e-06, 'epoch': 3.5152}
{'loss': 0.8634, 'grad_norm': 2.8756646756289506, 'learning_rate': 1.3571111111111113e-06, 'epoch': 3.5156}
{'loss': 0.8708, 'grad_norm': 2.8421894601958795, 'learning_rate': 1.356e-06, 'epoch': 3.516}
{'eval_valid_loss': 0.8408203125, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.894, 'eval_valid_steps_per_second': 279.473, 'epoch': 3.516}
{'loss': 0.8612, 'grad_norm': 3.0305625194922543, 'learning_rate': 1.354888888888889e-06, 'epoch': 3.5164}
{'loss': 0.8651, 'grad_norm': 2.9726788573840315, 'learning_rate': 1.353777777777778e-06, 'epoch': 3.5168}
{'loss': 0.8589, 'grad_norm': 2.8036429497618176, 'learning_rate': 1.3526666666666668e-06, 'epoch': 3.5172}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.8604, 'grad_norm': 2.9824311381797606, 'learning_rate': 1.3515555555555557e-06, 'epoch': 3.5176}
{'loss': 0.856, 'grad_norm': 3.0268413988288847, 'learning_rate': 1.3504444444444444e-06, 'epoch': 3.518}
{'loss': 0.8632, 'grad_norm': 2.807511759229781, 'learning_rate': 1.3493333333333333e-06, 'epoch': 3.5183999999999997}
{'loss': 0.8714, 'grad_norm': 3.0969119530714386, 'learning_rate': 1.3482222222222225e-06, 'epoch': 3.5188}
{'loss': 0.868, 'grad_norm': 2.9483289172015663, 'learning_rate': 1.3471111111111112e-06, 'epoch': 3.5192}
{'loss': 0.8673, 'grad_norm': 2.8026696014894426, 'learning_rate': 1.3460000000000001e-06, 'epoch': 3.5196}
{'loss': 0.8707, 'grad_norm': 2.8752469547604864, 'learning_rate': 1.3448888888888888e-06, 'epoch': 3.52}
{'eval_valid_loss': 0.84130859375, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.869, 'eval_valid_steps_per_second': 279.717, 'epoch': 3.52}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8754, 'grad_norm': 2.71986020648453, 'learning_rate': 1.343777777777778e-06, 'epoch': 3.5204}
{'loss': 0.8839, 'grad_norm': 2.9304239593556862, 'learning_rate': 1.3426666666666669e-06, 'epoch': 3.5208}
{'loss': 0.8586, 'grad_norm': 3.0527804341322504, 'learning_rate': 1.3415555555555556e-06, 'epoch': 3.5212}
{'loss': 0.871, 'grad_norm': 2.735618668152992, 'learning_rate': 1.3404444444444445e-06, 'epoch': 3.5216}
{'loss': 0.8615, 'grad_norm': 2.830535941151672, 'learning_rate': 1.3393333333333332e-06, 'epoch': 3.5220000000000002}
{'loss': 0.8802, 'grad_norm': 2.921804549647963, 'learning_rate': 1.3382222222222224e-06, 'epoch': 3.5224}
{'loss': 0.8633, 'grad_norm': 2.8572862479510484, 'learning_rate': 1.3371111111111113e-06, 'epoch': 3.5228}
{'loss': 0.8667, 'grad_norm': 2.8271701317780007, 'learning_rate': 1.336e-06, 'epoch': 3.5232}
{'loss': 0.8613, 'grad_norm': 3.3997857499637516, 'learning_rate': 1.334888888888889e-06, 'epoch': 3.5236}
{'loss': 0.8572, 'grad_norm': 3.0638446094281435, 'learning_rate': 1.333777777777778e-06, 'epoch': 3.524}
{'eval_valid_loss': 0.8408203125, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1128.955, 'eval_valid_steps_per_second': 282.239, 'epoch': 3.524}
{'loss': 0.8671, 'grad_norm': 2.9402729297966967, 'learning_rate': 1.3327777777777778e-06, 'epoch': 3.5244}
{'loss': 0.87, 'grad_norm': 3.037158850361938, 'learning_rate': 1.3316666666666667e-06, 'epoch': 3.5248}
{'loss': 0.8498, 'grad_norm': 3.0927277427022317, 'learning_rate': 1.3305555555555558e-06, 'epoch': 3.5252}
{'loss': 0.8612, 'grad_norm': 2.882387527941508, 'learning_rate': 1.3294444444444445e-06, 'epoch': 3.5256}
{'loss': 0.8696, 'grad_norm': 2.8407524208304276, 'learning_rate': 1.3283333333333335e-06, 'epoch': 3.526}
{'loss': 0.856, 'grad_norm': 2.828883411870474, 'learning_rate': 1.3272222222222222e-06, 'epoch': 3.5263999999999998}
{'loss': 0.8635, 'grad_norm': 3.2889179641969544, 'learning_rate': 1.3261111111111113e-06, 'epoch': 3.5268}
{'loss': 0.8698, 'grad_norm': 2.957156124527018, 'learning_rate': 1.3250000000000002e-06, 'epoch': 3.5272}
{'loss': 0.857, 'grad_norm': 3.240495750549831, 'learning_rate': 1.323888888888889e-06, 'epoch': 3.5276}
{'loss': 0.8661, 'grad_norm': 2.8499776433603223, 'learning_rate': 1.3227777777777779e-06, 'epoch': 3.528}
{'eval_valid_loss': 0.8408203125, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.559, 'eval_valid_steps_per_second': 278.14, 'epoch': 3.528}
{'loss': 0.8783, 'grad_norm': 3.0135919507171076, 'learning_rate': 1.3216666666666666e-06, 'epoch': 3.5284}
{'loss': 0.8632, 'grad_norm': 2.829924964314408, 'learning_rate': 1.3205555555555557e-06, 'epoch': 3.5288}
{'loss': 0.8723, 'grad_norm': 3.2934048546008263, 'learning_rate': 1.3194444444444446e-06, 'epoch': 3.5292}
{'loss': 0.8648, 'grad_norm': 3.101836223678905, 'learning_rate': 1.3183333333333333e-06, 'epoch': 3.5296}
{'loss': 0.8571, 'grad_norm': 2.939763556528873, 'learning_rate': 1.3172222222222223e-06, 'epoch': 3.5300000000000002}
{'loss': 0.8593, 'grad_norm': 3.010185484715741, 'learning_rate': 1.3161111111111114e-06, 'epoch': 3.5304}
{'loss': 0.8726, 'grad_norm': 3.2572561013028967, 'learning_rate': 1.3150000000000001e-06, 'epoch': 3.5308}
{'loss': 0.8635, 'grad_norm': 2.798038017091706, 'learning_rate': 1.313888888888889e-06, 'epoch': 3.5312}
{'loss': 0.8688, 'grad_norm': 3.2416201083526226, 'learning_rate': 1.3127777777777777e-06, 'epoch': 3.5316}
{'loss': 0.879, 'grad_norm': 3.060582280638877, 'learning_rate': 1.3116666666666667e-06, 'epoch': 3.532}
{'eval_valid_loss': 0.84033203125, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.08, 'eval_valid_steps_per_second': 277.02, 'epoch': 3.532}
{'loss': 0.8718, 'grad_norm': 2.864401797119952, 'learning_rate': 1.3105555555555558e-06, 'epoch': 3.5324}
{'loss': 0.8562, 'grad_norm': 2.744074935398351, 'learning_rate': 1.3094444444444445e-06, 'epoch': 3.5328}
{'loss': 0.877, 'grad_norm': 2.9805588314897165, 'learning_rate': 1.3083333333333334e-06, 'epoch': 3.5332}
{'loss': 0.8519, 'grad_norm': 2.949577481513306, 'learning_rate': 1.3072222222222222e-06, 'epoch': 3.5336}
{'loss': 0.8698, 'grad_norm': 2.6516432532364362, 'learning_rate': 1.3061111111111113e-06, 'epoch': 3.534}
{'loss': 0.8708, 'grad_norm': 3.1579890511832107, 'learning_rate': 1.3050000000000002e-06, 'epoch': 3.5343999999999998}
{'loss': 0.8674, 'grad_norm': 2.908548410181635, 'learning_rate': 1.303888888888889e-06, 'epoch': 3.5348}
{'loss': 0.8603, 'grad_norm': 3.046262865442432, 'learning_rate': 1.3027777777777778e-06, 'epoch': 3.5352}
{'loss': 0.868, 'grad_norm': 2.8957012041168655, 'learning_rate': 1.301666666666667e-06, 'epoch': 3.5356}
{'loss': 0.8622, 'grad_norm': 3.0149679802385227, 'learning_rate': 1.3005555555555557e-06, 'epoch': 3.536}
{'eval_valid_loss': 0.8408203125, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1121.802, 'eval_valid_steps_per_second': 280.45, 'epoch': 3.536}
{'loss': 0.8729, 'grad_norm': 2.819423984676243, 'learning_rate': 1.2994444444444446e-06, 'epoch': 3.5364}
{'loss': 0.8505, 'grad_norm': 2.955685086138668, 'learning_rate': 1.2983333333333333e-06, 'epoch': 3.5368}
{'loss': 0.8596, 'grad_norm': 2.831513324601449, 'learning_rate': 1.2972222222222222e-06, 'epoch': 3.5372}
{'loss': 0.8618, 'grad_norm': 2.6854041932089827, 'learning_rate': 1.2961111111111114e-06, 'epoch': 3.5376}
{'loss': 0.8662, 'grad_norm': 3.1805239334135478, 'learning_rate': 1.295e-06, 'epoch': 3.5380000000000003}
{'loss': 0.8698, 'grad_norm': 2.8545966218995322, 'learning_rate': 1.293888888888889e-06, 'epoch': 3.5384}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8801, 'grad_norm': 2.766847588089035, 'learning_rate': 1.2927777777777777e-06, 'epoch': 3.5388}
{'loss': 0.871, 'grad_norm': 2.9313949085522695, 'learning_rate': 1.2916666666666669e-06, 'epoch': 3.5392}
{'loss': 0.8607, 'grad_norm': 2.854165767521032, 'learning_rate': 1.2905555555555558e-06, 'epoch': 3.5396}
{'loss': 0.8676, 'grad_norm': 2.8798822847290175, 'learning_rate': 1.2894444444444445e-06, 'epoch': 3.54}
{'eval_valid_loss': 0.8408203125, 'eval_valid_runtime': 0.0885, 'eval_valid_samples_per_second': 1130.415, 'eval_valid_steps_per_second': 282.604, 'epoch': 3.54}
{'loss': 0.8746, 'grad_norm': 3.3272462141328014, 'learning_rate': 1.2883333333333334e-06, 'epoch': 3.5404}
{'loss': 0.8596, 'grad_norm': 2.7435918347471344, 'learning_rate': 1.2872222222222221e-06, 'epoch': 3.5408}
{'loss': 0.8681, 'grad_norm': 3.001462133061017, 'learning_rate': 1.2861111111111113e-06, 'epoch': 3.5412}
{'loss': 0.8797, 'grad_norm': 3.31082290803958, 'learning_rate': 1.2850000000000002e-06, 'epoch': 3.5416}
{'loss': 0.881, 'grad_norm': 2.927974352630478, 'learning_rate': 1.283888888888889e-06, 'epoch': 3.542}
{'loss': 0.8588, 'grad_norm': 3.156508765613477, 'learning_rate': 1.2827777777777778e-06, 'epoch': 3.5423999999999998}
{'loss': 0.8722, 'grad_norm': 2.878507195692161, 'learning_rate': 1.281666666666667e-06, 'epoch': 3.5427999999999997}
{'loss': 0.8684, 'grad_norm': 3.4389039293792925, 'learning_rate': 1.2805555555555557e-06, 'epoch': 3.5432}
{'loss': 0.8693, 'grad_norm': 2.959488033323332, 'learning_rate': 1.2794444444444446e-06, 'epoch': 3.5436}
{'loss': 0.8685, 'grad_norm': 3.385433503133527, 'learning_rate': 1.2783333333333333e-06, 'epoch': 3.544}
{'eval_valid_loss': 0.8408203125, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1120.052, 'eval_valid_steps_per_second': 280.013, 'epoch': 3.544}
{'loss': 0.8613, 'grad_norm': 3.147977714948044, 'learning_rate': 1.2772222222222222e-06, 'epoch': 3.5444}
{'loss': 0.874, 'grad_norm': 3.0924308449792623, 'learning_rate': 1.2761111111111114e-06, 'epoch': 3.5448}
{'loss': 0.8615, 'grad_norm': 3.0018298906236622, 'learning_rate': 1.275e-06, 'epoch': 3.5452}
{'loss': 0.8663, 'grad_norm': 3.1260517639247927, 'learning_rate': 1.273888888888889e-06, 'epoch': 3.5456}
{'loss': 0.8692, 'grad_norm': 3.194958683561649, 'learning_rate': 1.2727777777777777e-06, 'epoch': 3.5460000000000003}
{'loss': 0.875, 'grad_norm': 3.333262476565344, 'learning_rate': 1.2716666666666668e-06, 'epoch': 3.5464}
{'loss': 0.8592, 'grad_norm': 2.942247956759401, 'learning_rate': 1.2705555555555558e-06, 'epoch': 3.5468}
{'loss': 0.8805, 'grad_norm': 3.0488657976519695, 'learning_rate': 1.2694444444444445e-06, 'epoch': 3.5472}
{'loss': 0.8647, 'grad_norm': 2.9036294597397867, 'learning_rate': 1.2683333333333334e-06, 'epoch': 3.5476}
{'loss': 0.869, 'grad_norm': 3.087113989124617, 'learning_rate': 1.2672222222222225e-06, 'epoch': 3.548}
{'eval_valid_loss': 0.84033203125, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.162, 'eval_valid_steps_per_second': 279.541, 'epoch': 3.548}
{'loss': 0.8679, 'grad_norm': 2.9107955537629175, 'learning_rate': 1.2661111111111112e-06, 'epoch': 3.5484}
{'loss': 0.877, 'grad_norm': 2.9739556699474767, 'learning_rate': 1.2650000000000002e-06, 'epoch': 3.5488}
{'loss': 0.861, 'grad_norm': 2.7569585239831333, 'learning_rate': 1.2638888888888889e-06, 'epoch': 3.5492}
{'loss': 0.8559, 'grad_norm': 2.803649327668496, 'learning_rate': 1.2627777777777778e-06, 'epoch': 3.5496}
{'loss': 0.8605, 'grad_norm': 2.762219422750746, 'learning_rate': 1.261666666666667e-06, 'epoch': 3.55}
{'loss': 0.8658, 'grad_norm': 3.1266839449438315, 'learning_rate': 1.2605555555555557e-06, 'epoch': 3.5504}
{'loss': 0.8631, 'grad_norm': 3.0349767624501203, 'learning_rate': 1.2594444444444446e-06, 'epoch': 3.5507999999999997}
{'loss': 0.8621, 'grad_norm': 2.9707295493732016, 'learning_rate': 1.2583333333333333e-06, 'epoch': 3.5512}
{'loss': 0.8642, 'grad_norm': 3.067797691369315, 'learning_rate': 1.2572222222222224e-06, 'epoch': 3.5516}
{'loss': 0.8662, 'grad_norm': 3.1644188574280063, 'learning_rate': 1.2561111111111113e-06, 'epoch': 3.552}
{'eval_valid_loss': 0.8408203125, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1120.89, 'eval_valid_steps_per_second': 280.223, 'epoch': 3.552}
{'loss': 0.8781, 'grad_norm': 2.781115517954633, 'learning_rate': 1.255e-06, 'epoch': 3.5524}
{'loss': 0.86, 'grad_norm': 2.8258814000065535, 'learning_rate': 1.253888888888889e-06, 'epoch': 3.5528}
{'loss': 0.8535, 'grad_norm': 2.7612071285053967, 'learning_rate': 1.2527777777777777e-06, 'epoch': 3.5532}
{'loss': 0.8682, 'grad_norm': 2.9131714785603586, 'learning_rate': 1.2516666666666668e-06, 'epoch': 3.5536}
{'loss': 0.8707, 'grad_norm': 3.1939229112165077, 'learning_rate': 1.2505555555555557e-06, 'epoch': 3.5540000000000003}
{'loss': 0.8722, 'grad_norm': 3.0082304230697097, 'learning_rate': 1.2494444444444445e-06, 'epoch': 3.5544000000000002}
{'loss': 0.8691, 'grad_norm': 2.9158145863345646, 'learning_rate': 1.2483333333333334e-06, 'epoch': 3.5548}
{'loss': 0.8868, 'grad_norm': 3.052709519571268, 'learning_rate': 1.2472222222222223e-06, 'epoch': 3.5552}
{'loss': 0.8571, 'grad_norm': 2.893344986134201, 'learning_rate': 1.2461111111111112e-06, 'epoch': 3.5556}
{'loss': 0.8712, 'grad_norm': 3.0288384076486357, 'learning_rate': 1.2450000000000002e-06, 'epoch': 3.556}
{'eval_valid_loss': 0.84033203125, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1120.282, 'eval_valid_steps_per_second': 280.071, 'epoch': 3.556}
{'loss': 0.8866, 'grad_norm': 2.9759135953196876, 'learning_rate': 1.2438888888888889e-06, 'epoch': 3.5564}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.8774, 'grad_norm': 2.966788617763618, 'learning_rate': 1.2427777777777778e-06, 'epoch': 3.5568}
{'loss': 0.8664, 'grad_norm': 2.808260551711283, 'learning_rate': 1.2416666666666667e-06, 'epoch': 3.5572}
{'loss': 0.8531, 'grad_norm': 2.858496098337385, 'learning_rate': 1.2405555555555556e-06, 'epoch': 3.5576}
{'loss': 0.8705, 'grad_norm': 2.819024269945084, 'learning_rate': 1.2394444444444446e-06, 'epoch': 3.558}
{'loss': 0.866, 'grad_norm': 2.9251240092668267, 'learning_rate': 1.2383333333333335e-06, 'epoch': 3.5584}
{'loss': 0.8723, 'grad_norm': 3.310809459793838, 'learning_rate': 1.2372222222222224e-06, 'epoch': 3.5587999999999997}
{'loss': 0.8698, 'grad_norm': 3.1749785700397717, 'learning_rate': 1.2361111111111113e-06, 'epoch': 3.5592}
{'loss': 0.8743, 'grad_norm': 2.891759021083071, 'learning_rate': 1.235e-06, 'epoch': 3.5596}
{'loss': 0.8739, 'grad_norm': 2.9613634371818987, 'learning_rate': 1.233888888888889e-06, 'epoch': 3.56}
{'eval_valid_loss': 0.83984375, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1121.451, 'eval_valid_steps_per_second': 280.363, 'epoch': 3.56}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8642, 'grad_norm': 2.9176658922590857, 'learning_rate': 1.2327777777777779e-06, 'epoch': 3.5604}
{'loss': 0.8636, 'grad_norm': 3.1224891207484866, 'learning_rate': 1.2316666666666668e-06, 'epoch': 3.5608}
{'loss': 0.86, 'grad_norm': 3.312939875564646, 'learning_rate': 1.2305555555555557e-06, 'epoch': 3.5612}
{'loss': 0.8596, 'grad_norm': 2.7101464120439434, 'learning_rate': 1.2294444444444444e-06, 'epoch': 3.5616}
{'loss': 0.8697, 'grad_norm': 2.837668571002216, 'learning_rate': 1.2283333333333334e-06, 'epoch': 3.5620000000000003}
{'loss': 0.8625, 'grad_norm': 2.9181454207979782, 'learning_rate': 1.2272222222222223e-06, 'epoch': 3.5624000000000002}
{'loss': 0.8678, 'grad_norm': 3.233331198585107, 'learning_rate': 1.2261111111111112e-06, 'epoch': 3.5628}
{'loss': 0.8736, 'grad_norm': 2.9276658063497334, 'learning_rate': 1.2250000000000001e-06, 'epoch': 3.5632}
{'loss': 0.86, 'grad_norm': 2.8405015805315084, 'learning_rate': 1.223888888888889e-06, 'epoch': 3.5636}
{'loss': 0.8654, 'grad_norm': 2.9321313761145666, 'learning_rate': 1.2227777777777778e-06, 'epoch': 3.564}
{'eval_valid_loss': 0.84033203125, 'eval_valid_runtime': 0.091, 'eval_valid_samples_per_second': 1098.523, 'eval_valid_steps_per_second': 274.631, 'epoch': 3.564}
{'loss': 0.8504, 'grad_norm': 2.9934857343416934, 'learning_rate': 1.221777777777778e-06, 'epoch': 3.5644}
{'loss': 0.8729, 'grad_norm': 3.12341546894832, 'learning_rate': 1.2206666666666668e-06, 'epoch': 3.5648}
{'loss': 0.863, 'grad_norm': 3.0082794124300656, 'learning_rate': 1.2195555555555555e-06, 'epoch': 3.5652}
{'loss': 0.8725, 'grad_norm': 2.794857362953005, 'learning_rate': 1.2184444444444447e-06, 'epoch': 3.5656}
{'loss': 0.8627, 'grad_norm': 2.8136810842774027, 'learning_rate': 1.2173333333333334e-06, 'epoch': 3.566}
{'loss': 0.8635, 'grad_norm': 3.0198403973752495, 'learning_rate': 1.2162222222222223e-06, 'epoch': 3.5664}
{'loss': 0.8497, 'grad_norm': 2.694544879130079, 'learning_rate': 1.2151111111111112e-06, 'epoch': 3.5667999999999997}
{'loss': 0.8653, 'grad_norm': 2.7993469023694395, 'learning_rate': 1.214e-06, 'epoch': 3.5672}
{'loss': 0.8678, 'grad_norm': 3.0438560984329617, 'learning_rate': 1.212888888888889e-06, 'epoch': 3.5676}
{'loss': 0.8667, 'grad_norm': 3.043033858693496, 'learning_rate': 1.2117777777777778e-06, 'epoch': 3.568}
{'eval_valid_loss': 0.84033203125, 'eval_valid_runtime': 0.0906, 'eval_valid_samples_per_second': 1103.34, 'eval_valid_steps_per_second': 275.835, 'epoch': 3.568}
{'loss': 0.8585, 'grad_norm': 2.767387868174056, 'learning_rate': 1.2106666666666667e-06, 'epoch': 3.5684}
{'loss': 0.8622, 'grad_norm': 3.045967573363992, 'learning_rate': 1.2095555555555556e-06, 'epoch': 3.5688}
{'loss': 0.8761, 'grad_norm': 2.823953333791268, 'learning_rate': 1.2084444444444446e-06, 'epoch': 3.5692}
{'loss': 0.8544, 'grad_norm': 2.9637839090377573, 'learning_rate': 1.2073333333333335e-06, 'epoch': 3.5696}
{'loss': 0.8713, 'grad_norm': 2.88503989825105, 'learning_rate': 1.2062222222222224e-06, 'epoch': 3.57}
{'loss': 0.8583, 'grad_norm': 3.070375330236009, 'learning_rate': 1.2051111111111111e-06, 'epoch': 3.5704000000000002}
{'loss': 0.8804, 'grad_norm': 3.3993568842987827, 'learning_rate': 1.204e-06, 'epoch': 3.5708}
{'loss': 0.8644, 'grad_norm': 2.8864177603722623, 'learning_rate': 1.202888888888889e-06, 'epoch': 3.5712}
{'loss': 0.8684, 'grad_norm': 3.2712460559184198, 'learning_rate': 1.2017777777777779e-06, 'epoch': 3.5716}
{'loss': 0.8571, 'grad_norm': 3.1749445996745123, 'learning_rate': 1.2006666666666668e-06, 'epoch': 3.572}
{'eval_valid_loss': 0.83984375, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.605, 'eval_valid_steps_per_second': 279.151, 'epoch': 3.572}
{'loss': 0.8616, 'grad_norm': 3.166451378874152, 'learning_rate': 1.1995555555555555e-06, 'epoch': 3.5724}
{'loss': 0.8572, 'grad_norm': 2.933223177772343, 'learning_rate': 1.1984444444444446e-06, 'epoch': 3.5728}
{'loss': 0.8665, 'grad_norm': 2.986198377566248, 'learning_rate': 1.1973333333333334e-06, 'epoch': 3.5732}
{'loss': 0.8646, 'grad_norm': 3.1646555968709036, 'learning_rate': 1.1962222222222223e-06, 'epoch': 3.5736}
{'loss': 0.8603, 'grad_norm': 3.0426783686364653, 'learning_rate': 1.1951111111111112e-06, 'epoch': 3.574}
{'loss': 0.8682, 'grad_norm': 3.135662675417924, 'learning_rate': 1.1940000000000001e-06, 'epoch': 3.5744}
{'loss': 0.8596, 'grad_norm': 2.841146849653352, 'learning_rate': 1.192888888888889e-06, 'epoch': 3.5747999999999998}
{'loss': 0.8666, 'grad_norm': 2.9993907786719576, 'learning_rate': 1.1917777777777778e-06, 'epoch': 3.5752}
{'loss': 0.8549, 'grad_norm': 2.7585074600037953, 'learning_rate': 1.1906666666666667e-06, 'epoch': 3.5756}
{'loss': 0.8647, 'grad_norm': 3.168862232124296, 'learning_rate': 1.1895555555555556e-06, 'epoch': 3.576}
{'eval_valid_loss': 0.84033203125, 'eval_valid_runtime': 0.0907, 'eval_valid_samples_per_second': 1102.896, 'eval_valid_steps_per_second': 275.724, 'epoch': 3.576}
{'loss': 0.8705, 'grad_norm': 3.011441951554529, 'learning_rate': 1.1884444444444445e-06, 'epoch': 3.5764}
{'loss': 0.8649, 'grad_norm': 3.051710185175226, 'learning_rate': 1.1873333333333335e-06, 'epoch': 3.5768}
{'loss': 0.8723, 'grad_norm': 2.9484603917335486, 'learning_rate': 1.1862222222222224e-06, 'epoch': 3.5772}
{'loss': 0.87, 'grad_norm': 2.9172387952743852, 'learning_rate': 1.185111111111111e-06, 'epoch': 3.5776}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8707, 'grad_norm': 2.939516653597395, 'learning_rate': 1.1840000000000002e-06, 'epoch': 3.578}
{'loss': 0.8533, 'grad_norm': 2.6797746749800293, 'learning_rate': 1.182888888888889e-06, 'epoch': 3.5784000000000002}
{'loss': 0.8683, 'grad_norm': 3.0292730176762186, 'learning_rate': 1.1817777777777779e-06, 'epoch': 3.5788}
{'loss': 0.8591, 'grad_norm': 2.8942625716451147, 'learning_rate': 1.1806666666666668e-06, 'epoch': 3.5792}
{'loss': 0.866, 'grad_norm': 2.8854203924207815, 'learning_rate': 1.1795555555555555e-06, 'epoch': 3.5796}
{'loss': 0.8593, 'grad_norm': 2.969502203161242, 'learning_rate': 1.1784444444444446e-06, 'epoch': 3.58}
{'eval_valid_loss': 0.83984375, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.789, 'eval_valid_steps_per_second': 278.697, 'epoch': 3.58}
{'loss': 0.8606, 'grad_norm': 3.000470114669247, 'learning_rate': 1.1773333333333333e-06, 'epoch': 3.5804}
{'loss': 0.8616, 'grad_norm': 2.7464155657019345, 'learning_rate': 1.1762222222222223e-06, 'epoch': 3.5808}
{'loss': 0.8688, 'grad_norm': 2.959242176873799, 'learning_rate': 1.1751111111111112e-06, 'epoch': 3.5812}
{'loss': 0.8744, 'grad_norm': 2.980619234258283, 'learning_rate': 1.1740000000000001e-06, 'epoch': 3.5816}
{'loss': 0.8828, 'grad_norm': 2.9931625689803063, 'learning_rate': 1.172888888888889e-06, 'epoch': 3.582}
{'loss': 0.8656, 'grad_norm': 2.9923454023883207, 'learning_rate': 1.171777777777778e-06, 'epoch': 3.5824}
{'loss': 0.8653, 'grad_norm': 2.8806513669195053, 'learning_rate': 1.1706666666666667e-06, 'epoch': 3.5827999999999998}
{'loss': 0.8835, 'grad_norm': 3.0051017733523206, 'learning_rate': 1.1695555555555556e-06, 'epoch': 3.5832}
{'loss': 0.8677, 'grad_norm': 2.8951411460570182, 'learning_rate': 1.1684444444444445e-06, 'epoch': 3.5836}
{'loss': 0.8688, 'grad_norm': 2.9583589805691215, 'learning_rate': 1.1673333333333334e-06, 'epoch': 3.584}
{'eval_valid_loss': 0.83984375, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1110.198, 'eval_valid_steps_per_second': 277.549, 'epoch': 3.584}
{'loss': 0.8649, 'grad_norm': 2.9074332279433146, 'learning_rate': 1.1662222222222224e-06, 'epoch': 3.5844}
{'loss': 0.8776, 'grad_norm': 2.898771282214973, 'learning_rate': 1.165111111111111e-06, 'epoch': 3.5848}
{'loss': 0.8711, 'grad_norm': 2.976329724278631, 'learning_rate': 1.1640000000000002e-06, 'epoch': 3.5852}
{'loss': 0.8681, 'grad_norm': 3.038298974312845, 'learning_rate': 1.162888888888889e-06, 'epoch': 3.5856}
{'loss': 0.8646, 'grad_norm': 2.7704402900564884, 'learning_rate': 1.1617777777777778e-06, 'epoch': 3.586}
{'loss': 0.8686, 'grad_norm': 3.0223010389095424, 'learning_rate': 1.1606666666666668e-06, 'epoch': 3.5864000000000003}
{'loss': 0.8615, 'grad_norm': 2.9255143108751365, 'learning_rate': 1.1595555555555557e-06, 'epoch': 3.5868}
{'loss': 0.8631, 'grad_norm': 2.8957784182100834, 'learning_rate': 1.1584444444444446e-06, 'epoch': 3.5872}
{'loss': 0.8564, 'grad_norm': 3.0216660448039683, 'learning_rate': 1.1573333333333333e-06, 'epoch': 3.5876}
{'loss': 0.8691, 'grad_norm': 3.1100217347903496, 'learning_rate': 1.1562222222222222e-06, 'epoch': 3.588}
{'eval_valid_loss': 0.8408203125, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.171, 'eval_valid_steps_per_second': 281.793, 'epoch': 3.588}
{'loss': 0.8737, 'grad_norm': 2.8221513876844844, 'learning_rate': 1.1551111111111112e-06, 'epoch': 3.5884}
{'loss': 0.8702, 'grad_norm': 2.841957334202868, 'learning_rate': 1.154e-06, 'epoch': 3.5888}
{'loss': 0.8726, 'grad_norm': 2.7850775259483265, 'learning_rate': 1.152888888888889e-06, 'epoch': 3.5892}
{'loss': 0.8696, 'grad_norm': 2.8553855977821345, 'learning_rate': 1.151777777777778e-06, 'epoch': 3.5896}
{'loss': 0.8656, 'grad_norm': 2.851893418850547, 'learning_rate': 1.1506666666666666e-06, 'epoch': 3.59}
{'loss': 0.8763, 'grad_norm': 3.210878088556936, 'learning_rate': 1.1495555555555558e-06, 'epoch': 3.5904}
{'loss': 0.8731, 'grad_norm': 2.956906415554333, 'learning_rate': 1.1484444444444445e-06, 'epoch': 3.5907999999999998}
{'loss': 0.8612, 'grad_norm': 3.096924857807402, 'learning_rate': 1.1473333333333334e-06, 'epoch': 3.5911999999999997}
{'loss': 0.8745, 'grad_norm': 2.9316459111846496, 'learning_rate': 1.1462222222222223e-06, 'epoch': 3.5916}
{'loss': 0.8604, 'grad_norm': 2.8611285143284877, 'learning_rate': 1.145111111111111e-06, 'epoch': 3.592}
{'eval_valid_loss': 0.83984375, 'eval_valid_runtime': 0.0966, 'eval_valid_samples_per_second': 1035.301, 'eval_valid_steps_per_second': 258.825, 'epoch': 3.592}
{'loss': 0.8594, 'grad_norm': 2.992890337676379, 'learning_rate': 1.1440000000000002e-06, 'epoch': 3.5924}
{'loss': 0.873, 'grad_norm': 3.014134784976699, 'learning_rate': 1.142888888888889e-06, 'epoch': 3.5928}
{'loss': 0.8564, 'grad_norm': 2.854639760136546, 'learning_rate': 1.1417777777777778e-06, 'epoch': 3.5932}
{'loss': 0.8674, 'grad_norm': 2.8718667682948973, 'learning_rate': 1.1406666666666667e-06, 'epoch': 3.5936}
{'loss': 0.8686, 'grad_norm': 2.994041396435865, 'learning_rate': 1.1395555555555557e-06, 'epoch': 3.594}
{'loss': 0.8613, 'grad_norm': 3.6653173124021734, 'learning_rate': 1.1384444444444446e-06, 'epoch': 3.5944000000000003}
{'loss': 0.8644, 'grad_norm': 3.1373739164026744, 'learning_rate': 1.1373333333333335e-06, 'epoch': 3.5948}
{'loss': 0.8731, 'grad_norm': 2.834731662689252, 'learning_rate': 1.1362222222222222e-06, 'epoch': 3.5952}
{'loss': 0.866, 'grad_norm': 2.7904388226797248, 'learning_rate': 1.1351111111111111e-06, 'epoch': 3.5956}
{'loss': 0.8626, 'grad_norm': 3.145212381648406, 'learning_rate': 1.134e-06, 'epoch': 3.596}
{'eval_valid_loss': 0.83935546875, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1111.678, 'eval_valid_steps_per_second': 277.919, 'epoch': 3.596}
{'loss': 0.8549, 'grad_norm': 3.017506314012674, 'learning_rate': 1.132888888888889e-06, 'epoch': 3.5964}
{'loss': 0.8675, 'grad_norm': 2.885921930953712, 'learning_rate': 1.131777777777778e-06, 'epoch': 3.5968}
{'loss': 0.8634, 'grad_norm': 2.7556361399317466, 'learning_rate': 1.1306666666666666e-06, 'epoch': 3.5972}
{'loss': 0.8688, 'grad_norm': 2.918886727146842, 'learning_rate': 1.1295555555555558e-06, 'epoch': 3.5976}
{'loss': 0.8661, 'grad_norm': 2.835337909125086, 'learning_rate': 1.1284444444444445e-06, 'epoch': 3.598}
{'loss': 0.8653, 'grad_norm': 2.884429199188952, 'learning_rate': 1.1273333333333334e-06, 'epoch': 3.5984}
{'loss': 0.872, 'grad_norm': 2.898197272080126, 'learning_rate': 1.1262222222222223e-06, 'epoch': 3.5987999999999998}
{'loss': 0.8634, 'grad_norm': 3.120282436525926, 'learning_rate': 1.1251111111111112e-06, 'epoch': 3.5991999999999997}
{'loss': 0.8518, 'grad_norm': 2.965991008068083, 'learning_rate': 1.1240000000000002e-06, 'epoch': 3.5996}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8626, 'grad_norm': 3.1458705253591988, 'learning_rate': 1.1228888888888889e-06, 'epoch': 3.6}
{'eval_valid_loss': 0.83935546875, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.271, 'eval_valid_steps_per_second': 278.568, 'epoch': 3.6}
{'loss': 0.863, 'grad_norm': 3.153167711465813, 'learning_rate': 1.1217777777777778e-06, 'epoch': 3.6004}
{'loss': 0.8628, 'grad_norm': 3.0262713938763994, 'learning_rate': 1.1206666666666667e-06, 'epoch': 3.6008}
{'loss': 0.8499, 'grad_norm': 2.9996307364337933, 'learning_rate': 1.1195555555555556e-06, 'epoch': 3.6012}
{'loss': 0.8661, 'grad_norm': 2.7707592349553787, 'learning_rate': 1.1184444444444446e-06, 'epoch': 3.6016}
{'loss': 0.8682, 'grad_norm': 3.1402778813233083, 'learning_rate': 1.1173333333333335e-06, 'epoch': 3.602}
{'loss': 0.8564, 'grad_norm': 2.9645242229867566, 'learning_rate': 1.1162222222222222e-06, 'epoch': 3.6024000000000003}
{'loss': 0.8613, 'grad_norm': 3.0835194059020785, 'learning_rate': 1.1151111111111113e-06, 'epoch': 3.6028000000000002}
{'loss': 0.8798, 'grad_norm': 2.9022221854256594, 'learning_rate': 1.114e-06, 'epoch': 3.6032}
{'loss': 0.8684, 'grad_norm': 2.9417910178560174, 'learning_rate': 1.112888888888889e-06, 'epoch': 3.6036}
{'loss': 0.8629, 'grad_norm': 3.0393483534971932, 'learning_rate': 1.111777777777778e-06, 'epoch': 3.604}
{'eval_valid_loss': 0.83935546875, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1106.347, 'eval_valid_steps_per_second': 276.587, 'epoch': 3.604}
{'loss': 0.862, 'grad_norm': 2.9429395967380816, 'learning_rate': 1.1107777777777778e-06, 'epoch': 3.6044}
{'loss': 0.8657, 'grad_norm': 2.6303796309535645, 'learning_rate': 1.1096666666666667e-06, 'epoch': 3.6048}
{'loss': 0.8604, 'grad_norm': 2.9573594670833656, 'learning_rate': 1.1085555555555557e-06, 'epoch': 3.6052}
{'loss': 0.8691, 'grad_norm': 3.0310487582074193, 'learning_rate': 1.1074444444444446e-06, 'epoch': 3.6056}
{'loss': 0.8657, 'grad_norm': 2.7905956510258547, 'learning_rate': 1.1063333333333335e-06, 'epoch': 3.606}
{'loss': 0.8484, 'grad_norm': 2.721326330793954, 'learning_rate': 1.1052222222222222e-06, 'epoch': 3.6064}
{'loss': 0.8729, 'grad_norm': 2.8596176398882447, 'learning_rate': 1.1041111111111111e-06, 'epoch': 3.6068}
{'loss': 0.8574, 'grad_norm': 3.1102271613570402, 'learning_rate': 1.103e-06, 'epoch': 3.6071999999999997}
{'loss': 0.863, 'grad_norm': 2.7948156372608226, 'learning_rate': 1.101888888888889e-06, 'epoch': 3.6076}
{'loss': 0.8858, 'grad_norm': 3.2592349617592045, 'learning_rate': 1.100777777777778e-06, 'epoch': 3.608}
{'eval_valid_loss': 0.83935546875, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1116.944, 'eval_valid_steps_per_second': 279.236, 'epoch': 3.608}
{'loss': 0.863, 'grad_norm': 2.7139002086583806, 'learning_rate': 1.0996666666666668e-06, 'epoch': 3.6084}
{'loss': 0.8647, 'grad_norm': 2.887551906251266, 'learning_rate': 1.0985555555555556e-06, 'epoch': 3.6088}
{'loss': 0.8615, 'grad_norm': 2.8867201643799656, 'learning_rate': 1.0974444444444447e-06, 'epoch': 3.6092}
{'loss': 0.858, 'grad_norm': 2.8896536251742204, 'learning_rate': 1.0963333333333334e-06, 'epoch': 3.6096}
{'loss': 0.8604, 'grad_norm': 3.0623356522038914, 'learning_rate': 1.0952222222222223e-06, 'epoch': 3.61}
{'loss': 0.8722, 'grad_norm': 2.9451224571862413, 'learning_rate': 1.0941111111111112e-06, 'epoch': 3.6104000000000003}
{'loss': 0.8667, 'grad_norm': 2.9713253696304402, 'learning_rate': 1.093e-06, 'epoch': 3.6108000000000002}
{'loss': 0.8713, 'grad_norm': 2.948580277406948, 'learning_rate': 1.091888888888889e-06, 'epoch': 3.6112}
{'loss': 0.8611, 'grad_norm': 2.8674072888194084, 'learning_rate': 1.0907777777777778e-06, 'epoch': 3.6116}
{'loss': 0.8699, 'grad_norm': 3.0856352646015455, 'learning_rate': 1.0896666666666667e-06, 'epoch': 3.612}
{'eval_valid_loss': 0.83935546875, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.65, 'eval_valid_steps_per_second': 279.412, 'epoch': 3.612}
{'loss': 0.8675, 'grad_norm': 2.995562241420358, 'learning_rate': 1.0885555555555556e-06, 'epoch': 3.6124}
{'loss': 0.8696, 'grad_norm': 2.845583733205601, 'learning_rate': 1.0874444444444446e-06, 'epoch': 3.6128}
{'loss': 0.8595, 'grad_norm': 2.613704367740536, 'learning_rate': 1.0863333333333335e-06, 'epoch': 3.6132}
{'loss': 0.8688, 'grad_norm': 2.9232341338823, 'learning_rate': 1.0852222222222224e-06, 'epoch': 3.6136}
{'loss': 0.8506, 'grad_norm': 2.9847435149507295, 'learning_rate': 1.0841111111111111e-06, 'epoch': 3.614}
{'loss': 0.8632, 'grad_norm': 3.0306196638875735, 'learning_rate': 1.083e-06, 'epoch': 3.6144}
{'loss': 0.8573, 'grad_norm': 2.842528940404083, 'learning_rate': 1.081888888888889e-06, 'epoch': 3.6148}
{'loss': 0.8682, 'grad_norm': 2.993092402585168, 'learning_rate': 1.080777777777778e-06, 'epoch': 3.6151999999999997}
{'loss': 0.8492, 'grad_norm': 2.6433569302553934, 'learning_rate': 1.0796666666666668e-06, 'epoch': 3.6156}
{'loss': 0.8681, 'grad_norm': 2.8185503043087197, 'learning_rate': 1.0785555555555555e-06, 'epoch': 3.616}
{'eval_valid_loss': 0.83984375, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.028, 'eval_valid_steps_per_second': 278.007, 'epoch': 3.616}
{'loss': 0.8603, 'grad_norm': 2.7654088415382225, 'learning_rate': 1.0774444444444447e-06, 'epoch': 3.6164}
{'loss': 0.8588, 'grad_norm': 3.091927453932666, 'learning_rate': 1.0763333333333334e-06, 'epoch': 3.6168}
{'loss': 0.8575, 'grad_norm': 3.0171556096735075, 'learning_rate': 1.0752222222222223e-06, 'epoch': 3.6172}
{'loss': 0.8685, 'grad_norm': 3.293136402355819, 'learning_rate': 1.0741111111111112e-06, 'epoch': 3.6176}
{'loss': 0.8728, 'grad_norm': 3.0580552407795847, 'learning_rate': 1.0730000000000001e-06, 'epoch': 3.618}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8592, 'grad_norm': 2.903811287834595, 'learning_rate': 1.071888888888889e-06, 'epoch': 3.6184}
{'loss': 0.877, 'grad_norm': 3.153466952243742, 'learning_rate': 1.0707777777777778e-06, 'epoch': 3.6188000000000002}
{'loss': 0.8571, 'grad_norm': 2.936433628993715, 'learning_rate': 1.0696666666666667e-06, 'epoch': 3.6192}
{'loss': 0.8595, 'grad_norm': 3.034933428152061, 'learning_rate': 1.0685555555555556e-06, 'epoch': 3.6196}
{'loss': 0.8735, 'grad_norm': 3.08076684154305, 'learning_rate': 1.0674444444444445e-06, 'epoch': 3.62}
{'eval_valid_loss': 0.84033203125, 'eval_valid_runtime': 0.089, 'eval_valid_samples_per_second': 1123.563, 'eval_valid_steps_per_second': 280.891, 'epoch': 3.62}
{'loss': 0.865, 'grad_norm': 3.081085098527611, 'learning_rate': 1.0663333333333335e-06, 'epoch': 3.6204}
{'loss': 0.8615, 'grad_norm': 3.132717483106996, 'learning_rate': 1.0652222222222224e-06, 'epoch': 3.6208}
{'loss': 0.8582, 'grad_norm': 2.7804179661511137, 'learning_rate': 1.0641111111111111e-06, 'epoch': 3.6212}
{'loss': 0.8699, 'grad_norm': 2.907171785135204, 'learning_rate': 1.0630000000000002e-06, 'epoch': 3.6216}
{'loss': 0.8612, 'grad_norm': 3.1760691457134986, 'learning_rate': 1.061888888888889e-06, 'epoch': 3.622}
{'loss': 0.8678, 'grad_norm': 2.9032468045472575, 'learning_rate': 1.0607777777777779e-06, 'epoch': 3.6224}
{'loss': 0.864, 'grad_norm': 3.1195325420098956, 'learning_rate': 1.0596666666666668e-06, 'epoch': 3.6228}
{'loss': 0.8711, 'grad_norm': 3.1896716461729455, 'learning_rate': 1.0585555555555555e-06, 'epoch': 3.6231999999999998}
{'loss': 0.8635, 'grad_norm': 2.795746218541223, 'learning_rate': 1.0574444444444446e-06, 'epoch': 3.6236}
{'loss': 0.8666, 'grad_norm': 2.7713861535418993, 'learning_rate': 1.0563333333333334e-06, 'epoch': 3.624}
{'eval_valid_loss': 0.83984375, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1108.818, 'eval_valid_steps_per_second': 277.205, 'epoch': 3.624}
{'loss': 0.8817, 'grad_norm': 3.037365711838633, 'learning_rate': 1.0552222222222223e-06, 'epoch': 3.6244}
{'loss': 0.8681, 'grad_norm': 3.0340577129578103, 'learning_rate': 1.0541111111111112e-06, 'epoch': 3.6248}
{'loss': 0.8741, 'grad_norm': 3.0768713358737747, 'learning_rate': 1.0530000000000001e-06, 'epoch': 3.6252}
{'loss': 0.8693, 'grad_norm': 3.156964372296103, 'learning_rate': 1.051888888888889e-06, 'epoch': 3.6256}
{'loss': 0.8644, 'grad_norm': 2.8024016019934335, 'learning_rate': 1.050777777777778e-06, 'epoch': 3.626}
{'loss': 0.8758, 'grad_norm': 2.9697176711436386, 'learning_rate': 1.0496666666666667e-06, 'epoch': 3.6264}
{'loss': 0.8644, 'grad_norm': 2.9538995443579252, 'learning_rate': 1.0485555555555556e-06, 'epoch': 3.6268000000000002}
{'loss': 0.8586, 'grad_norm': 2.874582224014105, 'learning_rate': 1.0474444444444445e-06, 'epoch': 3.6272}
{'loss': 0.8559, 'grad_norm': 3.211968203140797, 'learning_rate': 1.0463333333333335e-06, 'epoch': 3.6276}
{'loss': 0.8558, 'grad_norm': 3.1105684070064665, 'learning_rate': 1.0452222222222224e-06, 'epoch': 3.628}
{'eval_valid_loss': 0.83984375, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.659, 'eval_valid_steps_per_second': 278.665, 'epoch': 3.628}
{'loss': 0.8712, 'grad_norm': 3.166847444485564, 'learning_rate': 1.044111111111111e-06, 'epoch': 3.6284}
{'loss': 0.864, 'grad_norm': 3.013944879256914, 'learning_rate': 1.0430000000000002e-06, 'epoch': 3.6288}
{'loss': 0.878, 'grad_norm': 2.8346091698392377, 'learning_rate': 1.041888888888889e-06, 'epoch': 3.6292}
{'loss': 0.8742, 'grad_norm': 2.9659514136003615, 'learning_rate': 1.0407777777777779e-06, 'epoch': 3.6296}
{'loss': 0.8573, 'grad_norm': 3.073250662977122, 'learning_rate': 1.0396666666666668e-06, 'epoch': 3.63}
{'loss': 0.8692, 'grad_norm': 2.761795015914199, 'learning_rate': 1.0385555555555557e-06, 'epoch': 3.6304}
{'loss': 0.875, 'grad_norm': 3.2490007900208897, 'learning_rate': 1.0374444444444446e-06, 'epoch': 3.6308}
{'loss': 0.8812, 'grad_norm': 3.293489046273462, 'learning_rate': 1.0363333333333333e-06, 'epoch': 3.6311999999999998}
{'loss': 0.8491, 'grad_norm': 3.00043820121322, 'learning_rate': 1.0352222222222223e-06, 'epoch': 3.6316}
{'loss': 0.8654, 'grad_norm': 3.008835209954117, 'learning_rate': 1.0341111111111112e-06, 'epoch': 3.632}
{'eval_valid_loss': 0.83935546875, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1119.147, 'eval_valid_steps_per_second': 279.787, 'epoch': 3.632}
{'loss': 0.8811, 'grad_norm': 3.037790409773425, 'learning_rate': 1.033e-06, 'epoch': 3.6324}
{'loss': 0.8673, 'grad_norm': 2.9972501808304544, 'learning_rate': 1.031888888888889e-06, 'epoch': 3.6328}
{'loss': 0.8595, 'grad_norm': 2.8789417363002245, 'learning_rate': 1.030777777777778e-06, 'epoch': 3.6332}
{'loss': 0.8724, 'grad_norm': 3.099698926549049, 'learning_rate': 1.0296666666666667e-06, 'epoch': 3.6336}
{'loss': 0.8726, 'grad_norm': 3.066506596763876, 'learning_rate': 1.0285555555555558e-06, 'epoch': 3.634}
{'loss': 0.8716, 'grad_norm': 3.0116135495981147, 'learning_rate': 1.0274444444444445e-06, 'epoch': 3.6344}
{'loss': 0.8565, 'grad_norm': 3.023230543125878, 'learning_rate': 1.0263333333333334e-06, 'epoch': 3.6348000000000003}
{'loss': 0.8656, 'grad_norm': 3.0778557422430377, 'learning_rate': 1.0252222222222224e-06, 'epoch': 3.6352}
{'loss': 0.8603, 'grad_norm': 2.95853406041576, 'learning_rate': 1.024111111111111e-06, 'epoch': 3.6356}
{'loss': 0.86, 'grad_norm': 2.880989201093493, 'learning_rate': 1.0230000000000002e-06, 'epoch': 3.636}
{'eval_valid_loss': 0.84033203125, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1121.319, 'eval_valid_steps_per_second': 280.33, 'epoch': 3.636}
{'loss': 0.8579, 'grad_norm': 3.026044869653754, 'learning_rate': 1.021888888888889e-06, 'epoch': 3.6364}
{'loss': 0.8768, 'grad_norm': 3.095409516571765, 'learning_rate': 1.0207777777777778e-06, 'epoch': 3.6368}
{'loss': 0.871, 'grad_norm': 2.894966700202609, 'learning_rate': 1.0196666666666668e-06, 'epoch': 3.6372}
{'loss': 0.8744, 'grad_norm': 2.8591186481482493, 'learning_rate': 1.0185555555555557e-06, 'epoch': 3.6376}
{'loss': 0.8565, 'grad_norm': 3.0855610195308505, 'learning_rate': 1.0174444444444446e-06, 'epoch': 3.638}
{'loss': 0.8652, 'grad_norm': 2.7071474884310285, 'learning_rate': 1.0163333333333335e-06, 'epoch': 3.6384}
{'loss': 0.8642, 'grad_norm': 3.3377572217681295, 'learning_rate': 1.0152222222222222e-06, 'epoch': 3.6388}
{'loss': 0.8608, 'grad_norm': 2.936133249364387, 'learning_rate': 1.0141111111111112e-06, 'epoch': 3.6391999999999998}
{'loss': 0.8629, 'grad_norm': 3.1339695950696527, 'learning_rate': 1.013e-06, 'epoch': 3.6395999999999997}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8652, 'grad_norm': 3.199386964877321, 'learning_rate': 1.011888888888889e-06, 'epoch': 3.64}
{'eval_valid_loss': 0.83935546875, 'eval_valid_runtime': 0.0908, 'eval_valid_samples_per_second': 1101.295, 'eval_valid_steps_per_second': 275.324, 'epoch': 3.64}
{'loss': 0.8658, 'grad_norm': 3.0736903084213667, 'learning_rate': 1.010777777777778e-06, 'epoch': 3.6404}
{'loss': 0.8514, 'grad_norm': 2.8250754074143107, 'learning_rate': 1.0096666666666666e-06, 'epoch': 3.6408}
{'loss': 0.8758, 'grad_norm': 3.2657250840761614, 'learning_rate': 1.0085555555555558e-06, 'epoch': 3.6412}
{'loss': 0.8677, 'grad_norm': 2.9843928950706937, 'learning_rate': 1.0074444444444445e-06, 'epoch': 3.6416}
{'loss': 0.8601, 'grad_norm': 3.2052410300521834, 'learning_rate': 1.0063333333333334e-06, 'epoch': 3.642}
{'loss': 0.8695, 'grad_norm': 3.1054873675861936, 'learning_rate': 1.0052222222222223e-06, 'epoch': 3.6424}
{'loss': 0.8739, 'grad_norm': 3.1130770971736443, 'learning_rate': 1.0041111111111113e-06, 'epoch': 3.6428000000000003}
{'loss': 0.8623, 'grad_norm': 2.899614614332672, 'learning_rate': 1.0030000000000002e-06, 'epoch': 3.6432}
{'loss': 0.8643, 'grad_norm': 3.0020376179903123, 'learning_rate': 1.0018888888888889e-06, 'epoch': 3.6436}
{'loss': 0.8616, 'grad_norm': 3.069513755742662, 'learning_rate': 1.0007777777777778e-06, 'epoch': 3.644}
{'eval_valid_loss': 0.83935546875, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1124.614, 'eval_valid_steps_per_second': 281.153, 'epoch': 3.644}
{'loss': 0.8635, 'grad_norm': 2.7514014953523676, 'learning_rate': 9.99777777777778e-07, 'epoch': 3.6444}
{'loss': 0.8562, 'grad_norm': 2.825332464217856, 'learning_rate': 9.986666666666669e-07, 'epoch': 3.6448}
{'loss': 0.874, 'grad_norm': 3.243289420149785, 'learning_rate': 9.975555555555556e-07, 'epoch': 3.6452}
{'loss': 0.8682, 'grad_norm': 3.0658452778264103, 'learning_rate': 9.964444444444445e-07, 'epoch': 3.6456}
{'loss': 0.8633, 'grad_norm': 3.130588931505911, 'learning_rate': 9.953333333333334e-07, 'epoch': 3.646}
{'loss': 0.8626, 'grad_norm': 2.8032136407656365, 'learning_rate': 9.942222222222224e-07, 'epoch': 3.6464}
{'loss': 0.8626, 'grad_norm': 2.954757128398923, 'learning_rate': 9.931111111111113e-07, 'epoch': 3.6468}
{'loss': 0.8639, 'grad_norm': 2.6513510462935237, 'learning_rate': 9.92e-07, 'epoch': 3.6471999999999998}
{'loss': 0.8594, 'grad_norm': 2.99210742016597, 'learning_rate': 9.90888888888889e-07, 'epoch': 3.6475999999999997}
{'loss': 0.862, 'grad_norm': 2.8452891701292264, 'learning_rate': 9.897777777777778e-07, 'epoch': 3.648}
{'eval_valid_loss': 0.83935546875, 'eval_valid_runtime': 0.0922, 'eval_valid_samples_per_second': 1085.086, 'eval_valid_steps_per_second': 271.272, 'epoch': 3.648}
{'loss': 0.8604, 'grad_norm': 3.180172379473005, 'learning_rate': 9.886666666666668e-07, 'epoch': 3.6484}
{'loss': 0.8666, 'grad_norm': 2.8729119700300534, 'learning_rate': 9.875555555555557e-07, 'epoch': 3.6488}
{'loss': 0.8718, 'grad_norm': 3.13996918316923, 'learning_rate': 9.864444444444446e-07, 'epoch': 3.6492}
{'loss': 0.8617, 'grad_norm': 2.972951355571146, 'learning_rate': 9.853333333333333e-07, 'epoch': 3.6496}
{'loss': 0.8619, 'grad_norm': 2.978652180527467, 'learning_rate': 9.842222222222222e-07, 'epoch': 3.65}
{'loss': 0.8799, 'grad_norm': 3.2607966507313058, 'learning_rate': 9.831111111111112e-07, 'epoch': 3.6504}
{'loss': 0.8683, 'grad_norm': 2.89351762431802, 'learning_rate': 9.82e-07, 'epoch': 3.6508000000000003}
{'loss': 0.8663, 'grad_norm': 3.2950543443137468, 'learning_rate': 9.80888888888889e-07, 'epoch': 3.6512000000000002}
{'loss': 0.8675, 'grad_norm': 2.764320496917303, 'learning_rate': 9.79777777777778e-07, 'epoch': 3.6516}
{'loss': 0.8647, 'grad_norm': 2.740251526129755, 'learning_rate': 9.786666666666669e-07, 'epoch': 3.652}
{'eval_valid_loss': 0.83935546875, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1114.203, 'eval_valid_steps_per_second': 278.551, 'epoch': 3.652}
{'loss': 0.8848, 'grad_norm': 2.8733681214561777, 'learning_rate': 9.775555555555556e-07, 'epoch': 3.6524}
{'loss': 0.8632, 'grad_norm': 3.0360454695228047, 'learning_rate': 9.764444444444445e-07, 'epoch': 3.6528}
{'loss': 0.8616, 'grad_norm': 3.0975867476489185, 'learning_rate': 9.753333333333334e-07, 'epoch': 3.6532}
{'loss': 0.8665, 'grad_norm': 2.9399722683257146, 'learning_rate': 9.742222222222223e-07, 'epoch': 3.6536}
{'loss': 0.8618, 'grad_norm': 2.91062681246802, 'learning_rate': 9.731111111111113e-07, 'epoch': 3.654}
{'loss': 0.8599, 'grad_norm': 3.0995654924503753, 'learning_rate': 9.72e-07, 'epoch': 3.6544}
{'loss': 0.8582, 'grad_norm': 3.1951809881702973, 'learning_rate': 9.708888888888889e-07, 'epoch': 3.6548}
{'loss': 0.8687, 'grad_norm': 3.086859620823138, 'learning_rate': 9.697777777777778e-07, 'epoch': 3.6552}
{'loss': 0.8756, 'grad_norm': 3.1673561378753505, 'learning_rate': 9.686666666666667e-07, 'epoch': 3.6555999999999997}
{'loss': 0.8621, 'grad_norm': 3.361876510049594, 'learning_rate': 9.675555555555557e-07, 'epoch': 3.656}
{'eval_valid_loss': 0.83935546875, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.189, 'eval_valid_steps_per_second': 279.547, 'epoch': 3.656}
{'loss': 0.8637, 'grad_norm': 2.9854520282306014, 'learning_rate': 9.664444444444446e-07, 'epoch': 3.6564}
{'loss': 0.8699, 'grad_norm': 2.8773184322307044, 'learning_rate': 9.653333333333333e-07, 'epoch': 3.6568}
{'loss': 0.8561, 'grad_norm': 3.0570364406355424, 'learning_rate': 9.642222222222224e-07, 'epoch': 3.6572}
{'loss': 0.8583, 'grad_norm': 3.190588912983963, 'learning_rate': 9.631111111111111e-07, 'epoch': 3.6576}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.8621, 'grad_norm': 3.0617770587326305, 'learning_rate': 9.62e-07, 'epoch': 3.658}
{'loss': 0.8713, 'grad_norm': 2.9724398921474946, 'learning_rate': 9.60888888888889e-07, 'epoch': 3.6584}
{'loss': 0.8589, 'grad_norm': 2.9886851318288725, 'learning_rate': 9.59777777777778e-07, 'epoch': 3.6588000000000003}
{'loss': 0.8688, 'grad_norm': 2.823420700715695, 'learning_rate': 9.586666666666668e-07, 'epoch': 3.6592000000000002}
{'loss': 0.8673, 'grad_norm': 3.0439780915508297, 'learning_rate': 9.575555555555555e-07, 'epoch': 3.6596}
{'loss': 0.8621, 'grad_norm': 2.938152778411404, 'learning_rate': 9.564444444444445e-07, 'epoch': 3.66}
{'eval_valid_loss': 0.83984375, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1124.276, 'eval_valid_steps_per_second': 281.069, 'epoch': 3.66}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.857, 'grad_norm': 2.6878573102512195, 'learning_rate': 9.553333333333334e-07, 'epoch': 3.6604}
{'loss': 0.8596, 'grad_norm': 2.7641969808724296, 'learning_rate': 9.542222222222223e-07, 'epoch': 3.6608}
{'loss': 0.8614, 'grad_norm': 2.741344327708342, 'learning_rate': 9.531111111111111e-07, 'epoch': 3.6612}
{'loss': 0.8696, 'grad_norm': 3.1795275317543434, 'learning_rate': 9.520000000000002e-07, 'epoch': 3.6616}
{'loss': 0.8728, 'grad_norm': 3.029028599639005, 'learning_rate': 9.50888888888889e-07, 'epoch': 3.662}
{'loss': 0.8586, 'grad_norm': 2.912507232771639, 'learning_rate': 9.497777777777778e-07, 'epoch': 3.6624}
{'loss': 0.8655, 'grad_norm': 2.870670147222484, 'learning_rate': 9.486666666666667e-07, 'epoch': 3.6628}
{'loss': 0.8621, 'grad_norm': 2.990235909848408, 'learning_rate': 9.475555555555556e-07, 'epoch': 3.6632}
{'loss': 0.8691, 'grad_norm': 2.9959608344445314, 'learning_rate': 9.464444444444446e-07, 'epoch': 3.6635999999999997}
{'loss': 0.8725, 'grad_norm': 2.7176331056530376, 'learning_rate': 9.453333333333334e-07, 'epoch': 3.664}
{'eval_valid_loss': 0.83935546875, 'eval_valid_runtime': 0.0891, 'eval_valid_samples_per_second': 1122.907, 'eval_valid_steps_per_second': 280.727, 'epoch': 3.664}
{'loss': 0.8729, 'grad_norm': 3.1117121189515116, 'learning_rate': 9.442222222222223e-07, 'epoch': 3.6644}
{'loss': 0.8667, 'grad_norm': 2.6965787649730646, 'learning_rate': 9.431111111111111e-07, 'epoch': 3.6648}
{'loss': 0.8587, 'grad_norm': 2.8063113205885735, 'learning_rate': 9.420000000000002e-07, 'epoch': 3.6652}
{'loss': 0.862, 'grad_norm': 2.955447893021207, 'learning_rate': 9.40888888888889e-07, 'epoch': 3.6656}
{'loss': 0.8665, 'grad_norm': 3.0496208240542826, 'learning_rate': 9.397777777777778e-07, 'epoch': 3.666}
{'loss': 0.8556, 'grad_norm': 2.7952005925814953, 'learning_rate': 9.386666666666667e-07, 'epoch': 3.6664}
{'loss': 0.864, 'grad_norm': 3.41560434577749, 'learning_rate': 9.375555555555556e-07, 'epoch': 3.6668}
{'loss': 0.8603, 'grad_norm': 2.8566351411783257, 'learning_rate': 9.364444444444446e-07, 'epoch': 3.6672000000000002}
{'loss': 0.8607, 'grad_norm': 2.8967518647575483, 'learning_rate': 9.353333333333334e-07, 'epoch': 3.6676}
{'loss': 0.8607, 'grad_norm': 2.895985823442472, 'learning_rate': 9.342222222222223e-07, 'epoch': 3.668}
{'eval_valid_loss': 0.83984375, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1121.142, 'eval_valid_steps_per_second': 280.285, 'epoch': 3.668}
{'loss': 0.8496, 'grad_norm': 3.0299654628025405, 'learning_rate': 9.331111111111111e-07, 'epoch': 3.6684}
{'loss': 0.8621, 'grad_norm': 2.7894281794072717, 'learning_rate': 9.320000000000001e-07, 'epoch': 3.6688}
{'loss': 0.8701, 'grad_norm': 2.9613543798231055, 'learning_rate': 9.30888888888889e-07, 'epoch': 3.6692}
{'loss': 0.8655, 'grad_norm': 2.849721967788006, 'learning_rate': 9.297777777777779e-07, 'epoch': 3.6696}
{'loss': 0.8578, 'grad_norm': 2.8698474955912077, 'learning_rate': 9.286666666666667e-07, 'epoch': 3.67}
{'loss': 0.8751, 'grad_norm': 3.1702234074241584, 'learning_rate': 9.275555555555556e-07, 'epoch': 3.6704}
{'loss': 0.8676, 'grad_norm': 3.2781990672017964, 'learning_rate': 9.264444444444445e-07, 'epoch': 3.6708}
{'loss': 0.8642, 'grad_norm': 2.761541984113249, 'learning_rate': 9.253333333333334e-07, 'epoch': 3.6712}
{'loss': 0.8717, 'grad_norm': 2.9895353989075644, 'learning_rate': 9.242222222222223e-07, 'epoch': 3.6715999999999998}
{'loss': 0.8749, 'grad_norm': 3.23471618548228, 'learning_rate': 9.231111111111111e-07, 'epoch': 3.672}
{'eval_valid_loss': 0.83935546875, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.528, 'eval_valid_steps_per_second': 278.632, 'epoch': 3.672}
{'loss': 0.8676, 'grad_norm': 3.0375672121029282, 'learning_rate': 9.220000000000001e-07, 'epoch': 3.6724}
{'loss': 0.8685, 'grad_norm': 2.7307691176947615, 'learning_rate': 9.208888888888889e-07, 'epoch': 3.6728}
{'loss': 0.8708, 'grad_norm': 3.15353192481482, 'learning_rate': 9.197777777777779e-07, 'epoch': 3.6732}
{'loss': 0.8736, 'grad_norm': 2.8812583052294145, 'learning_rate': 9.186666666666667e-07, 'epoch': 3.6736}
{'loss': 0.8735, 'grad_norm': 2.9982488408852612, 'learning_rate': 9.175555555555557e-07, 'epoch': 3.674}
{'loss': 0.8672, 'grad_norm': 2.9732929004142443, 'learning_rate': 9.164444444444445e-07, 'epoch': 3.6744}
{'loss': 0.8589, 'grad_norm': 2.9152999275583316, 'learning_rate': 9.153333333333334e-07, 'epoch': 3.6748}
{'loss': 0.8692, 'grad_norm': 2.95290353741873, 'learning_rate': 9.142222222222223e-07, 'epoch': 3.6752000000000002}
{'loss': 0.8616, 'grad_norm': 2.909229217957926, 'learning_rate': 9.131111111111111e-07, 'epoch': 3.6756}
{'loss': 0.8531, 'grad_norm': 3.0019582675152665, 'learning_rate': 9.120000000000001e-07, 'epoch': 3.676}
{'eval_valid_loss': 0.83984375, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.239, 'eval_valid_steps_per_second': 279.31, 'epoch': 3.676}
{'loss': 0.8676, 'grad_norm': 2.8645613958501333, 'learning_rate': 9.108888888888889e-07, 'epoch': 3.6764}
{'loss': 0.867, 'grad_norm': 2.877345709032298, 'learning_rate': 9.097777777777779e-07, 'epoch': 3.6768}
{'loss': 0.8685, 'grad_norm': 2.68606298307766, 'learning_rate': 9.086666666666667e-07, 'epoch': 3.6772}
{'loss': 0.8739, 'grad_norm': 3.044636241344716, 'learning_rate': 9.075555555555557e-07, 'epoch': 3.6776}
{'loss': 0.8613, 'grad_norm': 2.789958425283873, 'learning_rate': 9.064444444444445e-07, 'epoch': 3.678}
{'loss': 0.8665, 'grad_norm': 3.1126315828261952, 'learning_rate': 9.053333333333333e-07, 'epoch': 3.6784}
{'loss': 0.8678, 'grad_norm': 3.081542852695193, 'learning_rate': 9.042222222222223e-07, 'epoch': 3.6788}
{'loss': 0.8605, 'grad_norm': 2.8168156679420826, 'learning_rate': 9.031111111111111e-07, 'epoch': 3.6792}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8654, 'grad_norm': 2.961888550580259, 'learning_rate': 9.020000000000001e-07, 'epoch': 3.6795999999999998}
{'loss': 0.861, 'grad_norm': 2.842422636685365, 'learning_rate': 9.008888888888889e-07, 'epoch': 3.68}
{'eval_valid_loss': 0.83935546875, 'eval_valid_runtime': 0.089, 'eval_valid_samples_per_second': 1123.129, 'eval_valid_steps_per_second': 280.782, 'epoch': 3.68}
{'loss': 0.8753, 'grad_norm': 3.0964993966032868, 'learning_rate': 8.997777777777778e-07, 'epoch': 3.6804}
{'loss': 0.8658, 'grad_norm': 3.02634962925552, 'learning_rate': 8.986666666666667e-07, 'epoch': 3.6808}
{'loss': 0.8674, 'grad_norm': 3.027635365869029, 'learning_rate': 8.975555555555557e-07, 'epoch': 3.6812}
{'loss': 0.8647, 'grad_norm': 2.873705878489426, 'learning_rate': 8.964444444444445e-07, 'epoch': 3.6816}
{'loss': 0.856, 'grad_norm': 2.77471715929102, 'learning_rate': 8.953333333333334e-07, 'epoch': 3.682}
{'loss': 0.8626, 'grad_norm': 2.867345660054517, 'learning_rate': 8.942222222222223e-07, 'epoch': 3.6824}
{'loss': 0.8576, 'grad_norm': 3.089592886095633, 'learning_rate': 8.931111111111111e-07, 'epoch': 3.6828}
{'loss': 0.8753, 'grad_norm': 2.8461486268701996, 'learning_rate': 8.920000000000001e-07, 'epoch': 3.6832000000000003}
{'loss': 0.8595, 'grad_norm': 2.870903071085679, 'learning_rate': 8.908888888888889e-07, 'epoch': 3.6836}
{'loss': 0.8621, 'grad_norm': 3.237996262548197, 'learning_rate': 8.897777777777778e-07, 'epoch': 3.684}
{'eval_valid_loss': 0.83935546875, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.783, 'eval_valid_steps_per_second': 278.696, 'epoch': 3.684}
{'loss': 0.8599, 'grad_norm': 2.964405746171828, 'learning_rate': 8.887777777777779e-07, 'epoch': 3.6844}
{'loss': 0.8661, 'grad_norm': 3.2206445599581435, 'learning_rate': 8.876666666666667e-07, 'epoch': 3.6848}
{'loss': 0.8564, 'grad_norm': 2.9611571439336317, 'learning_rate': 8.865555555555556e-07, 'epoch': 3.6852}
{'loss': 0.854, 'grad_norm': 3.032325504672861, 'learning_rate': 8.854444444444444e-07, 'epoch': 3.6856}
{'loss': 0.8733, 'grad_norm': 3.2222796225824704, 'learning_rate': 8.843333333333335e-07, 'epoch': 3.686}
{'loss': 0.8659, 'grad_norm': 2.886855518368654, 'learning_rate': 8.832222222222223e-07, 'epoch': 3.6864}
{'loss': 0.8609, 'grad_norm': 3.1723829341900287, 'learning_rate': 8.821111111111112e-07, 'epoch': 3.6868}
{'loss': 0.8692, 'grad_norm': 2.8986765410774336, 'learning_rate': 8.81e-07, 'epoch': 3.6872}
{'loss': 0.8628, 'grad_norm': 3.1249921989343545, 'learning_rate': 8.79888888888889e-07, 'epoch': 3.6875999999999998}
{'loss': 0.8658, 'grad_norm': 2.697355828237459, 'learning_rate': 8.787777777777779e-07, 'epoch': 3.6879999999999997}
{'eval_valid_loss': 0.83935546875, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.265, 'eval_valid_steps_per_second': 278.316, 'epoch': 3.6879999999999997}
{'loss': 0.8624, 'grad_norm': 3.023066870079454, 'learning_rate': 8.776666666666668e-07, 'epoch': 3.6884}
{'loss': 0.8579, 'grad_norm': 3.21410245448654, 'learning_rate': 8.765555555555556e-07, 'epoch': 3.6888}
{'loss': 0.8533, 'grad_norm': 2.7854502818310167, 'learning_rate': 8.754444444444444e-07, 'epoch': 3.6892}
{'loss': 0.8601, 'grad_norm': 2.929865849470432, 'learning_rate': 8.743333333333334e-07, 'epoch': 3.6896}
{'loss': 0.8621, 'grad_norm': 2.8387988869318046, 'learning_rate': 8.732222222222223e-07, 'epoch': 3.69}
{'loss': 0.8797, 'grad_norm': 3.0871942689236223, 'learning_rate': 8.721111111111112e-07, 'epoch': 3.6904}
{'loss': 0.8597, 'grad_norm': 2.7313707359915793, 'learning_rate': 8.71e-07, 'epoch': 3.6908}
{'loss': 0.8714, 'grad_norm': 2.6692890513969174, 'learning_rate': 8.69888888888889e-07, 'epoch': 3.6912000000000003}
{'loss': 0.8591, 'grad_norm': 3.215339974615876, 'learning_rate': 8.687777777777779e-07, 'epoch': 3.6916}
{'loss': 0.8645, 'grad_norm': 2.940642877133196, 'learning_rate': 8.676666666666668e-07, 'epoch': 3.692}
{'eval_valid_loss': 0.83935546875, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.554, 'eval_valid_steps_per_second': 278.888, 'epoch': 3.692}
{'loss': 0.8533, 'grad_norm': 2.8515524929341347, 'learning_rate': 8.665555555555556e-07, 'epoch': 3.6924}
{'loss': 0.8689, 'grad_norm': 3.039356276328768, 'learning_rate': 8.654444444444444e-07, 'epoch': 3.6928}
{'loss': 0.852, 'grad_norm': 3.0970455784786415, 'learning_rate': 8.643333333333334e-07, 'epoch': 3.6932}
{'loss': 0.864, 'grad_norm': 2.910034059353073, 'learning_rate': 8.632222222222223e-07, 'epoch': 3.6936}
{'loss': 0.8645, 'grad_norm': 2.989281371060257, 'learning_rate': 8.621111111111112e-07, 'epoch': 3.694}
{'loss': 0.8648, 'grad_norm': 3.163982597566652, 'learning_rate': 8.61e-07, 'epoch': 3.6944}
{'loss': 0.8665, 'grad_norm': 3.0176785794916676, 'learning_rate': 8.59888888888889e-07, 'epoch': 3.6948}
{'loss': 0.8697, 'grad_norm': 2.8665167624254444, 'learning_rate': 8.587777777777778e-07, 'epoch': 3.6952}
{'loss': 0.8679, 'grad_norm': 2.9981807268811345, 'learning_rate': 8.576666666666668e-07, 'epoch': 3.6955999999999998}
{'loss': 0.8635, 'grad_norm': 3.0527781985524327, 'learning_rate': 8.565555555555556e-07, 'epoch': 3.6959999999999997}
{'eval_valid_loss': 0.8388671875, 'eval_valid_runtime': 0.0906, 'eval_valid_samples_per_second': 1104.247, 'eval_valid_steps_per_second': 276.062, 'epoch': 3.6959999999999997}
{'loss': 0.8719, 'grad_norm': 2.9623583602846795, 'learning_rate': 8.554444444444446e-07, 'epoch': 3.6964}
{'loss': 0.8619, 'grad_norm': 3.0754156242719937, 'learning_rate': 8.543333333333334e-07, 'epoch': 3.6968}
{'loss': 0.8624, 'grad_norm': 2.7428246814743695, 'learning_rate': 8.532222222222222e-07, 'epoch': 3.6972}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.8734, 'grad_norm': 3.1780503109508, 'learning_rate': 8.521111111111112e-07, 'epoch': 3.6976}
{'loss': 0.8689, 'grad_norm': 3.1042190820420466, 'learning_rate': 8.51e-07, 'epoch': 3.698}
{'loss': 0.8608, 'grad_norm': 2.8453060441255515, 'learning_rate': 8.49888888888889e-07, 'epoch': 3.6984}
{'loss': 0.8732, 'grad_norm': 2.7291415676875026, 'learning_rate': 8.487777777777778e-07, 'epoch': 3.6988}
{'loss': 0.8621, 'grad_norm': 3.3817739764669503, 'learning_rate': 8.476666666666668e-07, 'epoch': 3.6992000000000003}
{'loss': 0.8737, 'grad_norm': 2.8731212126595094, 'learning_rate': 8.465555555555556e-07, 'epoch': 3.6996}
{'loss': 0.8623, 'grad_norm': 3.086006009270929, 'learning_rate': 8.454444444444446e-07, 'epoch': 3.7}
{'eval_valid_loss': 0.83935546875, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1115.435, 'eval_valid_steps_per_second': 278.859, 'epoch': 3.7}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8685, 'grad_norm': 3.155292960755596, 'learning_rate': 8.443333333333334e-07, 'epoch': 3.7004}
{'loss': 0.87, 'grad_norm': 3.1182059247282914, 'learning_rate': 8.432222222222223e-07, 'epoch': 3.7008}
{'loss': 0.8589, 'grad_norm': 2.8800803783247027, 'learning_rate': 8.421111111111112e-07, 'epoch': 3.7012}
{'loss': 0.8604, 'grad_norm': 3.2540014191227584, 'learning_rate': 8.41e-07, 'epoch': 3.7016}
{'loss': 0.881, 'grad_norm': 3.1478117138685437, 'learning_rate': 8.39888888888889e-07, 'epoch': 3.702}
{'loss': 0.8608, 'grad_norm': 3.0378810576829895, 'learning_rate': 8.387777777777778e-07, 'epoch': 3.7024}
{'loss': 0.8554, 'grad_norm': 3.2241273054091684, 'learning_rate': 8.376666666666667e-07, 'epoch': 3.7028}
{'loss': 0.8585, 'grad_norm': 2.9604575943360407, 'learning_rate': 8.365555555555556e-07, 'epoch': 3.7032}
{'loss': 0.881, 'grad_norm': 3.1473091413186576, 'learning_rate': 8.354444444444446e-07, 'epoch': 3.7036}
{'loss': 0.8644, 'grad_norm': 3.051453344186876, 'learning_rate': 8.343333333333334e-07, 'epoch': 3.7039999999999997}
{'eval_valid_loss': 0.8388671875, 'eval_valid_runtime': 0.0903, 'eval_valid_samples_per_second': 1107.77, 'eval_valid_steps_per_second': 276.942, 'epoch': 3.7039999999999997}
{'loss': 0.8567, 'grad_norm': 2.962459827102427, 'learning_rate': 8.332222222222223e-07, 'epoch': 3.7044}
{'loss': 0.8627, 'grad_norm': 3.0029606614415036, 'learning_rate': 8.321111111111111e-07, 'epoch': 3.7048}
{'loss': 0.8623, 'grad_norm': 3.205602878383408, 'learning_rate': 8.31e-07, 'epoch': 3.7052}
{'loss': 0.8608, 'grad_norm': 2.980199899209302, 'learning_rate': 8.29888888888889e-07, 'epoch': 3.7056}
{'loss': 0.8764, 'grad_norm': 2.8254609759387868, 'learning_rate': 8.287777777777778e-07, 'epoch': 3.706}
{'loss': 0.8647, 'grad_norm': 3.140463146385213, 'learning_rate': 8.276666666666667e-07, 'epoch': 3.7064}
{'loss': 0.8646, 'grad_norm': 2.9251950571850034, 'learning_rate': 8.265555555555555e-07, 'epoch': 3.7068}
{'loss': 0.8704, 'grad_norm': 3.0420442187909496, 'learning_rate': 8.254444444444446e-07, 'epoch': 3.7072000000000003}
{'loss': 0.8565, 'grad_norm': 3.1083464335011275, 'learning_rate': 8.243333333333334e-07, 'epoch': 3.7076000000000002}
{'loss': 0.8661, 'grad_norm': 3.022949366630362, 'learning_rate': 8.232222222222223e-07, 'epoch': 3.708}
{'eval_valid_loss': 0.8388671875, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1120.447, 'eval_valid_steps_per_second': 280.112, 'epoch': 3.708}
{'loss': 0.8598, 'grad_norm': 2.8450604762368905, 'learning_rate': 8.221111111111111e-07, 'epoch': 3.7084}
{'loss': 0.8722, 'grad_norm': 3.142633744643197, 'learning_rate': 8.210000000000002e-07, 'epoch': 3.7088}
{'loss': 0.8604, 'grad_norm': 3.041115133804899, 'learning_rate': 8.19888888888889e-07, 'epoch': 3.7092}
{'loss': 0.8598, 'grad_norm': 2.986016765602349, 'learning_rate': 8.187777777777778e-07, 'epoch': 3.7096}
{'loss': 0.8683, 'grad_norm': 3.074982147242507, 'learning_rate': 8.176666666666667e-07, 'epoch': 3.71}
{'loss': 0.8565, 'grad_norm': 2.7993743107993523, 'learning_rate': 8.165555555555555e-07, 'epoch': 3.7104}
{'loss': 0.8792, 'grad_norm': 3.0813725762427544, 'learning_rate': 8.154444444444446e-07, 'epoch': 3.7108}
{'loss': 0.8681, 'grad_norm': 2.9944010781156005, 'learning_rate': 8.143333333333334e-07, 'epoch': 3.7112}
{'loss': 0.8706, 'grad_norm': 2.8373804126023887, 'learning_rate': 8.132222222222223e-07, 'epoch': 3.7116}
{'loss': 0.8638, 'grad_norm': 2.8805434179806464, 'learning_rate': 8.121111111111111e-07, 'epoch': 3.7119999999999997}
{'eval_valid_loss': 0.83935546875, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1115.364, 'eval_valid_steps_per_second': 278.841, 'epoch': 3.7119999999999997}
{'loss': 0.864, 'grad_norm': 2.8271124697490757, 'learning_rate': 8.110000000000002e-07, 'epoch': 3.7124}
{'loss': 0.8518, 'grad_norm': 2.8190893865482356, 'learning_rate': 8.09888888888889e-07, 'epoch': 3.7128}
{'loss': 0.8596, 'grad_norm': 2.94671637875782, 'learning_rate': 8.087777777777779e-07, 'epoch': 3.7132}
{'loss': 0.8683, 'grad_norm': 2.902695050548784, 'learning_rate': 8.076666666666667e-07, 'epoch': 3.7136}
{'loss': 0.865, 'grad_norm': 2.9273518419930182, 'learning_rate': 8.065555555555555e-07, 'epoch': 3.714}
{'loss': 0.8686, 'grad_norm': 3.213208789782883, 'learning_rate': 8.054444444444446e-07, 'epoch': 3.7144}
{'loss': 0.8547, 'grad_norm': 3.0284579287902673, 'learning_rate': 8.043333333333334e-07, 'epoch': 3.7148}
{'loss': 0.8592, 'grad_norm': 2.8367856966742737, 'learning_rate': 8.032222222222223e-07, 'epoch': 3.7152}
{'loss': 0.8652, 'grad_norm': 2.767781559070357, 'learning_rate': 8.021111111111111e-07, 'epoch': 3.7156000000000002}
{'loss': 0.8677, 'grad_norm': 3.021285540616735, 'learning_rate': 8.010000000000001e-07, 'epoch': 3.716}
{'eval_valid_loss': 0.8388671875, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1115.895, 'eval_valid_steps_per_second': 278.974, 'epoch': 3.716}
{'loss': 0.8562, 'grad_norm': 2.817695751032821, 'learning_rate': 7.99888888888889e-07, 'epoch': 3.7164}
{'loss': 0.868, 'grad_norm': 2.618182650723861, 'learning_rate': 7.987777777777779e-07, 'epoch': 3.7168}
{'loss': 0.8695, 'grad_norm': 3.059020440165831, 'learning_rate': 7.976666666666667e-07, 'epoch': 3.7172}
{'loss': 0.8657, 'grad_norm': 2.9844649336281113, 'learning_rate': 7.965555555555555e-07, 'epoch': 3.7176}
{'loss': 0.8633, 'grad_norm': 2.879838013712868, 'learning_rate': 7.954444444444445e-07, 'epoch': 3.718}
{'loss': 0.8593, 'grad_norm': 2.8412714626510254, 'learning_rate': 7.943333333333334e-07, 'epoch': 3.7184}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8617, 'grad_norm': 3.0767818270067995, 'learning_rate': 7.932222222222223e-07, 'epoch': 3.7188}
{'loss': 0.878, 'grad_norm': 2.96752284938625, 'learning_rate': 7.921111111111111e-07, 'epoch': 3.7192}
{'loss': 0.8658, 'grad_norm': 2.9517542266369583, 'learning_rate': 7.910000000000001e-07, 'epoch': 3.7196}
{'loss': 0.8635, 'grad_norm': 2.970412030024893, 'learning_rate': 7.89888888888889e-07, 'epoch': 3.7199999999999998}
{'eval_valid_loss': 0.83935546875, 'eval_valid_runtime': 0.0882, 'eval_valid_samples_per_second': 1133.678, 'eval_valid_steps_per_second': 283.42, 'epoch': 3.7199999999999998}
{'loss': 0.8681, 'grad_norm': 2.9754217069328797, 'learning_rate': 7.887777777777779e-07, 'epoch': 3.7204}
{'loss': 0.8637, 'grad_norm': 2.6874969116459333, 'learning_rate': 7.876666666666667e-07, 'epoch': 3.7208}
{'loss': 0.8591, 'grad_norm': 2.9213022598893796, 'learning_rate': 7.865555555555557e-07, 'epoch': 3.7212}
{'loss': 0.8747, 'grad_norm': 2.8224280129148234, 'learning_rate': 7.854444444444445e-07, 'epoch': 3.7216}
{'loss': 0.8545, 'grad_norm': 3.212306146301877, 'learning_rate': 7.843333333333334e-07, 'epoch': 3.722}
{'loss': 0.8598, 'grad_norm': 2.92632644116768, 'learning_rate': 7.832222222222223e-07, 'epoch': 3.7224}
{'loss': 0.8732, 'grad_norm': 2.9450485860025917, 'learning_rate': 7.821111111111111e-07, 'epoch': 3.7228}
{'loss': 0.8617, 'grad_norm': 3.0328361353083437, 'learning_rate': 7.810000000000001e-07, 'epoch': 3.7232}
{'loss': 0.8672, 'grad_norm': 2.8239099009202198, 'learning_rate': 7.798888888888889e-07, 'epoch': 3.7236000000000002}
{'loss': 0.8689, 'grad_norm': 2.953660240310855, 'learning_rate': 7.787777777777779e-07, 'epoch': 3.724}
{'eval_valid_loss': 0.8388671875, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1120.989, 'eval_valid_steps_per_second': 280.247, 'epoch': 3.724}
{'loss': 0.8582, 'grad_norm': 2.9441990961314564, 'learning_rate': 7.777777777777779e-07, 'epoch': 3.7244}
{'loss': 0.8748, 'grad_norm': 3.0669931421094407, 'learning_rate': 7.766666666666667e-07, 'epoch': 3.7248}
{'loss': 0.8707, 'grad_norm': 2.9199378584102154, 'learning_rate': 7.755555555555556e-07, 'epoch': 3.7252}
{'loss': 0.871, 'grad_norm': 3.1157267023014543, 'learning_rate': 7.744444444444445e-07, 'epoch': 3.7256}
{'loss': 0.8535, 'grad_norm': 2.7062255464026075, 'learning_rate': 7.733333333333335e-07, 'epoch': 3.726}
{'loss': 0.8646, 'grad_norm': 3.3595708479458666, 'learning_rate': 7.722222222222223e-07, 'epoch': 3.7264}
{'loss': 0.862, 'grad_norm': 2.8557558137182797, 'learning_rate': 7.711111111111112e-07, 'epoch': 3.7268}
{'loss': 0.8555, 'grad_norm': 2.8242338188054297, 'learning_rate': 7.7e-07, 'epoch': 3.7272}
{'loss': 0.8674, 'grad_norm': 2.946967836052388, 'learning_rate': 7.688888888888891e-07, 'epoch': 3.7276}
{'loss': 0.8523, 'grad_norm': 3.069429867672914, 'learning_rate': 7.677777777777779e-07, 'epoch': 3.7279999999999998}
{'eval_valid_loss': 0.8388671875, 'eval_valid_runtime': 0.0905, 'eval_valid_samples_per_second': 1105.163, 'eval_valid_steps_per_second': 276.291, 'epoch': 3.7279999999999998}
{'loss': 0.8614, 'grad_norm': 2.8101237643296733, 'learning_rate': 7.666666666666667e-07, 'epoch': 3.7284}
{'loss': 0.8697, 'grad_norm': 3.107595634277818, 'learning_rate': 7.655555555555556e-07, 'epoch': 3.7288}
{'loss': 0.8518, 'grad_norm': 3.295476047152745, 'learning_rate': 7.644444444444444e-07, 'epoch': 3.7292}
{'loss': 0.8681, 'grad_norm': 3.0467690767314033, 'learning_rate': 7.633333333333335e-07, 'epoch': 3.7296}
{'loss': 0.864, 'grad_norm': 2.728993875673145, 'learning_rate': 7.622222222222223e-07, 'epoch': 3.73}
{'loss': 0.8605, 'grad_norm': 2.969741485066477, 'learning_rate': 7.611111111111112e-07, 'epoch': 3.7304}
{'loss': 0.8608, 'grad_norm': 2.776182991758101, 'learning_rate': 7.6e-07, 'epoch': 3.7308}
{'loss': 0.8706, 'grad_norm': 3.2186568811520577, 'learning_rate': 7.588888888888891e-07, 'epoch': 3.7312}
{'loss': 0.8598, 'grad_norm': 2.8639189984405555, 'learning_rate': 7.577777777777779e-07, 'epoch': 3.7316000000000003}
{'loss': 0.8577, 'grad_norm': 3.1408320591172405, 'learning_rate': 7.566666666666667e-07, 'epoch': 3.732}
{'eval_valid_loss': 0.8388671875, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1114.01, 'eval_valid_steps_per_second': 278.503, 'epoch': 3.732}
{'loss': 0.8652, 'grad_norm': 3.2279688377312037, 'learning_rate': 7.555555555555556e-07, 'epoch': 3.7324}
{'loss': 0.8657, 'grad_norm': 2.8373569792779354, 'learning_rate': 7.544444444444444e-07, 'epoch': 3.7328}
{'loss': 0.8634, 'grad_norm': 2.854836029840687, 'learning_rate': 7.533333333333335e-07, 'epoch': 3.7332}
{'loss': 0.8641, 'grad_norm': 2.991115906065454, 'learning_rate': 7.522222222222223e-07, 'epoch': 3.7336}
{'loss': 0.868, 'grad_norm': 2.709546581585924, 'learning_rate': 7.511111111111112e-07, 'epoch': 3.734}
{'loss': 0.8587, 'grad_norm': 3.2519561859208097, 'learning_rate': 7.5e-07, 'epoch': 3.7344}
{'loss': 0.8578, 'grad_norm': 3.1155148466604694, 'learning_rate': 7.48888888888889e-07, 'epoch': 3.7348}
{'loss': 0.8713, 'grad_norm': 2.849081109427484, 'learning_rate': 7.477777777777779e-07, 'epoch': 3.7352}
{'loss': 0.861, 'grad_norm': 2.9894626052528674, 'learning_rate': 7.466666666666668e-07, 'epoch': 3.7356}
{'loss': 0.864, 'grad_norm': 2.763660735522355, 'learning_rate': 7.455555555555556e-07, 'epoch': 3.7359999999999998}
{'eval_valid_loss': 0.83935546875, 'eval_valid_runtime': 0.0908, 'eval_valid_samples_per_second': 1101.61, 'eval_valid_steps_per_second': 275.403, 'epoch': 3.7359999999999998}
{'loss': 0.8569, 'grad_norm': 2.8980988255796123, 'learning_rate': 7.444444444444444e-07, 'epoch': 3.7364}
{'loss': 0.8648, 'grad_norm': 2.9343736630275226, 'learning_rate': 7.433333333333335e-07, 'epoch': 3.7368}
{'loss': 0.8567, 'grad_norm': 2.7956882868169544, 'learning_rate': 7.422222222222223e-07, 'epoch': 3.7372}
{'loss': 0.8563, 'grad_norm': 3.211920158586896, 'learning_rate': 7.411111111111112e-07, 'epoch': 3.7376}
{'loss': 0.8629, 'grad_norm': 2.998466844996568, 'learning_rate': 7.4e-07, 'epoch': 3.738}
{'loss': 0.8627, 'grad_norm': 2.9832086430941023, 'learning_rate': 7.38888888888889e-07, 'epoch': 3.7384}
{'loss': 0.8764, 'grad_norm': 3.129902227523027, 'learning_rate': 7.377777777777779e-07, 'epoch': 3.7388}
{'loss': 0.8784, 'grad_norm': 3.1409691769102563, 'learning_rate': 7.366666666666668e-07, 'epoch': 3.7392}
{'loss': 0.8628, 'grad_norm': 2.7742490145322396, 'learning_rate': 7.355555555555556e-07, 'epoch': 3.7396000000000003}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8502, 'grad_norm': 3.046330266332545, 'learning_rate': 7.344444444444445e-07, 'epoch': 3.74}
{'eval_valid_loss': 0.8388671875, 'eval_valid_runtime': 0.0906, 'eval_valid_samples_per_second': 1104.267, 'eval_valid_steps_per_second': 276.067, 'epoch': 3.74}
{'loss': 0.8683, 'grad_norm': 3.09752040916351, 'learning_rate': 7.333333333333334e-07, 'epoch': 3.7404}
{'loss': 0.867, 'grad_norm': 2.871497768079743, 'learning_rate': 7.322222222222223e-07, 'epoch': 3.7408}
{'loss': 0.8657, 'grad_norm': 3.088160252727929, 'learning_rate': 7.311111111111112e-07, 'epoch': 3.7412}
{'loss': 0.8621, 'grad_norm': 3.250888143742485, 'learning_rate': 7.3e-07, 'epoch': 3.7416}
{'loss': 0.867, 'grad_norm': 3.008694606091993, 'learning_rate': 7.28888888888889e-07, 'epoch': 3.742}
{'loss': 0.8678, 'grad_norm': 3.026317752361752, 'learning_rate': 7.277777777777778e-07, 'epoch': 3.7424}
{'loss': 0.873, 'grad_norm': 2.9825853504077697, 'learning_rate': 7.266666666666668e-07, 'epoch': 3.7428}
{'loss': 0.8559, 'grad_norm': 3.101845120638307, 'learning_rate': 7.255555555555556e-07, 'epoch': 3.7432}
{'loss': 0.8663, 'grad_norm': 2.824623653991422, 'learning_rate': 7.244444444444446e-07, 'epoch': 3.7436}
{'loss': 0.8662, 'grad_norm': 2.7150187538879718, 'learning_rate': 7.233333333333334e-07, 'epoch': 3.7439999999999998}
{'eval_valid_loss': 0.83935546875, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1119.147, 'eval_valid_steps_per_second': 279.787, 'epoch': 3.7439999999999998}
{'loss': 0.8651, 'grad_norm': 3.083271333139836, 'learning_rate': 7.222222222222222e-07, 'epoch': 3.7443999999999997}
{'loss': 0.8675, 'grad_norm': 3.059201790306558, 'learning_rate': 7.211111111111112e-07, 'epoch': 3.7448}
{'loss': 0.8622, 'grad_norm': 2.8196077866497067, 'learning_rate': 7.2e-07, 'epoch': 3.7452}
{'loss': 0.8579, 'grad_norm': 3.1523358842451437, 'learning_rate': 7.18888888888889e-07, 'epoch': 3.7456}
{'loss': 0.8763, 'grad_norm': 2.9086708425448453, 'learning_rate': 7.177777777777778e-07, 'epoch': 3.746}
{'loss': 0.8685, 'grad_norm': 2.974765926256577, 'learning_rate': 7.166666666666668e-07, 'epoch': 3.7464}
{'loss': 0.8553, 'grad_norm': 2.8300492289814065, 'learning_rate': 7.155555555555556e-07, 'epoch': 3.7468}
{'loss': 0.8587, 'grad_norm': 2.799704271477701, 'learning_rate': 7.144444444444446e-07, 'epoch': 3.7472}
{'loss': 0.8803, 'grad_norm': 3.1592876433568704, 'learning_rate': 7.133333333333334e-07, 'epoch': 3.7476000000000003}
{'loss': 0.868, 'grad_norm': 3.3083265572741962, 'learning_rate': 7.122222222222223e-07, 'epoch': 3.748}
{'eval_valid_loss': 0.8388671875, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1121.109, 'eval_valid_steps_per_second': 280.277, 'epoch': 3.748}
{'loss': 0.8595, 'grad_norm': 2.8609569269725954, 'learning_rate': 7.111111111111112e-07, 'epoch': 3.7484}
{'loss': 0.867, 'grad_norm': 3.1294051116750214, 'learning_rate': 7.1e-07, 'epoch': 3.7488}
{'loss': 0.867, 'grad_norm': 3.0937855265725727, 'learning_rate': 7.08888888888889e-07, 'epoch': 3.7492}
{'loss': 0.8659, 'grad_norm': 3.0531879071067514, 'learning_rate': 7.077777777777778e-07, 'epoch': 3.7496}
{'loss': 0.8655, 'grad_norm': 2.9233679297776787, 'learning_rate': 7.066666666666667e-07, 'epoch': 3.75}
{'loss': 0.8707, 'grad_norm': 2.77983727633263, 'learning_rate': 7.055555555555556e-07, 'epoch': 3.7504}
{'loss': 0.8618, 'grad_norm': 3.0870475800507995, 'learning_rate': 7.044444444444446e-07, 'epoch': 3.7508}
{'loss': 0.8729, 'grad_norm': 3.1175408222857097, 'learning_rate': 7.033333333333334e-07, 'epoch': 3.7512}
{'loss': 0.8703, 'grad_norm': 3.06993338498648, 'learning_rate': 7.022222222222223e-07, 'epoch': 3.7516}
{'loss': 0.8563, 'grad_norm': 2.998566871305139, 'learning_rate': 7.011111111111112e-07, 'epoch': 3.752}
{'eval_valid_loss': 0.8388671875, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.323, 'eval_valid_steps_per_second': 278.081, 'epoch': 3.752}
{'loss': 0.8598, 'grad_norm': 2.9229164205556937, 'learning_rate': 7.000000000000001e-07, 'epoch': 3.7523999999999997}
{'loss': 0.8673, 'grad_norm': 3.0128976417003797, 'learning_rate': 6.98888888888889e-07, 'epoch': 3.7528}
{'loss': 0.8679, 'grad_norm': 3.094724742423016, 'learning_rate': 6.977777777777778e-07, 'epoch': 3.7532}
{'loss': 0.8383, 'grad_norm': 2.79404355156091, 'learning_rate': 6.966666666666667e-07, 'epoch': 3.7536}
{'loss': 0.8673, 'grad_norm': 3.111527095929254, 'learning_rate': 6.955555555555556e-07, 'epoch': 3.754}
{'loss': 0.8748, 'grad_norm': 2.778507058784562, 'learning_rate': 6.944444444444446e-07, 'epoch': 3.7544}
{'loss': 0.8612, 'grad_norm': 3.007672953155239, 'learning_rate': 6.933333333333334e-07, 'epoch': 3.7548}
{'loss': 0.8585, 'grad_norm': 2.908809678031474, 'learning_rate': 6.922222222222223e-07, 'epoch': 3.7552}
{'loss': 0.8604, 'grad_norm': 2.87160501923333, 'learning_rate': 6.911111111111111e-07, 'epoch': 3.7556000000000003}
{'loss': 0.8656, 'grad_norm': 2.859070485477385, 'learning_rate': 6.900000000000001e-07, 'epoch': 3.7560000000000002}
{'eval_valid_loss': 0.8388671875, 'eval_valid_runtime': 0.0883, 'eval_valid_samples_per_second': 1132.72, 'eval_valid_steps_per_second': 283.18, 'epoch': 3.7560000000000002}
{'loss': 0.8524, 'grad_norm': 3.116511154417398, 'learning_rate': 6.88888888888889e-07, 'epoch': 3.7564}
{'loss': 0.865, 'grad_norm': 2.7611797027912988, 'learning_rate': 6.877777777777778e-07, 'epoch': 3.7568}
{'loss': 0.8551, 'grad_norm': 2.84203160945272, 'learning_rate': 6.866666666666667e-07, 'epoch': 3.7572}
{'loss': 0.8652, 'grad_norm': 2.827429944189964, 'learning_rate': 6.855555555555555e-07, 'epoch': 3.7576}
{'loss': 0.8763, 'grad_norm': 3.1308860944249113, 'learning_rate': 6.844444444444446e-07, 'epoch': 3.758}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.869, 'grad_norm': 3.3845183055967047, 'learning_rate': 6.833333333333334e-07, 'epoch': 3.7584}
{'loss': 0.8705, 'grad_norm': 2.961942124854828, 'learning_rate': 6.822222222222223e-07, 'epoch': 3.7588}
{'loss': 0.8568, 'grad_norm': 3.0242687531626378, 'learning_rate': 6.811111111111111e-07, 'epoch': 3.7592}
{'loss': 0.8623, 'grad_norm': 2.842944665205604, 'learning_rate': 6.800000000000001e-07, 'epoch': 3.7596}
{'loss': 0.8641, 'grad_norm': 3.076167350644614, 'learning_rate': 6.78888888888889e-07, 'epoch': 3.76}
{'eval_valid_loss': 0.83837890625, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.745, 'eval_valid_steps_per_second': 279.436, 'epoch': 3.76}
{'loss': 0.8625, 'grad_norm': 3.016764954068742, 'learning_rate': 6.777777777777779e-07, 'epoch': 3.7603999999999997}
{'loss': 0.8714, 'grad_norm': 3.1765730974477515, 'learning_rate': 6.766666666666667e-07, 'epoch': 3.7608}
{'loss': 0.8508, 'grad_norm': 3.0115344415418233, 'learning_rate': 6.755555555555555e-07, 'epoch': 3.7612}
{'loss': 0.8612, 'grad_norm': 2.8782224321803715, 'learning_rate': 6.744444444444446e-07, 'epoch': 3.7616}
{'loss': 0.8689, 'grad_norm': 3.121819031130636, 'learning_rate': 6.733333333333334e-07, 'epoch': 3.762}
{'loss': 0.8565, 'grad_norm': 2.955273063971758, 'learning_rate': 6.722222222222223e-07, 'epoch': 3.7624}
{'loss': 0.8612, 'grad_norm': 2.8067472805286258, 'learning_rate': 6.711111111111111e-07, 'epoch': 3.7628}
{'loss': 0.8577, 'grad_norm': 3.1370519648720787, 'learning_rate': 6.7e-07, 'epoch': 3.7632}
{'loss': 0.8559, 'grad_norm': 2.837034617503582, 'learning_rate': 6.68888888888889e-07, 'epoch': 3.7636}
{'loss': 0.8577, 'grad_norm': 2.9922179178230643, 'learning_rate': 6.677777777777779e-07, 'epoch': 3.7640000000000002}
{'eval_valid_loss': 0.83837890625, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.016, 'eval_valid_steps_per_second': 279.254, 'epoch': 3.7640000000000002}
{'loss': 0.871, 'grad_norm': 2.988122893482097, 'learning_rate': 6.666666666666667e-07, 'epoch': 3.7644}
{'loss': 0.8664, 'grad_norm': 2.8763341658693764, 'learning_rate': 6.656666666666666e-07, 'epoch': 3.7648}
{'loss': 0.8695, 'grad_norm': 2.903112594610837, 'learning_rate': 6.645555555555557e-07, 'epoch': 3.7652}
{'loss': 0.8619, 'grad_norm': 2.79174352060876, 'learning_rate': 6.634444444444445e-07, 'epoch': 3.7656}
{'loss': 0.8701, 'grad_norm': 3.092432194184607, 'learning_rate': 6.623333333333334e-07, 'epoch': 3.766}
{'loss': 0.8629, 'grad_norm': 3.0712506557179773, 'learning_rate': 6.612222222222222e-07, 'epoch': 3.7664}
{'loss': 0.8649, 'grad_norm': 3.184633097048724, 'learning_rate': 6.601111111111112e-07, 'epoch': 3.7668}
{'loss': 0.8583, 'grad_norm': 2.975585967546844, 'learning_rate': 6.590000000000001e-07, 'epoch': 3.7672}
{'loss': 0.859, 'grad_norm': 2.9504589580479865, 'learning_rate': 6.578888888888889e-07, 'epoch': 3.7676}
{'loss': 0.8668, 'grad_norm': 2.9770814037828397, 'learning_rate': 6.567777777777778e-07, 'epoch': 3.768}
{'eval_valid_loss': 0.8388671875, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1120.492, 'eval_valid_steps_per_second': 280.123, 'epoch': 3.768}
{'loss': 0.8691, 'grad_norm': 3.2031148212550256, 'learning_rate': 6.556666666666666e-07, 'epoch': 3.7683999999999997}
{'loss': 0.8735, 'grad_norm': 2.8287187606344077, 'learning_rate': 6.545555555555557e-07, 'epoch': 3.7688}
{'loss': 0.8569, 'grad_norm': 3.0572174015589066, 'learning_rate': 6.534444444444445e-07, 'epoch': 3.7692}
{'loss': 0.8537, 'grad_norm': 2.677078717718833, 'learning_rate': 6.523333333333334e-07, 'epoch': 3.7696}
{'loss': 0.8654, 'grad_norm': 2.804605052448105, 'learning_rate': 6.512222222222222e-07, 'epoch': 3.77}
{'loss': 0.8726, 'grad_norm': 3.1572323582858863, 'learning_rate': 6.501111111111112e-07, 'epoch': 3.7704}
{'loss': 0.8514, 'grad_norm': 2.983541841697186, 'learning_rate': 6.490000000000001e-07, 'epoch': 3.7708}
{'loss': 0.8581, 'grad_norm': 3.1517264554710276, 'learning_rate': 6.478888888888889e-07, 'epoch': 3.7712}
{'loss': 0.8653, 'grad_norm': 3.0387896143136874, 'learning_rate': 6.467777777777778e-07, 'epoch': 3.7716}
{'loss': 0.8647, 'grad_norm': 3.1132174764153677, 'learning_rate': 6.456666666666666e-07, 'epoch': 3.7720000000000002}
{'eval_valid_loss': 0.8388671875, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1106.709, 'eval_valid_steps_per_second': 276.677, 'epoch': 3.7720000000000002}
{'loss': 0.8607, 'grad_norm': 2.794748206244416, 'learning_rate': 6.445555555555556e-07, 'epoch': 3.7724}
{'loss': 0.8594, 'grad_norm': 2.888279638684633, 'learning_rate': 6.434444444444445e-07, 'epoch': 3.7728}
{'loss': 0.8609, 'grad_norm': 3.064286003150888, 'learning_rate': 6.423333333333334e-07, 'epoch': 3.7732}
{'loss': 0.8594, 'grad_norm': 3.349811069655265, 'learning_rate': 6.412222222222222e-07, 'epoch': 3.7736}
{'loss': 0.8573, 'grad_norm': 3.018676607254668, 'learning_rate': 6.401111111111112e-07, 'epoch': 3.774}
{'loss': 0.8655, 'grad_norm': 2.6450479304165975, 'learning_rate': 6.39e-07, 'epoch': 3.7744}
{'loss': 0.8673, 'grad_norm': 3.033926765108096, 'learning_rate': 6.37888888888889e-07, 'epoch': 3.7748}
{'loss': 0.8588, 'grad_norm': 2.8789543344527098, 'learning_rate': 6.367777777777778e-07, 'epoch': 3.7752}
{'loss': 0.8691, 'grad_norm': 2.7684062184617124, 'learning_rate': 6.356666666666666e-07, 'epoch': 3.7756}
{'loss': 0.8604, 'grad_norm': 3.0954188171182753, 'learning_rate': 6.345555555555556e-07, 'epoch': 3.776}
{'eval_valid_loss': 0.83837890625, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1110.23, 'eval_valid_steps_per_second': 277.557, 'epoch': 3.776}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.8651, 'grad_norm': 3.134650492707825, 'learning_rate': 6.334444444444444e-07, 'epoch': 3.7763999999999998}
{'loss': 0.8602, 'grad_norm': 2.7457006452557096, 'learning_rate': 6.323333333333334e-07, 'epoch': 3.7768}
{'loss': 0.8651, 'grad_norm': 2.873329236893562, 'learning_rate': 6.312222222222222e-07, 'epoch': 3.7772}
{'loss': 0.8697, 'grad_norm': 3.0439819490383764, 'learning_rate': 6.301111111111112e-07, 'epoch': 3.7776}
{'loss': 0.859, 'grad_norm': 3.1952229979275293, 'learning_rate': 6.29e-07, 'epoch': 3.778}
{'loss': 0.8602, 'grad_norm': 2.886935281440061, 'learning_rate': 6.27888888888889e-07, 'epoch': 3.7784}
{'loss': 0.873, 'grad_norm': 3.1967440525207538, 'learning_rate': 6.267777777777778e-07, 'epoch': 3.7788}
{'loss': 0.863, 'grad_norm': 3.0602496696043806, 'learning_rate': 6.256666666666668e-07, 'epoch': 3.7792}
{'loss': 0.8573, 'grad_norm': 2.8462977995647107, 'learning_rate': 6.245555555555556e-07, 'epoch': 3.7796}
{'loss': 0.8606, 'grad_norm': 2.8301413813985836, 'learning_rate': 6.234444444444445e-07, 'epoch': 3.7800000000000002}
{'eval_valid_loss': 0.83837890625, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.575, 'eval_valid_steps_per_second': 279.394, 'epoch': 3.7800000000000002}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8645, 'grad_norm': 2.9896103141089974, 'learning_rate': 6.223333333333334e-07, 'epoch': 3.7804}
{'loss': 0.8529, 'grad_norm': 2.69397426499229, 'learning_rate': 6.212222222222223e-07, 'epoch': 3.7808}
{'loss': 0.8536, 'grad_norm': 2.755170999765599, 'learning_rate': 6.201111111111112e-07, 'epoch': 3.7812}
{'loss': 0.8664, 'grad_norm': 3.0452605934475536, 'learning_rate': 6.19e-07, 'epoch': 3.7816}
{'loss': 0.8598, 'grad_norm': 3.206371216220801, 'learning_rate': 6.178888888888889e-07, 'epoch': 3.782}
{'loss': 0.853, 'grad_norm': 2.8448154738072686, 'learning_rate': 6.167777777777778e-07, 'epoch': 3.7824}
{'loss': 0.8709, 'grad_norm': 3.011792257478141, 'learning_rate': 6.156666666666667e-07, 'epoch': 3.7828}
{'loss': 0.8601, 'grad_norm': 3.1270757456938814, 'learning_rate': 6.145555555555556e-07, 'epoch': 3.7832}
{'loss': 0.8705, 'grad_norm': 3.332411982439244, 'learning_rate': 6.134444444444445e-07, 'epoch': 3.7836}
{'loss': 0.8651, 'grad_norm': 2.802189285492161, 'learning_rate': 6.123333333333334e-07, 'epoch': 3.784}
{'eval_valid_loss': 0.83837890625, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.958, 'eval_valid_steps_per_second': 278.739, 'epoch': 3.784}
{'loss': 0.867, 'grad_norm': 3.350264122144615, 'learning_rate': 6.112222222222223e-07, 'epoch': 3.7843999999999998}
{'loss': 0.867, 'grad_norm': 2.838611277622055, 'learning_rate': 6.101111111111112e-07, 'epoch': 3.7848}
{'loss': 0.875, 'grad_norm': 2.952368301245104, 'learning_rate': 6.090000000000001e-07, 'epoch': 3.7852}
{'loss': 0.869, 'grad_norm': 3.1973245530241234, 'learning_rate': 6.078888888888889e-07, 'epoch': 3.7856}
{'loss': 0.8393, 'grad_norm': 3.0035721750600284, 'learning_rate': 6.067777777777778e-07, 'epoch': 3.786}
{'loss': 0.8663, 'grad_norm': 2.975360647748911, 'learning_rate': 6.056666666666667e-07, 'epoch': 3.7864}
{'loss': 0.852, 'grad_norm': 2.7448804082163587, 'learning_rate': 6.045555555555556e-07, 'epoch': 3.7868}
{'loss': 0.8682, 'grad_norm': 2.9806512499289486, 'learning_rate': 6.034444444444445e-07, 'epoch': 3.7872}
{'loss': 0.8663, 'grad_norm': 2.872537325258836, 'learning_rate': 6.023333333333333e-07, 'epoch': 3.7876}
{'loss': 0.856, 'grad_norm': 3.2843557827257555, 'learning_rate': 6.012222222222223e-07, 'epoch': 3.7880000000000003}
{'eval_valid_loss': 0.83837890625, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1120.456, 'eval_valid_steps_per_second': 280.114, 'epoch': 3.7880000000000003}
{'loss': 0.8693, 'grad_norm': 3.1597889803493557, 'learning_rate': 6.001111111111112e-07, 'epoch': 3.7884}
{'loss': 0.8693, 'grad_norm': 2.866014869651367, 'learning_rate': 5.990000000000001e-07, 'epoch': 3.7888}
{'loss': 0.8593, 'grad_norm': 2.6767386178654413, 'learning_rate': 5.978888888888889e-07, 'epoch': 3.7892}
{'loss': 0.86, 'grad_norm': 2.9684012509341575, 'learning_rate': 5.967777777777778e-07, 'epoch': 3.7896}
{'loss': 0.8593, 'grad_norm': 2.9464388952445946, 'learning_rate': 5.956666666666667e-07, 'epoch': 3.79}
{'loss': 0.8609, 'grad_norm': 2.909092036499, 'learning_rate': 5.945555555555556e-07, 'epoch': 3.7904}
{'loss': 0.8545, 'grad_norm': 2.8008359799963234, 'learning_rate': 5.934444444444445e-07, 'epoch': 3.7908}
{'loss': 0.8673, 'grad_norm': 2.901327979696979, 'learning_rate': 5.923333333333333e-07, 'epoch': 3.7912}
{'loss': 0.8627, 'grad_norm': 3.155727220429641, 'learning_rate': 5.912222222222223e-07, 'epoch': 3.7916}
{'loss': 0.8531, 'grad_norm': 2.995737793229752, 'learning_rate': 5.901111111111112e-07, 'epoch': 3.792}
{'eval_valid_loss': 0.83837890625, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1109.707, 'eval_valid_steps_per_second': 277.427, 'epoch': 3.792}
{'loss': 0.869, 'grad_norm': 2.9619495806024974, 'learning_rate': 5.890000000000001e-07, 'epoch': 3.7923999999999998}
{'loss': 0.8673, 'grad_norm': 2.777729731912478, 'learning_rate': 5.878888888888889e-07, 'epoch': 3.7927999999999997}
{'loss': 0.8587, 'grad_norm': 2.93852479793048, 'learning_rate': 5.867777777777778e-07, 'epoch': 3.7932}
{'loss': 0.8712, 'grad_norm': 3.2939735951977958, 'learning_rate': 5.856666666666668e-07, 'epoch': 3.7936}
{'loss': 0.8719, 'grad_norm': 2.9838643860515717, 'learning_rate': 5.845555555555556e-07, 'epoch': 3.794}
{'loss': 0.8567, 'grad_norm': 2.929895083384759, 'learning_rate': 5.834444444444445e-07, 'epoch': 3.7944}
{'loss': 0.8668, 'grad_norm': 2.963228743289371, 'learning_rate': 5.823333333333333e-07, 'epoch': 3.7948}
{'loss': 0.8596, 'grad_norm': 3.095686056288586, 'learning_rate': 5.812222222222222e-07, 'epoch': 3.7952}
{'loss': 0.8664, 'grad_norm': 2.833213822331332, 'learning_rate': 5.801111111111112e-07, 'epoch': 3.7956}
{'loss': 0.8622, 'grad_norm': 3.069916134164056, 'learning_rate': 5.790000000000001e-07, 'epoch': 3.7960000000000003}
{'eval_valid_loss': 0.83837890625, 'eval_valid_runtime': 0.0912, 'eval_valid_samples_per_second': 1096.137, 'eval_valid_steps_per_second': 274.034, 'epoch': 3.7960000000000003}
{'loss': 0.8764, 'grad_norm': 2.771725349085093, 'learning_rate': 5.778888888888889e-07, 'epoch': 3.7964}
{'loss': 0.8639, 'grad_norm': 3.4453631546552614, 'learning_rate': 5.767777777777778e-07, 'epoch': 3.7968}
{'loss': 0.8788, 'grad_norm': 3.044917176972803, 'learning_rate': 5.756666666666668e-07, 'epoch': 3.7972}
{'loss': 0.8671, 'grad_norm': 3.063603387310563, 'learning_rate': 5.745555555555557e-07, 'epoch': 3.7976}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.8672, 'grad_norm': 2.8454864670822633, 'learning_rate': 5.734444444444445e-07, 'epoch': 3.798}
{'loss': 0.8559, 'grad_norm': 3.2033127799468013, 'learning_rate': 5.723333333333333e-07, 'epoch': 3.7984}
{'loss': 0.8659, 'grad_norm': 2.9126714551582085, 'learning_rate': 5.712222222222222e-07, 'epoch': 3.7988}
{'loss': 0.8601, 'grad_norm': 2.8989567355828787, 'learning_rate': 5.701111111111112e-07, 'epoch': 3.7992}
{'loss': 0.8729, 'grad_norm': 3.0780094725331075, 'learning_rate': 5.690000000000001e-07, 'epoch': 3.7996}
{'loss': 0.8706, 'grad_norm': 2.9028596843047207, 'learning_rate': 5.678888888888889e-07, 'epoch': 3.8}
{'eval_valid_loss': 0.83837890625, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1114.016, 'eval_valid_steps_per_second': 278.504, 'epoch': 3.8}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8696, 'grad_norm': 3.090515787029075, 'learning_rate': 5.667777777777778e-07, 'epoch': 3.8004}
{'loss': 0.8805, 'grad_norm': 3.0454840688587814, 'learning_rate': 5.656666666666667e-07, 'epoch': 3.8007999999999997}
{'loss': 0.8686, 'grad_norm': 2.9685910383130083, 'learning_rate': 5.645555555555557e-07, 'epoch': 3.8012}
{'loss': 0.87, 'grad_norm': 2.844226147539837, 'learning_rate': 5.634444444444445e-07, 'epoch': 3.8016}
{'loss': 0.8623, 'grad_norm': 2.8963666134416903, 'learning_rate': 5.623333333333334e-07, 'epoch': 3.802}
{'loss': 0.8584, 'grad_norm': 3.027597015615659, 'learning_rate': 5.612222222222222e-07, 'epoch': 3.8024}
{'loss': 0.8709, 'grad_norm': 2.8562821196538497, 'learning_rate': 5.601111111111111e-07, 'epoch': 3.8028}
{'loss': 0.8435, 'grad_norm': 2.898631421000096, 'learning_rate': 5.590000000000001e-07, 'epoch': 3.8032}
{'loss': 0.8576, 'grad_norm': 2.9190588961236887, 'learning_rate': 5.578888888888889e-07, 'epoch': 3.8036}
{'loss': 0.8676, 'grad_norm': 3.430660266260598, 'learning_rate': 5.567777777777778e-07, 'epoch': 3.8040000000000003}
{'eval_valid_loss': 0.8388671875, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1120.824, 'eval_valid_steps_per_second': 280.206, 'epoch': 3.8040000000000003}
{'loss': 0.8635, 'grad_norm': 2.8055985333882654, 'learning_rate': 5.556666666666667e-07, 'epoch': 3.8044000000000002}
{'loss': 0.8582, 'grad_norm': 2.9477154822607394, 'learning_rate': 5.546666666666667e-07, 'epoch': 3.8048}
{'loss': 0.8635, 'grad_norm': 3.144484260160602, 'learning_rate': 5.535555555555556e-07, 'epoch': 3.8052}
{'loss': 0.8527, 'grad_norm': 3.170428824887522, 'learning_rate': 5.524444444444445e-07, 'epoch': 3.8056}
{'loss': 0.8553, 'grad_norm': 2.7308045918837305, 'learning_rate': 5.513333333333333e-07, 'epoch': 3.806}
{'loss': 0.875, 'grad_norm': 3.0187675526900417, 'learning_rate': 5.502222222222222e-07, 'epoch': 3.8064}
{'loss': 0.8679, 'grad_norm': 2.950399488337045, 'learning_rate': 5.491111111111112e-07, 'epoch': 3.8068}
{'loss': 0.8822, 'grad_norm': 2.8199074367117105, 'learning_rate': 5.480000000000001e-07, 'epoch': 3.8072}
{'loss': 0.856, 'grad_norm': 2.637374434713839, 'learning_rate': 5.468888888888889e-07, 'epoch': 3.8076}
{'loss': 0.8655, 'grad_norm': 2.9280678302798044, 'learning_rate': 5.457777777777778e-07, 'epoch': 3.808}
{'eval_valid_loss': 0.83837890625, 'eval_valid_runtime': 0.0904, 'eval_valid_samples_per_second': 1106.601, 'eval_valid_steps_per_second': 276.65, 'epoch': 3.808}
{'loss': 0.8514, 'grad_norm': 2.6957762996402823, 'learning_rate': 5.446666666666666e-07, 'epoch': 3.8084}
{'loss': 0.8519, 'grad_norm': 2.8101832914814464, 'learning_rate': 5.435555555555556e-07, 'epoch': 3.8087999999999997}
{'loss': 0.8604, 'grad_norm': 2.799766899521819, 'learning_rate': 5.424444444444445e-07, 'epoch': 3.8092}
{'loss': 0.8566, 'grad_norm': 2.964914313520778, 'learning_rate': 5.413333333333334e-07, 'epoch': 3.8096}
{'loss': 0.8677, 'grad_norm': 2.8005776803535887, 'learning_rate': 5.402222222222222e-07, 'epoch': 3.81}
{'loss': 0.8673, 'grad_norm': 3.159349685175114, 'learning_rate': 5.391111111111112e-07, 'epoch': 3.8104}
{'loss': 0.8509, 'grad_norm': 2.8303472629201045, 'learning_rate': 5.380000000000001e-07, 'epoch': 3.8108}
{'loss': 0.866, 'grad_norm': 2.878398451929037, 'learning_rate': 5.368888888888889e-07, 'epoch': 3.8112}
{'loss': 0.8721, 'grad_norm': 3.1948479965330243, 'learning_rate': 5.357777777777778e-07, 'epoch': 3.8116}
{'loss': 0.8612, 'grad_norm': 2.742689195477547, 'learning_rate': 5.346666666666667e-07, 'epoch': 3.8120000000000003}
{'eval_valid_loss': 0.83837890625, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.653, 'eval_valid_steps_per_second': 278.663, 'epoch': 3.8120000000000003}
{'loss': 0.8597, 'grad_norm': 2.844588040961265, 'learning_rate': 5.335555555555557e-07, 'epoch': 3.8124000000000002}
{'loss': 0.8562, 'grad_norm': 2.9752554339054824, 'learning_rate': 5.324444444444445e-07, 'epoch': 3.8128}
{'loss': 0.8639, 'grad_norm': 2.9455256701160115, 'learning_rate': 5.313333333333334e-07, 'epoch': 3.8132}
{'loss': 0.875, 'grad_norm': 3.0237428575031626, 'learning_rate': 5.302222222222222e-07, 'epoch': 3.8136}
{'loss': 0.8447, 'grad_norm': 2.800061158593712, 'learning_rate': 5.291111111111111e-07, 'epoch': 3.814}
{'loss': 0.8639, 'grad_norm': 3.154437328317682, 'learning_rate': 5.280000000000001e-07, 'epoch': 3.8144}
{'loss': 0.8682, 'grad_norm': 2.9325255926857734, 'learning_rate': 5.268888888888889e-07, 'epoch': 3.8148}
{'loss': 0.8518, 'grad_norm': 2.986595366391822, 'learning_rate': 5.257777777777778e-07, 'epoch': 3.8152}
{'loss': 0.8618, 'grad_norm': 2.9038135406005554, 'learning_rate': 5.246666666666667e-07, 'epoch': 3.8156}
{'loss': 0.8542, 'grad_norm': 2.7842636550663693, 'learning_rate': 5.235555555555557e-07, 'epoch': 3.816}
{'eval_valid_loss': 0.83837890625, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1120.896, 'eval_valid_steps_per_second': 280.224, 'epoch': 3.816}
{'loss': 0.8686, 'grad_norm': 2.974024168422662, 'learning_rate': 5.224444444444445e-07, 'epoch': 3.8164}
{'loss': 0.8651, 'grad_norm': 2.7792143712293704, 'learning_rate': 5.213333333333334e-07, 'epoch': 3.8167999999999997}
{'loss': 0.8584, 'grad_norm': 2.828414333148742, 'learning_rate': 5.202222222222222e-07, 'epoch': 3.8172}
{'loss': 0.8642, 'grad_norm': 2.9113489459148147, 'learning_rate': 5.191111111111111e-07, 'epoch': 3.8176}
{'loss': 0.8699, 'grad_norm': 2.9419024734073287, 'learning_rate': 5.180000000000001e-07, 'epoch': 3.818}
{'loss': 0.8671, 'grad_norm': 2.6341640897062644, 'learning_rate': 5.168888888888889e-07, 'epoch': 3.8184}
{'loss': 0.8641, 'grad_norm': 3.096077813899928, 'learning_rate': 5.157777777777778e-07, 'epoch': 3.8188}
{'loss': 0.8651, 'grad_norm': 2.859402640787573, 'learning_rate': 5.146666666666667e-07, 'epoch': 3.8192}
{'loss': 0.8679, 'grad_norm': 2.841719683355819, 'learning_rate': 5.135555555555556e-07, 'epoch': 3.8196}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8682, 'grad_norm': 2.947957558679651, 'learning_rate': 5.124444444444445e-07, 'epoch': 3.82}
{'eval_valid_loss': 0.83837890625, 'eval_valid_runtime': 0.0913, 'eval_valid_samples_per_second': 1095.367, 'eval_valid_steps_per_second': 273.842, 'epoch': 3.82}
{'loss': 0.8582, 'grad_norm': 2.972260287126272, 'learning_rate': 5.113333333333334e-07, 'epoch': 3.8204000000000002}
{'loss': 0.8538, 'grad_norm': 2.843293221436909, 'learning_rate': 5.102222222222222e-07, 'epoch': 3.8208}
{'loss': 0.8623, 'grad_norm': 3.0144350243875593, 'learning_rate': 5.091111111111111e-07, 'epoch': 3.8212}
{'loss': 0.8708, 'grad_norm': 2.9986324471556896, 'learning_rate': 5.08e-07, 'epoch': 3.8216}
{'loss': 0.8668, 'grad_norm': 3.1713406718775095, 'learning_rate': 5.068888888888889e-07, 'epoch': 3.822}
{'loss': 0.8614, 'grad_norm': 2.7763314206251146, 'learning_rate': 5.057777777777778e-07, 'epoch': 3.8224}
{'loss': 0.867, 'grad_norm': 2.7988632796236654, 'learning_rate': 5.046666666666667e-07, 'epoch': 3.8228}
{'loss': 0.8764, 'grad_norm': 3.007608907175275, 'learning_rate': 5.035555555555556e-07, 'epoch': 3.8232}
{'loss': 0.862, 'grad_norm': 3.2458158573778713, 'learning_rate': 5.024444444444445e-07, 'epoch': 3.8236}
{'loss': 0.8679, 'grad_norm': 2.7877972348148785, 'learning_rate': 5.013333333333334e-07, 'epoch': 3.824}
{'eval_valid_loss': 0.837890625, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1128.681, 'eval_valid_steps_per_second': 282.17, 'epoch': 3.824}
{'loss': 0.8636, 'grad_norm': 2.858713490411886, 'learning_rate': 5.002222222222223e-07, 'epoch': 3.8244}
{'loss': 0.855, 'grad_norm': 2.595328666365659, 'learning_rate': 4.991111111111112e-07, 'epoch': 3.8247999999999998}
{'loss': 0.8604, 'grad_norm': 2.993568958072493, 'learning_rate': 4.98e-07, 'epoch': 3.8252}
{'loss': 0.8586, 'grad_norm': 2.9675166027404862, 'learning_rate': 4.968888888888889e-07, 'epoch': 3.8256}
{'loss': 0.8702, 'grad_norm': 3.0519530796903998, 'learning_rate': 4.957777777777778e-07, 'epoch': 3.826}
{'loss': 0.8572, 'grad_norm': 2.9985251973950704, 'learning_rate': 4.946666666666667e-07, 'epoch': 3.8264}
{'loss': 0.8729, 'grad_norm': 2.8318887714193415, 'learning_rate': 4.935555555555556e-07, 'epoch': 3.8268}
{'loss': 0.8649, 'grad_norm': 2.9858247820520307, 'learning_rate': 4.924444444444444e-07, 'epoch': 3.8272}
{'loss': 0.8588, 'grad_norm': 3.146767194505621, 'learning_rate': 4.913333333333334e-07, 'epoch': 3.8276}
{'loss': 0.8746, 'grad_norm': 3.153998468887306, 'learning_rate': 4.902222222222223e-07, 'epoch': 3.828}
{'eval_valid_loss': 0.837890625, 'eval_valid_runtime': 0.0882, 'eval_valid_samples_per_second': 1133.541, 'eval_valid_steps_per_second': 283.385, 'epoch': 3.828}
{'loss': 0.8594, 'grad_norm': 2.878402505433509, 'learning_rate': 4.891111111111112e-07, 'epoch': 3.8284000000000002}
{'loss': 0.8581, 'grad_norm': 3.0419931769943465, 'learning_rate': 4.88e-07, 'epoch': 3.8288}
{'loss': 0.8556, 'grad_norm': 2.939398020484451, 'learning_rate': 4.868888888888888e-07, 'epoch': 3.8292}
{'loss': 0.8652, 'grad_norm': 3.302979526185493, 'learning_rate': 4.857777777777778e-07, 'epoch': 3.8296}
{'loss': 0.8645, 'grad_norm': 2.999645629457063, 'learning_rate': 4.846666666666667e-07, 'epoch': 3.83}
{'loss': 0.8576, 'grad_norm': 2.9160978250741296, 'learning_rate': 4.835555555555556e-07, 'epoch': 3.8304}
{'loss': 0.8534, 'grad_norm': 2.7636575543467305, 'learning_rate': 4.824444444444444e-07, 'epoch': 3.8308}
{'loss': 0.8669, 'grad_norm': 3.0050631404866586, 'learning_rate': 4.813333333333334e-07, 'epoch': 3.8312}
{'loss': 0.8507, 'grad_norm': 2.76864776723974, 'learning_rate': 4.802222222222223e-07, 'epoch': 3.8316}
{'loss': 0.8573, 'grad_norm': 2.6453163581815504, 'learning_rate': 4.791111111111112e-07, 'epoch': 3.832}
{'eval_valid_loss': 0.83837890625, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1126.496, 'eval_valid_steps_per_second': 281.624, 'epoch': 3.832}
{'loss': 0.8654, 'grad_norm': 2.8696741136589496, 'learning_rate': 4.78e-07, 'epoch': 3.8324}
{'loss': 0.8551, 'grad_norm': 2.9985059653810104, 'learning_rate': 4.768888888888889e-07, 'epoch': 3.8327999999999998}
{'loss': 0.8604, 'grad_norm': 2.9730120031195986, 'learning_rate': 4.7577777777777776e-07, 'epoch': 3.8332}
{'loss': 0.8517, 'grad_norm': 3.062725428633926, 'learning_rate': 4.746666666666667e-07, 'epoch': 3.8336}
{'loss': 0.8551, 'grad_norm': 2.772220027355628, 'learning_rate': 4.7355555555555555e-07, 'epoch': 3.834}
{'loss': 0.8573, 'grad_norm': 2.992342255178503, 'learning_rate': 4.724444444444445e-07, 'epoch': 3.8344}
{'loss': 0.8633, 'grad_norm': 3.1426581828349884, 'learning_rate': 4.7133333333333335e-07, 'epoch': 3.8348}
{'loss': 0.8609, 'grad_norm': 2.926107538667132, 'learning_rate': 4.7022222222222227e-07, 'epoch': 3.8352}
{'loss': 0.8634, 'grad_norm': 3.074490594538386, 'learning_rate': 4.6911111111111114e-07, 'epoch': 3.8356}
{'loss': 0.8756, 'grad_norm': 3.1308499512795342, 'learning_rate': 4.6800000000000006e-07, 'epoch': 3.836}
{'eval_valid_loss': 0.837890625, 'eval_valid_runtime': 0.0912, 'eval_valid_samples_per_second': 1096.246, 'eval_valid_steps_per_second': 274.062, 'epoch': 3.836}
{'loss': 0.869, 'grad_norm': 2.91713056054145, 'learning_rate': 4.6688888888888893e-07, 'epoch': 3.8364000000000003}
{'loss': 0.8645, 'grad_norm': 2.771963754155492, 'learning_rate': 4.6577777777777785e-07, 'epoch': 3.8368}
{'loss': 0.849, 'grad_norm': 2.9652063602857357, 'learning_rate': 4.646666666666667e-07, 'epoch': 3.8372}
{'loss': 0.8529, 'grad_norm': 2.9567696721474106, 'learning_rate': 4.6355555555555554e-07, 'epoch': 3.8376}
{'loss': 0.865, 'grad_norm': 2.959648405293557, 'learning_rate': 4.6244444444444446e-07, 'epoch': 3.838}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.8649, 'grad_norm': 3.104300705151459, 'learning_rate': 4.6133333333333334e-07, 'epoch': 3.8384}
{'loss': 0.8642, 'grad_norm': 3.1175896999391366, 'learning_rate': 4.6022222222222226e-07, 'epoch': 3.8388}
{'loss': 0.8677, 'grad_norm': 3.0584062150972877, 'learning_rate': 4.5911111111111113e-07, 'epoch': 3.8392}
{'loss': 0.8633, 'grad_norm': 2.7994249483521116, 'learning_rate': 4.5800000000000005e-07, 'epoch': 3.8396}
{'loss': 0.8628, 'grad_norm': 2.920128274722672, 'learning_rate': 4.568888888888889e-07, 'epoch': 3.84}
{'eval_valid_loss': 0.837890625, 'eval_valid_runtime': 0.0909, 'eval_valid_samples_per_second': 1100.246, 'eval_valid_steps_per_second': 275.062, 'epoch': 3.84}
{'loss': 0.8613, 'grad_norm': 2.945712383656861, 'learning_rate': 4.5577777777777784e-07, 'epoch': 3.8404}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8583, 'grad_norm': 3.020726301774473, 'learning_rate': 4.546666666666667e-07, 'epoch': 3.8407999999999998}
{'loss': 0.8575, 'grad_norm': 2.9882441200492744, 'learning_rate': 4.5355555555555564e-07, 'epoch': 3.8411999999999997}
{'loss': 0.8623, 'grad_norm': 2.888923742367313, 'learning_rate': 4.5244444444444445e-07, 'epoch': 3.8416}
{'loss': 0.8775, 'grad_norm': 3.389558255142509, 'learning_rate': 4.513333333333333e-07, 'epoch': 3.842}
{'loss': 0.8683, 'grad_norm': 2.929711130682693, 'learning_rate': 4.5022222222222225e-07, 'epoch': 3.8424}
{'loss': 0.8659, 'grad_norm': 3.045879220856041, 'learning_rate': 4.491111111111111e-07, 'epoch': 3.8428}
{'loss': 0.8545, 'grad_norm': 2.9757834488832295, 'learning_rate': 4.4800000000000004e-07, 'epoch': 3.8432}
{'loss': 0.8537, 'grad_norm': 3.2609700788507334, 'learning_rate': 4.468888888888889e-07, 'epoch': 3.8436}
{'loss': 0.8533, 'grad_norm': 2.713410247407552, 'learning_rate': 4.4577777777777783e-07, 'epoch': 3.844}
{'eval_valid_loss': 0.83837890625, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.834, 'eval_valid_steps_per_second': 279.958, 'epoch': 3.844}
{'loss': 0.8733, 'grad_norm': 3.2127842755094123, 'learning_rate': 4.446666666666667e-07, 'epoch': 3.8444000000000003}
{'loss': 0.8605, 'grad_norm': 2.9214488345974723, 'learning_rate': 4.436666666666667e-07, 'epoch': 3.8448}
{'loss': 0.8825, 'grad_norm': 3.1221279011302174, 'learning_rate': 4.425555555555556e-07, 'epoch': 3.8452}
{'loss': 0.8601, 'grad_norm': 2.94573977581271, 'learning_rate': 4.414444444444445e-07, 'epoch': 3.8456}
{'loss': 0.8576, 'grad_norm': 2.7927266849643835, 'learning_rate': 4.403333333333334e-07, 'epoch': 3.846}
{'loss': 0.8631, 'grad_norm': 2.8745957069574852, 'learning_rate': 4.3922222222222227e-07, 'epoch': 3.8464}
{'loss': 0.8694, 'grad_norm': 2.9340558253244953, 'learning_rate': 4.381111111111112e-07, 'epoch': 3.8468}
{'loss': 0.8545, 'grad_norm': 2.931626850377912, 'learning_rate': 4.3700000000000006e-07, 'epoch': 3.8472}
{'loss': 0.8707, 'grad_norm': 3.0797323493653, 'learning_rate': 4.35888888888889e-07, 'epoch': 3.8476}
{'loss': 0.8733, 'grad_norm': 2.977241168410819, 'learning_rate': 4.347777777777778e-07, 'epoch': 3.848}
{'eval_valid_loss': 0.83837890625, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.773, 'eval_valid_steps_per_second': 279.693, 'epoch': 3.848}
{'loss': 0.8657, 'grad_norm': 2.902739239902806, 'learning_rate': 4.3366666666666667e-07, 'epoch': 3.8484}
{'loss': 0.8554, 'grad_norm': 2.788335112278339, 'learning_rate': 4.325555555555556e-07, 'epoch': 3.8487999999999998}
{'loss': 0.8604, 'grad_norm': 3.379372122516257, 'learning_rate': 4.3144444444444447e-07, 'epoch': 3.8491999999999997}
{'loss': 0.8606, 'grad_norm': 3.2690560053593214, 'learning_rate': 4.303333333333334e-07, 'epoch': 3.8496}
{'loss': 0.8723, 'grad_norm': 3.0434834996135414, 'learning_rate': 4.2922222222222226e-07, 'epoch': 3.85}
{'loss': 0.8702, 'grad_norm': 2.8483996055539005, 'learning_rate': 4.281111111111112e-07, 'epoch': 3.8504}
{'loss': 0.863, 'grad_norm': 2.838555328382927, 'learning_rate': 4.2700000000000005e-07, 'epoch': 3.8508}
{'loss': 0.8764, 'grad_norm': 2.9525893800403007, 'learning_rate': 4.25888888888889e-07, 'epoch': 3.8512}
{'loss': 0.863, 'grad_norm': 2.9075263256134165, 'learning_rate': 4.2477777777777784e-07, 'epoch': 3.8516}
{'loss': 0.8606, 'grad_norm': 2.8064074329828026, 'learning_rate': 4.2366666666666666e-07, 'epoch': 3.852}
{'eval_valid_loss': 0.83837890625, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.38, 'eval_valid_steps_per_second': 279.595, 'epoch': 3.852}
{'loss': 0.8631, 'grad_norm': 3.1084090798040305, 'learning_rate': 4.225555555555556e-07, 'epoch': 3.8524000000000003}
{'loss': 0.8568, 'grad_norm': 3.0649141411011462, 'learning_rate': 4.2144444444444446e-07, 'epoch': 3.8528000000000002}
{'loss': 0.8522, 'grad_norm': 2.7710420176191755, 'learning_rate': 4.203333333333334e-07, 'epoch': 3.8532}
{'loss': 0.8694, 'grad_norm': 3.1979678995872467, 'learning_rate': 4.1922222222222225e-07, 'epoch': 3.8536}
{'loss': 0.8675, 'grad_norm': 2.8494992460392767, 'learning_rate': 4.1811111111111117e-07, 'epoch': 3.854}
{'loss': 0.853, 'grad_norm': 2.980295088722046, 'learning_rate': 4.1700000000000004e-07, 'epoch': 3.8544}
{'loss': 0.8677, 'grad_norm': 2.945954928166002, 'learning_rate': 4.1588888888888896e-07, 'epoch': 3.8548}
{'loss': 0.8715, 'grad_norm': 3.1452189386573113, 'learning_rate': 4.1477777777777783e-07, 'epoch': 3.8552}
{'loss': 0.8728, 'grad_norm': 2.8546944127993514, 'learning_rate': 4.136666666666667e-07, 'epoch': 3.8556}
{'loss': 0.8566, 'grad_norm': 2.8855905664053134, 'learning_rate': 4.125555555555556e-07, 'epoch': 3.856}
{'eval_valid_loss': 0.83837890625, 'eval_valid_runtime': 0.089, 'eval_valid_samples_per_second': 1123.469, 'eval_valid_steps_per_second': 280.867, 'epoch': 3.856}
{'loss': 0.8719, 'grad_norm': 2.8666881677372, 'learning_rate': 4.1144444444444445e-07, 'epoch': 3.8564}
{'loss': 0.8567, 'grad_norm': 2.9798919099527725, 'learning_rate': 4.1033333333333337e-07, 'epoch': 3.8568}
{'loss': 0.8666, 'grad_norm': 2.985215213595638, 'learning_rate': 4.0922222222222224e-07, 'epoch': 3.8571999999999997}
{'loss': 0.8668, 'grad_norm': 2.9844417165119417, 'learning_rate': 4.0811111111111116e-07, 'epoch': 3.8576}
{'loss': 0.8709, 'grad_norm': 3.1688135810357094, 'learning_rate': 4.0700000000000003e-07, 'epoch': 3.858}
{'loss': 0.8621, 'grad_norm': 3.1588140308485633, 'learning_rate': 4.0588888888888895e-07, 'epoch': 3.8584}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8663, 'grad_norm': 3.0253863793300835, 'learning_rate': 4.047777777777778e-07, 'epoch': 3.8588}
{'loss': 0.8622, 'grad_norm': 2.9375302029132393, 'learning_rate': 4.036666666666667e-07, 'epoch': 3.8592}
{'loss': 0.8479, 'grad_norm': 2.8101647907940714, 'learning_rate': 4.025555555555556e-07, 'epoch': 3.8596}
{'loss': 0.8639, 'grad_norm': 2.9190852775211877, 'learning_rate': 4.014444444444445e-07, 'epoch': 3.86}
{'eval_valid_loss': 0.837890625, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1120.702, 'eval_valid_steps_per_second': 280.175, 'epoch': 3.86}
{'loss': 0.8761, 'grad_norm': 3.1390784996616143, 'learning_rate': 4.0033333333333336e-07, 'epoch': 3.8604000000000003}
{'loss': 0.8674, 'grad_norm': 2.9921714742445507, 'learning_rate': 3.9922222222222223e-07, 'epoch': 3.8608000000000002}
{'loss': 0.8705, 'grad_norm': 2.8166486871209475, 'learning_rate': 3.9811111111111115e-07, 'epoch': 3.8612}
{'loss': 0.8588, 'grad_norm': 3.299427915243818, 'learning_rate': 3.97e-07, 'epoch': 3.8616}
{'loss': 0.8493, 'grad_norm': 3.0093137091142013, 'learning_rate': 3.9588888888888894e-07, 'epoch': 3.862}
{'loss': 0.8629, 'grad_norm': 2.941223159096904, 'learning_rate': 3.947777777777778e-07, 'epoch': 3.8624}
{'loss': 0.8707, 'grad_norm': 2.8357188926069448, 'learning_rate': 3.936666666666667e-07, 'epoch': 3.8628}
{'loss': 0.8608, 'grad_norm': 3.1852161509439356, 'learning_rate': 3.925555555555556e-07, 'epoch': 3.8632}
{'loss': 0.8697, 'grad_norm': 3.090841756689038, 'learning_rate': 3.914444444444445e-07, 'epoch': 3.8636}
{'loss': 0.8702, 'grad_norm': 3.074614328528955, 'learning_rate': 3.903333333333334e-07, 'epoch': 3.864}
{'eval_valid_loss': 0.837890625, 'eval_valid_runtime': 0.0889, 'eval_valid_samples_per_second': 1125.338, 'eval_valid_steps_per_second': 281.335, 'epoch': 3.864}
{'loss': 0.8542, 'grad_norm': 2.9682124404092347, 'learning_rate': 3.892222222222222e-07, 'epoch': 3.8644}
{'loss': 0.8569, 'grad_norm': 3.014290944445495, 'learning_rate': 3.8811111111111114e-07, 'epoch': 3.8648}
{'loss': 0.8647, 'grad_norm': 2.8928834134587627, 'learning_rate': 3.87e-07, 'epoch': 3.8651999999999997}
{'loss': 0.8763, 'grad_norm': 2.7959446451870327, 'learning_rate': 3.8588888888888893e-07, 'epoch': 3.8656}
{'loss': 0.8618, 'grad_norm': 3.176090474106625, 'learning_rate': 3.847777777777778e-07, 'epoch': 3.866}
{'loss': 0.8486, 'grad_norm': 3.1012981119540735, 'learning_rate': 3.8366666666666673e-07, 'epoch': 3.8664}
{'loss': 0.8527, 'grad_norm': 2.8290945361797077, 'learning_rate': 3.825555555555556e-07, 'epoch': 3.8668}
{'loss': 0.8665, 'grad_norm': 3.0467139763953406, 'learning_rate': 3.8144444444444447e-07, 'epoch': 3.8672}
{'loss': 0.862, 'grad_norm': 3.0209006378743926, 'learning_rate': 3.803333333333334e-07, 'epoch': 3.8676}
{'loss': 0.8731, 'grad_norm': 3.233563067854471, 'learning_rate': 3.7922222222222226e-07, 'epoch': 3.868}
{'eval_valid_loss': 0.83837890625, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.852, 'eval_valid_steps_per_second': 279.463, 'epoch': 3.868}
{'loss': 0.8566, 'grad_norm': 2.966412597842353, 'learning_rate': 3.7811111111111113e-07, 'epoch': 3.8684}
{'loss': 0.8527, 'grad_norm': 2.831345184359828, 'learning_rate': 3.77e-07, 'epoch': 3.8688000000000002}
{'loss': 0.8738, 'grad_norm': 3.0640860654400357, 'learning_rate': 3.758888888888889e-07, 'epoch': 3.8692}
{'loss': 0.8537, 'grad_norm': 3.103022147833843, 'learning_rate': 3.747777777777778e-07, 'epoch': 3.8696}
{'loss': 0.8627, 'grad_norm': 3.032195740080866, 'learning_rate': 3.736666666666667e-07, 'epoch': 3.87}
{'loss': 0.871, 'grad_norm': 3.0061457387883053, 'learning_rate': 3.725555555555556e-07, 'epoch': 3.8704}
{'loss': 0.8666, 'grad_norm': 2.9064017379280283, 'learning_rate': 3.7144444444444446e-07, 'epoch': 3.8708}
{'loss': 0.8699, 'grad_norm': 3.1837263582847304, 'learning_rate': 3.703333333333334e-07, 'epoch': 3.8712}
{'loss': 0.8709, 'grad_norm': 2.9538192033994926, 'learning_rate': 3.6922222222222225e-07, 'epoch': 3.8716}
{'loss': 0.8652, 'grad_norm': 3.016553483841061, 'learning_rate': 3.681111111111112e-07, 'epoch': 3.872}
{'eval_valid_loss': 0.837890625, 'eval_valid_runtime': 0.0885, 'eval_valid_samples_per_second': 1130.437, 'eval_valid_steps_per_second': 282.609, 'epoch': 3.872}
{'loss': 0.8669, 'grad_norm': 2.921182147410451, 'learning_rate': 3.6700000000000004e-07, 'epoch': 3.8724}
{'loss': 0.8627, 'grad_norm': 2.9650019426323375, 'learning_rate': 3.658888888888889e-07, 'epoch': 3.8728}
{'loss': 0.8675, 'grad_norm': 3.0061375896350913, 'learning_rate': 3.647777777777778e-07, 'epoch': 3.8731999999999998}
{'loss': 0.8685, 'grad_norm': 2.80666080008352, 'learning_rate': 3.636666666666667e-07, 'epoch': 3.8736}
{'loss': 0.8562, 'grad_norm': 2.6700880365912965, 'learning_rate': 3.625555555555556e-07, 'epoch': 3.874}
{'loss': 0.8614, 'grad_norm': 3.2379417563584636, 'learning_rate': 3.6144444444444445e-07, 'epoch': 3.8744}
{'loss': 0.8645, 'grad_norm': 2.9366089605748225, 'learning_rate': 3.6033333333333337e-07, 'epoch': 3.8748}
{'loss': 0.8484, 'grad_norm': 2.89284801576325, 'learning_rate': 3.5922222222222224e-07, 'epoch': 3.8752}
{'loss': 0.8533, 'grad_norm': 2.726313549579632, 'learning_rate': 3.5811111111111116e-07, 'epoch': 3.8756}
{'loss': 0.8632, 'grad_norm': 2.677805598429595, 'learning_rate': 3.5700000000000003e-07, 'epoch': 3.876}
{'eval_valid_loss': 0.837890625, 'eval_valid_runtime': 0.0886, 'eval_valid_samples_per_second': 1128.463, 'eval_valid_steps_per_second': 282.116, 'epoch': 3.876}
{'loss': 0.8581, 'grad_norm': 2.919092138278995, 'learning_rate': 3.5588888888888896e-07, 'epoch': 3.8764}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
{'loss': 0.8663, 'grad_norm': 3.082149720906554, 'learning_rate': 3.547777777777778e-07, 'epoch': 3.8768000000000002}
{'loss': 0.865, 'grad_norm': 3.0198630758849316, 'learning_rate': 3.536666666666667e-07, 'epoch': 3.8772}
{'loss': 0.8571, 'grad_norm': 3.0117937170215328, 'learning_rate': 3.5255555555555557e-07, 'epoch': 3.8776}
{'loss': 0.8629, 'grad_norm': 3.0265587164166403, 'learning_rate': 3.5144444444444444e-07, 'epoch': 3.878}
{'loss': 0.8705, 'grad_norm': 2.9724593880592507, 'learning_rate': 3.5033333333333336e-07, 'epoch': 3.8784}
{'loss': 0.8678, 'grad_norm': 3.176929224164291, 'learning_rate': 3.4922222222222223e-07, 'epoch': 3.8788}
{'loss': 0.8552, 'grad_norm': 3.2025557593703535, 'learning_rate': 3.4811111111111115e-07, 'epoch': 3.8792}
{'loss': 0.8669, 'grad_norm': 2.9855914208286864, 'learning_rate': 3.47e-07, 'epoch': 3.8796}
{'loss': 0.8481, 'grad_norm': 3.1985039011329333, 'learning_rate': 3.4588888888888895e-07, 'epoch': 3.88}
{'eval_valid_loss': 0.837890625, 'eval_valid_runtime': 0.0896, 'eval_valid_samples_per_second': 1116.026, 'eval_valid_steps_per_second': 279.006, 'epoch': 3.88}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8634, 'grad_norm': 3.031988761779118, 'learning_rate': 3.447777777777778e-07, 'epoch': 3.8804}
{'loss': 0.8641, 'grad_norm': 3.071218798490155, 'learning_rate': 3.436666666666667e-07, 'epoch': 3.8808}
{'loss': 0.8677, 'grad_norm': 2.889242511792046, 'learning_rate': 3.4255555555555556e-07, 'epoch': 3.8811999999999998}
{'loss': 0.8677, 'grad_norm': 2.7952117769572156, 'learning_rate': 3.4144444444444443e-07, 'epoch': 3.8816}
{'loss': 0.8656, 'grad_norm': 3.185892101598611, 'learning_rate': 3.4033333333333335e-07, 'epoch': 3.882}
{'loss': 0.8628, 'grad_norm': 3.094904279676901, 'learning_rate': 3.392222222222222e-07, 'epoch': 3.8824}
{'loss': 0.8585, 'grad_norm': 3.0158685081874688, 'learning_rate': 3.3811111111111114e-07, 'epoch': 3.8828}
{'loss': 0.8605, 'grad_norm': 2.963841712502816, 'learning_rate': 3.37e-07, 'epoch': 3.8832}
{'loss': 0.8727, 'grad_norm': 3.0114397050793142, 'learning_rate': 3.3588888888888894e-07, 'epoch': 3.8836}
{'loss': 0.8703, 'grad_norm': 3.444268481483889, 'learning_rate': 3.347777777777778e-07, 'epoch': 3.884}
{'eval_valid_loss': 0.837890625, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.222, 'eval_valid_steps_per_second': 279.555, 'epoch': 3.884}
{'loss': 0.868, 'grad_norm': 2.9598587758536157, 'learning_rate': 3.3366666666666673e-07, 'epoch': 3.8844}
{'loss': 0.8624, 'grad_norm': 3.0841987540183724, 'learning_rate': 3.326666666666667e-07, 'epoch': 3.8848000000000003}
{'loss': 0.8674, 'grad_norm': 3.1070881298096884, 'learning_rate': 3.315555555555556e-07, 'epoch': 3.8852}
{'loss': 0.8493, 'grad_norm': 2.747813075896452, 'learning_rate': 3.304444444444445e-07, 'epoch': 3.8856}
{'loss': 0.8613, 'grad_norm': 3.1727078758384866, 'learning_rate': 3.2933333333333337e-07, 'epoch': 3.886}
{'loss': 0.8744, 'grad_norm': 2.9887704386504863, 'learning_rate': 3.282222222222223e-07, 'epoch': 3.8864}
{'loss': 0.8661, 'grad_norm': 2.987943862119585, 'learning_rate': 3.2711111111111116e-07, 'epoch': 3.8868}
{'loss': 0.8596, 'grad_norm': 3.0065296459395343, 'learning_rate': 3.26e-07, 'epoch': 3.8872}
{'loss': 0.8564, 'grad_norm': 2.8345420809186064, 'learning_rate': 3.248888888888889e-07, 'epoch': 3.8876}
{'loss': 0.8556, 'grad_norm': 3.2051812805647444, 'learning_rate': 3.237777777777778e-07, 'epoch': 3.888}
{'eval_valid_loss': 0.837890625, 'eval_valid_runtime': 0.0917, 'eval_valid_samples_per_second': 1090.73, 'eval_valid_steps_per_second': 272.682, 'epoch': 3.888}
{'loss': 0.858, 'grad_norm': 2.846188547679744, 'learning_rate': 3.226666666666667e-07, 'epoch': 3.8884}
{'loss': 0.8659, 'grad_norm': 2.78170165134423, 'learning_rate': 3.2155555555555557e-07, 'epoch': 3.8888}
{'loss': 0.8755, 'grad_norm': 2.8733854476835687, 'learning_rate': 3.204444444444445e-07, 'epoch': 3.8891999999999998}
{'loss': 0.8648, 'grad_norm': 3.009652295909311, 'learning_rate': 3.1933333333333336e-07, 'epoch': 3.8895999999999997}
{'loss': 0.8604, 'grad_norm': 3.044588659233202, 'learning_rate': 3.182222222222223e-07, 'epoch': 3.89}
{'loss': 0.8663, 'grad_norm': 3.158307575676059, 'learning_rate': 3.1711111111111115e-07, 'epoch': 3.8904}
{'loss': 0.8582, 'grad_norm': 2.6751592428278554, 'learning_rate': 3.160000000000001e-07, 'epoch': 3.8908}
{'loss': 0.8552, 'grad_norm': 2.7503034738505883, 'learning_rate': 3.148888888888889e-07, 'epoch': 3.8912}
{'loss': 0.8644, 'grad_norm': 2.738735286493793, 'learning_rate': 3.1377777777777776e-07, 'epoch': 3.8916}
{'loss': 0.8604, 'grad_norm': 2.6509567758025088, 'learning_rate': 3.126666666666667e-07, 'epoch': 3.892}
{'eval_valid_loss': 0.837890625, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.656, 'eval_valid_steps_per_second': 278.664, 'epoch': 3.892}
{'loss': 0.863, 'grad_norm': 2.987016088406632, 'learning_rate': 3.1155555555555556e-07, 'epoch': 3.8924}
{'loss': 0.8601, 'grad_norm': 2.867673588431527, 'learning_rate': 3.104444444444445e-07, 'epoch': 3.8928000000000003}
{'loss': 0.8767, 'grad_norm': 2.756638879655175, 'learning_rate': 3.0933333333333335e-07, 'epoch': 3.8932}
{'loss': 0.8533, 'grad_norm': 2.8203202720387064, 'learning_rate': 3.0822222222222227e-07, 'epoch': 3.8936}
{'loss': 0.8599, 'grad_norm': 2.8866155603365393, 'learning_rate': 3.0711111111111114e-07, 'epoch': 3.894}
{'loss': 0.8673, 'grad_norm': 2.7786348241977032, 'learning_rate': 3.06e-07, 'epoch': 3.8944}
{'loss': 0.87, 'grad_norm': 3.0048987407094803, 'learning_rate': 3.048888888888889e-07, 'epoch': 3.8948}
{'loss': 0.8676, 'grad_norm': 3.0091240438557163, 'learning_rate': 3.037777777777778e-07, 'epoch': 3.8952}
{'loss': 0.8612, 'grad_norm': 2.9020628247674, 'learning_rate': 3.026666666666667e-07, 'epoch': 3.8956}
{'loss': 0.8576, 'grad_norm': 2.8439066183537336, 'learning_rate': 3.015555555555556e-07, 'epoch': 3.896}
{'eval_valid_loss': 0.837890625, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1125.815, 'eval_valid_steps_per_second': 281.454, 'epoch': 3.896}
{'loss': 0.876, 'grad_norm': 3.290739379517909, 'learning_rate': 3.0044444444444447e-07, 'epoch': 3.8964}
{'loss': 0.8594, 'grad_norm': 3.0611424161854917, 'learning_rate': 2.9933333333333334e-07, 'epoch': 3.8968}
{'loss': 0.8721, 'grad_norm': 2.9041671980515753, 'learning_rate': 2.9822222222222226e-07, 'epoch': 3.8971999999999998}
{'loss': 0.876, 'grad_norm': 2.9113543610725903, 'learning_rate': 2.9711111111111113e-07, 'epoch': 3.8975999999999997}
{'loss': 0.8621, 'grad_norm': 2.999126913259984, 'learning_rate': 2.9600000000000006e-07, 'epoch': 3.898}
{'loss': 0.8577, 'grad_norm': 3.013289293194877, 'learning_rate': 2.948888888888889e-07, 'epoch': 3.8984}
{'loss': 0.8698, 'grad_norm': 2.93788382883583, 'learning_rate': 2.937777777777778e-07, 'epoch': 3.8988}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8717, 'grad_norm': 3.0489030495234215, 'learning_rate': 2.9266666666666667e-07, 'epoch': 3.8992}
{'loss': 0.855, 'grad_norm': 3.0334321357457195, 'learning_rate': 2.915555555555556e-07, 'epoch': 3.8996}
{'loss': 0.8621, 'grad_norm': 2.9065550572253174, 'learning_rate': 2.9044444444444446e-07, 'epoch': 3.9}
{'eval_valid_loss': 0.837890625, 'eval_valid_runtime': 0.0888, 'eval_valid_samples_per_second': 1125.553, 'eval_valid_steps_per_second': 281.388, 'epoch': 3.9}
{'loss': 0.8715, 'grad_norm': 3.019509047689399, 'learning_rate': 2.8933333333333333e-07, 'epoch': 3.9004}
{'loss': 0.869, 'grad_norm': 3.04230944600185, 'learning_rate': 2.8822222222222225e-07, 'epoch': 3.9008000000000003}
{'loss': 0.8663, 'grad_norm': 2.9756257292713153, 'learning_rate': 2.871111111111111e-07, 'epoch': 3.9012000000000002}
{'loss': 0.8665, 'grad_norm': 2.9884035172724763, 'learning_rate': 2.8600000000000005e-07, 'epoch': 3.9016}
{'loss': 0.8604, 'grad_norm': 2.723790319791192, 'learning_rate': 2.848888888888889e-07, 'epoch': 3.902}
{'loss': 0.8566, 'grad_norm': 2.8734528483592303, 'learning_rate': 2.837777777777778e-07, 'epoch': 3.9024}
{'loss': 0.8648, 'grad_norm': 3.1510057788833414, 'learning_rate': 2.8266666666666666e-07, 'epoch': 3.9028}
{'loss': 0.86, 'grad_norm': 2.9466989830742105, 'learning_rate': 2.815555555555556e-07, 'epoch': 3.9032}
{'loss': 0.868, 'grad_norm': 2.8751495985832456, 'learning_rate': 2.8044444444444445e-07, 'epoch': 3.9036}
{'loss': 0.8563, 'grad_norm': 2.923181680224004, 'learning_rate': 2.7933333333333337e-07, 'epoch': 3.904}
{'eval_valid_loss': 0.83740234375, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.801, 'eval_valid_steps_per_second': 278.2, 'epoch': 3.904}
{'loss': 0.8694, 'grad_norm': 2.9050761385048243, 'learning_rate': 2.7822222222222224e-07, 'epoch': 3.9044}
{'loss': 0.8657, 'grad_norm': 3.182104649435944, 'learning_rate': 2.771111111111111e-07, 'epoch': 3.9048}
{'loss': 0.8733, 'grad_norm': 3.141593650876685, 'learning_rate': 2.7600000000000004e-07, 'epoch': 3.9052}
{'loss': 0.8537, 'grad_norm': 2.726221516883235, 'learning_rate': 2.748888888888889e-07, 'epoch': 3.9055999999999997}
{'loss': 0.8596, 'grad_norm': 2.738381850589056, 'learning_rate': 2.7377777777777783e-07, 'epoch': 3.906}
{'loss': 0.8666, 'grad_norm': 2.8956886325104554, 'learning_rate': 2.726666666666667e-07, 'epoch': 3.9064}
{'loss': 0.8498, 'grad_norm': 2.9074072020884514, 'learning_rate': 2.7155555555555557e-07, 'epoch': 3.9068}
{'loss': 0.8648, 'grad_norm': 3.3192497976225033, 'learning_rate': 2.7044444444444444e-07, 'epoch': 3.9072}
{'loss': 0.8688, 'grad_norm': 3.2547128508812557, 'learning_rate': 2.6933333333333336e-07, 'epoch': 3.9076}
{'loss': 0.8646, 'grad_norm': 3.299572451305754, 'learning_rate': 2.6822222222222223e-07, 'epoch': 3.908}
{'eval_valid_loss': 0.83740234375, 'eval_valid_runtime': 0.0901, 'eval_valid_samples_per_second': 1110.109, 'eval_valid_steps_per_second': 277.527, 'epoch': 3.908}
{'loss': 0.8601, 'grad_norm': 3.323611480793284, 'learning_rate': 2.6711111111111116e-07, 'epoch': 3.9084}
{'loss': 0.8535, 'grad_norm': 2.687009799962829, 'learning_rate': 2.66e-07, 'epoch': 3.9088000000000003}
{'loss': 0.8617, 'grad_norm': 3.0275842977139042, 'learning_rate': 2.648888888888889e-07, 'epoch': 3.9092000000000002}
{'loss': 0.8733, 'grad_norm': 3.2699621684542857, 'learning_rate': 2.637777777777778e-07, 'epoch': 3.9096}
{'loss': 0.8689, 'grad_norm': 3.0150738049325625, 'learning_rate': 2.626666666666667e-07, 'epoch': 3.91}
{'loss': 0.8683, 'grad_norm': 2.779272898331333, 'learning_rate': 2.615555555555556e-07, 'epoch': 3.9104}
{'loss': 0.8686, 'grad_norm': 3.076190679583521, 'learning_rate': 2.6044444444444443e-07, 'epoch': 3.9108}
{'loss': 0.8655, 'grad_norm': 3.112728917079978, 'learning_rate': 2.5933333333333335e-07, 'epoch': 3.9112}
{'loss': 0.8514, 'grad_norm': 2.724903399828835, 'learning_rate': 2.582222222222222e-07, 'epoch': 3.9116}
{'loss': 0.8533, 'grad_norm': 2.7499103477342848, 'learning_rate': 2.5711111111111115e-07, 'epoch': 3.912}
{'eval_valid_loss': 0.837890625, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1121.469, 'eval_valid_steps_per_second': 280.367, 'epoch': 3.912}
{'loss': 0.8626, 'grad_norm': 2.9677425682920218, 'learning_rate': 2.56e-07, 'epoch': 3.9124}
{'loss': 0.8615, 'grad_norm': 2.7895606101043366, 'learning_rate': 2.548888888888889e-07, 'epoch': 3.9128}
{'loss': 0.8611, 'grad_norm': 2.9977393672933657, 'learning_rate': 2.537777777777778e-07, 'epoch': 3.9132}
{'loss': 0.8513, 'grad_norm': 2.6412793370164347, 'learning_rate': 2.526666666666667e-07, 'epoch': 3.9135999999999997}
{'loss': 0.8731, 'grad_norm': 3.0393420779723757, 'learning_rate': 2.515555555555556e-07, 'epoch': 3.914}
{'loss': 0.8688, 'grad_norm': 2.907178786772253, 'learning_rate': 2.5044444444444447e-07, 'epoch': 3.9144}
{'loss': 0.8598, 'grad_norm': 3.3787733547999683, 'learning_rate': 2.4933333333333334e-07, 'epoch': 3.9148}
{'loss': 0.8736, 'grad_norm': 2.947721826474517, 'learning_rate': 2.482222222222222e-07, 'epoch': 3.9152}
{'loss': 0.8648, 'grad_norm': 3.1357471866383677, 'learning_rate': 2.4711111111111114e-07, 'epoch': 3.9156}
{'loss': 0.8549, 'grad_norm': 3.005542433539268, 'learning_rate': 2.46e-07, 'epoch': 3.916}
{'eval_valid_loss': 0.83740234375, 'eval_valid_runtime': 0.0895, 'eval_valid_samples_per_second': 1117.039, 'eval_valid_steps_per_second': 279.26, 'epoch': 3.916}
{'loss': 0.8698, 'grad_norm': 2.869894563569117, 'learning_rate': 2.4488888888888893e-07, 'epoch': 3.9164}
{'loss': 0.8585, 'grad_norm': 3.0492125127730034, 'learning_rate': 2.437777777777778e-07, 'epoch': 3.9168}
{'loss': 0.8542, 'grad_norm': 3.0648442900669575, 'learning_rate': 2.4266666666666667e-07, 'epoch': 3.9172000000000002}
{'loss': 0.8756, 'grad_norm': 2.7949775674257524, 'learning_rate': 2.415555555555556e-07, 'epoch': 3.9176}
{'loss': 0.8661, 'grad_norm': 2.922252620427914, 'learning_rate': 2.4044444444444446e-07, 'epoch': 3.918}
{'loss': 0.8654, 'grad_norm': 2.979451197039355, 'learning_rate': 2.393333333333334e-07, 'epoch': 3.9184}
{'loss': 0.864, 'grad_norm': 2.9492897176730235, 'learning_rate': 2.3822222222222226e-07, 'epoch': 3.9188}
{'loss': 0.8575, 'grad_norm': 3.0014615571634087, 'learning_rate': 2.3711111111111113e-07, 'epoch': 3.9192}
{'loss': 0.8562, 'grad_norm': 3.140944298602204, 'learning_rate': 2.3600000000000002e-07, 'epoch': 3.9196}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8669, 'grad_norm': 3.23397337221296, 'learning_rate': 2.3488888888888892e-07, 'epoch': 3.92}
{'eval_valid_loss': 0.83740234375, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1121.673, 'eval_valid_steps_per_second': 280.418, 'epoch': 3.92}
{'loss': 0.8665, 'grad_norm': 2.9768461107321493, 'learning_rate': 2.3377777777777782e-07, 'epoch': 3.9204}
{'loss': 0.863, 'grad_norm': 3.4102491735358518, 'learning_rate': 2.326666666666667e-07, 'epoch': 3.9208}
{'loss': 0.8595, 'grad_norm': 2.9563023060532383, 'learning_rate': 2.3155555555555556e-07, 'epoch': 3.9212}
{'loss': 0.8609, 'grad_norm': 2.945011113377377, 'learning_rate': 2.3044444444444445e-07, 'epoch': 3.9215999999999998}
{'loss': 0.8515, 'grad_norm': 3.1813069374070464, 'learning_rate': 2.2933333333333335e-07, 'epoch': 3.922}
{'loss': 0.8723, 'grad_norm': 3.033388189895707, 'learning_rate': 2.2822222222222225e-07, 'epoch': 3.9224}
{'loss': 0.8632, 'grad_norm': 2.963051245301928, 'learning_rate': 2.2711111111111114e-07, 'epoch': 3.9228}
{'loss': 0.8757, 'grad_norm': 2.995497832039456, 'learning_rate': 2.26e-07, 'epoch': 3.9232}
{'loss': 0.8595, 'grad_norm': 2.939047507248338, 'learning_rate': 2.248888888888889e-07, 'epoch': 3.9236}
{'loss': 0.8656, 'grad_norm': 2.952131144491584, 'learning_rate': 2.237777777777778e-07, 'epoch': 3.924}
{'eval_valid_loss': 0.83740234375, 'eval_valid_runtime': 0.0887, 'eval_valid_samples_per_second': 1127.44, 'eval_valid_steps_per_second': 281.86, 'epoch': 3.924}
{'loss': 0.8567, 'grad_norm': 2.7531295546841483, 'learning_rate': 2.226666666666667e-07, 'epoch': 3.9244}
{'loss': 0.8576, 'grad_norm': 2.865471672107786, 'learning_rate': 2.216666666666667e-07, 'epoch': 3.9248}
{'loss': 0.8646, 'grad_norm': 2.9965386290419445, 'learning_rate': 2.2055555555555555e-07, 'epoch': 3.9252000000000002}
{'loss': 0.8668, 'grad_norm': 2.7667848180960304, 'learning_rate': 2.1944444444444445e-07, 'epoch': 3.9256}
{'loss': 0.8555, 'grad_norm': 3.189627619850544, 'learning_rate': 2.1833333333333334e-07, 'epoch': 3.926}
{'loss': 0.8637, 'grad_norm': 3.2553641086181297, 'learning_rate': 2.1722222222222224e-07, 'epoch': 3.9264}
{'loss': 0.8603, 'grad_norm': 2.6895838576202316, 'learning_rate': 2.1611111111111114e-07, 'epoch': 3.9268}
{'loss': 0.8545, 'grad_norm': 2.635063833181934, 'learning_rate': 2.15e-07, 'epoch': 3.9272}
{'loss': 0.8648, 'grad_norm': 3.040020298984166, 'learning_rate': 2.138888888888889e-07, 'epoch': 3.9276}
{'loss': 0.8598, 'grad_norm': 2.779145242679, 'learning_rate': 2.127777777777778e-07, 'epoch': 3.928}
{'eval_valid_loss': 0.83740234375, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.587, 'eval_valid_steps_per_second': 278.397, 'epoch': 3.928}
{'loss': 0.8553, 'grad_norm': 3.1864941543323164, 'learning_rate': 2.116666666666667e-07, 'epoch': 3.9284}
{'loss': 0.8613, 'grad_norm': 3.0622511587446732, 'learning_rate': 2.105555555555556e-07, 'epoch': 3.9288}
{'loss': 0.8616, 'grad_norm': 3.1065070953509144, 'learning_rate': 2.094444444444445e-07, 'epoch': 3.9292}
{'loss': 0.8573, 'grad_norm': 3.0054273784458374, 'learning_rate': 2.0833333333333333e-07, 'epoch': 3.9295999999999998}
{'loss': 0.8621, 'grad_norm': 3.0240041413726657, 'learning_rate': 2.0722222222222223e-07, 'epoch': 3.93}
{'loss': 0.8633, 'grad_norm': 3.0587195008834227, 'learning_rate': 2.0611111111111113e-07, 'epoch': 3.9304}
{'loss': 0.8538, 'grad_norm': 3.0667849070017033, 'learning_rate': 2.0500000000000002e-07, 'epoch': 3.9308}
{'loss': 0.8476, 'grad_norm': 2.862183759953237, 'learning_rate': 2.0388888888888892e-07, 'epoch': 3.9312}
{'loss': 0.8616, 'grad_norm': 2.684073592685025, 'learning_rate': 2.027777777777778e-07, 'epoch': 3.9316}
{'loss': 0.8606, 'grad_norm': 2.8246383777343316, 'learning_rate': 2.0166666666666669e-07, 'epoch': 3.932}
{'eval_valid_loss': 0.837890625, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1110.924, 'eval_valid_steps_per_second': 277.731, 'epoch': 3.932}
{'loss': 0.8565, 'grad_norm': 2.964541574390906, 'learning_rate': 2.0055555555555558e-07, 'epoch': 3.9324}
{'loss': 0.8637, 'grad_norm': 3.0891547513933864, 'learning_rate': 1.9944444444444448e-07, 'epoch': 3.9328}
{'loss': 0.8654, 'grad_norm': 2.8705148230002004, 'learning_rate': 1.9833333333333338e-07, 'epoch': 3.9332000000000003}
{'loss': 0.8666, 'grad_norm': 3.0326434501532313, 'learning_rate': 1.9722222222222222e-07, 'epoch': 3.9336}
{'loss': 0.8779, 'grad_norm': 3.1775063571821316, 'learning_rate': 1.9611111111111112e-07, 'epoch': 3.934}
{'loss': 0.8666, 'grad_norm': 2.9318082737368805, 'learning_rate': 1.95e-07, 'epoch': 3.9344}
{'loss': 0.8506, 'grad_norm': 3.1268285932648485, 'learning_rate': 1.938888888888889e-07, 'epoch': 3.9348}
{'loss': 0.8666, 'grad_norm': 3.065988422282357, 'learning_rate': 1.927777777777778e-07, 'epoch': 3.9352}
{'loss': 0.8487, 'grad_norm': 3.0120642200439964, 'learning_rate': 1.9166666666666668e-07, 'epoch': 3.9356}
{'loss': 0.8613, 'grad_norm': 2.841737789837602, 'learning_rate': 1.9055555555555557e-07, 'epoch': 3.936}
{'eval_valid_loss': 0.83740234375, 'eval_valid_runtime': 0.0885, 'eval_valid_samples_per_second': 1129.648, 'eval_valid_steps_per_second': 282.412, 'epoch': 3.936}
{'loss': 0.8653, 'grad_norm': 2.8184140121440544, 'learning_rate': 1.8944444444444447e-07, 'epoch': 3.9364}
{'loss': 0.8635, 'grad_norm': 2.949915287046129, 'learning_rate': 1.8833333333333337e-07, 'epoch': 3.9368}
{'loss': 0.8644, 'grad_norm': 2.9355218699717063, 'learning_rate': 1.8722222222222226e-07, 'epoch': 3.9372}
{'loss': 0.8619, 'grad_norm': 2.961287082779695, 'learning_rate': 1.861111111111111e-07, 'epoch': 3.9375999999999998}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8687, 'grad_norm': 3.1065396651635706, 'learning_rate': 1.85e-07, 'epoch': 3.9379999999999997}
{'loss': 0.8555, 'grad_norm': 3.1423726225068975, 'learning_rate': 1.838888888888889e-07, 'epoch': 3.9384}
{'loss': 0.8621, 'grad_norm': 2.7562027304352306, 'learning_rate': 1.827777777777778e-07, 'epoch': 3.9388}
{'loss': 0.8696, 'grad_norm': 2.852407377312299, 'learning_rate': 1.816666666666667e-07, 'epoch': 3.9392}
{'loss': 0.851, 'grad_norm': 2.9997189857077546, 'learning_rate': 1.8055555555555556e-07, 'epoch': 3.9396}
{'loss': 0.8724, 'grad_norm': 2.77434649010894, 'learning_rate': 1.7944444444444446e-07, 'epoch': 3.94}
{'eval_valid_loss': 0.83740234375, 'eval_valid_runtime': 0.0902, 'eval_valid_samples_per_second': 1109.032, 'eval_valid_steps_per_second': 277.258, 'epoch': 3.94}
{'loss': 0.8711, 'grad_norm': 3.238227171958271, 'learning_rate': 1.7833333333333336e-07, 'epoch': 3.9404}
{'loss': 0.867, 'grad_norm': 2.9853631375942875, 'learning_rate': 1.7722222222222225e-07, 'epoch': 3.9408}
{'loss': 0.8594, 'grad_norm': 3.053211479953532, 'learning_rate': 1.7611111111111115e-07, 'epoch': 3.9412000000000003}
{'loss': 0.856, 'grad_norm': 2.8796082239938623, 'learning_rate': 1.7500000000000002e-07, 'epoch': 3.9416}
{'loss': 0.8682, 'grad_norm': 2.850589476449566, 'learning_rate': 1.738888888888889e-07, 'epoch': 3.942}
{'loss': 0.8534, 'grad_norm': 3.015237002539006, 'learning_rate': 1.7277777777777779e-07, 'epoch': 3.9424}
{'loss': 0.8464, 'grad_norm': 2.865883023741834, 'learning_rate': 1.7166666666666668e-07, 'epoch': 3.9428}
{'loss': 0.8653, 'grad_norm': 2.834996174939248, 'learning_rate': 1.7055555555555558e-07, 'epoch': 3.9432}
{'loss': 0.8523, 'grad_norm': 3.030268038155715, 'learning_rate': 1.6944444444444448e-07, 'epoch': 3.9436}
{'loss': 0.8558, 'grad_norm': 3.123076027834906, 'learning_rate': 1.6833333333333335e-07, 'epoch': 3.944}
{'eval_valid_loss': 0.83740234375, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.243, 'eval_valid_steps_per_second': 279.561, 'epoch': 3.944}
{'loss': 0.8538, 'grad_norm': 3.1096261560421277, 'learning_rate': 1.6722222222222224e-07, 'epoch': 3.9444}
{'loss': 0.8646, 'grad_norm': 2.9841966725541473, 'learning_rate': 1.6611111111111114e-07, 'epoch': 3.9448}
{'loss': 0.8612, 'grad_norm': 2.849291741262797, 'learning_rate': 1.65e-07, 'epoch': 3.9452}
{'loss': 0.8607, 'grad_norm': 2.9527545676441114, 'learning_rate': 1.638888888888889e-07, 'epoch': 3.9455999999999998}
{'loss': 0.8633, 'grad_norm': 2.86023502236169, 'learning_rate': 1.6277777777777778e-07, 'epoch': 3.9459999999999997}
{'loss': 0.856, 'grad_norm': 2.956446656627945, 'learning_rate': 1.6166666666666667e-07, 'epoch': 3.9464}
{'loss': 0.8518, 'grad_norm': 2.731499276908431, 'learning_rate': 1.6055555555555557e-07, 'epoch': 3.9468}
{'loss': 0.8565, 'grad_norm': 2.8426502113276046, 'learning_rate': 1.5944444444444446e-07, 'epoch': 3.9472}
{'loss': 0.8614, 'grad_norm': 3.0319605908789358, 'learning_rate': 1.5833333333333336e-07, 'epoch': 3.9476}
{'loss': 0.8641, 'grad_norm': 3.285374628490295, 'learning_rate': 1.5722222222222223e-07, 'epoch': 3.948}
{'eval_valid_loss': 0.83740234375, 'eval_valid_runtime': 0.0894, 'eval_valid_samples_per_second': 1118.896, 'eval_valid_steps_per_second': 279.724, 'epoch': 3.948}
{'loss': 0.8675, 'grad_norm': 3.131067098959766, 'learning_rate': 1.5611111111111113e-07, 'epoch': 3.9484}
{'loss': 0.8639, 'grad_norm': 2.790276490264409, 'learning_rate': 1.5500000000000002e-07, 'epoch': 3.9488}
{'loss': 0.8673, 'grad_norm': 3.267680547902341, 'learning_rate': 1.538888888888889e-07, 'epoch': 3.9492000000000003}
{'loss': 0.8622, 'grad_norm': 2.894414665149721, 'learning_rate': 1.527777777777778e-07, 'epoch': 3.9496}
{'loss': 0.8634, 'grad_norm': 3.0939937071708496, 'learning_rate': 1.516666666666667e-07, 'epoch': 3.95}
{'loss': 0.8694, 'grad_norm': 3.0358226540400604, 'learning_rate': 1.5055555555555558e-07, 'epoch': 3.9504}
{'loss': 0.872, 'grad_norm': 2.8397501650663353, 'learning_rate': 1.4944444444444445e-07, 'epoch': 3.9508}
{'loss': 0.8619, 'grad_norm': 2.8751901947218483, 'learning_rate': 1.4833333333333335e-07, 'epoch': 3.9512}
{'loss': 0.8788, 'grad_norm': 2.949847698725291, 'learning_rate': 1.4722222222222222e-07, 'epoch': 3.9516}
{'loss': 0.8609, 'grad_norm': 2.875192122674342, 'learning_rate': 1.4611111111111112e-07, 'epoch': 3.952}
{'eval_valid_loss': 0.83740234375, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1120.669, 'eval_valid_steps_per_second': 280.167, 'epoch': 3.952}
{'loss': 0.8566, 'grad_norm': 3.0455055876301556, 'learning_rate': 1.4500000000000001e-07, 'epoch': 3.9524}
{'loss': 0.8672, 'grad_norm': 3.0483469708646713, 'learning_rate': 1.4388888888888888e-07, 'epoch': 3.9528}
{'loss': 0.8612, 'grad_norm': 2.571553808854237, 'learning_rate': 1.4277777777777778e-07, 'epoch': 3.9532}
{'loss': 0.8634, 'grad_norm': 2.9767361988238825, 'learning_rate': 1.4166666666666668e-07, 'epoch': 3.9536}
{'loss': 0.855, 'grad_norm': 3.003668320537856, 'learning_rate': 1.4055555555555557e-07, 'epoch': 3.9539999999999997}
{'loss': 0.8714, 'grad_norm': 3.263940887961322, 'learning_rate': 1.3944444444444447e-07, 'epoch': 3.9544}
{'loss': 0.8603, 'grad_norm': 2.7661590760927437, 'learning_rate': 1.3833333333333334e-07, 'epoch': 3.9548}
{'loss': 0.8678, 'grad_norm': 3.091334536715768, 'learning_rate': 1.3722222222222224e-07, 'epoch': 3.9552}
{'loss': 0.8649, 'grad_norm': 2.995495573606931, 'learning_rate': 1.361111111111111e-07, 'epoch': 3.9556}
{'loss': 0.8654, 'grad_norm': 2.852815258807452, 'learning_rate': 1.35e-07, 'epoch': 3.956}
{'eval_valid_loss': 0.83740234375, 'eval_valid_runtime': 0.0905, 'eval_valid_samples_per_second': 1104.692, 'eval_valid_steps_per_second': 276.173, 'epoch': 3.956}
{'loss': 0.8795, 'grad_norm': 2.9221401808245204, 'learning_rate': 1.338888888888889e-07, 'epoch': 3.9564}
{'loss': 0.8522, 'grad_norm': 2.7691514490545344, 'learning_rate': 1.3277777777777777e-07, 'epoch': 3.9568}
{'loss': 0.8782, 'grad_norm': 3.015764411115173, 'learning_rate': 1.3166666666666667e-07, 'epoch': 3.9572000000000003}
{'loss': 0.8571, 'grad_norm': 3.060484699915256, 'learning_rate': 1.3055555555555556e-07, 'epoch': 3.9576000000000002}
{'loss': 0.874, 'grad_norm': 2.823345993915111, 'learning_rate': 1.2944444444444446e-07, 'epoch': 3.958}
{'loss': 0.8662, 'grad_norm': 2.9332242649204083, 'learning_rate': 1.2833333333333336e-07, 'epoch': 3.9584}
{'loss': 0.8666, 'grad_norm': 3.2446416459934944, 'learning_rate': 1.2722222222222223e-07, 'epoch': 3.9588}
{'loss': 0.8641, 'grad_norm': 3.0197781638275276, 'learning_rate': 1.2611111111111112e-07, 'epoch': 3.9592}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8779, 'grad_norm': 2.796856166200786, 'learning_rate': 1.2500000000000002e-07, 'epoch': 3.9596}
{'loss': 0.8707, 'grad_norm': 3.0049081230399337, 'learning_rate': 1.238888888888889e-07, 'epoch': 3.96}
{'eval_valid_loss': 0.83740234375, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.944, 'eval_valid_steps_per_second': 279.986, 'epoch': 3.96}
{'loss': 0.8584, 'grad_norm': 3.1440683154420235, 'learning_rate': 1.227777777777778e-07, 'epoch': 3.9604}
{'loss': 0.8583, 'grad_norm': 2.9629690804904025, 'learning_rate': 1.2166666666666666e-07, 'epoch': 3.9608}
{'loss': 0.8515, 'grad_norm': 2.95250424920147, 'learning_rate': 1.2055555555555555e-07, 'epoch': 3.9612}
{'loss': 0.8638, 'grad_norm': 3.017390668163584, 'learning_rate': 1.1944444444444445e-07, 'epoch': 3.9616}
{'loss': 0.8736, 'grad_norm': 2.938324051788422, 'learning_rate': 1.1833333333333333e-07, 'epoch': 3.9619999999999997}
{'loss': 0.8497, 'grad_norm': 2.777477674300102, 'learning_rate': 1.1722222222222223e-07, 'epoch': 3.9624}
{'loss': 0.8596, 'grad_norm': 2.9939440558818675, 'learning_rate': 1.1611111111111111e-07, 'epoch': 3.9628}
{'loss': 0.8614, 'grad_norm': 2.92762013034962, 'learning_rate': 1.1500000000000001e-07, 'epoch': 3.9632}
{'loss': 0.8592, 'grad_norm': 2.77463701662164, 'learning_rate': 1.1388888888888891e-07, 'epoch': 3.9636}
{'loss': 0.8658, 'grad_norm': 2.6857815171819244, 'learning_rate': 1.1277777777777778e-07, 'epoch': 3.964}
{'eval_valid_loss': 0.83740234375, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1111.663, 'eval_valid_steps_per_second': 277.916, 'epoch': 3.964}
{'loss': 0.8703, 'grad_norm': 3.150832663716512, 'learning_rate': 1.1166666666666667e-07, 'epoch': 3.9644}
{'loss': 0.8717, 'grad_norm': 2.844486293212205, 'learning_rate': 1.1066666666666668e-07, 'epoch': 3.9648}
{'loss': 0.8691, 'grad_norm': 2.8577738484500554, 'learning_rate': 1.0955555555555556e-07, 'epoch': 3.9652}
{'loss': 0.8602, 'grad_norm': 2.744174541657477, 'learning_rate': 1.0844444444444446e-07, 'epoch': 3.9656000000000002}
{'loss': 0.8576, 'grad_norm': 3.3029143173314006, 'learning_rate': 1.0733333333333333e-07, 'epoch': 3.966}
{'loss': 0.863, 'grad_norm': 3.076563211405116, 'learning_rate': 1.0622222222222222e-07, 'epoch': 3.9664}
{'loss': 0.855, 'grad_norm': 2.7335803048761784, 'learning_rate': 1.0511111111111112e-07, 'epoch': 3.9668}
{'loss': 0.8698, 'grad_norm': 3.0339694752971327, 'learning_rate': 1.04e-07, 'epoch': 3.9672}
{'loss': 0.8713, 'grad_norm': 2.99424535440029, 'learning_rate': 1.028888888888889e-07, 'epoch': 3.9676}
{'loss': 0.8525, 'grad_norm': 3.1253449249645273, 'learning_rate': 1.017777777777778e-07, 'epoch': 3.968}
{'eval_valid_loss': 0.83740234375, 'eval_valid_runtime': 0.0892, 'eval_valid_samples_per_second': 1121.55, 'eval_valid_steps_per_second': 280.387, 'epoch': 3.968}
{'loss': 0.8541, 'grad_norm': 2.9129533779101062, 'learning_rate': 1.0066666666666667e-07, 'epoch': 3.9684}
{'loss': 0.8495, 'grad_norm': 2.877776339574362, 'learning_rate': 9.955555555555556e-08, 'epoch': 3.9688}
{'loss': 0.857, 'grad_norm': 2.9036210280016026, 'learning_rate': 9.844444444444445e-08, 'epoch': 3.9692}
{'loss': 0.8729, 'grad_norm': 2.9438916884985886, 'learning_rate': 9.733333333333334e-08, 'epoch': 3.9696}
{'loss': 0.8588, 'grad_norm': 3.0944640655274096, 'learning_rate': 9.622222222222224e-08, 'epoch': 3.9699999999999998}
{'loss': 0.8551, 'grad_norm': 2.982981231537056, 'learning_rate': 9.511111111111111e-08, 'epoch': 3.9704}
{'loss': 0.8609, 'grad_norm': 3.0510978485609694, 'learning_rate': 9.400000000000001e-08, 'epoch': 3.9708}
{'loss': 0.8743, 'grad_norm': 2.998664136335509, 'learning_rate': 9.288888888888889e-08, 'epoch': 3.9712}
{'loss': 0.8591, 'grad_norm': 2.9679885791297917, 'learning_rate': 9.177777777777779e-08, 'epoch': 3.9716}
{'loss': 0.8703, 'grad_norm': 3.0357766321335515, 'learning_rate': 9.066666666666668e-08, 'epoch': 3.972}
{'eval_valid_loss': 0.83740234375, 'eval_valid_runtime': 0.0898, 'eval_valid_samples_per_second': 1113.371, 'eval_valid_steps_per_second': 278.343, 'epoch': 3.972}
{'loss': 0.8557, 'grad_norm': 3.120880406121883, 'learning_rate': 8.955555555555555e-08, 'epoch': 3.9724}
{'loss': 0.8567, 'grad_norm': 2.9049291942112228, 'learning_rate': 8.844444444444445e-08, 'epoch': 3.9728}
{'loss': 0.8552, 'grad_norm': 2.5622039484342163, 'learning_rate': 8.733333333333333e-08, 'epoch': 3.9732}
{'loss': 0.8589, 'grad_norm': 2.964773395894749, 'learning_rate': 8.622222222222223e-08, 'epoch': 3.9736000000000002}
{'loss': 0.8639, 'grad_norm': 3.1633020782350276, 'learning_rate': 8.511111111111113e-08, 'epoch': 3.974}
{'loss': 0.8546, 'grad_norm': 3.241495807814458, 'learning_rate': 8.4e-08, 'epoch': 3.9744}
{'loss': 0.8533, 'grad_norm': 2.7809868752386917, 'learning_rate': 8.28888888888889e-08, 'epoch': 3.9748}
{'loss': 0.8591, 'grad_norm': 2.9734020625942934, 'learning_rate': 8.177777777777779e-08, 'epoch': 3.9752}
{'loss': 0.8561, 'grad_norm': 2.845648514348633, 'learning_rate': 8.066666666666667e-08, 'epoch': 3.9756}
{'loss': 0.8729, 'grad_norm': 2.9714768083801255, 'learning_rate': 7.955555555555557e-08, 'epoch': 3.976}
{'eval_valid_loss': 0.83740234375, 'eval_valid_runtime': 0.0893, 'eval_valid_samples_per_second': 1119.296, 'eval_valid_steps_per_second': 279.824, 'epoch': 3.976}
{'loss': 0.8558, 'grad_norm': 3.11518311652701, 'learning_rate': 7.844444444444444e-08, 'epoch': 3.9764}
{'loss': 0.8769, 'grad_norm': 2.9579827656131, 'learning_rate': 7.733333333333334e-08, 'epoch': 3.9768}
{'loss': 0.8593, 'grad_norm': 3.138361119646861, 'learning_rate': 7.622222222222222e-08, 'epoch': 3.9772}
{'loss': 0.8688, 'grad_norm': 3.2231341475822415, 'learning_rate': 7.511111111111112e-08, 'epoch': 3.9776}
{'loss': 0.8566, 'grad_norm': 2.8790573846491463, 'learning_rate': 7.400000000000001e-08, 'epoch': 3.9779999999999998}
{'loss': 0.8656, 'grad_norm': 2.903916837535951, 'learning_rate': 7.28888888888889e-08, 'epoch': 3.9784}
{'loss': 0.8677, 'grad_norm': 2.918569183875231, 'learning_rate': 7.177777777777778e-08, 'epoch': 3.9788}
{'loss': 0.8681, 'grad_norm': 3.1556251303902636, 'learning_rate': 7.066666666666666e-08, 'epoch': 3.9792}
{'loss': 0.8702, 'grad_norm': 2.8171461300131746, 'learning_rate': 6.955555555555556e-08, 'epoch': 3.9796}
{'loss': 0.8699, 'grad_norm': 2.8246764079999904, 'learning_rate': 6.844444444444446e-08, 'epoch': 3.98}
{'eval_valid_loss': 0.83740234375, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1111.71, 'eval_valid_steps_per_second': 277.928, 'epoch': 3.98}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
{'loss': 0.8603, 'grad_norm': 2.780585338166478, 'learning_rate': 6.733333333333334e-08, 'epoch': 3.9804}
{'loss': 0.8542, 'grad_norm': 2.7605554269906754, 'learning_rate': 6.622222222222222e-08, 'epoch': 3.9808}
{'loss': 0.8784, 'grad_norm': 3.0725692240100084, 'learning_rate': 6.511111111111111e-08, 'epoch': 3.9812}
{'loss': 0.8579, 'grad_norm': 2.948462635654841, 'learning_rate': 6.4e-08, 'epoch': 3.9816000000000003}
{'loss': 0.8597, 'grad_norm': 3.0656005481207553, 'learning_rate': 6.28888888888889e-08, 'epoch': 3.982}
{'loss': 0.8532, 'grad_norm': 3.157553214895563, 'learning_rate': 6.177777777777778e-08, 'epoch': 3.9824}
{'loss': 0.8673, 'grad_norm': 2.9512400893804753, 'learning_rate': 6.066666666666667e-08, 'epoch': 3.9828}
{'loss': 0.8599, 'grad_norm': 2.858946919316397, 'learning_rate': 5.9555555555555564e-08, 'epoch': 3.9832}
{'loss': 0.8553, 'grad_norm': 3.034290308020869, 'learning_rate': 5.8444444444444454e-08, 'epoch': 3.9836}
{'loss': 0.8566, 'grad_norm': 2.878034400135241, 'learning_rate': 5.733333333333334e-08, 'epoch': 3.984}
{'eval_valid_loss': 0.83740234375, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.087, 'eval_valid_steps_per_second': 278.022, 'epoch': 3.984}
{'loss': 0.8708, 'grad_norm': 3.1048820901142675, 'learning_rate': 5.622222222222223e-08, 'epoch': 3.9844}
{'loss': 0.8631, 'grad_norm': 2.734944959911338, 'learning_rate': 5.511111111111111e-08, 'epoch': 3.9848}
{'loss': 0.8462, 'grad_norm': 2.8498092487145237, 'learning_rate': 5.400000000000001e-08, 'epoch': 3.9852}
{'loss': 0.8736, 'grad_norm': 2.892787835596126, 'learning_rate': 5.28888888888889e-08, 'epoch': 3.9856}
{'loss': 0.8559, 'grad_norm': 3.0388999248256208, 'learning_rate': 5.177777777777778e-08, 'epoch': 3.9859999999999998}
{'loss': 0.8525, 'grad_norm': 2.896504387300877, 'learning_rate': 5.066666666666667e-08, 'epoch': 3.9864}
{'loss': 0.8641, 'grad_norm': 2.7403654201764516, 'learning_rate': 4.955555555555556e-08, 'epoch': 3.9868}
{'loss': 0.8604, 'grad_norm': 3.1962068980163614, 'learning_rate': 4.844444444444445e-08, 'epoch': 3.9872}
{'loss': 0.8649, 'grad_norm': 2.8729005175978735, 'learning_rate': 4.733333333333334e-08, 'epoch': 3.9876}
{'loss': 0.8519, 'grad_norm': 3.0162683008320936, 'learning_rate': 4.6222222222222224e-08, 'epoch': 3.988}
{'eval_valid_loss': 0.83740234375, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1110.512, 'eval_valid_steps_per_second': 277.628, 'epoch': 3.988}
{'loss': 0.8598, 'grad_norm': 2.8492528157646615, 'learning_rate': 4.5111111111111114e-08, 'epoch': 3.9884}
{'loss': 0.8672, 'grad_norm': 2.946436083358833, 'learning_rate': 4.4000000000000004e-08, 'epoch': 3.9888}
{'loss': 0.8778, 'grad_norm': 3.0087277989030006, 'learning_rate': 4.2888888888888894e-08, 'epoch': 3.9892}
{'loss': 0.8512, 'grad_norm': 2.720103179222855, 'learning_rate': 4.177777777777778e-08, 'epoch': 3.9896000000000003}
{'loss': 0.8606, 'grad_norm': 3.016235862885349, 'learning_rate': 4.066666666666667e-08, 'epoch': 3.99}
{'loss': 0.8669, 'grad_norm': 3.294165478656367, 'learning_rate': 3.9555555555555563e-08, 'epoch': 3.9904}
{'loss': 0.8604, 'grad_norm': 3.0925781361786, 'learning_rate': 3.844444444444445e-08, 'epoch': 3.9908}
{'loss': 0.862, 'grad_norm': 3.1913418337275314, 'learning_rate': 3.733333333333334e-08, 'epoch': 3.9912}
{'loss': 0.8607, 'grad_norm': 2.8247488059974777, 'learning_rate': 3.622222222222223e-08, 'epoch': 3.9916}
{'loss': 0.8568, 'grad_norm': 3.090749537906613, 'learning_rate': 3.511111111111112e-08, 'epoch': 3.992}
{'eval_valid_loss': 0.83740234375, 'eval_valid_runtime': 0.0899, 'eval_valid_samples_per_second': 1112.229, 'eval_valid_steps_per_second': 278.057, 'epoch': 3.992}
{'loss': 0.8548, 'grad_norm': 2.734269123752388, 'learning_rate': 3.4e-08, 'epoch': 3.9924}
{'loss': 0.866, 'grad_norm': 2.8196020103194153, 'learning_rate': 3.288888888888889e-08, 'epoch': 3.9928}
{'loss': 0.8658, 'grad_norm': 2.968776190792802, 'learning_rate': 3.177777777777778e-08, 'epoch': 3.9932}
{'loss': 0.8687, 'grad_norm': 3.0315018382116428, 'learning_rate': 3.066666666666667e-08, 'epoch': 3.9936}
{'loss': 0.8481, 'grad_norm': 2.7400746805392737, 'learning_rate': 2.9555555555555557e-08, 'epoch': 3.9939999999999998}
{'loss': 0.8681, 'grad_norm': 3.1901699085338366, 'learning_rate': 2.8444444444444447e-08, 'epoch': 3.9943999999999997}
{'loss': 0.884, 'grad_norm': 3.1155174198536035, 'learning_rate': 2.7333333333333337e-08, 'epoch': 3.9948}
{'loss': 0.8623, 'grad_norm': 2.9073969208377948, 'learning_rate': 2.6222222222222227e-08, 'epoch': 3.9952}
{'loss': 0.8773, 'grad_norm': 2.8780186085705792, 'learning_rate': 2.5111111111111113e-08, 'epoch': 3.9956}
{'loss': 0.8584, 'grad_norm': 2.874971472557176, 'learning_rate': 2.4e-08, 'epoch': 3.996}
{'eval_valid_loss': 0.83740234375, 'eval_valid_runtime': 0.0897, 'eval_valid_samples_per_second': 1114.466, 'eval_valid_steps_per_second': 278.617, 'epoch': 3.996}
{'loss': 0.8677, 'grad_norm': 3.368897492854857, 'learning_rate': 2.288888888888889e-08, 'epoch': 3.9964}
{'loss': 0.8616, 'grad_norm': 2.869360716021229, 'learning_rate': 2.1777777777777776e-08, 'epoch': 3.9968}
{'loss': 0.856, 'grad_norm': 2.7294425454854445, 'learning_rate': 2.066666666666667e-08, 'epoch': 3.9972}
{'loss': 0.8604, 'grad_norm': 2.9656481600260047, 'learning_rate': 1.9555555555555556e-08, 'epoch': 3.9976000000000003}
{'loss': 0.8549, 'grad_norm': 3.138011433229837, 'learning_rate': 1.8444444444444446e-08, 'epoch': 3.998}
{'loss': 0.8606, 'grad_norm': 2.9731374343634505, 'learning_rate': 1.7333333333333333e-08, 'epoch': 3.9984}
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/torch/nn/modules/module.py:1898: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Traceback (most recent call last):
  File "/n/home05/sqin/self-correct/sudoku/train.py", line 221, in <module>
    main(args)
  File "/n/home05/sqin/self-correct/sudoku/train.py", line 209, in main
    trainer.train()
  File "/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/transformers/trainer.py", line 2653, in _inner_training_loop
    self._load_best_model()
  File "/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/transformers/trainer.py", line 2889, in _load_best_model
    deepspeed_load_checkpoint(
  File "/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/transformers/integrations/deepspeed.py", line 447, in deepspeed_load_checkpoint
    raise ValueError(f"Can't find a valid checkpoint at {checkpoint_path}")
ValueError: Can't find a valid checkpoint at /n/netscratch/dam_lab/Lab/sqin/reason/sudoku/outputs/oft/checkpoint-97600
[rank0]: Traceback (most recent call last):
[rank0]:   File "/n/home05/sqin/self-correct/sudoku/train.py", line 221, in <module>
[rank0]:     main(args)
[rank0]:   File "/n/home05/sqin/self-correct/sudoku/train.py", line 209, in main
[rank0]:     trainer.train()
[rank0]:   File "/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/transformers/trainer.py", line 2171, in train
[rank0]:     return inner_training_loop(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/transformers/trainer.py", line 2653, in _inner_training_loop
[rank0]:     self._load_best_model()
[rank0]:   File "/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/transformers/trainer.py", line 2889, in _load_best_model
[rank0]:     deepspeed_load_checkpoint(
[rank0]:   File "/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/transformers/integrations/deepspeed.py", line 447, in deepspeed_load_checkpoint
[rank0]:     raise ValueError(f"Can't find a valid checkpoint at {checkpoint_path}")
[rank0]: ValueError: Can't find a valid checkpoint at /n/netscratch/dam_lab/Lab/sqin/reason/sudoku/outputs/oft/checkpoint-97600
{'loss': 0.8562, 'grad_norm': 2.8703793421870567, 'learning_rate': 1.6222222222222223e-08, 'epoch': 3.9988}
{'loss': 0.8603, 'grad_norm': 2.801713397317885, 'learning_rate': 1.5111111111111113e-08, 'epoch': 3.9992}
{'loss': 0.8717, 'grad_norm': 3.2261856173033148, 'learning_rate': 1.4000000000000001e-08, 'epoch': 3.9996}
{'loss': 0.8673, 'grad_norm': 2.891642077109657, 'learning_rate': 1.288888888888889e-08, 'epoch': 4.0}
{'eval_valid_loss': 0.83740234375, 'eval_valid_runtime': 0.09, 'eval_valid_samples_per_second': 1111.471, 'eval_valid_steps_per_second': 277.868, 'epoch': 4.0}