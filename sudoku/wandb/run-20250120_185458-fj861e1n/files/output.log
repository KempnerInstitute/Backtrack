Tokenizer vocab size: 109
Model size: 21,068,288









Map:  90%|█████████ | 9000/10000 [00:48<00:04, 200.17 examples/s]

Map: 100%|██████████| 10000/10000 [00:57<00:00, 174.62 examples/s]