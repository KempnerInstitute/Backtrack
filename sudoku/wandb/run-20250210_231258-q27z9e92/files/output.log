
Tokenizer vocab size: 109
Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(109, 1024)
    (layers): ModuleList(
      (0-23): 24 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (k_proj): Linear(in_features=1024, out_features=256, bias=True)
          (v_proj): Linear(in_features=1024, out_features=256, bias=True)
          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)
          (up_proj): Linear(in_features=1024, out_features=4096, bias=False)
          (down_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((1024,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((1024,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((1024,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1024, out_features=109, bias=False)
)
Model size: 365,214,720
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
Traceback (most recent call last):
  File "/n/home05/sqin/self-correct/sudoku/train.py", line 296, in <module>
    main(args)
  File "/n/home05/sqin/self-correct/sudoku/train.py", line 239, in main
    per_device_train_batch_size=config["per_device_batch_size"],  # Adjusted for GPUs
                                ~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
KeyError: 'per_device_batch_size'
[rank0]: Traceback (most recent call last):
[rank0]:   File "/n/home05/sqin/self-correct/sudoku/train.py", line 296, in <module>
[rank0]:     main(args)
[rank0]:   File "/n/home05/sqin/self-correct/sudoku/train.py", line 239, in main
[rank0]:     per_device_train_batch_size=config["per_device_batch_size"],  # Adjusted for GPUs
[rank0]:                                 ~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: KeyError: 'per_device_batch_size'
Tokenized dataset example
tokenized dataset DatasetDict({
    train: Dataset({
        features: ['input_ids'],
        num_rows: 10723154
    })
    val: Dataset({
        features: ['input_ids'],
        num_rows: 371
    })
})