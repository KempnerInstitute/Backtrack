Tokenizer vocab size: 109
Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(109, 512)
    (layers): ModuleList(
      (0-7): 8 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (k_proj): Linear(in_features=512, out_features=128, bias=True)
          (v_proj): Linear(in_features=512, out_features=128, bias=True)
          (o_proj): Linear(in_features=512, out_features=512, bias=False)
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=512, out_features=1024, bias=False)
          (up_proj): Linear(in_features=512, out_features=1024, bias=False)
          (down_proj): Linear(in_features=1024, out_features=512, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((512,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((512,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((512,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=512, out_features=109, bias=False)
)
Model size: 17,952,256
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.
  table = cls._concat_blocks(blocks, axis=0)
/n/holylabs/LABS/dam_lab/Lab/sqin/envs/reason/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/n/home05/sqin/self-correct/sudoku/train.py:269: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Tokenized dataset example
tokenized dataset DatasetDict({
    train: Dataset({
        features: ['input_ids'],
        num_rows: 10723154
    })
    val: Dataset({
        features: ['input_ids'],
        num_rows: 371
    })
